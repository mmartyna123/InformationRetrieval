{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79e1864f",
      "metadata": {
        "id": "79e1864f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52d70d58",
      "metadata": {
        "id": "52d70d58"
      },
      "source": [
        "Today we will focus on finding similarities between documents. For this purpose, we will compare the content of these documents. The same techniques can be used for a query in a search engine. Then simply we can treat the query like another document, calculate similarities and return the most similar documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4286948a",
      "metadata": {
        "id": "4286948a"
      },
      "outputs": [],
      "source": [
        "documents = ['Machine Learning',\n",
        " 'Five Advanced Plots in Python - Matplotlib',\n",
        " 'How to Make your Computer Talk with Python',\n",
        " 'Anomaly Detection on Servo Drives',\n",
        " 'Key takeaways from Kaggle’s most recent time series competition - Ventilator Pressure Prediction',\n",
        " 'Animated Mathematical Analysis',\n",
        " 'How to Perform Speech Recognition with Python',\n",
        " 'Beyond The Semesters: E04',\n",
        " 'How to improve classification of e-commerce pages, incorporating multiple modalities',\n",
        " 'Time Series Forecasting with ThymeBoost',\n",
        " 'CHAPTER 2: Why I Chose Data Science!',\n",
        " 'Training Provably-Robust Neural Networks',\n",
        " 'Time Series Forecasting with ThymeBoost',\n",
        " 'How to improve classification of e-commerce pages, incorporating multiple modalities',\n",
        " '5 Cute Features of CatBoost',\n",
        " 'Variance Inflation Factor (VIF) and it’s relationship with multicollinearity&nbsp;.',\n",
        " 'Beyond The Semesters: E04',\n",
        " 'Efficient Digital Transformation - Particle Swarm Optimiser',\n",
        " 'MEASURE OF ASYMMETRY',\n",
        " 'What is linear regression? A quick cover with a tutorial',\n",
        " 'Correlation VS Covariance: The easy way',\n",
        " 'Are Recommender System harming us?',\n",
        " '1 Line of Python Code That Will Speed Up Your AI by Up to 6x',\n",
        " 'If You Are Serious About Data Science Job. You Must Know These 3 Things.',\n",
        " 'Recommender System With Machine Learning and Statistics',\n",
        " 'Bias detection and mitigation in IBM AutoAI',\n",
        " 'Data Engineering: Create your own Dataset',\n",
        " 'Graph Neural Networks and Generalizable Models in Neuroscience',\n",
        " 'Fastest Way of Deploying Your Machine Learning Models',\n",
        " 'A Novel Approach to Integrate Speech Recognition into Authentication Systems',\n",
        " '3 Lessons Learned in Teaching Machine Learning for Earth Observation Techniques',\n",
        " 'Vision Transformer in Galaxy Morphology Classification',\n",
        " 'Exploring Methods of Deep Reinforcement Learning with NLP Applications',\n",
        " '6 Essential Tips to Solve Data Science Projects',\n",
        " 'Data Science Interview Questions My Friends and I got asked recently (III)',\n",
        " 'Understanding Uber’s Generative Teaching Networks',\n",
        " 'How to achieve efficient large-batch training?',\n",
        " 'How Parallelization and Large Batch Size Improve the Performance of Deep Neural Networks.',\n",
        " 'Why You Need to Know the Inner Workings of Models',\n",
        " 'Let’s Build A Simple Object Classification Task I']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb57cc0",
      "metadata": {
        "id": "dfb57cc0"
      },
      "outputs": [],
      "source": [
        "CountVec = CountVectorizer(ngram_range=(1,1), stop_words='english')\n",
        "CountData = CountVec.fit_transform(documents)\n",
        "\n",
        "CountData"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b621173",
      "metadata": {
        "id": "0b621173"
      },
      "source": [
        "The very basic way of storing information about documents is word count. Simply for each document we store an information how many times each word appears. It can be stored in an array, however, it's not the best option since it will be filled mostly with 0s. That's why it's stored in a sparse matrix, but we can expand it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4ef7983",
      "metadata": {
        "scrolled": true,
        "id": "c4ef7983"
      },
      "outputs": [],
      "source": [
        "df=pd.DataFrame(CountData.toarray(), columns=CountVec.get_feature_names_out(), index=documents)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e5f4c41",
      "metadata": {
        "id": "2e5f4c41"
      },
      "source": [
        "## Task 1\n",
        "We can reduce the size of an array, get rid of unnecesary words, and improve the quality of comparison by firstly preprocessing the docuemnts.\n",
        "Check array size after stemming/lemmatization and without stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f87bab12",
      "metadata": {
        "id": "f87bab12"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b300849b",
      "metadata": {
        "id": "b300849b"
      },
      "source": [
        "## Task 2\n",
        "\n",
        "Easy technique to compare two documents is a jaccard similarity.\n",
        "$J={\\frac {|A\\cap B|}{|A\\cup B|}}.$\n",
        "\n",
        "Implement Jaccard similarity, and function finding closest document to a provided query. Test different queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a82f68b4",
      "metadata": {
        "id": "a82f68b4"
      },
      "outputs": [],
      "source": [
        "def jaccard(d1, d2):\n",
        "    pass\n",
        "\n",
        "def closest(query, df):\n",
        "    pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83d10b06",
      "metadata": {
        "id": "83d10b06"
      },
      "source": [
        "<a href=\"https://ibb.co/k4rRpf9\"><img src=\"https://i.ibb.co/GW1KXLt/ir4.jpg\" alt=\"ir4\" border=\"0\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e64780c4",
      "metadata": {
        "id": "e64780c4"
      },
      "outputs": [],
      "source": [
        "queries = [\n",
        "    \"python\",\n",
        "    \"plot neural network\",\n",
        "    \"plot neural networks\",\n",
        "    \"ploting neural networks\",\n",
        "    \"data science\",\n",
        "]\n",
        "for q in queries:\n",
        "    print(q)\n",
        "    print(closest(q, df))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae7a42c0",
      "metadata": {
        "id": "ae7a42c0"
      },
      "source": [
        "## Task 3\n",
        "\n",
        "TFIDF (term frequency–inverse document frequency) is a much better approach. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\n",
        "\n",
        "This approach consists of 2 steps:\n",
        "TF (term frequency) -  $tf(t,d)$, is the relative frequency of term $t$ within document $d$, can be expressed e.g. as a word count divided by number of terms in a given document or by the maximum term count in a given document.\n",
        "\n",
        "IDF (inverse document frequency) - is a measure of how much information the word provides. If a word appears in every document it does not provide much information, but if it just appears in two documents then its impact on similiarity between these two documents is higher. The standard approach to compute this value is logarithm of number of documents divided by number of documents containing a given term $IDF(t) = log(\\frac{N}{n_t})$\n",
        "\n",
        "TFIDF is then just TF multiplied by IDF\n",
        "\n",
        "\n",
        "Implement tf idf, compare it with sklearn TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4728b41",
      "metadata": {
        "id": "c4728b41"
      },
      "outputs": [],
      "source": [
        "tfidf=TfidfVectorizer(use_idf=True, smooth_idf=False)\n",
        "\n",
        "dfTFIDF = pd.DataFrame(tfidf.fit_transform(documents).toarray(), index=documents, columns=tfidf.get_feature_names_out())\n",
        "dfTFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1436cd3",
      "metadata": {
        "id": "b1436cd3"
      },
      "outputs": [],
      "source": [
        "pd.Series(tfidf.idf_, index=tfidf.get_feature_names_out()).sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c07325d2",
      "metadata": {
        "id": "c07325d2"
      },
      "outputs": [],
      "source": [
        "query = \"how to machine learning\"\n",
        "query = tfidf.transform([query]).toarray()[0]\n",
        "1-dfTFIDF.apply(lambda x: cosine(x, query), axis=1).sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fe39e13",
      "metadata": {
        "scrolled": true,
        "id": "6fe39e13"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce4c97be",
      "metadata": {
        "id": "ce4c97be"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "be692110",
      "metadata": {
        "id": "be692110"
      },
      "source": [
        "## Task 4\n",
        "Create a search engine based on TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b884af5",
      "metadata": {
        "id": "6b884af5"
      },
      "outputs": [],
      "source": [
        "def search(query, df):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "973207bf",
      "metadata": {
        "id": "973207bf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3b5e9c4a",
      "metadata": {
        "id": "3b5e9c4a"
      },
      "source": [
        "## Task 5\n",
        "Create a search engine based on history containing more than one document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff24faf",
      "metadata": {
        "id": "4ff24faf"
      },
      "outputs": [],
      "source": [
        "def search(history, df):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8846314",
      "metadata": {
        "id": "b8846314"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e59dbf7",
      "metadata": {
        "id": "9e59dbf7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ab0675e6",
      "metadata": {
        "id": "ab0675e6"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}