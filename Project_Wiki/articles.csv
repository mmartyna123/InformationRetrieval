title,link,content,processedContent
Machine learning,https://en.wikipedia.org/wiki/Machine_learning,"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Advances in the field of deep learning have allowed neural networks to surpass many previous approaches in performance.[2]
 ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[3][4] The application of ML to business problems is known as predictive analytics.
 Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.[6][7]
 From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.
 The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[8][9] The synonym self-teaching computers was also used in this time period.[10][11]
 Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes.[12] In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells.[13] Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data.[12] Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.[12]
 By the early 1960s, an experimental ""learning machine"" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively ""trained"" by a human operator/teacher to recognize patterns and equipped with a ""goof"" button to cause it to reevaluate incorrect decisions.[14] A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[15] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[16] In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.[17]
 Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: ""A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.""[18] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper ""Computing Machinery and Intelligence"", in which the question ""Can machines think?"" is replaced with the question ""Can machines do what we (as thinking entities) can do?"".[19]
 Modern-day machine learning has two objectives.  One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.[20]
 As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed ""neural networks""; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[22] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[23]: 488 
 However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[23]: 488  By 1980, expert systems had come to dominate AI, and statistics was out of favor.[24] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[23]: 708–710, 755  Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as ""connectionism"", by researchers from other disciplines including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[23]: 25 
 Machine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.[24]
 There is a close connection between machine learning and compression. A system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution). Conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for ""general intelligence"".[25][26][27]
 An alternative view can show compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. For each compressor C(.) we define an associated vector space ℵ, such that C(.) maps an input string x, corresponding to the vector norm ||~x||. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, LZW, LZ77, and PPM.[28]
 According to AIXI theory, a connection more directly explained in Hutter Prize, the best possible compression of x is the smallest possible software that generates x. For example, in that model, a zip file's compressed size includes both the zip file and the unzipping software, since you can not unzip it without both, but there may be an even smaller combined form.
 Examples of AI-powered audio/video compression software include NVIDIA Maxine, AIVC.[29] Examples of software that can perform AI-powered image compression include OpenCV, TensorFlow, MATLAB's Image Processing Toolbox (IPT) and High-Fidelity Generative Image Compression.[30]
 In unsupervised machine learning, k-means clustering can be utilized to compress data by grouping similar data points into clusters. This technique simplifies handling extensive datasets that lack predefined labels and finds widespread use in fields such as image compression.[31]
 Data compression aims to reduce the size of data files, enhancing storage efficiency and speeding up data transmission. K-means clustering, an unsupervised machine learning algorithm, is employed to partition a dataset into a specified number of clusters, k, each represented by the centroid of its points. This process condenses extensive datasets into a more compact set of representative points. Particularly beneficial in image and signal processing, k-means clustering aids in data reduction by replacing groups of data points with their centroids, thereby preserving the core information of the original data while significantly decreasing the required storage space.[32]
 Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as ""unsupervised learning"" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.
 Machine learning also has intimate ties to optimization: Many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).[34]
 Characterizing the generalization of various learning algorithms is an active topic of current research, especially for deep learning algorithms.
 Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns.[35] According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[36] He also suggested the term data science as a placeholder to call the overall field.[36]
 Conventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.[37]
 Leo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model,[38] wherein ""algorithmic model"" means more or less the machine learning algorithms like Random Forest.
 Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[39]
 Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyze the weight space of deep neural networks.[40] Statistical physics is thus finding applications in the area of medical diagnostics.[41]
 A core objective of a learner is to generalize from its experience.[5][42] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.
 The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the Probably Approximately Correct Learning (PAC) model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.
 For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[43]
 In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.
 
 Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the ""signal"" or ""feedback"" available to the learning system:
 Although each algorithm has advantages and limitations, no single algorithm works for all problems.[44][45][46]
 Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[47] The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[48] An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[18]
 Types of supervised-learning algorithms include active learning, classification and regression.[49] Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. As an example, for a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email. Examples of regression would be predicting the height of a person, or the future temperature. [50]
 Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.
 Unsupervised learning algorithms find structures in data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction,[7] and density estimation.[51]
 Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.
 A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself.[52][53]
 Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy.
 In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.[54]
 Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcements learning algorithms use dynamic programming techniques.[55] Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.
 Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables.[56] In other words, it is a process of reducing the dimension of the feature set, also called the ""number of features"". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).
The manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularization.
 Other approaches have been developed which do not fit neatly into this three-fold categorization, and sometimes more than one is used by the same machine learning system. For example, topic modeling, meta-learning.[57]
 Self-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA).[58] It is learning with no external rewards and no external teacher advice. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.[59]
The self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: 
 It is a system with only one input, situation, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioral environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behavior, in an environment that contains both desirable and undesirable situations.[60]
 Several learning algorithms aim at discovering better representations of the inputs provided during training.[61] Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.
 Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization[62] and various forms of clustering.[63][64][65]
 Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.[66] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[67]
 Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.
 Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately.[68] A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[69]
 In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.[70] Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.[71]
 In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.[72]
 Three broad categories of anomaly detection techniques exist.[73] Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as ""normal"" and ""abnormal"" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model.
 Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning,[74][75] and finally meta-learning (e.g. MAML).
 Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of ""interestingness"".[76]
 Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves ""rules"" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[77] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.
 Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets.[78] For example, the rule 



{

o
n
i
o
n
s
,
p
o
t
a
t
o
e
s

}
⇒
{

b
u
r
g
e
r

}


{\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}

 found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.
 Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[79]
 Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.
 Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[80][81][82] Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.[83] The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.
 A machine learning model is a type of mathematical model that, once ""trained"" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimize errors in its predictions.[84] By extension, the term ""model"" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.[85]
 Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.
 Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems ""learn"" to perform tasks by considering examples, generally without being programmed with any task-specific rules.
 An ANN is a model based on a collection of connected units or nodes called ""artificial neurons"", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a ""signal"", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called ""edges"". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.
 The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
 Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[86]
 Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.
 Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category.[87] An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
 Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel[88]), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.
 A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.
 A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.
 Given a set of observed points, or input–output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.
 Gaussian processes are popular surrogate models in Bayesian optimization used to do hyperparameter optimization.
 A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.[90][91] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[92]
 The theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster's rule of combination), just like how in a pmf-based Bayesian approach[clarification needed] would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving.[4][9] However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches.
 Typically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams.
 Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.[93]
 There are many applications for machine learning, including:
 In 2006, the media-services provider Netflix held the first ""Netflix Prize"" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[96] Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (""everything is a recommendation"") and they changed their recommendation engine accordingly.[97] In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis.[98] In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[99] In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists.[100] In 2019 Springer Nature published the first research book created using machine learning.[101] In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19.[102] Machine learning was recently applied to predict the pro-environmental behavior of travelers.[103] Recently, machine learning technology was also applied to optimize smartphone's performance and thermal behavior based on the user's interaction with the phone.[104][105][106] When applied correctly, machine learning algorithms (MLAs) can utilize a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.[107]
 Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.[108]
 Machine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes.[109][110][111] Other applications have been focusing on pre evacuation decisions in building fires.[112][113]
 Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.[114][115][116] Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.[117]
 The ""black box theory"" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data.[118] The House of Lords Select Committee, which claimed that such an ""intelligence system"" that could have a ""substantial impact on an individual's life"" would not be considered acceptable unless it provided ""a full and satisfactory explanation for the decisions"" it makes.[118]
 In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.[119] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.[120][121] Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.[122]
 Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.[123]
 Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI.[124] It contrasts with the ""black box"" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision.[125] By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.
 Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalizing the theory in accordance with how complex the theory is.[126]
 Learners can also disappoint by ""learning the wrong lesson"". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses.[127] A real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in ""adversarial"" images that the system misclassifies.[128][129]
 Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel.[130] Machine learning models are often vulnerable to manipulation and/or evasion via adversarial machine learning.[131]
 Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories ""spam"" and well-visible ""not spam"" of posts) machine learning models that are often developed and/or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.[132][133][134]
 Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[135]
 In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning true positive rate (TPR) and true negative rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. Receiver operating characteristic (ROC) along with the accompanying Area Under the ROC Curve (AUC) offer additional tools for classification model assessment. Higher AUC is associated with a better performing model.[136]
 The ethics of artificial intelligence covers a broad range of topics within the field that are considered to have particular ethical stakes.[137] This includes algorithmic biases, fairness, automated decision-making, accountability, privacy, and regulation. 
It also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks.[137]
 Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.[138]
 Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[139] For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names.[138] Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants.[140][141] Another example includes predictive policing company Geolitica's predictive algorithm that resulted in ""disproportionately high levels of over-policing in low-income and minority communities"" after being trained with historical crime data.[142]
 While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases.[143] In fact, according to research carried out by the Computing Research Association (CRA) in 2021, ""female faculty merely make up 16.1%"" of all faculty members who focus on AI among several universities around the world.[144] Furthermore, among the group of ""new U.S. resident AI PhD graduates,"" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.[144]
 Language models learned from data have been shown to contain human-like biases.[145][146] Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.[147][148] In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.[149]
 In an experiment carried out by ProPublica, an investigative journalism organization, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged ""black defendants high risk twice as often as white defendants.""[142] In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognize gorillas.[150] Similar issues with recognizing non-white people have been found in many other systems.[151]
 Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.[152] Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that ""[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.""[153]
 There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.[154]
 Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units.[155] By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[156] OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.[157][158]
 Neuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialized hardware architectures.[159]
 A physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. The term ""physical neural network"" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses.[160][161]
 Embedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers.[162][163][164] Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as hardware acceleration,[165][166] approximate computing,[167] and model optimization.[168][169] Common optimization techniques include pruning, quantization, knowledge distillation, low-rank factorization, network architecture search, and parameter sharing.
 Software suites containing a variety of machine learning algorithms include the following:
",machin learn ml field studi artifici intellig concern develop studi statist algorithm learn data gener unseen data thu perform task without explicit instruct advanc field deep learn allow neural network surpass mani previou approach perform ml find applic mani field includ natur languag process comput vision speech recognit email filter agricultur medicin applic ml busi problem known predict analyt statist mathemat optim mathemat program method compris foundat machin learn data mine relat field studi focus exploratori data analysi eda via unsupervis learn theoret viewpoint probabl approxim correct pac learn provid framework describ machin learn term machin learn coin arthur samuel ibm employe pioneer field comput game artifici intellig synonym comput also use time period although earliest machin learn model introduc arthur samuel invent program calcul win chanc checker side histori machin learn root back decad human desir effort studi human cognit process canadian psychologist donald hebb publish book organ behavior introduc theoret neural structur form certain interact among nerv cell hebb model neuron interact one anoth set groundwork ai machin learn algorithm work node artifici neuron use comput commun data research studi human cognit system contribut modern machin learn technolog well includ logician walter pitt warren mcculloch propos earli mathemat model neural network come algorithm mirror human thought process earli experiment learn machin punch tape memori call cybertron develop raytheon compani analyz sonar signal electrocardiogram speech pattern use rudimentari reinforc learn repetit train human recogn pattern equip goof button caus reevalu incorrect decis repres book research machin learn nilsson book learn machin deal mostli machin learn pattern classif interest relat pattern recognit continu describ duda hart report given use teach strategi artifici neural network learn recogn charact letter digit special symbol comput termin tom mitchel provid wide quot formal definit algorithm studi machin learn field comput program said learn experi e respect class task perform measur p perform task measur p improv experi definit task machin learn concern offer fundament oper definit rather defin field cognit term follow alan ture propos paper comput machineri intellig question machin think replac question machin think entiti machin learn two object one classifi data base model develop purpos make predict futur outcom base model hypothet algorithm specif classifi data may use comput vision mole coupl supervis learn order train classifi cancer mole machin learn algorithm stock trade may inform trader futur potenti predict scientif endeavor machin learn grew quest artifici intellig ai earli day ai academ disciplin research interest machin learn data attempt approach problem variou symbol method well term neural network mostli perceptron model later found reinvent gener linear model statist probabilist reason also employ especi autom medic diagnosi howev increas emphasi logic approach caus rift ai machin learn probabilist system plagu theoret practic problem data acquisit represent expert system come domin ai statist favor work learn continu within ai lead induct logic program ilp statist line research outsid field ai proper pattern recognit inform retriev neural network research abandon ai comput scienc around time line continu outsid field connection research disciplin includ john hopfield david rumelhart geoffrey hinton main success came reinvent backpropag machin learn ml reorgan recogn field start flourish field chang goal achiev artifici intellig tackl solvabl problem practic natur shift focu away symbol approach inherit ai toward method model borrow statist fuzzi logic probabl theori close connect machin learn compress system predict posterior probabl sequenc given entir histori use optim data compress use arithmet code output distribut convers optim compressor use predict find symbol compress best given previou histori equival use justif use data compress benchmark gener intellig altern view show compress algorithm implicitli map string implicit featur space vector similar measur comput similar within featur space compressor c defin associ vector space ℵ c map input string x correspond vector norm exhaust examin featur space underli compress algorithm preclud space instead featur vector choos examin three repres lossless compress method lzw ppm accord aixi theori connect directli explain hutter prize best possibl compress x smallest possibl softwar gener exampl model zip file compress size includ zip file unzip softwar sinc unzip without may even smaller combin form exampl compress softwar includ nvidia maxin aivc exampl softwar perform imag compress includ opencv tensorflow matlab imag process toolbox ipt gener imag compress unsupervis machin learn cluster util compress data group similar data point cluster techniqu simplifi handl extens dataset lack predefin label find widespread use field imag compress data compress aim reduc size data file enhanc storag effici speed data transmiss cluster unsupervis machin learn algorithm employ partit dataset specifi number cluster k repres centroid point process condens extens dataset compact set repres point particularli benefici imag signal process cluster aid data reduct replac group data point centroid therebi preserv core inform origin data significantli decreas requir storag space machin learn data mine often employ method overlap significantli machin learn focus predict base known properti learn train data data mine focus discoveri previous unknown properti data analysi step knowledg discoveri databas data mine use mani machin learn method differ goal hand machin learn also employ data mine method unsupervis learn preprocess step improv learner accuraci much confus two research commun often separ confer separ journal ecml pkdd major except come basic assumpt work machin learn perform usual evalu respect abil reproduc known knowledg knowledg discoveri data mine kdd key task discoveri previous unknown knowledg evalu respect known knowledg uninform unsupervis method easili outperform supervis method typic kdd task supervis method use due unavail train data machin learn also intim tie optim mani learn problem formul minim loss function train set exampl loss function express discrep predict model train actual problem instanc exampl classif one want assign label instanc model train correctli predict preassign label set exampl character gener variou learn algorithm activ topic current research especi deep learn algorithm machin learn statist close relat field term method distinct princip goal statist draw popul infer sampl machin learn find generaliz predict pattern accord michael jordan idea machin learn methodolog principl theoret tool long statist also suggest term data scienc placehold call overal field convent statist analys requir priori select model suitabl studi data set addit signific theoret relev variabl base previou experi includ analysi contrast machin learn built model rather data shape model detect underli pattern variabl input use train model accur ultim model leo breiman distinguish two statist model paradigm data model algorithm model wherein algorithm model mean less machin learn algorithm like random forest statistician adopt method machin learn lead combin field call statist learn analyt comput techniqu deriv physic disord system extend problem includ machin learn analyz weight space deep neural network statist physic thu find applic area medic diagnost core object learner gener experi gener context abil learn machin perform accur new unseen experienc learn data set train exampl come gener unknown probabl distribut consid repres space occurr learner build gener model space enabl produc suffici accur predict new case comput analysi machin learn algorithm perform branch theoret comput scienc known comput learn theori via probabl approxim correct learn pac model train set finit futur uncertain learn theori usual yield guarante perform algorithm instead probabilist bound perform quit common decomposit one way quantifi gener error best perform context gener complex hypothesi match complex function underli data hypothesi less complex function model fit data complex model increas respons train error decreas hypothesi complex model subject overfit gener poorer addit perform bound learn theorist studi time complex feasibl learn comput learn theori comput consid feasibl done polynomi time two kind time complex result posit result show certain class function learn polynomi time neg result show certain class learn polynomi time machin learn approach tradit divid three broad categori correspond learn paradigm depend natur signal feedback avail learn system although algorithm advantag limit singl algorithm work problem supervis learn algorithm build mathemat model set data contain input desir output data known train data consist set train exampl train exampl one input desir output also known supervisori signal mathemat model train exampl repres array vector sometim call featur vector train data repres matrix iter optim object function supervis learn algorithm learn function use predict output associ new input optim function allow algorithm correctli determin output input part train data algorithm improv accuraci output predict time said learn perform task type algorithm includ activ learn classif regress classif algorithm use output restrict limit set valu regress algorithm use output may numer valu within rang exampl classif algorithm filter email input would incom email output would name folder file email exampl regress would predict height person futur temperatur similar learn area supervis machin learn close relat regress classif goal learn exampl use similar function measur similar relat two object applic rank recommend system visual ident track face verif speaker verif unsupervis learn algorithm find structur data label classifi categor instead respond feedback unsupervis learn algorithm identifi common data react base presenc absenc common new piec data central applic unsupervis machin learn includ cluster dimension reduct densiti estim cluster analysi assign set observ subset call cluster observ within cluster similar accord one predesign criteria observ drawn differ cluster dissimilar differ cluster techniqu make differ assumpt structur data often defin similar metric evalu exampl intern compact similar member cluster separ differ cluster method base estim densiti graph connect special type unsupervis learn call learn involv train model gener supervisori signal data learn fall unsupervis learn without label train data supervis learn complet label train data train exampl miss train label yet mani research found unlabel data use conjunct small amount label data produc consider improv learn accuraci weakli supervis learn train label noisi limit imprecis howev label often cheaper obtain result larger effect train set reinforc learn area machin learn concern softwar agent ought take action environ maxim notion cumul reward due gener field studi mani disciplin game theori control theori oper research inform theori optim system swarm intellig statist genet algorithm reinforc learn environ typic repres markov decis process mdp mani reinforc learn algorithm use dynam program techniqu reinforc learn algorithm assum knowledg exact mathemat model mdp use exact model infeas reinforc learn algorithm use autonom vehicl learn play game human oppon dimension reduct process reduc number random variabl consider obtain set princip variabl word process reduc dimens featur set also call number featur dimension reduct techniqu consid either featur elimin extract one popular method dimension reduct princip compon analysi pca pca involv chang data smaller space manifold hypothesi propos data set lie along manifold mani dimension reduct techniqu make assumpt lead area manifold learn manifold regular approach develop fit neatli categor sometim one use machin learn system exampl topic model machin learn paradigm introduc along neural network capabl name crossbar adapt array caa learn extern reward extern teacher advic caa algorithm comput crossbar fashion decis action emot feel consequ situat system driven interact cognit emot algorithm updat memori matrix w iter execut follow machin learn routin system one input situat one output action behavior neither separ reinforc input advic input environ backpropag valu secondari reinforc emot toward consequ situat caa exist two environ one behavior environ behav genet environ wherefrom initi receiv initi emot situat encount behavior environ receiv genom speci vector genet environ caa learn behavior environ contain desir undesir situat sever learn algorithm aim discov better represent input provid train classic exampl includ princip compon analysi cluster analysi featur learn algorithm also call represent learn algorithm often attempt preserv inform input also transform way make use often step perform classif predict techniqu allow reconstruct input come unknown distribut necessarili faith configur implaus distribut replac manual featur engin allow machin learn featur use perform specif task featur learn either supervis unsupervis supervis featur learn featur learn use label input data exampl includ artifici neural network multilay perceptron supervis dictionari learn unsupervis featur learn featur learn unlabel input data exampl includ dictionari learn independ compon analysi autoencod matrix factor variou form cluster manifold learn algorithm attempt constraint learn represent spars code algorithm attempt constraint learn represent spars mean mathemat model mani zero multilinear subspac learn algorithm aim learn represent directli tensor represent multidimension data without reshap vector deep learn algorithm discov multipl level represent hierarchi featur abstract featur defin term gener featur argu intellig machin one learn represent disentangl underli factor variat explain observ data featur learn motiv fact machin learn task classif often requir input mathemat comput conveni process howev data imag video sensori data yield attempt algorithm defin specif featur altern discov featur represent examin without reli explicit algorithm spars dictionari learn featur learn method train exampl repres linear combin basi function assum spars matrix method strongli difficult solv approxim popular heurist method spars dictionari learn algorithm spars dictionari learn appli sever context classif problem determin class previous unseen train exampl belong dictionari class alreadi built new train exampl associ class best spars repres correspond dictionari spars dictionari learn also appli imag key idea clean imag patch spars repres imag dictionari nois data mine anomali detect also known outlier detect identif rare item event observ rais suspicion differ significantli major data typic anomal item repres issu bank fraud structur defect medic problem error text anomali refer outlier novelti nois deviat except particular context abus network intrus detect interest object often rare object unexpect burst inact pattern adher common statist definit outlier rare object mani outlier detect method particular unsupervis algorithm fail data unless aggreg appropri instead cluster analysi algorithm may abl detect form pattern three broad categori anomali detect techniqu exist unsupervis anomali detect techniqu detect anomali unlabel test data set assumpt major instanc data set normal look instanc seem fit least remaind data set supervis anomali detect techniqu requir data set label normal abnorm involv train classifi key differ mani statist classif problem inher unbalanc natur outlier detect anomali detect techniqu construct model repres normal behavior given normal train data set test likelihood test instanc gener model robot learn inspir multitud machin learn method start supervis learn reinforc learn final maml associ rule learn machin learn method discov relationship variabl larg databas intend identifi strong rule discov databas use measur interesting machin learn gener term machin learn method identifi learn evolv rule store manipul appli knowledg defin characterist machin learn algorithm identif util set relat rule collect repres knowledg captur system contrast machin learn algorithm commonli identifi singular model univers appli instanc order make predict machin learn approach includ learn classifi system associ rule learn artifici immun system base concept strong rule rakesh agraw tomasz imieliński arun swami introduc associ rule discov regular product transact data record po system supermarket exampl rule n n p e b u r g e r onion potato burger found sale data supermarket would indic custom buy onion potato togeth like also buy hamburg meat inform use basi decis market activ promot price product placement addit market basket analysi associ rule employ today applic area includ web usag mine intrus detect continu product bioinformat contrast sequenc mine associ rule learn typic consid order item either within transact across transact learn classifi system lc famili machin learn algorithm combin discoveri compon typic genet algorithm learn compon perform either supervis learn reinforc learn unsupervis learn seek identifi set rule collect store appli knowledg piecewis manner order make predict induct logic program ilp approach rule learn use logic program uniform represent input exampl background knowledg hypothes given encod known background knowledg set exampl repres logic databas fact ilp system deriv hypothes logic program entail posit neg exampl induct program relat field consid kind program languag repres hypothes logic program function program induct logic program particularli use bioinformat natur languag process gordon plotkin ehud shapiro laid initi theoret foundat induct machin learn logic set shapiro built first implement model infer system prolog program induct infer logic program posit neg exampl term induct refer philosoph induct suggest theori explain observ fact rather mathemat induct prove properti member set machin learn model type mathemat model train given dataset use make predict classif new data train learn algorithm iter adjust model intern paramet minim error predict extens term model refer sever level specif gener class model associ learn algorithm fulli train model intern paramet tune variou type model use research machin learn system pick best model task call model select artifici neural network ann connectionist system comput system vagu inspir biolog neural network constitut anim brain system learn perform task consid exampl gener without program rule ann model base collect connect unit node call artifici neuron loos model neuron biolog brain connect like synaps biolog brain transmit inform signal one artifici neuron anoth artifici neuron receiv signal process signal addit artifici neuron connect common ann implement signal connect artifici neuron real number output artifici neuron comput function sum input connect artifici neuron call edg artifici neuron edg typic weight adjust learn proce weight increas decreas strength signal connect artifici neuron may threshold signal sent aggreg signal cross threshold typic artifici neuron aggreg layer differ layer may perform differ kind transform input signal travel first layer input layer last layer output layer possibl travers layer multipl time origin goal ann approach solv problem way human brain would howev time attent move perform specif task lead deviat biolog artifici neural network use varieti task includ comput vision speech recognit machin translat social network filter play board video game medic diagnosi deep learn consist multipl hidden layer artifici neural network approach tri model way human brain process light sound vision hear success applic deep learn comput vision speech recognit decis tree learn use decis tree predict model go observ item repres branch conclus item target valu repres leav one predict model approach use statist data mine machin learn tree model target variabl take discret set valu call classif tree tree structur leav repres class label branch repres conjunct featur lead class label decis tree target variabl take continu valu typic real number call regress tree decis analysi decis tree use visual explicitli repres decis decis make data mine decis tree describ data result classif tree input machin svm also known network set relat supervis learn method use classif regress given set train exampl mark belong one two categori svm train algorithm build model predict whether new exampl fall one categori svm train algorithm binari linear classifi although method platt scale exist use svm probabilist classif set addit perform linear classif svm effici perform classif use call kernel trick implicitli map input featur space regress analysi encompass larg varieti statist method estim relationship input variabl associ featur common form linear regress singl line drawn best fit given data accord mathemat criterion ordinari least squar latter often extend regular method mitig overfit bia ridg regress deal problem model includ polynomi regress exampl use trendlin fit microsoft excel logist regress often use statist classif even kernel regress introduc take advantag kernel trick implicitli map input variabl space bayesian network belief network direct acycl graphic model probabilist graphic model repres set random variabl condit independ direct acycl graph dag exampl bayesian network could repres probabilist relationship diseas symptom given symptom network use comput probabl presenc variou diseas effici algorithm exist perform infer learn bayesian network model sequenc variabl like speech signal protein sequenc call dynam bayesian network gener bayesian network repres solv decis problem uncertainti call influenc diagram gaussian process stochast process everi finit collect random variabl process multivari normal distribut reli covari function kernel model pair point relat depend locat given set observ point exampl distribut unobserv output new point function input data directli comput look like observ point covari point new unobserv point gaussian process popular surrog model bayesian optim use hyperparamet optim genet algorithm ga search algorithm heurist techniqu mimic process natur select use method mutat crossov gener new genotyp hope find good solut given problem machin learn genet algorithm use convers machin learn techniqu use improv perform genet evolutionari algorithm theori belief function also refer evid theori theori gener framework reason uncertainti understood connect framework probabl possibl imprecis probabl theori theoret framework thought kind learner analog properti evid combin dempster rule combin like bayesian approach clarif need would combin probabl howev mani caveat belief function compar bayesian approach order incorpor ignor uncertainti quantif belief function approach implement within machin learn domain typic leverag fusion approach variou ensembl method better handl learner decis boundari low sampl ambigu class issu standard machin learn approach tend difficulti resolv howev comput complex algorithm depend number proposit class lead much higher comput time compar machin learn approach typic machin learn model requir high quantiti reliabl data perform accur predict train machin learn model machin learn engin need target collect larg repres sampl data data train set vari corpu text collect imag sensor data data collect individu user servic overfit someth watch train machin learn model train model deriv bias data result skew undesir predict bias model may result detriment outcom therebi further neg impact societi object algorithm bia potenti result data fulli prepar train machin learn ethic becom field studi notabl becom integr within machin learn engin team feder learn adapt form distribut artifici intellig train machin learn model decentr train process allow user privaci maintain need send data central server also increas effici decentr train process mani devic exampl gboard use feder machin learn train search queri predict model user mobil phone without send individu search back googl mani applic machin learn includ provid netflix held first netflix prize competit find program better predict user prefer improv accuraci exist cinematch movi recommend algorithm least joint team made research collabor team big chao pragmat theori built ensembl model win grand prize million shortli prize award netflix realiz viewer rate best indic view pattern everyth recommend chang recommend engin accordingli wall street journal wrote firm rebellion research use machin learn predict financi crisi sun microsystem vinod khosla predict medic doctor job would lost next two decad autom machin learn medic diagnost softwar report machin learn algorithm appli field art histori studi fine art paint may reveal previous unrecogn influenc among artist springer natur publish first research book creat use machin learn machin learn technolog use help make diagnos aid research develop cure machin learn recent appli predict behavior travel recent machin learn technolog also appli optim smartphon perform thermal behavior base user interact phone appli correctli machin learn algorithm mla util wide rang compani characterist predict stock return without overfit employ effect featur engin combin forecast mla gener result far surpass obtain basic linear techniqu like ol recent advanc machin learn extend field quantum chemistri novel algorithm enabl predict solvent effect chemic reaction therebi offer new tool chemist tailor experiment condit optim outcom machin learn becom use tool investig predict evacu decis make larg scale small scale disast differ solut test predict household decid evacu wildfir hurrican applic focus pre evacu decis build fire although machin learn transform field program often fail deliv expect result reason numer lack suitabl data lack access data data bia privaci problem badli chosen task algorithm wrong tool peopl lack resourc evalu problem black box theori pose anoth yet signific challeng black box refer situat algorithm process produc output entir opaqu mean even coder algorithm audit pattern machin extract data hous lord select committe claim intellig system could substanti impact individu life would consid accept unless provid full satisfactori explan decis make car uber fail detect pedestrian kill collis attempt use machin learn healthcar ibm watson system fail deliv even year time billion dollar invest microsoft bing chat chatbot report produc hostil offens respons user machin learn use strategi updat evid relat systemat review increas review burden relat growth biomed literatur improv train set yet develop suffici reduc workload burden without limit necessari sensit find research explain ai xai interpret ai explain machin learn xml artifici intellig ai human understand decis predict made ai contrast black box concept machin learn even design explain ai arriv specif decis refin mental model user system dismantl misconcept xai promis help user perform effect xai may implement social right explan settl bad overli complex theori gerrymand fit past train data known overfit mani system attempt reduc overfit reward theori accord well fit data penal theori accord complex theori learner also disappoint learn wrong lesson toy exampl imag classifi train pictur brown hors black cat might conclud brown patch like hors exampl unlik human current imag classifi often primarili make judgment spatial relationship compon pictur learn relationship pixel human oblivi still correl imag certain type real object modifi pattern legitim imag result adversari imag system misclassifi adversari vulner also result nonlinear system perturb system possibl chang output chang singl adversari chosen pixel machin learn model often vulner manipul evas via adversari machin learn research demonstr backdoor place undetect classifi categori spam spam post machin learn model often develop train third parti parti chang classif input includ case type transpar provid possibl includ access classif machin learn model valid accuraci estim techniqu like holdout method split data train test set convent train set test set design evalu perform train model test set comparison method randomli partit data k subset k experi perform respect consid subset evalu remain subset train model addit holdout method bootstrap sampl n instanc replac dataset use assess model accuraci addit overal accuraci investig frequent report sensit specif mean true posit rate tpr true neg rate tnr respect similarli investig sometim report fals posit rate fpr well fals neg rate fnr howev rate ratio fail reveal numer denomin receiv oper characterist roc along accompani area roc curv auc offer addit tool classif model assess higher auc associ better perform model ethic artifici intellig cover broad rang topic within field consid particular ethic stake includ algorithm bias fair autom account privaci regul also cover variou emerg potenti futur challeng machin ethic make machin behav ethic lethal autonom weapon system arm race dynam ai safeti align technolog unemploy misinform treat certain ai system moral statu ai welfar right artifici superintellig existenti risk differ machin learn approach suffer differ data bias machin learn system train specif current custom may abl predict need new custom group repres train data train data machin learn like pick constitut unconsci bias alreadi present societi system train dataset collect bias may exhibit bias upon use algorithm bia thu digit cultur prejudic exampl uk commiss racial equal found georg medic school use comput program train data previou admiss staff program deni nearli candid found either women sound name use job hire data firm racist hire polici may lead machin learn system duplic bia score job applic similar previou success applic anoth exampl includ predict polic compani geolitica predict algorithm result disproportion high level minor commun train histor crime data respons collect data document algorithm rule use system consid critic part machin learn research blame lack particip represent minor popul field ai machin learn vulner bias fact accord research carri comput research associ cra femal faculti mere make faculti member focu ai among sever univers around world furthermor among group new resid ai phd graduat identifi white asian hispan african american demonstr lack divers field ai languag model learn data shown contain bias human languag contain bias machin train languag corpora necessarili also learn bias microsoft test tay chatbot learn twitter quickli pick racist sexist languag experi carri propublica investig journal organ machin learn algorithm insight recidiv rate among prison fals flag black defend high risk twice often white defend googl photo tag coupl black peopl gorilla caus controversi gorilla label subsequ remov still recogn gorilla similar issu recogn peopl found mani system challeng effect use machin learn may take longer adopt domain concern fair machin learn reduc bia machin learn propel use human good increasingli express artifici intellig scientist includ li said noth artifici ai inspir peopl creat peopl impact peopl power tool begin understand profound respons concern among health care profession system might design public interest machin especi true unit state ethic dilemma improv health care also increas profit exampl algorithm could design provid patient unnecessari test medic algorithm proprietari owner hold stake potenti machin learn health care provid profession addit tool diagnos medic plan recoveri path patient requir bias mitig sinc advanc machin learn algorithm comput hardwar led effici method train deep neural network particular narrow subdomain machin learn contain mani layer nonlinear hidden unit graphic process unit gpu often enhanc displac cpu domin method train commerci cloud ai openai estim hardwar comput use largest deep learn project alexnet alphazero found increas amount comput requir trendlin month neuromorph comput refer class comput system design emul structur function biolog neural network system may implement simul convent hardwar special hardwar architectur physic neural network specif type neuromorph hardwar reli electr adjust materi memristor emul function neural synaps term physic neural network highlight use physic hardwar comput oppos implement broadli refer artifici neural network use materi adjust resist replic neural synaps embed machin learn machin learn model deploy embed system limit comput resourc wearabl comput edg devic microcontrol run model directli devic elimin need transfer store data cloud server process therebi reduc risk data breach privaci leak theft intellectu properti person data busi secret embed machin learn achiev variou techniqu hardwar acceler approxim comput model optim common optim techniqu includ prune quantiz knowledg distil factor network architectur search paramet share softwar suit contain varieti machin learn algorithm includ follow
Machine Learning (journal),https://en.wikipedia.org/wiki/Machine_Learning_(journal),"Machine Learning  is a peer-reviewed scientific journal, published since 1986.
 In 2001, forty editors and members of the editorial board of Machine Learning resigned in order to support the Journal of Machine Learning Research (JMLR), saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet.[1]
 Following the mass resignation, Kluwer changed their publishing policy to allow authors to self-archive their papers online after peer-review.[2]
 
 This article about a computer science journal is a stub. You can help Wikipedia by expanding it. See tips for writing articles about academic journals. Further suggestions might be found on the article's talk page.",machin learn scientif journal publish sinc forti editor member editori board machin learn resign order support journal machin learn research jmlr say era internet detriment research continu publish paper expens journal archiv instead wrote support model jmlr author retain copyright paper archiv freeli avail internet follow mass resign kluwer chang publish polici allow author paper onlin articl comput scienc journal stub help wikipedia expand see tip write articl academ journal suggest might found articl talk page
Statistical learning in language acquisition,https://en.wikipedia.org/wiki/Statistical_learning_in_language_acquisition,"Statistical learning is the ability for humans and other animals to extract statistical regularities from the world around them to learn about the environment. Although statistical learning is now thought to be a generalized learning mechanism, the phenomenon was first identified in human infant language acquisition.
 The earliest evidence for these statistical learning abilities comes from a study by Jenny Saffran, Richard Aslin, and Elissa Newport, in which 8-month-old infants were presented with nonsense streams of monotone speech. Each stream was composed of four three-syllable ""pseudowords"" that were repeated randomly. After exposure to the speech streams for two minutes, infants reacted differently to hearing ""pseudowords"" as opposed to ""nonwords"" from the speech stream, where nonwords were composed of the same syllables that the infants had been exposed to, but in a different order. This suggests that infants are able to learn statistical relationships between syllables even with very limited exposure to a language. That is, infants learn which syllables are always paired together and which ones only occur together relatively rarely, suggesting that they are parts of two different units. This method of learning is thought to be one way that children learn which groups of syllables form individual words. [citation needed]
 Since the initial discovery of the role of statistical learning in lexical acquisition, the same mechanism has been proposed for elements of phonological acquisition, and syntactical acquisition, as well as in non-linguistic domains. Further research has also indicated that statistical learning is likely a domain-general and even species-general learning mechanism, occurring for visual as well as auditory information, and in both primates and non-primates.
 The role of statistical learning in language acquisition has been particularly well documented in the area of lexical acquisition.[1] One important contribution to infants' understanding of segmenting words from a continuous stream of speech is their ability to recognize statistical regularities of the speech heard in their environments.[1] Although many factors play an important role, this specific mechanism is powerful and can operate over a short time scale.[1]
 It is a well-established finding that, unlike written language, spoken language does not have any clear boundaries between words; spoken language is a continuous stream of sound rather than individual words with silences between them.[2] This lack of segmentation between linguistic units presents a problem for young children learning language, who must be able to pick out individual units from the continuous speech streams that they hear.[3] One proposed method of how children are able to solve this problem is that they are attentive to the statistical regularities of the world around them.[2][3] For example, in the phrase ""pretty baby"", children are more likely to hear the sounds pre and ty heard together during the entirety of the lexical input around them than they are to hear the sounds ty and ba together.[3] In an artificial grammar learning study with adult participants, Saffran, Newport, and Aslin found that participants were able to locate word boundaries based only on transitional probabilities, suggesting that adults are capable of using statistical regularities in a language-learning task.[4] This is a robust finding that has been widely replicated.[1]
 To determine if young children have these same abilities Saffran Aslin and Newport exposed 8-month-old infants to an artificial grammar.[3] The grammar was composed of four words, each composed of three nonsense syllables. During the experiment, infants heard a continuous speech stream of these words. The speech was presented in a monotone with no cues (such as pauses, intonation, etc.) to word boundaries other than the statistical probabilities. Within a word, the transitional probability of two syllable pairs was 1.0: in the word bidaku, for example, the probability of hearing the syllable da immediately after the syllable bi was 100%. Between words, however, the transitional probability of hearing a syllable pair was much lower: After any given word (e.g., bidaku) was presented, one of three words could follow (in this case, padoti, golabu, or tupiro), so the likelihood of hearing any given syllable after ku was only 33%.
 To determine if infants were picking up on the statistical information, each infant was presented with multiple presentations of either a word from the artificial grammar or a nonword made up of the same syllables but presented in a random order. Infants who were presented with nonwords during the test phase listened significantly longer to these words than infants who were presented with words from the artificial grammar, showing a novelty preference for these new nonwords. However, the implementation of the test could also be due to infants learning serial-order information and not to actually learning transitional probabilities between words. That is, at test, infants heard strings such as dapiku and tilado that were never presented during learning; they could simply have learned that the syllable ku never followed the syllable pi.[3]
 To look more closely at this issue, Saffran Aslin and Newport conducted another study in which infants underwent the same training with the artificial grammar but then were presented with either words or part-words rather than words or nonwords.[3] The part-words were syllable sequences composed of the last syllable from one word and the first two syllables from another (such as kupado). Because the part-words had been heard during the time when children were listening to the artificial grammar, preferential listening to these part-words would indicate that children were learning not only serial-order information, but also the statistical likelihood of hearing particular syllable sequences. Again, infants showed greater listening times to the novel (part-) words, indicating that 8-month-old infants were able to extract these statistical regularities from a continuous speech stream.
 This result has been the impetus for much more research on the role of statistical learning in lexical acquisition and other areas (see[1]). In a follow-up to the original report,[3] Aslin, Saffran, and Newport found that even when words and part words occurred equally often in the speech stream, but with different transitional probabilities between syllables of words and part words, infants were still able to detect the statistical regularities and still preferred to listen to the novel part-words over the familiarized words.[5] This finding provides stronger evidence that infants are able to pick up transitional probabilities from the speech they hear, rather than just being aware of frequencies of individual syllable sequences.[1]
 Another follow-up study examined the extent to which the statistical information learned during this type of artificial grammar learning feeds into knowledge that infants may already have about their native language.[6] Infants preferred to listen to words over part-words, whereas there was no significant difference in the nonsense frame condition. This finding suggests that even pre-linguistic infants are able to integrate the statistical cues they learn in a laboratory into their previously acquired knowledge of a language.[1][6] In other words, once infants have acquired some linguistic knowledge, they incorporate newly acquired information into that previously acquired learning.
 A related finding indicates that slightly older infants can acquire both lexical and grammatical regularities from a single set of input,[7] suggesting that they are able to use outputs of one type of statistical learning (cues that lead to the discovery of word boundaries) as input to a second type (cues that lead to the discovery of syntactical regularities.[1][7] At test, 12-month-olds preferred to listen to sentences that had the same grammatical structure as the artificial language they had been tested on rather than sentences that had a different (ungrammatical) structure. Because learning grammatical regularities requires infants to be able to determine boundaries between individual words, this indicates that infants who are still quite young are able to acquire multiple levels of language knowledge (both lexical and syntactical) simultaneously, indicating that statistical learning is a powerful mechanism at play in language learning.[1][7]
 Despite the large role that statistical learning appears to play in lexical acquisition, it is likely not the only mechanism by which infants learn to segment words. Statistical learning studies are generally conducted with artificial grammars that have no cues to word boundary information other than transitional probabilities between words. Real speech, though, has many different types of cues to word boundaries, including prosodic and phonotactic information.[8]
 Together, the findings from these studies of statistical learning in language acquisition indicate that statistical properties of the language are a strong cue in helping infants learn their first language.[1]
 There is much evidence that statistical learning is an important component of both discovering which phonemes are important for a given language and which contrasts within phonemes are important.[9][10][11] Having this knowledge is important for aspects of both speech perception and speech production.
 Since the discovery of infants' statistical learning abilities in word learning, the same general mechanism has also been studied in other facets of language learning. For example, it is well-established that infants can discriminate between phonemes of many different languages but eventually become unable to discriminate between phonemes that do not appear in their native language;[12] however, it was not clear how this decrease in discriminatory ability came about. Maye et al. suggested that the mechanism responsible might be a statistical learning mechanism in which infants track the distributional regularities of the sounds in their native language.[12] To test this idea, Maye et al. exposed 6- and 8-month-old infants to a continuum of speech sounds that varied on the degree to which they were voiced. The distribution that the infants heard was either bimodal, with sounds from both ends of the voicing continuum heard most often, or unimodal, with sounds from the middle of the distribution heard most often. The results indicated that infants from both age groups were sensitive to the distribution of phonemes. At test, infants heard either non-alternating (repeated exemplars of tokens 3 or 6 from an 8-token continuum) or alternating (exemplars of tokens 1 and 8) exposures to specific phonemes on the continuum. Infants exposed to the bimodal distribution listened longer to the alternating trials than the non-alternating trials while there was no difference in listening times for infants exposed to the unimodal distribution. This finding indicates that infants exposed the bimodal distribution were better able to discriminate sounds from the two ends of the distribution than were infants in the unimodal condition, regardless of age. This type of statistical learning differs from that used in lexical acquisition, as it requires infants to track frequencies rather than transitional probabilities, and has been named ""distributional learning"".[10]
 Distributional learning has also been found to help infants contrast two phonemes that they initially have difficulty in discriminating between. Maye, Weiss, and Aslin found that infants who were exposed to a bimodal distribution of a non-native contrast that was initially difficult to discriminate were better able to discriminate the contrast than infants exposed to a unimodal distribution of the same contrast.[13] Maye et al. also found that infants were able to abstract features of a contrast (i.e., voicing onset time) and generalize that feature to the same type of contrast at a different place of articulation, a finding that has not been found in adults.
 In a review of the role of distributional learning on phonological acquisition, Werker et al. note that distributional learning cannot be the only mechanism by which phonetic categories are acquired.[10] However, it does seem clear that this type of statistical learning mechanism can play a role in this skill, although research is ongoing.[10]
 A related finding regarding statistical cues to phonological acquisition is a phenomenon known as the perceptual magnet effect.[14][15][16] In this effect, a prototypical phoneme of a person's native language acts as a ""magnet"" for similar phonemes, which are perceived as belonging to the same category as the prototypical phoneme. In the original test of this effect, adult participants were asked to indicate if a given exemplar of a particular phoneme differed from a referent phoneme.[14] If the referent phoneme is a non-prototypical phoneme for that language, both adults and 6-month-old infants show less generalization to other sounds than they do for prototypical phonemes, even if the subjective distance between the sounds is the same.[14][16] That is, adults and infants are both more likely to notice that a particular phoneme differs from the referent phoneme if that referent phoneme is a non-prototypical exemplar than if it is a prototypical exemplar. The prototypes themselves are apparently discovered through a distributional learning process, in which infants are sensitive to the frequencies with which certain sounds occur and treat those that occur most often as the prototypical phonemes of their language.[11]
 A statistical learning device has also been proposed as a component of syntactical acquisition for young children.[1][9][17] Early evidence for this mechanism came largely from studies of computer modeling or analyses of natural language corpora.[18][19] These early studies focused largely on distributional information specifically rather than statistical learning mechanisms generally. Specifically, in these early papers it was proposed that children created templates of possible sentence structures involving unnamed categories of word types (i.e., nouns or verbs, although children would not put these labels on their categories). Children were thought to learn which words belonged to the same categories by tracking the similar contexts in which words of the same category appeared.
 Later studies expanded these results by looking at the actual behavior of children or adults who had been exposed to artificial grammars.[9] These later studies also considered the role of statistical learning more broadly than the earlier studies, placing their results in the context of the statistical learning mechanisms thought to be involved with other aspects of language learning, such as lexical acquisition.
 Evidence from a series of four experiments conducted by Gomez and Gerken suggests that children are able to generalize grammatical structures with less than two minutes of exposure to an artificial grammar.[9][20] In the first experiment, 11-12 month-old infants were trained on an artificial grammar composed of nonsense words with a set grammatical structure. At test, infants heard both novel grammatical and ungrammatical sentences. Infants oriented longer toward the grammatical sentences, in line with previous research that suggests that infants generally orient for a longer amount of time to natural instances of language rather than altered instances of language e.g.,.[21] (This familiarity preference differs from the novelty preference generally found in word-learning studies, due to the differences between lexical acquisition and syntactical acquisition.) This finding indicates that young children are sensitive to the grammatical structure of language even after minimal exposure. Gomez and Gerken also found that this sensitivity is evident when ungrammatical transitions are located in the middle of the sentence (unlike in the first experiment, in which all the errors occurred at the beginning and end of the sentences), that the results could not be due to an innate preference for the grammatical sentences caused by something other than grammar, and that children are able to generalize the grammatical rules to new vocabulary.
 Together these studies suggest that infants are able to extract a substantial amount of syntactic knowledge even from limited exposure to a language.[9][20] Children apparently detected grammatical anomalies whether the grammatical violation in the test sentences occurred at the end or in the middle of the sentence. Additionally, even when the individual words of the grammar were changed, infants were still able to discriminate between grammatical and ungrammatical strings during the test phase. This generalization indicates that infants were not learning vocabulary-specific grammatical structures, but abstracting the general rules of that grammar and applying those rules to novel vocabulary. Furthermore, in all four experiments, the test of grammatical structures occurred five minutes after the initial exposure to the artificial grammar had ended, suggesting that the infants were able to maintain the grammatical abstractions they had learned even after a short delay.
 In a similar study, Saffran found that adults and older children (first- and second grade children) were also sensitive to syntactical information after exposure to an artificial language which had no cues to phrase structure other than the statistical regularities that were present.[22] Both adults and children were able to pick out sentences that were ungrammatical at a rate greater than chance, even under an ""incidental"" exposure condition in which participants' primary goal was to complete a different task while hearing the language.
 Although the number of studies dealing with statistical learning of syntactical information is limited, the available evidence does indicate that the statistical learning mechanisms are likely a contributing factor to children's ability to learn their language.[9][17]
 Much of the early work using statistical learning paradigms focused on the ability for children or adults to learn a single language,[1] consistent with the process of language acquisition for monolingual speakers or learners. However, it is estimated that approximately 60-75% of people in the world are bilingual.[23] More recently, researchers have begun looking at the role of statistical learning for those who speak more than one language. Although there are no reviews on this topic yet, Weiss, Gerfen, and Mitchel examined how hearing input from multiple artificial languages simultaneously can affect the ability to learn either or both languages.[24] Over four experiments, Weiss et al. found that, after exposure to two artificial languages, adult learners are capable of determining word boundaries in both languages when each language is spoken by a different speaker. However, when the two languages were spoken by the same speaker, participants were able learn both languages only when they were ""congruent""—when the word boundaries of one language matched the word boundaries of the other. When the languages were incongruent—a syllable that appeared in the middle of a word in one language appeared at the end of the word in the other language—and spoken by a single speaker, participants were able to learn, at best, one of the two languages. A final experiment showed that the inability to learn incongruent languages spoken in the same voice was not due to syllable overlap between the languages but due to differing word boundaries.
 Similar work replicates the finding that learners are able to learn two sets of statistical representations when an additional cue is present (two different male voices in this case).[25] In their paradigm, the two languages were presented consecutively, rather than interleaved as in Weiss et al.'s paradigm,[24] and participants did learn the first artificial language to which they had been exposed better than the second, although participants' performance was above chance for both languages.
 While statistical learning improves and strengthens multilingualism, it appears that the inverse is not true. In a study by Yim and Rudoy[26] it was found that both monolingual and bilingual children perform statistical learning tasks equally well.
 Antovich and Graf Estes[27] found that 14-month-old bilingual children are better than monolinguals at segmenting two different artificial languages using transitional probability cues. They suggest that a bilingual environment in early childhood trains children to rely on statistical regularities to segment the speech flow and access two lexical systems.
 A statistical learning mechanism has also been proposed for learning the meaning of words. Specifically, Yu and Smith conducted a pair of studies in which adults were exposed to pictures of objects and heard nonsense words.[28] Each nonsense word was paired with a particular object. There were 18 total word-referent pairs, and each participant was presented with either 2, 3, or 4 objects at a time, depending on the condition, and heard the nonsense word associated with one of those objects. Each word-referent pair was presented 6 times over the course of the training trials; after the completion of the training trials, participants completed a forced-alternative test in which they were asked to choose the correct referent that matched a nonsense word they were given. Participants were able to choose the correct item more often than would happen by chance, indicating, according to the authors, that they were using statistical learning mechanisms to track co-occurrence probabilities across training trials.
 An alternative hypothesis is that learners in this type of task may be using a ""propose-but-verify"" mechanism rather than a statistical learning mechanism.[29][30] Medina et al. and Trueswell et al. argue that, because Yu and Smith only tracked knowledge at the end of the training, rather than tracking knowledge on a trial-by-trial basis, it is impossible to know if participants were truly updating statistical probabilities of co-occurrence (and therefore maintaining multiple hypotheses simultaneously), or if, instead, they were forming a single hypothesis and checking it on the next trial.[28][29][30] For example, if a participant is presented with a picture of a dog and a picture of a shoe, and hears the nonsense word vash she might hypothesize that vash refers to the dog. On a future trial, she may see a picture of a shoe and a picture of a door and again hear the word vash. If statistical learning is the mechanism by which word-referent mappings are learned, then the participant would be more likely to select the picture of the shoe than the door, as shoe would have appeared in conjunction with the word vash 100% of the time. However, if participants are simply forming a single hypothesis, they may fail to remember the context of the previous presentation of vash (especially if, as in the experimental conditions, there are multiple trials with other words in between the two presentations of vash) and therefore be at chance in this second trial. According to this proposed mechanism of word learning, if the participant had correctly guessed that vash referred to the shoe in the first trial, her hypothesis would be confirmed in the subsequent trial.
 To distinguish between these two possibilities, Trueswell et al. conducted a series of experiments similar to those conducted by Yu and Smith except that participants were asked to indicate their choice of the word-referent mapping on each trial, and only a single object name was presented on each trial (with varying numbers of objects).[28][30] Participants would therefore have been at chance when they are forced to make a choice in their first trial. The results from the subsequent trials indicate that participants were not using a statistical learning mechanism in these experiments, but instead were using a propose-and-verify mechanism, holding only one potential hypothesis in mind at a time. Specifically, if participants had chosen an incorrect word-referent mapping in an initial presentation of a nonsense word (from a display of five possible choices), their likelihood of choosing the correct word-referent mapping in the next trial of that word was still at chance, or 20%. If, though, the participant had chosen the correct word-referent mapping on an initial presentation of a nonsense word, the likelihood of choosing the correct word-referent mapping on the subsequent presentation of that word was approximately 50%. These results were also replicated in a condition where participants were choosing between only two alternatives. These results suggest that participants did not remember the surrounding context of individual presentations and were therefore not using statistical cues to determine the word-referent mappings. Instead, participants make a hypothesis regarding a word-referent mapping and, on the next presentation of that word, either confirm or reject the hypothesis accordingly.
 Overall, these results, along with similar results from Medina et al., indicate that word meanings may not be learned through a statistical learning mechanism in these experiments, which ask participants to hypothesize a mapping even on the first occurrence (i.e., not cross-situationally).[29] However, when the propose-but-verify mechanism has been compared to a statistical learning mechanism, the former was unable to reproduce individual learning trajectories nor fit as well as the latter.[31]
 Additionally, statistical learning by itself cannot account even for those aspects of language acquisition for which it has been shown to play a large role. For example, Kuhl, Tsao, and Liu found that young English-learning infants who spent time in a laboratory session with a native Mandarin speaker were able to distinguish between phonemes that occur in Mandarin but not in English, unlike infants who were in a control condition.[32] Infants in this control condition came to the lab as often as infants in the experimental condition, but were exposed only to English; when tested at a later date, they were unable to distinguish the Mandarin phonemes. In a second experiment, the authors presented infants with audio or audiovisual recordings of Mandarin speakers and tested the infants' ability to distinguish between the Mandarin phonemes. In this condition, infants failed to distinguish the foreign language phonemes. This finding indicates that social interaction is a necessary component of language learning and that, even if infants are presented with the raw data of hearing a language, they are unable to take advantage of the statistical cues present in that data if they are not also experiencing the social interaction.[11]
 Although the phenomenon of statistical learning was first discovered in the context of language acquisition and there is much evidence of its role in that purpose, work since the original discovery has suggested that statistical learning may be a domain general skill and is likely not unique to humans.[3][33] For example, Saffran, Johnson, Aslin, and Newport found that both adults and infants were able to learn statistical probabilities of ""words"" created by playing different musical tones (i.e., participants heard the musical notes D, E, and F presented together during training and were able to recognize those notes as a unit at test as compared to three notes that had not been presented together).[34] In non-auditory domains, there is evidence that humans are able to learn statistical visual information whether that information is presented across space, e.g.,[35] or time, e.g.,.[36] Evidence of statistical learning has also been found in other primates, e.g.,[37] and some limited statistical learning abilities have been found even in non-primates like rats.[38] Together these findings suggest that statistical learning may be a generalized learning mechanism that happens to be utilized in language acquisition, rather than a mechanism that is unique to the human infant's ability to learn his or her language(s).
 Further evidence for domain general statistical learning was suggested in a study run through the University of Cornell Department of Psychology concerning visual statistical learning in infancy. Researchers in this study questioned whether domain generality of statistical learning in infancy would be seen using visual information. After first viewing images in statistically predictable patterns, infants were then exposed to the same familiar patterns in addition to novel sequences of the same identical stimulus components. Interest in the visuals was measured by the amount of time the child looked at the stimuli in which the researchers named ""looking time"". All ages of infant participants showed more interest in the novel sequence relative to the familiar sequence. In demonstrating a preference for the novel sequences (which violated the transitional probability that defined the grouping of the original stimuli) the results of the study support the likelihood of domain general statistical learning in infancy.[39]
",statist learn abil human anim extract statist regular world around learn environ although statist learn thought gener learn mechan phenomenon first identifi human infant languag acquisit earliest evid statist learn abil come studi jenni saffran richard aslin elissa newport infant present nonsens stream monoton speech stream compos four pseudoword repeat randomli exposur speech stream two minut infant react differ hear pseudoword oppos nonword speech stream nonword compos syllabl infant expos differ order suggest infant abl learn statist relationship syllabl even limit exposur languag infant learn syllabl alway pair togeth one occur togeth rel rare suggest part two differ unit method learn thought one way children learn group syllabl form individu word citat need sinc initi discoveri role statist learn lexic acquisit mechan propos element phonolog acquisit syntact acquisit well domain research also indic statist learn like even learn mechan occur visual well auditori inform primat role statist learn languag acquisit particularli well document area lexic acquisit one import contribut infant understand segment word continu stream speech abil recogn statist regular speech heard environ although mani factor play import role specif mechan power oper short time scale find unlik written languag spoken languag clear boundari word spoken languag continu stream sound rather individu word silenc lack segment linguist unit present problem young children learn languag must abl pick individu unit continu speech stream hear one propos method children abl solv problem attent statist regular world around exampl phrase pretti babi children like hear sound pre ty heard togeth entireti lexic input around hear sound ty ba togeth artifici grammar learn studi adult particip saffran newport aslin found particip abl locat word boundari base transit probabl suggest adult capabl use statist regular task robust find wide replic determin young children abil saffran aslin newport expos infant artifici grammar grammar compos four word compos three nonsens syllabl experi infant heard continu speech stream word speech present monoton cue paus inton etc word boundari statist probabl within word transit probabl two syllabl pair word bidaku exampl probabl hear syllabl da immedi syllabl bi word howev transit probabl hear syllabl pair much lower given word bidaku present one three word could follow case padoti golabu tupiro likelihood hear given syllabl ku determin infant pick statist inform infant present multipl present either word artifici grammar nonword made syllabl present random order infant present nonword test phase listen significantli longer word infant present word artifici grammar show novelti prefer new nonword howev implement test could also due infant learn inform actual learn transit probabl word test infant heard string dapiku tilado never present learn could simpli learn syllabl ku never follow syllabl pi look close issu saffran aslin newport conduct anoth studi infant underw train artifici grammar present either word rather word nonword syllabl sequenc compos last syllabl one word first two syllabl anoth kupado heard time children listen artifici grammar preferenti listen would indic children learn inform also statist likelihood hear particular syllabl sequenc infant show greater listen time novel word indic infant abl extract statist regular continu speech stream result impetu much research role statist learn lexic acquisit area see origin report aslin saffran newport found even word part word occur equal often speech stream differ transit probabl syllabl word part word infant still abl detect statist regular still prefer listen novel familiar word find provid stronger evid infant abl pick transit probabl speech hear rather awar frequenc individu syllabl sequenc anoth studi examin extent statist inform learn type artifici grammar learn feed knowledg infant may alreadi nativ languag infant prefer listen word wherea signific differ nonsens frame condit find suggest even infant abl integr statist cue learn laboratori previous acquir knowledg languag word infant acquir linguist knowledg incorpor newli acquir inform previous acquir learn relat find indic slightli older infant acquir lexic grammat regular singl set input suggest abl use output one type statist learn cue lead discoveri word boundari input second type cue lead discoveri syntact regular test prefer listen sentenc grammat structur artifici languag test rather sentenc differ ungrammat structur learn grammat regular requir infant abl determin boundari individu word indic infant still quit young abl acquir multipl level languag knowledg lexic syntact simultan indic statist learn power mechan play languag learn despit larg role statist learn appear play lexic acquisit like mechan infant learn segment word statist learn studi gener conduct artifici grammar cue word boundari inform transit probabl word real speech though mani differ type cue word boundari includ prosod phonotact inform togeth find studi statist learn languag acquisit indic statist properti languag strong cue help infant learn first languag much evid statist learn import compon discov phonem import given languag contrast within phonem import knowledg import aspect speech percept speech product sinc discoveri infant statist learn abil word learn gener mechan also studi facet languag learn exampl infant discrimin phonem mani differ languag eventu becom unabl discrimin phonem appear nativ languag howev clear decreas discriminatori abil came may et al suggest mechan respons might statist learn mechan infant track distribut regular sound nativ languag test idea may et al expos infant continuum speech sound vari degre voic distribut infant heard either bimod sound end voic continuum heard often unimod sound middl distribut heard often result indic infant age group sensit distribut phonem test infant heard either repeat exemplar token continuum altern exemplar token exposur specif phonem continuum infant expos bimod distribut listen longer altern trial trial differ listen time infant expos unimod distribut find indic infant expos bimod distribut better abl discrimin sound two end distribut infant unimod condit regardless age type statist learn differ use lexic acquisit requir infant track frequenc rather transit probabl name distribut learn distribut learn also found help infant contrast two phonem initi difficulti discrimin may weiss aslin found infant expos bimod distribut contrast initi difficult discrimin better abl discrimin contrast infant expos unimod distribut contrast may et al also found infant abl abstract featur contrast voic onset time gener featur type contrast differ place articul find found adult review role distribut learn phonolog acquisit werker et al note distribut learn mechan phonet categori acquir howev seem clear type statist learn mechan play role skill although research ongo relat find regard statist cue phonolog acquisit phenomenon known perceptu magnet effect effect prototyp phonem person nativ languag act magnet similar phonem perceiv belong categori prototyp phonem origin test effect adult particip ask indic given exemplar particular phonem differ refer phonem refer phonem phonem languag adult infant show less gener sound prototyp phonem even subject distanc sound adult infant like notic particular phonem differ refer phonem refer phonem exemplar prototyp exemplar prototyp appar discov distribut learn process infant sensit frequenc certain sound occur treat occur often prototyp phonem languag statist learn devic also propos compon syntact acquisit young children earli evid mechan came larg studi comput model analys natur languag corpora earli studi focus larg distribut inform specif rather statist learn mechan gener specif earli paper propos children creat templat possibl sentenc structur involv unnam categori word type noun verb although children would put label categori children thought learn word belong categori track similar context word categori appear later studi expand result look actual behavior children adult expos artifici grammar later studi also consid role statist learn broadli earlier studi place result context statist learn mechan thought involv aspect languag learn lexic acquisit evid seri four experi conduct gomez gerken suggest children abl gener grammat structur less two minut exposur artifici grammar first experi infant train artifici grammar compos nonsens word set grammat structur test infant heard novel grammat ungrammat sentenc infant orient longer toward grammat sentenc line previou research suggest infant gener orient longer amount time natur instanc languag rather alter instanc languag familiar prefer differ novelti prefer gener found studi due differ lexic acquisit syntact acquisit find indic young children sensit grammat structur languag even minim exposur gomez gerken also found sensit evid ungrammat transit locat middl sentenc unlik first experi error occur begin end sentenc result could due innat prefer grammat sentenc caus someth grammar children abl gener grammat rule new vocabulari togeth studi suggest infant abl extract substanti amount syntact knowledg even limit exposur languag children appar detect grammat anomali whether grammat violat test sentenc occur end middl sentenc addit even individu word grammar chang infant still abl discrimin grammat ungrammat string test phase gener indic infant learn grammat structur abstract gener rule grammar appli rule novel vocabulari furthermor four experi test grammat structur occur five minut initi exposur artifici grammar end suggest infant abl maintain grammat abstract learn even short delay similar studi saffran found adult older children second grade children also sensit syntact inform exposur artifici languag cue phrase structur statist regular present adult children abl pick sentenc ungrammat rate greater chanc even incident exposur condit particip primari goal complet differ task hear languag although number studi deal statist learn syntact inform limit avail evid indic statist learn mechan like contribut factor children abil learn languag much earli work use statist learn paradigm focus abil children adult learn singl languag consist process languag acquisit monolingu speaker learner howev estim approxim peopl world bilingu recent research begun look role statist learn speak one languag although review topic yet weiss gerfen mitchel examin hear input multipl artifici languag simultan affect abil learn either languag four experi weiss et al found exposur two artifici languag adult learner capabl determin word boundari languag languag spoken differ speaker howev two languag spoken speaker particip abl learn languag congruent word boundari one languag match word boundari languag syllabl appear middl word one languag appear end word spoken singl speaker particip abl learn best one two languag final experi show inabl learn incongru languag spoken voic due syllabl overlap languag due differ word boundari similar work replic find learner abl learn two set statist represent addit cue present two differ male voic case paradigm two languag present consecut rather interleav weiss et al paradigm particip learn first artifici languag expos better second although particip perform chanc languag statist learn improv strengthen multilingu appear invers true studi yim rudoy found monolingu bilingu children perform statist learn task equal well antovich graf est found bilingu children better monolingu segment two differ artifici languag use transit probabl cue suggest bilingu environ earli childhood train children reli statist regular segment speech flow access two lexic system statist learn mechan also propos learn mean word specif yu smith conduct pair studi adult expos pictur object heard nonsens word nonsens word pair particular object total pair particip present either object time depend condit heard nonsens word associ one object pair present time cours train trial complet train trial particip complet test ask choos correct refer match nonsens word given particip abl choos correct item often would happen chanc indic accord author use statist learn mechan track probabl across train trial altern hypothesi learner type task may use mechan rather statist learn mechan medina et al trueswel et al argu yu smith track knowledg end train rather track knowledg basi imposs know particip truli updat statist probabl therefor maintain multipl hypothes simultan instead form singl hypothesi check next trial exampl particip present pictur dog pictur shoe hear nonsens word vash might hypothes vash refer dog futur trial may see pictur shoe pictur door hear word vash statist learn mechan map learn particip would like select pictur shoe door shoe would appear conjunct word vash time howev particip simpli form singl hypothesi may fail rememb context previou present vash especi experiment condit multipl trial word two present vash therefor chanc second trial accord propos mechan word learn particip correctli guess vash refer shoe first trial hypothesi would confirm subsequ trial distinguish two possibl trueswel et al conduct seri experi similar conduct yu smith except particip ask indic choic map trial singl object name present trial vari number object particip would therefor chanc forc make choic first trial result subsequ trial indic particip use statist learn mechan experi instead use mechan hold one potenti hypothesi mind time specif particip chosen incorrect map initi present nonsens word display five possibl choic likelihood choos correct map next trial word still chanc though particip chosen correct map initi present nonsens word likelihood choos correct map subsequ present word approxim result also replic condit particip choos two altern result suggest particip rememb surround context individu present therefor use statist cue determin map instead particip make hypothesi regard map next present word either confirm reject hypothesi accordingli overal result along similar result medina et indic word mean may learn statist learn mechan experi ask particip hypothes map even first occurr howev mechan compar statist learn mechan former unabl reproduc individu learn trajectori fit well latter addit statist learn account even aspect languag acquisit shown play larg role exampl kuhl tsao liu found young infant spent time laboratori session nativ mandarin speaker abl distinguish phonem occur mandarin english unlik infant control condit infant control condit came lab often infant experiment condit expos english test later date unabl distinguish mandarin phonem second experi author present infant audio audiovisu record mandarin speaker test infant abil distinguish mandarin phonem condit infant fail distinguish foreign languag phonem find indic social interact necessari compon languag learn even infant present raw data hear languag unabl take advantag statist cue present data also experienc social interact although phenomenon statist learn first discov context languag acquisit much evid role purpos work sinc origin discoveri suggest statist learn may domain gener skill like uniqu human exampl saffran johnson aslin newport found adult infant abl learn statist probabl word creat play differ music tone particip heard music note e f present togeth train abl recogn note unit test compar three note present togeth domain evid human abl learn statist visual inform whether inform present across space time evid statist learn also found primat limit statist learn abil found even like rat togeth find suggest statist learn may gener learn mechan happen util languag acquisit rather mechan uniqu human infant abil learn languag evid domain gener statist learn suggest studi run univers cornel depart psycholog concern visual statist learn infanc research studi question whether domain gener statist learn infanc would seen use visual inform first view imag statist predict pattern infant expos familiar pattern addit novel sequenc ident stimulu compon interest visual measur amount time child look stimuli research name look time age infant particip show interest novel sequenc rel familiar sequenc demonstr prefer novel sequenc violat transit probabl defin group origin stimuli result studi support likelihood domain gener statist learn infanc
Data mining,https://en.wikipedia.org/wiki/Data_mining,"Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1]
 The term ""data mining"" is a misnomer because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
 The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, although they do belong to the overall KDD process as additional steps.
 The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data. In contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[8]
 The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
 In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[9][10] Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
 The term data mining appeared around 1990 in the database community, with generally positive connotations. For a short time in 1980s, the phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego–based company, to pitch their Database Mining Workstation;[11] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in the AI and machine learning communities. However, the term data mining became more popular in the business and press communities.[12] Currently, the terms data mining and knowledge discovery are used interchangeably.
 The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s).[13] The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[14] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
 The knowledge discovery in databases (KDD) process is commonly defined with the stages:
 It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:
 or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation.
 Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[15][16][17][18]
 The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[19] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[20]
 Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.
 Data mining involves six common classes of tasks:[5]
 Data mining can unintentionally be misused, producing results that appear to be significant but which do not actually predict future behavior and cannot be reproduced on a new sample of data, therefore bearing little use. This is sometimes caused by investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[21]
 The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" e-mails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
 If the learned patterns do not meet the desired standards, it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.
 The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[22][23] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[24] and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".[25]
 Computer science conferences on data mining include:
 Data mining topics are also present in many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases.
 There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
 For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[26]
 Data mining is used wherever there is digital data available. Notable examples of data mining can be found throughout business, medicine, science, finance, construction, and surveillance.
 While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to user behavior (ethical and otherwise).[27]
 The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[28] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[29][30]
 Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[31] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[32]
 It is recommended[according to whom?] to be aware of the following before data are collected:[31]
 Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[31] However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[33]
 The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.[34]
 Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[35]
 In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[36]
 In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.""[37] This underscores the necessity for data anonymity in data aggregation and mining practices.
 U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.
 Under European copyright database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist, so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[38] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions.
Since 2020 also Switzerland has been regulating data mining by allowing it in the research field under certain conditions laid down by art. 24d of the Swiss Copyright Act. This new article entered into force on 1 April 2020.[39]
 The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[40] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[41]
 US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[42]
 The following applications are available under free/open-source licenses. Public access to application source code is also available.
 The following applications are available under proprietary licenses.
 For more information about extracting information out of data (as opposed to analyzing data), see:
",data mine process extract discov pattern larg data set involv method intersect machin learn statist databas system data mine interdisciplinari subfield comput scienc statist overal goal extract inform intellig method data set transform inform comprehens structur use data mine analysi step knowledg discoveri databas process kdd asid raw analysi step also involv databas data manag aspect data model infer consider interesting metric complex consider discov structur visual onlin updat term data mine misnom goal extract pattern knowledg larg amount data extract mine data also buzzword frequent appli form data inform process collect extract wareh analysi statist well applic comput decis support system includ artifici intellig machin learn busi intellig often gener term larg scale data analysi refer actual method artifici intellig machin appropri actual data mine task automat analysi larg quantiti data extract previous unknown interest pattern group data record cluster analysi unusu record anomali detect depend associ rule mine sequenti pattern mine usual involv use databas techniqu spatial indic pattern seen kind summari input data may use analysi exampl machin learn predict analyt exampl data mine step might identifi multipl group data use obtain accur predict result decis support system neither data collect data prepar result interpret report part data mine step although belong overal kdd process addit step differ data analysi data mine data analysi use test model hypothes dataset analyz effect market campaign regardless amount data contrast data mine use machin learn statist model uncov clandestin hidden pattern larg volum data relat term data dredg data fish data snoop refer use data mine method sampl part larger popul data set may small reliabl statist infer made valid pattern discov method howev use creat new hypothes test larger data popul statistician economist use term like data fish data dredg refer consid bad practic analyz data without hypothesi term data mine use similarli critic way economist michael lovel articl publish review econom studi lovel indic practic masquerad varieti alias rang experiment posit fish snoop neg term data mine appear around databas commun gener posit connot short time phrase databas mine use sinc trademark hnc san compani pitch databas mine workstat research consequ turn data mine term use includ data archaeolog inform harvest inform discoveri knowledg extract etc gregori coin term knowledg discoveri databas first workshop topic term becam popular ai machin learn commun howev term data mine becam popular busi press commun current term data mine knowledg discoveri use interchang manual extract pattern data occur centuri earli method identifi pattern data includ bay theorem regress analysi prolifer ubiqu increas power comput technolog dramat increas data collect storag manipul abil data set grown size complex direct data analysi increasingli augment indirect autom data process aid discoveri comput scienc special field machin learn neural network cluster analysi genet algorithm decis tree decis rule support vector machin data mine process appli method intent uncov hidden pattern larg data set bridg gap appli statist artifici intellig usual provid mathemat background databas manag exploit way data store index databas execut actual learn discoveri algorithm effici allow method appli data set knowledg discoveri databas kdd process commonli defin stage exist howev mani variat theme standard process data mine defin six phase simplifi process data mine result valid poll conduct show methodolog lead methodolog use data miner data mine standard name poll semma howev time mani peopl report use sever team research publish review data mine process model azevedo santo conduct comparison semma data mine algorithm use target data set must assembl data mine uncov pattern actual present data target data set must larg enough contain pattern remain concis enough mine within accept time limit common sourc data data mart data warehous essenti analyz multivari data set data mine target set clean data clean remov observ contain nois miss data data mine involv six common class task data mine unintent misus produc result appear signific actual predict futur behavior reproduc new sampl data therefor bear littl use sometim caus investig mani hypothes perform proper statist hypothesi test simpl version problem machin learn known overfit problem aris differ phase process thu applic suffici prevent happen final step knowledg discoveri data verifi pattern produc data mine algorithm occur wider data set pattern found algorithm necessarili valid common data mine algorithm find pattern train set present gener data set call overfit overcom evalu use test set data data mine algorithm train learn pattern appli test set result output compar desir output exampl data mine algorithm tri distinguish spam legitim would train train set sampl train learn pattern would appli test set train accuraci pattern measur mani correctli classifi sever statist method may use evalu algorithm roc curv learn pattern meet desir standard necessari chang data mine step learn pattern meet desir standard final step interpret learn pattern turn knowledg premier profession bodi field associ comput machineri acm special interest group sig knowledg discoveri data mine sigkdd sinc acm sig host annual intern confer publish proceed sinc publish biannual academ journal titl sigkdd explor comput scienc confer data mine includ data mine topic also present mani data confer icd confer sigmod confer intern confer larg data base effort defin standard data mine process exampl european cross industri standard process data mine java data mine standard jdm develop successor process jdm activ stall sinc jdm withdrawn without reach final draft exchang extract particular use predict key standard predict model markup languag pmml languag develop data mine group dmg support exchang format mani data mine applic name suggest cover predict model particular data mine task high import busi applic howev extens cover exampl subspac cluster propos independ dmg data mine use wherev digit data avail notabl exampl data mine found throughout busi medicin scienc financ construct surveil term data mine may ethic implic often associ mine inform relat user behavior ethic otherwis way data mine use case context rais question regard privaci legal ethic particular data mine govern commerci data set nation secur law enforc purpos total inform awar program advis rais privaci concern data mine requir data prepar uncov inform pattern compromis confidenti privaci oblig common way occur data aggreg data aggreg involv combin data togeth possibl variou sourc way facilit analysi also might make identif privat data deduc otherwis appar data mine per se result prepar data purpos analysi threat individu privaci come play data compil caus data miner anyon access newli compil data set abl identifi specif individu especi data origin anonym recommend accord awar follow data collect data may also modifi becom anonym individu may readili identifi howev even anonym data set potenti contain enough inform allow identif individu occur journalist abl find sever individu base set search histori inadvert releas aol inadvert revel person identifi inform lead provid violat fair inform practic indiscret caus financi emot bodili harm indic individu one instanc privaci violat patron walgreen file lawsuit compani sell prescript inform data mine compani turn provid data pharmaceut compani europ rather strong privaci law effort underway strengthen right consum howev safe harbor principl develop current effect expos european user privaci exploit compani consequ edward snowden global surveil disclosur increas discuss revok agreement particular data fulli expos nation secur agenc attempt reach agreement unit state fail unit kingdom particular case corpor use data mine way target certain group custom forc pay unfairli high price group tend peopl lower statu savvi way exploit digit market place unit state privaci concern address us congress via passag regulatori control health insur portabl account act hipaa hipaa requir individu give inform consent regard inform provid intend present futur use accord articl biotech busi week n practic hipaa may offer greater protect longstand regul research arena say aahc importantli rule goal protect inform consent approach level incomprehens averag individu underscor necess data anonym data aggreg mine practic inform privaci legisl hipaa famili educ right privaci act ferpa appli specif area law address use data mine major busi control legisl european copyright databas law mine work web mine without permiss copyright owner legal databas pure data europ may databas right may exist data mine becom subject intellectu properti owner right protect databas direct recommend hargreav review led uk govern amend copyright law allow content mine limit except uk second countri world japan introduc except data mine howev due restrict inform societi direct uk except allow content mine purpos uk copyright law also allow provis overridden contractu term condit sinc also switzerland regul data mine allow research field certain condit laid art swiss copyright act new articl enter forc april european commiss facilit stakehold discuss text data mine titl licenc europ focu solut legal issu licens rather limit except led repres univers research librari civil societi group open access publish leav stakehold dialogu may us copyright law particular provis fair use uphold legal content mine america fair use countri israel taiwan south korea content mine transform supplant origin work view law fair use exampl part googl book settlement presid judg case rule googl digit project book law part transform use digit project text data mine follow applic avail licens public access applic sourc code also avail follow applic avail proprietari licens inform extract inform data oppos analyz data see
Supervised learning,https://en.wikipedia.org/wiki/Supervised_learning,"In machine learning, supervised learning (SL) is a paradigm where a model is trained using input objects (e.g. a vector of predictor variables) and desired output values (also known as a supervisory signal), which are often human-made labels. The training process builds a function that maps new data to expected output values.[1] An optimal scenario will allow for the algorithm to accurately determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias). This statistical quality of an algorithm is measured via a generalization error.
 To solve a given problem of supervised learning, the following steps must be performed:
 A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem).
 There are four major issues to consider in supervised learning:
 A first issue is the tradeoff between bias and variance.[2] Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input 



x


{\displaystyle x}

 if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for 



x


{\displaystyle x}

. A learning algorithm has high variance for a particular input 



x


{\displaystyle x}

 if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm.[3] Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be ""flexible"" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).
 The second issue is of the amount of training data available relative to the complexity of the ""true"" function (classifier or regression function). If the true function is simple, then an ""inflexible"" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a ""flexible"" learning algorithm with low bias and high variance.
 A third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many ""extra"" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.
 A fourth issue is the degree of noise in the desired output values (the supervisory target variables). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to overfitting. You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled ""corrupts"" your training data - this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator.
 In practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance.[4][5]
 Other factors to consider when choosing and applying a learning algorithm include the following:
 When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see  cross-validation). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.
 The most widely used learning algorithms are: 
 Given a set of 



N


{\displaystyle N}

 training examples of the form 



{
(

x

1


,

y

1


)
,
.
.
.
,
(

x

N


,


y

N


)
}


{\displaystyle \{(x_{1},y_{1}),...,(x_{N},\;y_{N})\}}

 such that 




x

i




{\displaystyle x_{i}}

 is the feature vector of the 



i


{\displaystyle i}

-th example and 




y

i




{\displaystyle y_{i}}

 is its label (i.e., class), a learning algorithm seeks a function 



g
:
X
→
Y


{\displaystyle g:X\to Y}

, where 



X


{\displaystyle X}

 is the input space and 



Y


{\displaystyle Y}

 is the output space. The function 



g


{\displaystyle g}

 is an element of some space of possible functions 



G


{\displaystyle G}

, usually called the hypothesis space. It is sometimes convenient to represent 



g


{\displaystyle g}

 using a scoring function 



f
:
X
×
Y
→

R



{\displaystyle f:X\times Y\to \mathbb {R} }

 such that 



g


{\displaystyle g}

 is defined as returning the 



y


{\displaystyle y}

 value that gives the highest score: 



g
(
x
)
=



arg
⁡
max

y



f
(
x
,
y
)


{\displaystyle g(x)={\underset {y}{\arg \max }}\;f(x,y)}

. Let 



F


{\displaystyle F}

 denote the space of scoring functions.
 Although 



G


{\displaystyle G}

 and 



F


{\displaystyle F}

 can be any space of functions, many learning algorithms are probabilistic models where 



g


{\displaystyle g}

 takes the form of a conditional probability model 



g
(
x
)
=



arg
⁡
max

y



P
(
y

|

x
)


{\displaystyle g(x)={\underset {y}{\arg \max }}\;P(y|x)}

, or 



f


{\displaystyle f}

 takes the form of a joint probability model 



f
(
x
,
y
)
=
P
(
x
,
y
)


{\displaystyle f(x,y)=P(x,y)}

. For example, naive Bayes and linear discriminant analysis are joint probability models, whereas logistic regression is a conditional probability model.
 There are two basic approaches to choosing 



f


{\displaystyle f}

 or 



g


{\displaystyle g}

: empirical risk minimization and structural risk minimization.[6] Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a penalty function that controls the bias/variance tradeoff.
 In both cases, it is assumed that the training set consists of a sample of independent and identically distributed pairs, 



(

x

i


,


y

i


)


{\displaystyle (x_{i},\;y_{i})}

. In order to measure how well a function fits the training data, a loss function 



L
:
Y
×
Y
→


R


≥
0




{\displaystyle L:Y\times Y\to \mathbb {R} ^{\geq 0}}

 is defined. For training example 



(

x

i


,


y

i


)


{\displaystyle (x_{i},\;y_{i})}

, the loss of predicting the value 






y
^





{\displaystyle {\hat {y}}}

 is 



L
(

y

i


,



y
^



)


{\displaystyle L(y_{i},{\hat {y}})}

.
 The risk 



R
(
g
)


{\displaystyle R(g)}

 of function 



g


{\displaystyle g}

 is defined as the expected loss of 



g


{\displaystyle g}

. This can be estimated from the training data as
 In empirical risk minimization, the supervised learning algorithm seeks the function 



g


{\displaystyle g}

 that minimizes 



R
(
g
)


{\displaystyle R(g)}

. Hence, a supervised learning algorithm can be constructed by applying an optimization algorithm to find 



g


{\displaystyle g}

.
 When 



g


{\displaystyle g}

 is a conditional probability distribution 



P
(
y

|

x
)


{\displaystyle P(y|x)}

 and the loss function is the negative log likelihood: 



L
(
y
,



y
^



)
=
−
log
⁡
P
(
y

|

x
)


{\displaystyle L(y,{\hat {y}})=-\log P(y|x)}

, then empirical risk minimization is equivalent to maximum likelihood estimation.
 When 



G


{\displaystyle G}

 contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well (overfitting).
 Structural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization. The regularization penalty can be viewed as implementing a form of Occam's razor that prefers simpler functions over more complex ones.
 A wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function 



g


{\displaystyle g}

 is a linear function of the form
 A popular regularization penalty is 




∑

j



β

j


2




{\displaystyle \sum _{j}\beta _{j}^{2}}

, which is the squared Euclidean norm of the weights, also known as the 




L

2




{\displaystyle L_{2}}

 norm. Other norms include the 




L

1




{\displaystyle L_{1}}

 norm, 




∑

j



|


β

j



|



{\displaystyle \sum _{j}|\beta _{j}|}

, and the 




L

0




{\displaystyle L_{0}}

 ""norm"", which is the number of non-zero 




β

j




{\displaystyle \beta _{j}}

s. The penalty will be denoted by 



C
(
g
)


{\displaystyle C(g)}

.
 The supervised learning optimization problem is to find the function 



g


{\displaystyle g}

 that minimizes
 The parameter 



λ


{\displaystyle \lambda }

 controls the bias-variance tradeoff. When 



λ
=
0


{\displaystyle \lambda =0}

, this gives empirical risk minimization with low bias and high variance. When 



λ


{\displaystyle \lambda }

 is large, the learning algorithm will have high bias and low variance. The value of 



λ


{\displaystyle \lambda }

 can be chosen empirically via  cross-validation.
 The complexity penalty has a Bayesian interpretation as the negative log prior probability of 



g


{\displaystyle g}

, 



−
log
⁡
P
(
g
)


{\displaystyle -\log P(g)}

, in which case 



J
(
g
)


{\displaystyle J(g)}

 is the posterior probability of 



g


{\displaystyle g}

.
 The training methods described above are discriminative training methods, because they seek to find a function 



g


{\displaystyle g}

 that discriminates well between the different output values (see discriminative model). For the special case where 



f
(
x
,
y
)
=
P
(
x
,
y
)


{\displaystyle f(x,y)=P(x,y)}

 is a joint probability distribution and the loss function is the negative log likelihood 



−

∑

i


log
⁡
P
(

x

i


,

y

i


)
,


{\displaystyle -\sum _{i}\log P(x_{i},y_{i}),}

 a risk minimization algorithm is said to perform generative training, because 



f


{\displaystyle f}

 can be regarded as a generative model that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in naive Bayes and linear discriminant analysis.
 There are several ways in which the standard supervised learning problem can be generalized:
",machin learn supervis learn sl paradigm model train use input object vector predictor variabl desir output valu also known supervisori signal often label train process build function map new data expect output valu optim scenario allow algorithm accur determin output valu unseen instanc requir learn algorithm gener train data unseen situat reason way see induct bia statist qualiti algorithm measur via gener error solv given problem supervis learn follow step must perform wide rang supervis learn algorithm avail strength weak singl learn algorithm work best supervis learn problem see free lunch theorem four major issu consid supervis learn first issu tradeoff bia varianc imagin avail sever differ equal good train data set learn algorithm bias particular input x x train data set systemat incorrect predict correct output x x learn algorithm high varianc particular input x x predict differ output valu train differ train set predict error learn classifi relat sum bia varianc learn algorithm gener tradeoff bia varianc learn algorithm low bia must flexibl fit data well learn algorithm flexibl fit train data set differ henc high varianc key aspect mani supervis learn method abl adjust tradeoff bia varianc either automat provid paramet user adjust second issu amount train data avail rel complex true function classifi regress function true function simpl inflex learn algorithm high bia low varianc abl learn small amount data true function highli complex involv complex interact among mani differ input featur behav differ differ part input space function abl learn larg amount train data pair flexibl learn algorithm low bia high varianc third issu dimension input space input featur vector larg dimens learn function difficult even true function depend small number featur mani extra dimens confus learn algorithm caus high varianc henc input data larg dimens typic requir tune classifi low varianc high bia practic engin manual remov irrelev featur input data like improv accuraci learn function addit mani algorithm featur select seek identifi relev featur discard irrelev one instanc gener strategi dimension reduct seek map input data space prior run supervis learn algorithm fourth issu degre nois desir output valu supervisori target variabl desir output valu often incorrect human error sensor error learn algorithm attempt find function exactli match train exampl attempt fit data care lead overfit overfit even measur error stochast nois function tri learn complex learn model situat part target function model corrupt train data phenomenon call determinist nois either type nois present better go higher bia lower varianc estim practic sever approach allevi nois output valu earli stop prevent overfit well detect remov noisi train exampl prior train supervis learn algorithm sever algorithm identifi noisi train exampl remov suspect noisi train exampl prior train decreas gener error statist signific factor consid choos appli learn algorithm includ follow consid new applic engin compar multipl learn algorithm experiment determin one work best problem hand see tune perform learn algorithm given fix resourc often better spend time collect addit train data inform featur spend extra time tune learn algorithm wide use learn algorithm given set n n train exampl form x x n n n n x featur vector exampl label class learn algorithm seek function g x g x x input space output space function g g element space possibl function g g usual call hypothesi space sometim conveni repres g g use score function f x r f r g g defin return valu give highest score g x arg max f x g x f x let f f denot space score function although g g f f space function mani learn algorithm probabilist model g g take form condit probabl model g x arg max p x g x p f f take form joint probabl model f x p x f x x exampl naiv bay linear discrimin analysi joint probabl model wherea logist regress condit probabl model two basic approach choos f f g g empir risk minim structur risk minim empir risk minim seek function best fit train data structur risk minim includ penalti function control tradeoff case assum train set consist sampl independ ident distribut pair x order measur well function fit train data loss function l r l r defin train exampl x loss predict valu l l risk r g r g function g g defin expect loss g g estim train data empir risk minim supervis learn algorithm seek function g g minim r g r g henc supervis learn algorithm construct appli optim algorithm find g g g g condit probabl distribut p x p loss function neg log likelihood l log p x l p empir risk minim equival maximum likelihood estim g g contain mani candid function train set suffici larg empir risk minim lead high varianc poor gener learn algorithm abl memor train exampl without gener well overfit structur risk minim seek prevent overfit incorpor regular penalti optim regular penalti view implement form occam razor prefer simpler function complex one wide varieti penalti employ correspond differ definit complex exampl consid case function g g linear function form popular regular penalti j β j j j squar euclidean norm weight also known l norm norm includ l norm j β j j j l norm number β j j penalti denot c g c g supervis learn optim problem find function g g minim paramet λ control tradeoff λ give empir risk minim low bia high varianc λ larg learn algorithm high bia low varianc valu λ chosen empir via complex penalti bayesian interpret neg log prior probabl g g log p g p g case j g j g posterior probabl g g train method describ discrimin train method seek find function g g discrimin well differ output valu see discrimin model special case f x p x f x x joint probabl distribut loss function neg log likelihood log p x p risk minim algorithm said perform gener train f f regard gener model explain data gener gener train algorithm often simpler comput effici discrimin train algorithm case solut comput close form naiv bay linear discrimin analysi sever way standard supervis learn problem gener
Unsupervised learning,https://en.wikipedia.org/wiki/Unsupervised_learning,"Unsupervised learning is a framework in machine learning where, in contrast to supervised learning, algorithms learn patterns exclusively from unlabeled data.[1] Other frameworks in the spectrum of supervisions include weak- or semi-supervision, where a small portion of the data is tagged, and self-supervision. Some researchers consider self-supervised learning a form of unsupervised learning.[2]
 Conceptually, unsupervised learning divides into the aspects of data, training, algorithm, and downstream applications. Typically, the dataset is harvested cheaply ""in the wild"", such as massive text corpus obtained by web crawling, with only minor filtering (such as Common Crawl). This compares favorably to supervised learning, where the dataset (such as the ImageNet1000) is typically constructed manually, which is much more expensive.
 There were algorithms designed specifically for unsupervised learning, such as clustering algorithms like k-means, dimensionality reduction techniques like principal component analysis (PCA), Boltzmann machine learning, and autoencoders. After the rise of deep learning, most large-scale unsupervised learning have been done by training general-purpose neural network architectures by gradient descent, adapted to performing unsupervised learning by designing an appropriate training procedure.
 Sometimes a trained model can be used as-is, but more often they are modified for downstream applications. For example, the generative pretraining method trains a model to generate a textual dataset, before finetuning it for other applications, such as text classification.[3][4] As another example, autoencoders are trained to good features, which can then be used as a module for other models, such as in a latent diffusion model.
 Tasks are often categorized as discriminative (recognition) or generative (imagination).  Often but not always, discriminative tasks use supervised methods and generative tasks use unsupervised (see Venn diagram); however, the separation is very hazy.  For example, object recognition favors supervised learning but unsupervised learning can also cluster objects into groups.  Furthermore, as progress marches onward some tasks employ both methods, and some tasks swing from one to another.  For example, image recognition started off as heavily supervised, but became hybrid by employing unsupervised pre-training, and then moved towards supervision again with the advent of dropout, ReLU, and adaptive learning rates.
 A typical generative task is as follows. At each step, a datapoint is sampled from the dataset, and part of the data is removed, and the model must infer the removed part. This is particularly clear for the denoising autoencoders and BERT.
 During the learning phase, an unsupervised network tries to mimic the data it's given and uses the error in its mimicked output to correct itself (i.e. correct its weights and biases). Sometimes the error is expressed as a low probability that the erroneous output occurs, or it might be expressed as an unstable high energy state in the network.
 In contrast to supervised methods' dominant use of backpropagation, unsupervised learning also employs other methods  including: Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations. See the table below for more details.
 An energy function is a macroscopic measure of a network's activation state.  In Boltzmann machines, it plays the role of the Cost function.  This analogy with physics is inspired by Ludwig Boltzmann's analysis of a gas' macroscopic energy from the microscopic probabilities of particle motion 



p
∝

e

−
E

/

k
T




{\displaystyle p\propto e^{-E/kT}}

, where k is the Boltzmann constant and T is temperature. In the RBM network the relation is 



p
=

e

−
E



/

Z


{\displaystyle p=e^{-E}/Z}

,[5] where 



p


{\displaystyle p}

 and 



E


{\displaystyle E}

 vary over every possible activation pattern and 





Z
=

∑



All Patterns





e

−
E
(

pattern

)






{\displaystyle \textstyle {Z=\sum _{\scriptscriptstyle {\text{All Patterns}}}e^{-E({\text{pattern}})}}}

. To be more precise, 



p
(
a
)
=

e

−
E
(
a
)



/

Z


{\displaystyle p(a)=e^{-E(a)}/Z}

, where 



a


{\displaystyle a}

 is an activation pattern of all neurons (visible and hidden). Hence, some early neural networks bear the name Boltzmann Machine.  Paul Smolensky calls 



−
E



{\displaystyle -E\,}

 the Harmony. A network seeks low energy which is high Harmony.
 This table shows connection diagrams of various unsupervised networks, the details of which will be given in the section Comparison of Networks.  Circles are neurons and edges between them are connection weights.  As network design changes, features are added on to enable new capabilities or removed to make learning faster.  For instance, neurons change between deterministic (Hopfield) and stochastic (Boltzmann) to allow robust output, weights are removed within a layer (RBM) to hasten learning, or connections are allowed to become asymmetric (Helmholtz).
 Of the networks bearing people's names, only Hopfield worked directly with neural networks.  Boltzmann and Helmholtz came before artificial neural networks, but their work in physics and physiology inspired the analytical methods that were used.
 Here, we highlight some characteristics of select networks.  The details of each are given in the comparison table below. 
 The classical example of unsupervised learning in the study of neural networks is Donald Hebb's principle, that is, neurons that fire together wire together.[8] In Hebbian learning, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons.[9] A similar version that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.
 Among neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used in unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing.[10]
 Two of the main methods used in unsupervised learning are principal component and cluster analysis. Cluster analysis is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships.[11] Cluster analysis is a branch of machine learning that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. This approach helps detect anomalous data points that do not fit into either group.
 A central application of unsupervised learning is in the field of density estimation in statistics,[12] though unsupervised learning encompasses many other domains involving summarizing and explaining data features. It can be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution  conditioned on the label  of input data; unsupervised learning intends to infer an a priori probability distribution .
 Some of the most common algorithms used in unsupervised learning include: (1) Clustering, (2) Anomaly detection, (3) Approaches for learning latent variable models. Each approach uses several methods as follows:
 One of the statistical approaches for unsupervised learning is the method of moments. In the method of moments, the unknown parameters (of interest) in the model are related to the moments of one or more random variables, and thus, these unknown parameters can be estimated given the moments. The moments are usually estimated from samples empirically. The basic moments are first and second order moments. For a random vector, the first order moment is the mean vector, and the second order moment is the covariance matrix (when the mean is zero). Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multi-dimensional arrays.
 In particular, the method of moments is shown to be effective in learning the parameters of latent variable models. Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. A highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words (observed variables) in the document based on the topic (latent variable) of the document. In the topic modeling, the words in the document are generated according to different statistical parameters when the topic of the document is changed. It is shown that method of moments (tensor decomposition techniques) consistently recover the parameters of a large class of latent variable models under some assumptions.[15]
 The Expectation–maximization algorithm (EM) is also one of the most practical methods for learning latent variable models. However, it can get stuck in local optima, and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model. In contrast, for the method of moments, the global convergence is guaranteed under some conditions.
",unsupervis learn framework machin learn contrast supervis learn algorithm learn pattern exclus unlabel data framework spectrum supervis includ small portion data tag research consid learn form unsupervis learn conceptu unsupervis learn divid aspect data train algorithm downstream applic typic dataset harvest cheapli wild massiv text corpu obtain web crawl minor filter common crawl compar favor supervis learn dataset typic construct manual much expens algorithm design specif unsupervis learn cluster algorithm like dimension reduct techniqu like princip compon analysi pca boltzmann machin learn autoencod rise deep learn unsupervis learn done train neural network architectur gradient descent adapt perform unsupervis learn design appropri train procedur sometim train model use often modifi downstream applic exampl gener pretrain method train model gener textual dataset finetun applic text classif anoth exampl autoencod train good featur use modul model latent diffus model task often categor discrimin recognit gener imagin often alway discrimin task use supervis method gener task use unsupervis see venn diagram howev separ hazi exampl object recognit favor supervis learn unsupervis learn also cluster object group furthermor progress march onward task employ method task swing one anoth exampl imag recognit start heavili supervis becam hybrid employ unsupervis move toward supervis advent dropout relu adapt learn rate typic gener task follow step datapoint sampl dataset part data remov model must infer remov part particularli clear denois autoencod bert learn phase unsupervis network tri mimic data given use error mimick output correct correct weight bias sometim error express low probabl erron output occur might express unstabl high energi state network contrast supervis method domin use backpropag unsupervis learn also employ method includ hopfield learn rule boltzmann learn rule contrast diverg wake sleep variat infer maximum likelihood maximum posteriori gibb sampl backpropag reconstruct error hidden state reparameter see tabl detail energi function macroscop measur network activ state boltzmann machin play role cost function analog physic inspir ludwig boltzmann analysi ga macroscop energi microscop probabl particl motion p e e k k boltzmann constant temperatur rbm network relat p e e z p p e e vari everi possibl activ pattern z pattern e e pattern pattern pattern precis p e e z p activ pattern neuron visibl hidden henc earli neural network bear name boltzmann machin paul smolenski call e harmoni network seek low energi high harmoni tabl show connect diagram variou unsupervis network detail given section comparison network circl neuron edg connect weight network design chang featur ad enabl new capabl remov make learn faster instanc neuron chang determinist hopfield stochast boltzmann allow robust output weight remov within layer rbm hasten learn connect allow becom asymmetr helmholtz network bear peopl name hopfield work directli neural network boltzmann helmholtz came artifici neural network work physic physiolog inspir analyt method use highlight characterist select network detail given comparison tabl classic exampl unsupervis learn studi neural network donald hebb principl neuron fire togeth wire togeth hebbian learn connect reinforc irrespect error exclus function coincid action potenti two neuron similar version modifi synapt weight take account time action potenti plastic stdp hebbian learn hypothes underli rang cognit function pattern recognit experienti learn among neural network model map som adapt reson theori art commonli use unsupervis learn algorithm som topograph organ nearbi locat map repres input similar properti art model allow number cluster vari problem size let user control degre similar member cluster mean constant call vigil paramet art network use mani pattern recognit task automat target recognit seismic signal process two main method use unsupervis learn princip compon cluster analysi cluster analysi use unsupervis learn group segment dataset share attribut order extrapol algorithm relationship cluster analysi branch machin learn group data label classifi categor instead respond feedback cluster analysi identifi common data react base presenc absenc common new piec data approach help detect anomal data point fit either group central applic unsupervis learn field densiti estim statist though unsupervis learn encompass mani domain involv summar explain data featur contrast supervis learn say wherea supervis learn intend infer condit probabl distribut condit label input data unsupervis learn intend infer priori probabl distribut common algorithm use unsupervis learn includ cluster anomali detect approach learn latent variabl model approach use sever method follow one statist approach unsupervis learn method moment method moment unknown paramet interest model relat moment one random variabl thu unknown paramet estim given moment moment usual estim sampl empir basic moment first second order moment random vector first order moment mean vector second order moment covari matrix mean zero higher order moment usual repres use tensor gener matric higher order array particular method moment shown effect learn paramet latent variabl model latent variabl model statist model addit observ variabl set latent variabl also exist observ highli practic exampl latent variabl model machin learn topic model statist model gener word observ variabl document base topic latent variabl document topic model word document gener accord differ statist paramet topic document chang shown method moment tensor decomposit techniqu consist recov paramet larg class latent variabl model assumpt algorithm em also one practic method learn latent variabl model howev get stuck local optima guarante algorithm converg true unknown paramet model contrast method moment global converg guarante condit
Weak supervision,https://en.wikipedia.org/wiki/Semi-supervised_learning,"Weak supervision (also known as semi-supervised learning) is a paradigm in machine learning, the relevance and notability of which increased with the advent of large language models due to large amount of data required to train them.  It is characterized by using a combination of a small amount of human-labeled data (exclusively used in more expensive and time-consuming supervised learning paradigm), followed by a large amount of unlabeled data (used exclusively in unsupervised learning paradigm). In other words, the desired output values are provided only for a subset of the training data. The remaining data is unlabeled or imprecisely labeled. Intuitively, it can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam. Technically, it could be viewed as performing clustering and then labeling the clusters with the labeled data, pushing the decision boundary away from high-density regions, or learning an underlying one-dimensional manifold where the data reside.
 The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render large, fully labeled training sets infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.
 More formally, semi-supervised learning assumes a set of 



l


{\displaystyle l}

 independently identically distributed examples 




x

1


,
…
,

x

l


∈
X


{\displaystyle x_{1},\dots ,x_{l}\in X}

 with corresponding labels 




y

1


,
…
,

y

l


∈
Y


{\displaystyle y_{1},\dots ,y_{l}\in Y}

 and 



u


{\displaystyle u}

 unlabeled examples 




x

l
+
1


,
…
,

x

l
+
u


∈
X


{\displaystyle x_{l+1},\dots ,x_{l+u}\in X}

 are processed. Semi-supervised learning combines this information to surpass the classification performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning.
 Semi-supervised learning may refer to either transductive learning or inductive learning.[1] The goal of transductive learning is to infer the correct labels for the given unlabeled data 




x

l
+
1


,
…
,

x

l
+
u




{\displaystyle x_{l+1},\dots ,x_{l+u}}

 only. The goal of inductive learning is to infer the correct mapping from 



X


{\displaystyle X}

 to 



Y


{\displaystyle Y}

.
 It is unnecessary (and, according to Vapnik's principle, imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.
 In order to make any use of unlabeled data, some relationship to the underlying distribution of data must exist. Semi-supervised learning algorithms make use of at least one of the following assumptions:[2]
 Points that are close to each other are more likely to share a label. This is also generally assumed in supervised learning and yields a preference for geometrically simple decision boundaries. In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so few points are close to each other but in different classes.[3]
 The data tend to form discrete clusters, and points in the same cluster are more likely to share a label (although data that shares a label may spread across multiple clusters). This is a special case of the smoothness assumption and gives rise to feature learning with clustering algorithms.
 The data lie approximately on a manifold of much lower dimension than the input space. In this case learning the manifold using both the labeled and unlabeled data can avoid the curse of dimensionality. Then learning can proceed using distances and densities defined on the manifold.
 The manifold assumption is practical when high-dimensional data are generated by some process that may be hard to model directly, but which has only a few degrees of freedom. For instance, human voice is controlled by a few vocal folds,[4] and images of various facial expressions are controlled by a few muscles. In these cases, it is better to consider distances and smoothness in the natural space of the generating problem, rather than in the space of all possible acoustic waves or images, respectively.
 The heuristic approach of self-training (also known as self-learning or self-labeling) is historically the oldest approach to semi-supervised learning,[2] with examples of applications starting in the 1960s.[5]
 The transductive learning framework was formally introduced by Vladimir Vapnik in the 1970s.[6] Interest in inductive learning using generative models also began in the 1970s. A probably approximately correct learning bound for semi-supervised learning of a Gaussian mixture was demonstrated by Ratsaby and Venkatesh in 1995.[7]
 Generative approaches to statistical learning first seek to estimate 



p
(
x

|

y
)


{\displaystyle p(x|y)}

,[disputed – discuss] the distribution of data points belonging to each class. The probability 



p
(
y

|

x
)


{\displaystyle p(y|x)}

 that a given point 



x


{\displaystyle x}

 has label 



y


{\displaystyle y}

 is then proportional to 



p
(
x

|

y
)
p
(
y
)


{\displaystyle p(x|y)p(y)}

 by Bayes' rule. Semi-supervised learning with generative models can be viewed either as an extension of supervised learning (classification plus information about 



p
(
x
)


{\displaystyle p(x)}

) or as an extension of unsupervised learning (clustering plus some labels).
 Generative models assume that the distributions take some particular form 



p
(
x

|

y
,
θ
)


{\displaystyle p(x|y,\theta )}

 parameterized by the vector 



θ


{\displaystyle \theta }

. If these assumptions are incorrect, the unlabeled data may actually decrease the accuracy of the solution relative to what would have been obtained from labeled data alone.[8] 
However, if the assumptions are correct, then the unlabeled data necessarily improves performance.[7]
 The unlabeled data are distributed according to a mixture of individual-class distributions. In order to learn the mixture distribution from the unlabeled data, it must be identifiable, that is, different parameters must yield different summed distributions. Gaussian mixture distributions are identifiable and commonly used for generative models.
 The parameterized joint distribution can be written as 



p
(
x
,
y

|

θ
)
=
p
(
y

|

θ
)
p
(
x

|

y
,
θ
)


{\displaystyle p(x,y|\theta )=p(y|\theta )p(x|y,\theta )}

 by using the chain rule. Each parameter vector 



θ


{\displaystyle \theta }

 is associated with a decision function 




f

θ


(
x
)
=


argmax
y


 
p
(
y

|

x
,
θ
)


{\displaystyle f_{\theta }(x)={\underset {y}{\operatorname {argmax} }}\ p(y|x,\theta )}

. 
The parameter is then chosen based on fit to both the labeled and unlabeled data, weighted by 



λ


{\displaystyle \lambda }

:
 Another major class of methods attempts to place boundaries in regions with few data points (labeled or unlabeled). One of the most commonly used algorithms is the transductive support vector machine, or TSVM (which, despite its name, may be used for inductive learning as well). Whereas support vector machines for supervised learning seek a decision boundary with maximal margin over the labeled data, the goal of TSVM is a labeling of the unlabeled data such that the decision boundary has maximal margin over all of the data. In addition to the standard hinge loss 



(
1
−
y
f
(
x
)

)

+




{\displaystyle (1-yf(x))_{+}}

 for labeled data, a loss function 



(
1
−

|

f
(
x
)

|


)

+




{\displaystyle (1-|f(x)|)_{+}}

 is introduced over the unlabeled data by letting 



y
=
sign
⁡

f
(
x
)



{\displaystyle y=\operatorname {sign} {f(x)}}

. TSVM then selects 




f

∗


(
x
)
=

h

∗


(
x
)
+
b


{\displaystyle f^{*}(x)=h^{*}(x)+b}

 from a reproducing kernel Hilbert space 





H




{\displaystyle {\mathcal {H}}}

 by minimizing the regularized empirical risk:
 An exact solution is intractable due to the non-convex term 



(
1
−

|

f
(
x
)

|


)

+




{\displaystyle (1-|f(x)|)_{+}}

, so research focuses on useful approximations.[9]
 Other approaches that implement low-density separation include Gaussian process models, information regularization, and entropy minimization (of which TSVM is a special case).
 Laplacian regularization has been historically approached through graph-Laplacian.
Graph-based methods for semi-supervised learning use a graph representation of the data, with a node for each labeled and unlabeled example. The graph may be constructed using domain knowledge or similarity of examples; two common methods are to connect each data point to its 



k


{\displaystyle k}

 nearest neighbors or to examples within some distance 



ϵ


{\displaystyle \epsilon }

. The weight 




W

i
j




{\displaystyle W_{ij}}

 of an edge between 




x

i




{\displaystyle x_{i}}

 and 




x

j




{\displaystyle x_{j}}

 is then set to 




e

−
‖

x

i


−

x

j



‖

2



/


ϵ

2






{\displaystyle e^{-\|x_{i}-x_{j}\|^{2}/\epsilon ^{2}}}

.
 Within the framework of manifold regularization,[10][11] the graph serves as a proxy for the manifold. A term is added to the standard Tikhonov regularization problem to enforce smoothness of the solution relative to the manifold (in the intrinsic space of the problem) as well as relative to the ambient input space. The minimization problem becomes
 where 





H




{\displaystyle {\mathcal {H}}}

 is a reproducing kernel Hilbert space and 





M




{\displaystyle {\mathcal {M}}}

 is the manifold on which the data lie. The regularization parameters 




λ

A




{\displaystyle \lambda _{A}}

 and 




λ

I




{\displaystyle \lambda _{I}}

 control smoothness in the ambient and intrinsic spaces respectively. The graph is used to approximate the intrinsic regularization term. Defining the graph Laplacian 



L
=
D
−
W


{\displaystyle L=D-W}

 where 




D

i
i


=

∑

j
=
1


l
+
u



W

i
j




{\displaystyle D_{ii}=\sum _{j=1}^{l+u}W_{ij}}

 and 




f



{\displaystyle \mathbf {f} }

 is the vector 



[
f
(

x

1


)
…
f
(

x

l
+
u


)
]


{\displaystyle [f(x_{1})\dots f(x_{l+u})]}

, we have
 The graph-based approach to Laplacian regularization is to put in relation with finite difference method.[clarification needed][citation needed]
 The Laplacian can also be used to extend the supervised learning algorithms: regularized least squares and support vector machines (SVM) to semi-supervised versions Laplacian regularized least squares and Laplacian SVM.
 Some methods for semi-supervised learning are not intrinsically geared to learning from both unlabeled and labeled data, but instead make use of unlabeled data within a supervised learning framework. For instance, the labeled and unlabeled examples 




x

1


,
…
,

x

l
+
u




{\displaystyle x_{1},\dots ,x_{l+u}}

 may inform a choice of representation, distance metric, or kernel for the data in an unsupervised first step. Then supervised learning proceeds from only the labeled examples. In this vein, some methods learn a low-dimensional representation using the supervised data and then apply either low-density separation or graph-based methods to the learned representation.[12][13] Iteratively refining the representation and then performing semi-supervised learning on said representation may further improve performance.
 Self-training is a wrapper method for semi-supervised learning.[14] First a supervised learning algorithm is trained based on the labeled data only. This classifier is then applied to the unlabeled data to generate more labeled examples as input for the supervised learning algorithm. Generally only the labels the classifier is most confident in are added at each step.[15] In natural language processing, a common self-training algorithm is the Yarowsky algorithm for problems like word sense disambiguation, accent restoration, and spelling correction.[16]
 Co-training is an extension of self-training in which multiple classifiers are trained on different (ideally disjoint) sets of features and generate labeled examples for one another.[17]
 Human responses to formal semi-supervised learning problems have yielded varying conclusions about the degree of influence of the unlabeled data.[18] More natural learning problems may also be viewed as instances of semi-supervised learning. Much of human concept learning involves a small amount of direct instruction (e.g. parental labeling of objects during childhood) combined with large amounts of unlabeled experience (e.g. observation of objects without naming or counting them, or at least without feedback).
 Human infants are sensitive to the structure of unlabeled natural categories such as images of dogs and cats or male and female faces.[19] Infants and children take into account not only unlabeled examples, but the sampling process from which labeled examples arise.[20][21]
",weak supervis also known learn paradigm machin learn relev notabl increas advent larg languag model due larg amount data requir train character use combin small amount data exclus use expens supervis learn paradigm follow larg amount unlabel data use exclus unsupervis learn paradigm word desir output valu provid subset train data remain data unlabel imprecis label intuit seen exam label data sampl problem teacher solv class aid solv anoth set problem transduct set unsolv problem act exam question induct set becom practic problem sort make exam technic could view perform cluster label cluster label data push decis boundari away region learn underli manifold data resid acquisit label data learn problem often requir skill human agent transcrib audio segment physic experi determin structur protein determin whether oil particular locat cost associ label process thu may render larg fulli label train set infeas wherea acquisit unlabel data rel inexpens situat learn great practic valu learn also theoret interest machin learn model human learn formal learn assum set l l independ ident distribut exampl x x l x l x correspond label l l u u unlabel exampl x l x l u x x process learn combin inform surpass classif perform obtain either discard unlabel data supervis learn discard label unsupervis learn learn may refer either transduct learn induct learn goal transduct learn infer correct label given unlabel data x l x l u goal induct learn infer correct map x x unnecessari accord vapnik principl imprud perform transduct learn way infer classif rule entir input space howev practic algorithm formal design transduct induct often use interchang order make use unlabel data relationship underli distribut data must exist learn algorithm make use least one follow assumpt point close like share label also gener assum supervis learn yield prefer geometr simpl decis boundari case learn smooth assumpt addit yield prefer decis boundari region point close differ class data tend form discret cluster point cluster like share label although data share label may spread across multipl cluster special case smooth assumpt give rise featur learn cluster algorithm data lie approxim manifold much lower dimens input space case learn manifold use label unlabel data avoid curs dimension learn proceed use distanc densiti defin manifold manifold assumpt practic data gener process may hard model directli degre freedom instanc human voic control vocal fold imag variou facial express control muscl case better consid distanc smooth natur space gener problem rather space possibl acoust wave imag respect heurist approach also known histor oldest approach learn exampl applic start transduct learn framework formal introduc vladimir vapnik interest induct learn use gener model also began probabl approxim correct learn bound learn gaussian mixtur demonstr ratsabi venkatesh gener approach statist learn first seek estim p x p disput discuss distribut data point belong class probabl p x p given point x x label proport p x p p p bay rule learn gener model view either extens supervis learn classif plu inform p x p x extens unsupervis learn cluster plu label gener model assum distribut take particular form p x θ p parameter vector θ assumpt incorrect unlabel data may actual decreas accuraci solut rel would obtain label data alon howev assumpt correct unlabel data necessarili improv perform unlabel data distribut accord mixtur distribut order learn mixtur distribut unlabel data must identifi differ paramet must yield differ sum distribut gaussian mixtur distribut identifi commonli use gener model parameter joint distribut written p x θ p θ p x θ p x p use chain rule paramet vector θ associ decis function f θ x argmax p x θ x argmax p paramet chosen base fit label unlabel data weight λ anoth major class method attempt place boundari region data point label unlabel one commonli use algorithm transduct support vector machin tsvm despit name may use induct learn well wherea support vector machin supervis learn seek decis boundari maxim margin label data goal tsvm label unlabel data decis boundari maxim margin data addit standard hing loss f x x label data loss function f x x introduc unlabel data let sign f x sign f x tsvm select f x h x b x x reproduc kernel hilbert space h h minim regular empir risk exact solut intract due term f x x research focus use approxim approach implement separ includ gaussian process model inform regular entropi minim tsvm special case laplacian regular histor approach method learn use graph represent data node label unlabel exampl graph may construct use domain knowledg similar exampl two common method connect data point k k nearest neighbor exampl within distanc ϵ weight w j ij edg x x j j set e x x j ϵ j within framework manifold regular graph serv proxi manifold term ad standard tikhonov regular problem enforc smooth solut rel manifold intrins space problem well rel ambient input space minim problem becom h h reproduc kernel hilbert space manifold data lie regular paramet λ λ control smooth ambient intrins space respect graph use approxim intrins regular term defin graph laplacian l w j l u w j ii ij f f vector f x f x l u f f approach laplacian regular put relat finit differ method clarif need citat need laplacian also use extend supervis learn algorithm regular least squar support vector machin svm version laplacian regular least squar laplacian svm method learn intrins gear learn unlabel label data instead make use unlabel data within supervis learn framework instanc label unlabel exampl x x l u may inform choic represent distanc metric kernel data unsupervis first step supervis learn proce label exampl vein method learn represent use supervis data appli either separ method learn represent iter refin represent perform learn said represent may improv perform wrapper method learn first supervis learn algorithm train base label data classifi appli unlabel data gener label exampl input supervis learn algorithm gener label classifi confid ad step natur languag process common algorithm yarowski algorithm problem like word sens disambigu accent restor spell correct extens multipl classifi train differ ideal disjoint set featur gener label exampl one anoth human respons formal learn problem yield vari conclus degre influenc unlabel data natur learn problem may also view instanc learn much human concept learn involv small amount direct instruct parent label object childhood combin larg amount unlabel experi observ object without name count least without feedback human infant sensit structur unlabel natur categori imag dog cat male femal face infant children take account unlabel exampl sampl process label exampl aris
Self-supervised learning,https://en.wikipedia.org/wiki/Self-supervised_learning,"

 Self-supervised learning (SSL) is a paradigm in machine learning where a model is trained on a task using the data itself to generate supervisory signals, rather than relying on externally-provided labels. In the context of neural networks, self-supervised learning aims to leverage inherent structures or relationships within the input data to create meaningful training signals. SSL tasks are designed so that solving them requires capturing essential features or relationships in the data. The input data is typically augmented or transformed in a way that creates pairs of related samples, where one sample serves as the input, and the other is used to formulate the supervisory signal. This augmentation can involve introducing noise, cropping, rotation, or other transformations. Self-supervised learning more closely imitates the way humans learn to classify objects.[1]
 During SSL, the model learns in two steps. First, the task is solved based on an auxiliary or pretext classification task using pseudo-labels, which help to initialize the model parameters.[2][3] Next, the actual task is performed with supervised or unsupervised learning.[4][5][6]
 Self-supervised learning has produced promising results in recent years, and has found practical application in fields such as audio processing, and is being used by Facebook and others for speech recognition.[7]
 Autoassociative self-supervised learning is a specific category of self-supervised learning where a neural network is trained to reproduce or reconstruct its own input data.[8] In other words, the model is tasked with learning a representation of the data that captures its essential features or structure, allowing it to regenerate the original input.
 The term ""autoassociative"" comes from the fact that the model is essentially associating the input data with itself. This is often achieved using autoencoders, which are a type of neural network architecture used for representation learning. Autoencoders consist of an encoder network that maps the input data to a lower-dimensional representation (latent space), and a decoder network that reconstructs the input from this representation. 
 The training process involves presenting the model with input data and requiring it to reconstruct the same data as closely as possible. The loss function used during training typically penalizes the difference between the original input and the reconstructed output (e.g. mean squared error). By minimizing this reconstruction error, the autoencoder learns a meaningful representation of the data in its latent space.
 For a binary classification task, training data can be divided into positive examples and negative examples. Positive examples are those that match the target. For example, if training a classifier to identify birds, the positive training data would include images that contain birds. Negative examples would be images that do not.[9] Contrastive self-supervised learning uses both positive and negative examples. The loss function in contrastive learning is used to minimize the distance between positive sample pairs, while maximizing the distance between negative sample pairs.[9]
 An early example uses a pair of 1-dimensional convolutional neural networks to process a pair of images and maximize their agreement.[10]
 Contrastive Language-Image Pre-training (CLIP) allows joint pretraining of a text encoder and an image encoder, such that a matching image-text pair have image encoding vector and text encoding vector that span a small angle (having a large cosine similarity).
 InfoNCE (Noise-Contrastive Estimation)[11] is a method to optimize two models jointly, based on Noise Contrastive Estimation (NCE).[12] Given a set 



X
=

{


x

1


,
…

x

N



}



{\displaystyle X=\left\{x_{1},\ldots x_{N}\right\}}

 of 



N


{\displaystyle N}

 random samples containing one positive sample from 



p

(


x

t
+
k


∣

c

t



)



{\displaystyle p\left(x_{t+k}\mid c_{t}\right)}

 and 



N
−
1


{\displaystyle N-1}

 negative samples from the 'proposal' distribution 



p

(

x

t
+
k


)



{\displaystyle p\left(x_{t+k}\right)}

, it minimizes the following loss function:
 






L




N



=
−


E


X



[

log
⁡




f

k



(


x

t
+
k


,

c

t



)




∑


x

j


∈
X



f

k



(


x

j


,

c

t



)





]



{\displaystyle {\mathcal {L}}_{\mathrm {N} }=-\mathbb {E} _{X}\left[\log {\frac {f_{k}\left(x_{t+k},c_{t}\right)}{\sum _{x_{j}\in X}f_{k}\left(x_{j},c_{t}\right)}}\right]}


 Non-contrastive self-supervised learning (NCSSL) uses only positive examples. Counterintuitively, NCSSL converges on a useful local minimum rather than reaching a trivial solution, with zero loss. For the example of binary classification, it would trivially learn to classify each example as positive. Effective NCSSL requires an extra predictor on the online side that does not back-propagate on the target side.[9]
 SSL belongs to supervised learning methods insofar as the goal is to generate a classified output from the input. At the same time, however, it does not require the explicit use of labeled input-output pairs. Instead, correlations, metadata embedded in the data, or domain knowledge present in the input are implicitly and autonomously extracted from the data. These supervisory signals, extracted from the data, can then be used for training.[1]
 SSL is similar to unsupervised learning in that it does not require labels in the sample data. Unlike unsupervised learning, however, learning is not done using inherent data structures.
 Semi-supervised learning combines supervised and unsupervised learning, requiring only a small portion of the learning data be labeled.[3]
 In transfer learning, a model designed for one task is reused on a different task.[13]
 Training an autoencoder intrinsically constitutes a self-supervised process, because the output pattern needs to become an optimal reconstruction of the input pattern itself. However, in current jargon, the term 'self-supervised' often refers to tasks based on a pretext-task training setup. This involves the (human) design of such pretext task(s), unlike 
the case of fully self-contained autoencoder training.[8]
 In reinforcement learning, self-supervising learning from a combination of losses can create abstract representations where only the most important information about the state are kept in a compressed way.[14]
 Self-supervised learning is particularly suitable for speech recognition. For example, Facebook developed wav2vec, a self-supervised algorithm, to perform speech recognition using two deep convolutional neural networks that build on each other.[7]
 Google's Bidirectional Encoder Representations from Transformers (BERT) model is used to better understand the context of search queries.[15]
 OpenAI's GPT-3 is an autoregressive language model that can be used in language processing. It can be used to translate texts or answer questions, among other things.[16]
 Bootstrap Your Own Latent (BYOL) is a NCSSL that produced excellent results on ImageNet and on transfer and semi-supervised benchmarks.[17]
 The Yarowsky algorithm is an example of self-supervised learning in natural language processing. From a small number of labeled examples, it learns to predict which word sense of a polysemous word is being used at a given point in text.
 DirectPred is a NCSSL that directly sets the predictor weights instead of learning it via typical gradient descent.[9]
 Self-GenomeNet is an example of self-supervised learning in genomics.[18]
 Self-supervised learning continues to gain prominence as a new approach across diverse fields. Its ability to leverage unlabeled data effectively opens new possibilities for advancement in machine learning, especially in data-driven application domains.
",learn ssl paradigm machin learn model train task use data gener supervisori signal rather reli label context neural network learn aim leverag inher structur relationship within input data creat meaning train signal ssl task design solv requir captur essenti featur relationship data input data typic augment transform way creat pair relat sampl one sampl serv input use formul supervisori signal augment involv introduc nois crop rotat transform learn close imit way human learn classifi object ssl model learn two step first task solv base auxiliari pretext classif task use help initi model paramet next actual task perform supervis unsupervis learn learn produc promis result recent year found practic applic field audio process use facebook other speech recognit autoassoci learn specif categori learn neural network train reproduc reconstruct input data word model task learn represent data captur essenti featur structur allow regener origin input term autoassoci come fact model essenti associ input data often achiev use autoencod type neural network architectur use represent learn autoencod consist encod network map input data represent latent space decod network reconstruct input represent train process involv present model input data requir reconstruct data close possibl loss function use train typic penal differ origin input reconstruct output mean squar error minim reconstruct error autoencod learn meaning represent data latent space binari classif task train data divid posit exampl neg exampl posit exampl match target exampl train classifi identifi bird posit train data would includ imag contain bird neg exampl would imag contrast learn use posit neg exampl loss function contrast learn use minim distanc posit sampl pair maxim distanc neg sampl pair earli exampl use pair convolut neural network process pair imag maxim agreement contrast clip allow joint pretrain text encod imag encod match pair imag encod vector text encod vector span small angl larg cosin similar infonc estim method optim two model jointli base nois contrast estim nce given set x x x n n n n random sampl contain one posit sampl p x k c n neg sampl distribut p x k minim follow loss function l n e x log f k x k c x j x f k x j c l n e x k j x k j learn ncssl use posit exampl counterintuit ncssl converg use local minimum rather reach trivial solut zero loss exampl binari classif would trivial learn classifi exampl posit effect ncssl requir extra predictor onlin side target side ssl belong supervis learn method insofar goal gener classifi output input time howev requir explicit use label pair instead correl metadata embed data domain knowledg present input implicitli autonom extract data supervisori signal extract data use train ssl similar unsupervis learn requir label sampl data unlik unsupervis learn howev learn done use inher data structur learn combin supervis unsupervis learn requir small portion learn data label transfer learn model design one task reus differ task train autoencod intrins constitut process output pattern need becom optim reconstruct input pattern howev current jargon term often refer task base train setup involv human design pretext task unlik case fulli autoencod train reinforc learn learn combin loss creat abstract represent import inform state kept compress way learn particularli suitabl speech recognit exampl facebook develop algorithm perform speech recognit use two deep convolut neural network build googl bidirect encod represent transform bert model use better understand context search queri openai autoregress languag model use languag process use translat text answer question among thing bootstrap latent byol ncssl produc excel result imagenet transfer benchmark yarowski algorithm exampl learn natur languag process small number label exampl learn predict word sens polysem word use given point text directpr ncssl directli set predictor weight instead learn via typic gradient descent exampl learn genom learn continu gain promin new approach across divers field abil leverag unlabel data effect open new possibl advanc machin learn especi applic domain
Reinforcement learning,https://en.wikipedia.org/wiki/Reinforcement_learning,"Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent should take actions in a dynamic environment in order to maximize a reward signal. Reinforcement learning is one of the three basic machine learning paradigms, alongside supervised learning and unsupervised learning.
 Q-learning at its simplest stores data in tables. This approach becomes infeasible as the number of states/actions increases (e.g., if the state space or action space were continuous), as the probability of the agent visiting a particular state and performing a particular action diminishes. 
 Reinforcement learning differs from supervised learning in not needing labelled input-output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead, the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the cumulative reward (the feedback of which might be incomplete or delayed).[1] The search for this balance is known as the exploration-exploitation dilemma.
 
The environment is typically stated in the form of a Markov decision process (MDP), as many reinforcement learning algorithms use dynamic programming techniques.[2] The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process, and they target large MDPs where exact methods become infeasible.[3]  Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, RL is called approximate dynamic programming, or neuro-dynamic programming. The problems of interest in RL have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation (particularly in the absence of a mathematical model of the environment).
 Basic reinforcement learning is modeled as a Markov decision process:
 The purpose of reinforcement learning is for the agent to learn an optimal (or near-optimal) policy that maximizes the reward function or other user-provided reinforcement signal that accumulates from immediate rewards. This is similar to processes that appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals learn to adopt behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.[4][5]
 A basic reinforcement learning agent interacts with its environment in discrete time steps. At each time step t, the agent receives the current state 




S

t




{\displaystyle S_{t}}

 and reward 




R

t




{\displaystyle R_{t}}

. It then chooses an action 




A

t




{\displaystyle A_{t}}

 from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state 




S

t
+
1




{\displaystyle S_{t+1}}

 and the reward 




R

t
+
1




{\displaystyle R_{t+1}}

 associated with the transition 



(

S

t


,

A

t


,

S

t
+
1


)


{\displaystyle (S_{t},A_{t},S_{t+1})}

 is determined. The goal of a reinforcement learning agent is to learn a policy: 
 



π
:


S


×


A


→
[
0
,
1
]


{\displaystyle \pi :{\mathcal {S}}\times {\mathcal {A}}\rightarrow [0,1]}

, 



π
(
s
,
a
)
=
Pr
(

A

t


=
a
∣

S

t


=
s
)


{\displaystyle \pi (s,a)=\Pr(A_{t}=a\mid S_{t}=s)}


 that maximizes the expected cumulative reward.
 Formulating the problem as a Markov decision process assumes the agent directly observes the current environmental state; in this case, the problem is said to have full observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have partial observability, and formally the problem must be formulated as a partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.
 When the agent's performance is compared to that of an agent that acts optimally, the difference in performance yields the notion of regret. In order to act near optimally, the agent must reason about long-term consequences of its actions (i.e., maximize future rewards), although the immediate reward associated with this might be negative.
 Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including energy storage,[6] robot control,[7] photovoltaic generators,[8] backgammon, checkers,[9] Go (AlphaGo), and autonomous driving systems.[10]
 Two elements make reinforcement learning powerful: the use of samples to optimize performance, and the use of function approximation to deal with large environments. Thanks to these two key components, RL can be used in large environments in the following situations:
 The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.
 The exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and for finite state space Markov decision processes in Burnetas and Katehakis (1997).[12]
 Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.
 One such method is 



ε


{\displaystyle \varepsilon }

-greedy, where 



0
<
ε
<
1


{\displaystyle 0<\varepsilon <1}

 is a parameter controlling the amount of exploration vs. exploitation.  With probability 



1
−
ε


{\displaystyle 1-\varepsilon }

, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability 



ε


{\displaystyle \varepsilon }

, exploration is chosen, and the action is chosen uniformly at random. 



ε


{\displaystyle \varepsilon }

 is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.[13]
 Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.
 The agent's action selection is modeled as a map called policy:
 The policy map gives the probability of taking action 



a


{\displaystyle a}

 when in state 



s


{\displaystyle s}

.[14]: 61  There are also deterministic policies.
 The state-value function 




V

π


(
s
)


{\displaystyle V_{\pi }(s)}

 is defined as, expected discounted return starting with state 



s


{\displaystyle s}

, i.e. 




S

0


=
s


{\displaystyle S_{0}=s}

, and successively following policy 



π


{\displaystyle \pi }

. Hence, roughly speaking, the value function estimates ""how good"" it is to be in a given state.[14]: 60 
 where the random variable 



G


{\displaystyle G}

 denotes the discounted return, and is defined as the sum of future discounted rewards:
 where 




R

t
+
1




{\displaystyle R_{t+1}}

 is the reward for transitioning from state 




S

t




{\displaystyle S_{t}}

 to 




S

t
+
1




{\displaystyle S_{t+1}}

, 



0
≤
γ
<
1


{\displaystyle 0\leq \gamma <1}

 is the discount rate. 



γ


{\displaystyle \gamma }

 is less than 1, so rewards in the distant future are weighted less than rewards in the immediate future.
 The algorithm must find a policy with maximum expected discounted return. From the theory of Markov decision processes it is known that, without loss of generality, the search can be restricted to the set of so-called stationary policies. A policy is stationary if the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted to deterministic stationary policies. A deterministic stationary policy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.
 The brute force approach entails two steps:
 One problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the discounted return of each policy.
 These problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this are value function estimation and direct policy search.
 Value function approaches attempt to find a policy that maximizes the discounted return by maintaining a set of estimates of expected discounted returns 





E


⁡
[
G
]


{\displaystyle \operatorname {\mathbb {E} } [G]}

 for some policy (usually either the ""current"" [on-policy] or the optimal [off-policy] one).
 These methods rely on the theory of Markov decision processes, where optimality is defined in a sense stronger than the one above: A policy is optimal if it achieves the best-expected discounted return from any initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found among stationary policies.
 To define optimality in a formal manner, define the state-value of a policy 



π


{\displaystyle \pi }

 by
 where 



G


{\displaystyle G}

 stands for the discounted return associated with following 



π


{\displaystyle \pi }

 from the initial state 



s


{\displaystyle s}

. Defining 




V

∗


(
s
)


{\displaystyle V^{*}(s)}

 as the maximum possible state-value of 




V

π


(
s
)


{\displaystyle V^{\pi }(s)}

, where 



π


{\displaystyle \pi }

 is allowed to change,
 A policy that achieves these optimal state-values in each state is called optimal. Clearly, a policy that is optimal in this sense is also optimal in the sense that it maximizes the expected discounted return, since 




V

∗


(
s
)
=

max

π



E

[
G
∣
s
,
π
]


{\displaystyle V^{*}(s)=\max _{\pi }\mathbb {E} [G\mid s,\pi ]}

, where 



s


{\displaystyle s}

 is a state randomly sampled from the distribution 



μ


{\displaystyle \mu }

 of initial states (so 



μ
(
s
)
=
Pr
(

S

0


=
s
)


{\displaystyle \mu (s)=\Pr(S_{0}=s)}

).
 Although state-values suffice to define optimality, it is useful to define action-values. Given a state 



s


{\displaystyle s}

, an action 



a


{\displaystyle a}

 and a policy 



π


{\displaystyle \pi }

, the action-value of the pair 



(
s
,
a
)


{\displaystyle (s,a)}

 under 



π


{\displaystyle \pi }

 is defined by
 where 



G


{\displaystyle G}

 now stands for the random discounted return associated with first taking action 



a


{\displaystyle a}

 in state 



s


{\displaystyle s}

 and following 



π


{\displaystyle \pi }

, thereafter.
 The theory of Markov decision processes states that if 




π

∗




{\displaystyle \pi ^{*}}

 is an optimal policy, we act optimally (take the optimal action) by choosing the action from 




Q


π

∗




(
s
,
⋅
)


{\displaystyle Q^{\pi ^{*}}(s,\cdot )}

 with the highest action-value at each state, 



s


{\displaystyle s}

. The action-value function of such an optimal policy (




Q


π

∗






{\displaystyle Q^{\pi ^{*}}}

) is called the optimal action-value function and is commonly denoted by 




Q

∗




{\displaystyle Q^{*}}

. In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.
 Assuming full knowledge of the Markov decision process, the two basic approaches to compute the optimal action-value function are value iteration and policy iteration. Both algorithms compute a sequence of functions 




Q

k




{\displaystyle Q_{k}}

 (



k
=
0
,
1
,
2
,
…


{\displaystyle k=0,1,2,\ldots }

) that converge to 




Q

∗




{\displaystyle Q^{*}}

. Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) Markov decision processes. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces.
 Monte Carlo methods[15] are used to solve reinforcement learning problems by averaging sample returns. Unlike methods that require full knowledge of the environment’s dynamics, Monte Carlo methods rely solely on actual or simulated experience—sequences of states, actions, and rewards obtained from interaction with an environment. This makes them applicable in situations where the complete dynamics are unknown. Learning from actual experience does not require prior knowledge of the environment and can still lead to optimal behavior. When using simulated experience, only a model capable of generating sample transitions is required, rather than a full specification of transition probabilities, which is necessary for dynamic programming methods.
 Monte Carlo methods apply to episodic tasks, where experience is divided into episodes that eventually terminate. Policy and value function updates occur only after the completion of an episode, making these methods incremental on an episode-by-episode basis, though not on a step-by-step (online) basis. The term “Monte Carlo” generally refers to any method involving random sampling; however, in this context, it specifically refers to methods that compute averages from complete returns, rather than partial returns.
 These methods function similarly to the bandit algorithms, in which returns are averaged for each state-action pair. The key difference is that actions taken in one state affect the returns of subsequent states within the same episode, making the problem non-stationary. To address this non-stationarity, Monte Carlo methods use the framework of general policy iteration (GPI). While dynamic programming computes value functions using full knowledge of the Markov decision process (MDP), Monte Carlo methods learn these functions through sample returns. The value functions and policies interact similarly to dynamic programming to achieve optimality, first addressing the prediction problem and then extending to policy improvement and control, all based on sampled experience.[14]
 The first problem is corrected by allowing the procedure to change the policy (at some or all states) before the values settle. This too may be problematic as it might prevent convergence. Most current algorithms do this, giving rise to the class of generalized policy iteration algorithms. Many actor-critic methods belong to this category.
 The second issue can be corrected by allowing trajectories to contribute to any state-action pair in them. This may also help to some extent with the third problem, although a better solution when returns have high variance is Sutton's temporal difference (TD) methods that are based on the recursive Bellman equation.[16][17] The computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are batched and the estimates are computed once based on the batch). Batch methods, such as the least-squares temporal difference method,[18] may use the information in the samples better, while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity. Some methods try to combine the two approaches. Methods based on temporal differences also overcome the fourth issue.
 Another problem specific to TD comes from their reliance on the recursive Bellman equation. Most TD methods have a so-called 



λ


{\displaystyle \lambda }

 parameter 



(
0
≤
λ
≤
1
)


{\displaystyle (0\leq \lambda \leq 1)}

 that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.
 In order to address the fifth issue, function approximation methods are used. Linear function approximation starts with a mapping 



ϕ


{\displaystyle \phi }

 that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair 



(
s
,
a
)


{\displaystyle (s,a)}

 are obtained by linearly combining the components of 



ϕ
(
s
,
a
)


{\displaystyle \phi (s,a)}

 with some weights 



θ


{\displaystyle \theta }

:
 The algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas from nonparametric statistics (which can be seen to construct their own features) have been explored.
 Value iteration can also be used as a starting point, giving rise to the Q-learning algorithm and its many variants.[19] Including Deep Q-learning methods when a neural network is used to represent Q, with various applications in stochastic search problems.[20]
 The problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency.
 An alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of stochastic optimization. The two approaches available are gradient-based and gradient-free methods.
 Gradient-based methods (policy gradient methods) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector 



θ


{\displaystyle \theta }

, let 




π

θ




{\displaystyle \pi _{\theta }}

 denote the policy associated to 



θ


{\displaystyle \theta }

. Defining the performance function by 



ρ
(
θ
)
=

ρ


π

θ






{\displaystyle \rho (\theta )=\rho ^{\pi _{\theta }}}

 under mild conditions this function will be differentiable as a function of the parameter vector 



θ


{\displaystyle \theta }

. If the gradient of 



ρ


{\displaystyle \rho }

 was known, one could use gradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams' REINFORCE method[21] (which is known as the likelihood ratio method in the simulation-based optimization literature).[22]
 A large class of methods avoids relying on gradient information. These include simulated annealing, cross-entropy search or methods of evolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.
 Policy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years, actor–critic methods have been proposed and performed well on various problems.[23]
 Policy search methods have been used in the robotics context.[24] Many policy search methods may get stuck in local optima (as they are based on local search).
 Finally, all of the above methods can be combined with algorithms that first learn a model of the Markov Decision Process, the probability of each next state given an action taken from an existing state. For instance, the Dyna algorithm[25] learns a model from experience, and uses that to provide more modelled transitions for a value function, in addition to the real transitions.  Such methods can sometimes be extended to use of non-parametric models, such as when the transitions are simply stored and 'replayed'[26] to the learning algorithm.
 Model-based methods can be more computationally intensive than model-free approaches, and their utility can be limited by the extent to which the Markov Decision Process can be learnt.[27]
 There are other ways to use models than to update a value function.[28] For instance, in model predictive control the model is used to update the behavior directly.
 Both the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.
 Efficient exploration of Markov decision processes is given in  Burnetas and Katehakis (1997).[12] Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.
 For incremental algorithms, asymptotic convergence issues have been settled[clarification needed]. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).
 Research topics include:
 Associative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks. In associative reinforcement learning tasks, the learning system interacts in a closed loop with its environment.[46]
 This approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space.[47] The work on learning ATARI games by Google DeepMind increased attention to deep reinforcement learning or end-to-end reinforcement learning.[48]
 Adversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations.[49][50][51] While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies.[52]
 By introducing fuzzy inference in reinforcement learning,[53] approximating the state-action value function with fuzzy rules in continuous space becomes possible. The IF - THEN form of fuzzy rules make this approach suitable for expressing the results in a form close to natural language. Extending FRL with Fuzzy Rule Interpolation [54] allows the use of reduced size sparse fuzzy rule-bases to emphasize cardinal rules (most important state-action values).
 In inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal.[55] One popular IRL paradigm is named maximum entropy inverse reinforcement learning (MaxEnt IRL). [56] MaxEnt IRL estimates the parameters of a linear model of the reward function by maximizing the entropy of the probability distribution of observed trajectories subject to constraints related to matching expected feature counts. Recently it has been shown that MaxEnt IRL is a particular case of a more general framework named random utility inverse reinforcement learning (RU-IRL). [57] RU-IRL is based on random utility theory and Markov decision processes. While prior IRL approaches assume that the apparent random behavior of an observed agent is due to it following a random policy, RU-IRL assumes that the observed agent follows a deterministic policy but randomness in observed behavior is due to the fact that an observer only has partial access to the features the observed agent uses in decision making. The utility function is modeled as a random variable to account for the ignorance of the observer regarding the features the observed agent actually considers in its utility function.
 Safe reinforcement learning (SRL) can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes.[58] An alternative approach is risk-averse reinforcement learning, where instead of the expected return, a risk-measure of the return is optimized, such as the Conditional Value at Risk (CVaR).[59] In addition to mitigating risk, the CVaR objective increases robustness to model uncertainties.[60][61] However, CVaR optimization in risk-averse RL requires special care, to prevent gradient bias[62] and blindness to success.[63]
 Self-reinforcement learning (or self learning), is a learning paradigm which does not use the concept of immediate reward Ra(s,s') after transition from s to s' with action a. It does not use an external reinforcement, it only uses the agent internal self-reinforcement. The internal self-reinforcement is provided by mechanism of feelings and emotions. In the learning process emotions are backpropagated by a mechanism of secondary reinforcement. The learning equation does not include the immediate reward, it only includes the state evaluation. 
 The self-reinforcement algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine:
1. in situation s perform action a
2. receive a consequence situation s'
3. compute state evaluation v(s') of how good is to be in the consequence situation s'
4. update crossbar memory w'(a,s) = w(a,s) + v(s')
 Initial conditions of the memory are received as input from the genetic environment. It is a system with only one input (situation), and only one output (action, or behavior). 
 Self reinforcement (self learning) was introduced in 1982 along with a neural network capable of self-reinforcement learning, named Crossbar Adaptive Array (CAA).[64][65] The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence states. The system is driven by the interaction between cognition and emotion. [66]
 Efficient comparison of RL algorithms is essential for research, deployment and monitoring of RL systems. To compare different algorithms on a given environment, an agent can be trained for each algorithm. Since the performance is sensitive to implementation details, all algorithms should be implemented as closely as possible to each other.[67] After the training is finished, the agents can be run on a sample of test episodes, and their scores (returns) can be compared. Since episodes are typically assumed to be i.i.d, standard statistical tools can be used for hypothesis testing, such as T-test and permutation test.[68] This requires to accumulate all the rewards within an episode into a single number - the episodic return. However, this causes a loss of information, as different time-steps are averaged together, possibly with different levels of noise. Whenever the noise level varies across the episode, the statistical power can be improved significantly, by weighting the rewards according to their estimated noise.[69]
 Reinforcement Learning algorithms contain unfair/unequal actions in the scenarios in which they can make significant social impacts. This is mainly caused by focusing on myopic models that do not account for how short-term actions influence long-term outcomes. Filtering a specific group of people according to this principle causes unfairness. The examples are applying RL during the processes of hiring, lending, and admissions.[70] Without the notion of fairness constraints, the decisions made by the RL system might be unethical and harmful.[71]
 The first example is during the hiring process. A company may prefer hiring applicants from a well-understood demographic for short-term productivity gains. However, this approach may exclude diverse candidates who could contribute significantly in the long term. A fair RL in hiring should ensure that actions (e.g., who to interview or hire) are guided by long-term productivity and fairness, not just immediate metrics. For instance, if the RL only filters for the relevant skills on the resume for short-term convenience, it could miss the potential diverse applicant who has strong learning skills and a great passion for long-term gains. Although hiring diverse candidates might incur short-term costs (e.g., training or adjustment periods), it could result in a stronger, more innovative workforce over time. Another particular example is for the graduated international student. If the RL only focuses on short-term gain, the sponsorship could be a liability to the company. However, international students are usually smart individuals with good resources which could potentially help the company in the long-term gains. A short-sighted RL could cause unfairness.[70]
 In the example of lending, if it is only based mainly on immediate risk assessments, it might systematically disadvantage certain groups, even when those groups could exhibit equal long-term creditworthiness. Some lending systems from the government are designed to help the people who need the help the most. However, a short-sighted RL that focuses on whether the applicant can pay back the money would not make the people who need the help the most pass the application. This is because they are highly likely to have the least ability to make money in the short term. This is unfair and against the original Intention of the program. Also, immediate default rates might penalize groups that show equal or better financial reliability in the future given time. In addition, many successful businesses with great impact on society usually sounded unrealistic at the beginning (e.g. Amazon, and Tesla.). If the RL only evaluates the risk of breaking a contract in the short-term, it would not only be unfair to the people who have big dreams but also would miss great development opportunities for society. [70]
 For the admission example, a university's admission strategy may focus on candidates who align with historical success metrics but fail to invest in potential students from underrepresented backgrounds, impacting long-term diversity and institutional strength. This is very dangerous as it can create a biased RL on the race, area, gender, and financial/education condition of a family.[70] According to the research, if the robot system trained in RL is biased in certain groups of people (sensitive identity). The robot may provide limited support to those people and shows relatively less interest compared to non-sensitive identity.[72]
 In that case, fairness is defined as requiring that an algorithm not prefer one action over another unless the long-term (discounted) reward is higher. This principle prohibits targeting or excluding populations unless the long-term benefit justifies such actions.[70]
 The RL system may also marginalize a certain group of people if the training data set shows different groups of people have notably different capabilities to work. The research shows the system wants to achieve maximum efficiency so that it will always choose to assign tasks to more efficient groups. If the system thinks this group of people has lower efficiency in working due to gender, race, or religious perspectives, the system may assign very little work or even abandon that group of people. For example, the paper talks about the videos made by minority people may have a lower chance to appear in the recommendation system because white creators are significantly more than minority and have the advantage in gaining traffic. [73]
 The challenge in holding this fairness is that ensuring fairness in these scenarios often requires RL to make short-term sacrifices (e.g., reduced immediate productivity or profit) for long-term equity and benefits.[70] The system may show lower performance and efficiency as trade-offs but can achieve equity in the long term.[74] Developers using RL-based systems need to ensure, by law, that their algorithms do not discriminate on protected attributes like gender, race, or religion. The research also recommends the multi-agent RL which can reduce the bias in the RL system. The successful application includes fair YouTube recommendations, fair IoT, fair stock trading, and reducing bias in facial recognition systems.  [73]
",reinforc learn rl interdisciplinari area machin learn optim control concern intellig agent take action dynam environ order maxim reward signal reinforc learn one three basic machin learn paradigm alongsid supervis learn unsupervis learn simplest store data tabl approach becom infeas number increas state space action space continu probabl agent visit particular state perform particular action diminish reinforc learn differ supervis learn need label pair present need action explicitli correct instead focu find balanc explor unchart territori exploit current knowledg goal maxim cumul reward feedback might incomplet delay search balanc known dilemma environ typic state form markov decis process mdp mani reinforc learn algorithm use dynam program techniqu main differ classic dynam program method reinforc learn algorithm latter assum knowledg exact mathemat model markov decis process target larg mdp exact method becom infeas due gener reinforc learn studi mani disciplin game theori control theori oper research inform theori optim system swarm intellig statist oper research control literatur rl call approxim dynam program program problem interest rl also studi theori optim control concern mostli exist character optim solut algorithm exact comput less learn approxim particularli absenc mathemat model environ basic reinforc learn model markov decis process purpos reinforc learn agent learn optim polici maxim reward function reinforc signal accumul immedi reward similar process appear occur anim psycholog exampl biolog brain hardwir interpret signal pain hunger neg reinforc interpret pleasur food intak posit reinforc circumst anim learn adopt behavior optim reward suggest anim capabl reinforc learn basic reinforc learn agent interact environ discret time step time step agent receiv current state reward r choos action set avail action subsequ sent environ environ move new state reward r associ transit determin goal reinforc learn agent learn polici π π pr maxim expect cumul reward formul problem markov decis process assum agent directli observ current environment state case problem said full observ agent access subset state observ state corrupt nois agent said partial observ formal problem must formul partial observ markov decis process case set action avail agent restrict exampl state account balanc could restrict posit current valu state state transit attempt reduc valu transit allow agent perform compar agent act optim differ perform yield notion regret order act near optim agent must reason consequ action maxim futur reward although immedi reward associ might neg thu reinforc learn particularli problem includ versu reward appli success variou problem includ energi storag robot control photovolta gener backgammon checker go alphago autonom drive system two element make reinforc learn power use sampl optim perform use function approxim deal larg environ thank two key compon rl use larg environ follow situat first two problem could consid plan problem sinc form model avail last one could consid genuin learn problem howev reinforc learn convert plan problem machin learn problem explor exploit thoroughli studi bandit problem finit state space markov decis process burneta katehaki reinforc learn requir clever explor mechan randomli select action without refer estim probabl distribut show poor perform case small finit markov decis process rel well understood howev due lack algorithm scale well number state scale problem infinit state space simpl explor method practic one method ε ε paramet control amount explor exploit probabl ε exploit chosen agent choos action believ best effect tie action broken uniformli random altern probabl ε explor chosen action chosen uniformli random ε usual fix paramet adjust either accord schedul make agent explor progress less adapt base heurist even issu explor disregard even state observ assum hereaft problem remain use past experi find action lead higher cumul reward agent action select model map call polici polici map give probabl take action state also determinist polici function v π defin expect discount return start state success follow polici π henc roughli speak valu function estim good given state random variabl g g denot discount return defin sum futur discount reward r reward transit state γ discount rate γ less reward distant futur weight less reward immedi futur algorithm must find polici maximum expect discount return theori markov decis process known without loss gener search restrict set stationari polici polici stationari return depend last state visit observ agent histori search restrict determinist stationari polici determinist stationari polici determinist select action base current state sinc polici identifi map set state set action polici identifi map loss gener brute forc approach entail two step one problem number polici larg even infinit anoth varianc return may larg requir mani sampl accur estim discount return polici problem amelior assum structur allow sampl gener one polici influenc estim made other two main approach achiev valu function estim direct polici search valu function approach attempt find polici maxim discount return maintain set estim expect discount return e g e g polici usual either current optim one method reli theori markov decis process optim defin sens stronger one polici optim achiev discount return initi state initi distribut play role definit optim polici alway found among stationari polici defin optim formal manner defin polici π g g stand discount return associ follow π initi state defin v maximum possibl v π π allow chang polici achiev optim state call optim clearli polici optim sens also optim sens maxim expect discount return sinc v max π e g π e state randomli sampl distribut μ initi state μ pr although suffic defin optim use defin given state action polici π pair π defin g g stand random discount return associ first take action state follow π thereaft theori markov decis process state π optim polici act optim take optim action choos action q π highest state function optim polici q π call optim function commonli denot q summari knowledg optim function alon suffic know act optim assum full knowledg markov decis process two basic approach comput optim function valu iter polici iter algorithm comput sequenc function q k k k converg q comput function involv comput expect whole impract smallest finit markov decis process reinforc learn method expect approxim averag sampl use function approxim techniqu cope need repres valu function larg space mont carlo method use solv reinforc learn problem averag sampl return unlik method requir full knowledg environ dynam mont carlo method reli sole actual simul state action reward obtain interact environ make applic situat complet dynam unknown learn actual experi requir prior knowledg environ still lead optim behavior use simul experi model capabl gener sampl transit requir rather full specif transit probabl necessari dynam program method mont carlo method appli episod task experi divid episod eventu termin polici valu function updat occur complet episod make method increment basi though onlin basi term mont carlo gener refer method involv random sampl howev context specif refer method comput averag complet return rather partial return method function similarli bandit algorithm return averag pair key differ action taken one state affect return subsequ state within episod make problem address mont carlo method use framework gener polici iter gpi dynam program comput valu function use full knowledg markov decis process mdp mont carlo method learn function sampl return valu function polici interact similarli dynam program achiev optim first address predict problem extend polici improv control base sampl experi first problem correct allow procedur chang polici state valu settl may problemat might prevent converg current algorithm give rise class gener polici iter algorithm mani method belong categori second issu correct allow trajectori contribut pair may also help extent third problem although better solut return high varianc sutton tempor differ td method base recurs bellman equat comput td method increment transit memori chang transit thrown away batch transit batch estim comput base batch batch method tempor differ method may use inform sampl better increment method choic batch method infeas due high comput memori complex method tri combin two approach method base tempor differ also overcom fourth issu anoth problem specif td come relianc recurs bellman equat td method λ paramet λ continu interpol mont carlo method reli bellman equat basic td method reli entir bellman equat effect palliat issu order address fifth issu function approxim method use linear function approxim start map ϕ assign vector pair action valu pair obtain linearli combin compon ϕ weight θ algorithm adjust weight instead adjust valu associ individu pair method base idea nonparametr statist seen construct featur explor valu iter also use start point give rise algorithm mani variant includ deep method neural network use repres q variou applic stochast search problem problem use may need highli precis estim compet action valu hard obtain return noisi though problem mitig extent tempor differ method use compat function approxim method compromis gener effici altern method search directli subset polici space case problem becom case stochast optim two approach avail method method polici gradient method start map paramet space space polici given paramet vector θ let π θ denot polici associ θ defin perform function ρ θ ρ π θ mild condit function differenti function paramet vector θ gradient ρ known one could use gradient ascent sinc analyt express gradient avail noisi estim avail estim construct mani way give rise algorithm william reinforc method known likelihood ratio method optim literatur larg class method avoid reli gradient inform includ simul anneal search method evolutionari comput mani method achiev theori limit global optimum polici search method may converg slowli given noisi data exampl happen episod problem trajectori long varianc return larg base method reli tempor differ might help case recent year method propos perform well variou problem polici search method use robot context mani polici search method may get stuck local optima base local search final method combin algorithm first learn model markov decis process probabl next state given action taken exist state instanc dyna algorithm learn model experi use provid model transit valu function addit real transit method sometim extend use model transit simpli store learn algorithm method comput intens approach util limit extent markov decis process learnt way use model updat valu function instanc model predict control model use updat behavior directli asymptot behavior algorithm well understood algorithm provabl good onlin perform address explor issu known effici explor markov decis process given burneta katehaki perform bound also appear mani algorithm bound expect rather loos thu work need better understand rel advantag limit increment algorithm asymptot converg issu settl clarif need algorithm converg wider set condit previous possibl exampl use arbitrari smooth function approxim research topic includ associ reinforc learn task combin facet stochast learn automata task supervis learn pattern classif task associ reinforc learn task learn system interact close loop environ approach extend reinforc learn use deep neural network without explicitli design state space work learn atari game googl deepmind increas attent deep reinforc learn reinforc learn adversari deep reinforc learn activ area research reinforc learn focus vulner learn polici research area studi initi show reinforc learn polici suscept impercept adversari manipul method propos overcom suscept recent studi shown propos solut far provid accur represent current vulner deep reinforc learn polici introduc fuzzi infer reinforc learn approxim valu function fuzzi rule continu space becom possibl form fuzzi rule make approach suitabl express result form close natur languag extend frl fuzzi rule interpol allow use reduc size spars fuzzi emphas cardin rule import valu invers reinforc learn irl reward function given instead reward function infer given observ behavior expert idea mimic observ behavior often optim close optim one popular irl paradigm name maximum entropi invers reinforc learn maxent irl maxent irl estim paramet linear model reward function maxim entropi probabl distribut observ trajectori subject constraint relat match expect featur count recent shown maxent irl particular case gener framework name random util invers reinforc learn base random util theori markov decis process prior irl approach assum appar random behavior observ agent due follow random polici assum observ agent follow determinist polici random observ behavior due fact observ partial access featur observ agent use decis make util function model random variabl account ignor observ regard featur observ agent actual consid util function safe reinforc learn srl defin process learn polici maxim expect return problem import ensur reason system perform respect safeti constraint learn deploy process altern approach reinforc learn instead expect return return optim condit valu risk cvar addit mitig risk cvar object increas robust model uncertainti howev cvar optim rl requir special care prevent gradient bia blind success learn self learn learn paradigm use concept immedi reward ra transit action use extern reinforc use agent intern intern provid mechan feel emot learn process emot backpropag mechan secondari reinforc learn equat includ immedi reward includ state evalu algorithm updat memori matrix w iter execut follow machin learn routin situat perform action receiv consequ situat comput state evalu v good consequ situat updat crossbar memori w w v initi condit memori receiv input genet environ system one input situat one output action behavior self reinforc self learn introduc along neural network capabl learn name crossbar adapt array caa caa comput crossbar fashion decis action emot feel consequ state system driven interact cognit emot effici comparison rl algorithm essenti research deploy monitor rl system compar differ algorithm given environ agent train algorithm sinc perform sensit implement detail algorithm implement close possibl train finish agent run sampl test episod score return compar sinc episod typic assum standard statist tool use hypothesi test permut test requir accumul reward within episod singl number episod return howev caus loss inform differ averag togeth possibl differ level nois whenev nois level vari across episod statist power improv significantli weight reward accord estim nois reinforc learn algorithm contain action scenario make signific social impact mainli caus focus myopic model account action influenc outcom filter specif group peopl accord principl caus unfair exampl appli rl process hire lend admiss without notion fair constraint decis made rl system might uneth harm first exampl hire process compani may prefer hire applic demograph product gain howev approach may exclud divers candid could contribut significantli long term fair rl hire ensur action interview hire guid product fair immedi metric instanc rl filter relev skill resum conveni could miss potenti divers applic strong learn skill great passion gain although hire divers candid might incur cost train adjust period could result stronger innov workforc time anoth particular exampl graduat intern student rl focus gain sponsorship could liabil compani howev intern student usual smart individu good resourc could potenti help compani gain rl could caus unfair exampl lend base mainli immedi risk assess might systemat disadvantag certain group even group could exhibit equal creditworthi lend system govern design help peopl need help howev rl focus whether applic pay back money would make peopl need help pass applic highli like least abil make money short term unfair origin intent program also immedi default rate might penal group show equal better financi reliabl futur given time addit mani success busi great impact societi usual sound unrealist begin amazon rl evalu risk break contract would unfair peopl big dream also would miss great develop opportun societi admiss exampl univers admiss strategi may focu candid align histor success metric fail invest potenti student underrepres background impact divers institut strength danger creat bias rl race area gender condit famili accord research robot system train rl bias certain group peopl sensit ident robot may provid limit support peopl show rel less interest compar ident case fair defin requir algorithm prefer one action anoth unless discount reward higher principl prohibit target exclud popul unless benefit justifi action rl system may also margin certain group peopl train data set show differ group peopl notabl differ capabl work research show system want achiev maximum effici alway choos assign task effici group system think group peopl lower effici work due gender race religi perspect system may assign littl work even abandon group peopl exampl paper talk video made minor peopl may lower chanc appear recommend system white creator significantli minor advantag gain traffic challeng hold fair ensur fair scenario often requir rl make sacrific reduc immedi product profit equiti benefit system may show lower perform effici achiev equiti long term develop use system need ensur law algorithm discrimin protect attribut like gender race religion research also recommend rl reduc bia rl system success applic includ fair youtub recommend fair iot fair stock trade reduc bia facial recognit system
Meta-learning (computer science),https://en.wikipedia.org/wiki/Meta-learning_(computer_science),"Meta-learning[1][2]
is a subfield of machine learning where automatic learning algorithms are applied to metadata about machine learning experiments. As of 2017, the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing learning algorithms or to learn (induce) the learning algorithm itself, hence the alternative term learning to learn.[1]
 Flexibility is important because each learning algorithm is based on a set of assumptions about the data, its inductive bias.[3] This means that it will only learn well if the bias matches the learning problem. A learning algorithm may perform very well in one domain, but not on the next. This poses strong restrictions on the use of machine learning or data mining techniques, since the relationship between the learning problem (often some kind of database) and the effectiveness of different learning algorithms is not yet understood.
 By using different kinds of metadata, like properties of the learning problem, algorithm properties (like performance measures), or patterns previously derived from the data, it is possible to learn, select, alter or combine different learning algorithms to effectively solve a given learning problem. Critiques of meta-learning approaches bear a strong resemblance to the critique of metaheuristic, a possibly related problem. A good analogy to meta-learning, and the inspiration for Jürgen Schmidhuber's early work (1987)[1] and Yoshua Bengio et al.'s work (1991),[4] considers that genetic evolution learns the learning procedure encoded in genes and executed in each individual's brain. In an open-ended hierarchical meta-learning system[1] using genetic programming, better evolutionary methods can be learned by meta evolution, which itself can be improved by meta meta evolution, etc.[1]
 A proposed definition[5] for a meta-learning system combines three requirements:
 Bias refers to the assumptions that influence the choice of explanatory hypotheses[6] and not the notion of bias represented in the bias-variance dilemma. Meta-learning is concerned with two aspects of learning bias.
 There are three common approaches:[8]
 Model-based meta-learning models updates its parameters rapidly with a few training steps, which can be achieved by its internal architecture or controlled by another meta-learner model.[8]
 A Memory-Augmented Neural Network, or MANN for short, is claimed to be able to encode new information quickly and thus to adapt to new tasks after only a few examples.[9]
 Meta Networks (MetaNet) learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization.[10]
 The core idea in metric-based meta-learning is similar to nearest neighbors algorithms, which weight is generated by a kernel function. It aims to learn a metric or distance function over objects. The notion of a good metric is problem-dependent. It should represent the relationship between inputs in the task space and facilitate problem solving.[8]
 Siamese neural network is composed of two twin networks whose output is jointly trained. There is a function above to learn the relationship between input data sample pairs. The two networks are the same, sharing the same weight and network parameters.[11]
 Matching Networks learn a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types.[12]
 The Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting.[13]
 Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve satisfied results.[14]
 What optimization-based meta-learning algorithms intend for is to adjust the optimization algorithm so that the model can be good at learning with a few examples.[8]
 LSTM-based meta-learner is to learn the exact optimization algorithm used to train another learner neural network classifier in the few-shot regime. The parametrization allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner (classifier) network that allows for quick convergence of training.[15]
 Model-Agnostic Meta-Learning (MAML) is a fairly general optimization algorithm, compatible with any model that learns through gradient descent.[16]
 Reptile is a remarkably simple meta-learning optimization algorithm, given that both of its components rely on meta-optimization through gradient descent and both are model-agnostic.[17]
 Some approaches which have been viewed as instances of meta-learning:
",subfield machin learn automat learn algorithm appli metadata machin learn experi term found standard interpret howev main goal use metadata understand automat learn becom flexibl solv learn problem henc improv perform exist learn algorithm learn induc learn algorithm henc altern term learn learn flexibl import learn algorithm base set assumpt data induct bia mean learn well bia match learn problem learn algorithm may perform well one domain next pose strong restrict use machin learn data mine techniqu sinc relationship learn problem often kind databas effect differ learn algorithm yet understood use differ kind metadata like properti learn problem algorithm properti like perform measur pattern previous deriv data possibl learn select alter combin differ learn algorithm effect solv given learn problem critiqu approach bear strong resembl critiqu metaheurist possibl relat problem good analog inspir jürgen schmidhub earli work yoshua bengio et al work consid genet evolut learn learn procedur encod gene execut individu brain hierarch system use genet program better evolutionari method learn meta evolut improv meta meta evolut etc propos definit system combin three requir bia refer assumpt influenc choic explanatori hypothes notion bia repres dilemma concern two aspect learn bia three common approach model updat paramet rapidli train step achiev intern architectur control anoth model neural network mann short claim abl encod new inform quickli thu adapt new task exampl meta network metanet learn knowledg across task shift induct bias via fast parameter rapid gener core idea similar nearest neighbor algorithm weight gener kernel function aim learn metric distanc function object notion good metric repres relationship input task space facilit problem solv siames neural network compos two twin network whose output jointli train function learn relationship input data sampl pair two network share weight network paramet match network learn network map small label support set unlabel exampl label obviat need adapt new class type relat network rn train scratch learn learn deep distanc metric compar small number imag within episod design simul set prototyp network learn metric space classif perform comput distanc prototyp represent class compar recent approach learn reflect simpler induct bia benefici regim achiev satisfi result algorithm intend adjust optim algorithm model good learn exampl learn exact optim algorithm use train anoth learner neural network classifi regim parametr allow learn appropri paramet updat specif scenario set amount updat made also learn gener initi learner classifi network allow quick converg train maml fairli gener optim algorithm compat model learn gradient descent reptil remark simpl optim algorithm given compon reli gradient descent approach view instanc
Online machine learning,https://en.wikipedia.org/wiki/Online_machine_learning,"In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., prediction of prices in the financial international markets. Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.
 In the setting of supervised learning, a function of 



f
:
X
→
Y


{\displaystyle f:X\to Y}

 is to be learned, where 



X


{\displaystyle X}

 is thought of as a space of inputs and 



Y


{\displaystyle Y}

 as a space of outputs, that predicts well on instances that are drawn from a joint probability distribution 



p
(
x
,
y
)


{\displaystyle p(x,y)}

 on 



X
×
Y


{\displaystyle X\times Y}

. In reality, the learner never knows the true distribution 



p
(
x
,
y
)


{\displaystyle p(x,y)}

 over instances. Instead, the learner usually has access to a training set of examples 



(

x

1


,

y

1


)
,
…
,
(

x

n


,

y

n


)


{\displaystyle (x_{1},y_{1}),\ldots ,(x_{n},y_{n})}

. In this setting, the loss function is given as 



V
:
Y
×
Y
→

R



{\displaystyle V:Y\times Y\to \mathbb {R} }

, such that 



V
(
f
(
x
)
,
y
)


{\displaystyle V(f(x),y)}

 measures the difference between the predicted value 



f
(
x
)


{\displaystyle f(x)}

 and the true value 



y


{\displaystyle y}

. The ideal goal is to select a function 



f
∈


H




{\displaystyle f\in {\mathcal {H}}}

, where 





H




{\displaystyle {\mathcal {H}}}

 is a space of functions called a hypothesis space, so that some notion of total loss is minimized. Depending on the type of model (statistical or adversarial), one can devise different notions of loss, which lead to different learning algorithms.
 In statistical learning models, the training sample 



(

x

i


,

y

i


)


{\displaystyle (x_{i},y_{i})}

 are assumed to have been drawn from the true distribution 



p
(
x
,
y
)


{\displaystyle p(x,y)}

 and the objective is to minimize the expected ""risk""




I
[
f
]
=

E

[
V
(
f
(
x
)
,
y
)
]
=
∫
V
(
f
(
x
)
,
y
)

d
p
(
x
,
y
)
 
.


{\displaystyle I[f]=\mathbb {E} [V(f(x),y)]=\int V(f(x),y)\,dp(x,y)\ .}


A common paradigm in this situation is to estimate a function 






f
^





{\displaystyle {\hat {f}}}

 through empirical risk minimization or regularized empirical risk minimization (usually Tikhonov regularization). The choice of loss function here gives rise to several well-known learning algorithms such as regularized least squares and support vector machines.
A purely online model in this category would learn based on just the new input 



(

x

t
+
1


,

y

t
+
1


)


{\displaystyle (x_{t+1},y_{t+1})}

, the current best predictor 




f

t




{\displaystyle f_{t}}

 and some extra stored information (which is usually expected to have storage requirements independent of training data size). For many formulations, for example nonlinear kernel methods, true online learning is not possible, though a form of hybrid online learning with recursive algorithms can be used where 




f

t
+
1




{\displaystyle f_{t+1}}

 is permitted to depend on 




f

t




{\displaystyle f_{t}}

 and all previous data points 



(

x

1


,

y

1


)
,
…
,
(

x

t


,

y

t


)


{\displaystyle (x_{1},y_{1}),\ldots ,(x_{t},y_{t})}

. In this case, the space requirements are no longer guaranteed to be constant since it requires storing all previous data points, but the solution may take less time to compute with the addition of a new data point, as compared to batch learning techniques.
 A common strategy to overcome the above issues is to learn using mini-batches, which process a small batch of 



b
≥
1


{\displaystyle b\geq 1}

 data points at a time, this can be considered as pseudo-online learning for 



b


{\displaystyle b}

 much smaller than the total number of training points. Mini-batch techniques are used with repeated passing over the training data to obtain optimized out-of-core versions of machine learning algorithms, for example, stochastic gradient descent. When combined with backpropagation, this is currently the de facto training method for training artificial neural networks.
 The simple example of linear least squares is used to explain a variety of ideas in online learning. The ideas are general enough to be applied to other settings, for example, with other convex loss functions.
 Consider the setting of supervised learning with 



f


{\displaystyle f}

 being a linear function to be learned:




f
(

x

j


)
=
⟨
w
,

x

j


⟩
=
w
⋅

x

j




{\displaystyle f(x_{j})=\langle w,x_{j}\rangle =w\cdot x_{j}}


where 




x

j


∈


R


d




{\displaystyle x_{j}\in \mathbb {R} ^{d}}

 is a vector of inputs (data points) and 



w
∈


R


d




{\displaystyle w\in \mathbb {R} ^{d}}

 is a linear filter vector.
The goal is to compute the filter vector 



w


{\displaystyle w}

.
To this end, a square loss function 




V
(
f
(

x

j


)
,

y

j


)
=
(
f
(

x

j


)
−

y

j



)

2


=
(
⟨
w
,

x

j


⟩
−

y

j



)

2




{\displaystyle V(f(x_{j}),y_{j})=(f(x_{j})-y_{j})^{2}=(\langle w,x_{j}\rangle -y_{j})^{2}}


is used to compute the vector 



w


{\displaystyle w}

 that minimizes the empirical loss





I

n


[
w
]
=

∑

j
=
1


n


V
(
⟨
w
,

x

j


⟩
,

y

j


)
=

∑

j
=
1


n


(

x

j



T



w
−

y

j



)

2




{\displaystyle I_{n}[w]=\sum _{j=1}^{n}V(\langle w,x_{j}\rangle ,y_{j})=\sum _{j=1}^{n}(x_{j}^{\mathsf {T}}w-y_{j})^{2}}

 
where





y

j


∈

R

.


{\displaystyle y_{j}\in \mathbb {R} .}


 Let 



X


{\displaystyle X}

 be the 



i
×
d


{\displaystyle i\times d}

 data matrix and 



y
∈


R


i




{\displaystyle y\in \mathbb {R} ^{i}}

 is the column vector of target values after the arrival of the first 



i


{\displaystyle i}

 data points.
Assuming that the covariance matrix 




Σ

i


=

X


T



X


{\displaystyle \Sigma _{i}=X^{\mathsf {T}}X}

 is invertible (otherwise it is preferential to proceed in a similar fashion with Tikhonov regularization), the best solution 




f

∗


(
x
)
=
⟨

w

∗


,
x
⟩


{\displaystyle f^{*}(x)=\langle w^{*},x\rangle }

 to the linear least squares problem is given by





w

∗


=
(

X


T



X

)

−
1



X


T



y
=

Σ

i


−
1



∑

j
=
1


i



x

j



y

j


.


{\displaystyle w^{*}=(X^{\mathsf {T}}X)^{-1}X^{\mathsf {T}}y=\Sigma _{i}^{-1}\sum _{j=1}^{i}x_{j}y_{j}.}


 Now, calculating the covariance matrix 




Σ

i


=

∑

j
=
1


i



x

j



x

j



T





{\displaystyle \Sigma _{i}=\sum _{j=1}^{i}x_{j}x_{j}^{\mathsf {T}}}

 takes time 



O
(
i

d

2


)


{\displaystyle O(id^{2})}

, inverting the 



d
×
d


{\displaystyle d\times d}

 matrix takes time 



O
(

d

3


)


{\displaystyle O(d^{3})}

, while the rest of the multiplication takes time 



O
(

d

2


)


{\displaystyle O(d^{2})}

, giving a total time of 



O
(
i

d

2


+

d

3


)


{\displaystyle O(id^{2}+d^{3})}

. When there are 



n


{\displaystyle n}

 total points in the dataset, to recompute the solution after the arrival of every datapoint 



i
=
1
,
…
,
n


{\displaystyle i=1,\ldots ,n}

, the naive approach will have a total complexity 



O
(

n

2



d

2


+
n

d

3


)


{\displaystyle O(n^{2}d^{2}+nd^{3})}

. Note that when storing the matrix 




Σ

i




{\displaystyle \Sigma _{i}}

, then updating it at each step needs only adding 




x

i
+
1



x

i
+
1



T





{\displaystyle x_{i+1}x_{i+1}^{\mathsf {T}}}

, which takes 



O
(

d

2


)


{\displaystyle O(d^{2})}

 time, reducing the total time to 



O
(
n

d

2


+
n

d

3


)
=
O
(
n

d

3


)


{\displaystyle O(nd^{2}+nd^{3})=O(nd^{3})}

, but with an additional storage space of 



O
(

d

2


)


{\displaystyle O(d^{2})}

 to store 




Σ

i




{\displaystyle \Sigma _{i}}

.[1]
 The recursive least squares (RLS) algorithm considers an online approach to the least squares problem. It can be shown that by initialising 





w

0


=
0
∈


R


d





{\displaystyle \textstyle w_{0}=0\in \mathbb {R} ^{d}}

 and 





Γ

0


=
I
∈


R


d
×
d





{\displaystyle \textstyle \Gamma _{0}=I\in \mathbb {R} ^{d\times d}}

, the solution of the linear least squares problem given in the previous section can be computed by the following iteration:





Γ

i


=

Γ

i
−
1


−




Γ

i
−
1



x

i



x

i



T




Γ

i
−
1




1
+

x

i



T




Γ

i
−
1



x

i







{\displaystyle \Gamma _{i}=\Gamma _{i-1}-{\frac {\Gamma _{i-1}x_{i}x_{i}^{\mathsf {T}}\Gamma _{i-1}}{1+x_{i}^{\mathsf {T}}\Gamma _{i-1}x_{i}}}}







w

i


=

w

i
−
1


−

Γ

i



x

i



(


x

i



T




w

i
−
1


−

y

i



)



{\displaystyle w_{i}=w_{i-1}-\Gamma _{i}x_{i}\left(x_{i}^{\mathsf {T}}w_{i-1}-y_{i}\right)}


The above iteration algorithm can be proved using induction on 



i


{\displaystyle i}

.[2] The proof also shows that 




Γ

i


=

Σ

i


−
1




{\displaystyle \Gamma _{i}=\Sigma _{i}^{-1}}

. One can look at RLS also in the context of adaptive filters (see RLS).
 The complexity for 



n


{\displaystyle n}

 steps of this algorithm is 



O
(
n

d

2


)


{\displaystyle O(nd^{2})}

, which is an order of magnitude faster than the corresponding batch learning complexity. The storage requirements at every step 



i


{\displaystyle i}

 here are to store the matrix 




Γ

i




{\displaystyle \Gamma _{i}}

, which is constant at 



O
(

d

2


)


{\displaystyle O(d^{2})}

. For the case when 




Σ

i




{\displaystyle \Sigma _{i}}

 is not invertible, consider the regularised version of the problem loss function 




∑

j
=
1


n




(


x

j



T



w
−

y

j



)


2


+
λ


‖
w
‖


2


2




{\displaystyle \sum _{j=1}^{n}\left(x_{j}^{\mathsf {T}}w-y_{j}\right)^{2}+\lambda \left\|w\right\|_{2}^{2}}

. Then, it's easy to show that the same algorithm works with 




Γ

0


=
(
I
+
λ
I

)

−
1




{\displaystyle \Gamma _{0}=(I+\lambda I)^{-1}}

, and the iterations proceed to give 




Γ

i


=
(

Σ

i


+
λ
I

)

−
1




{\displaystyle \Gamma _{i}=(\Sigma _{i}+\lambda I)^{-1}}

.[1]
 When this 





w

i


=

w

i
−
1


−

Γ

i



x

i



(


x

i



T




w

i
−
1


−

y

i



)



{\displaystyle w_{i}=w_{i-1}-\Gamma _{i}x_{i}\left(x_{i}^{\mathsf {T}}w_{i-1}-y_{i}\right)}

 
is replaced by





w

i


=

w

i
−
1


−

γ

i



x

i



(


x

i



T




w

i
−
1


−

y

i



)

=

w

i
−
1


−

γ

i


∇
V
(
⟨

w

i
−
1


,

x

i


⟩
,

y

i


)


{\displaystyle w_{i}=w_{i-1}-\gamma _{i}x_{i}\left(x_{i}^{\mathsf {T}}w_{i-1}-y_{i}\right)=w_{i-1}-\gamma _{i}\nabla V(\langle w_{i-1},x_{i}\rangle ,y_{i})}

 
or 




Γ

i


∈


R


d
×
d




{\displaystyle \Gamma _{i}\in \mathbb {R} ^{d\times d}}

 by 




γ

i


∈

R



{\displaystyle \gamma _{i}\in \mathbb {R} }

, this becomes the stochastic gradient descent algorithm. In this case, the complexity for 



n


{\displaystyle n}

 steps of this algorithm reduces to 



O
(
n
d
)


{\displaystyle O(nd)}

. The storage requirements at every step 



i


{\displaystyle i}

 are constant at 



O
(
d
)


{\displaystyle O(d)}

.
 However, the stepsize 




γ

i




{\displaystyle \gamma _{i}}

 needs to be chosen carefully to solve the expected risk minimization problem, as detailed above. By choosing a decaying step size 




γ

i


≈


1

i



,


{\displaystyle \gamma _{i}\approx {\frac {1}{\sqrt {i}}},}

 one can prove the convergence of the average iterate 






w
¯



n


=


1
n



∑

i
=
1


n



w

i




{\textstyle {\overline {w}}_{n}={\frac {1}{n}}\sum _{i=1}^{n}w_{i}}

. This setting is a special case of stochastic optimization, a well known problem in optimization.[1]
 In practice, one can perform multiple stochastic gradient passes (also called cycles or epochs) over the data. The algorithm thus obtained is called incremental gradient method and corresponds to an iteration





w

i


=

w

i
−
1


−

γ

i


∇
V
(
⟨

w

i
−
1


,

x


t

i




⟩
,

y


t

i




)


{\displaystyle w_{i}=w_{i-1}-\gamma _{i}\nabla V(\langle w_{i-1},x_{t_{i}}\rangle ,y_{t_{i}})}

 
The main difference with the stochastic gradient method is that here a sequence 




t

i




{\displaystyle t_{i}}

 is chosen to decide which training point is visited in the 



i


{\displaystyle i}

-th step. Such a sequence can be stochastic or deterministic. The number of iterations is then decoupled to the number of points (each point can be considered more than once). The incremental gradient method can be shown to provide a minimizer to the empirical risk.[3] Incremental techniques can be advantageous when considering objective functions made up of a sum of many terms e.g. an empirical error corresponding to a very large dataset.[1]
 Kernels can be used to extend the above algorithms to non-parametric models (or models where the parameters form an infinite dimensional space). The corresponding procedure will no longer be truly online and instead involve storing all the data points, but is still faster than the brute force method. This discussion is restricted to the case of the square loss, though it can be extended to any convex loss. It can be shown by an easy induction [1] that if 




X

i




{\displaystyle X_{i}}

 is the data matrix and 




w

i




{\displaystyle w_{i}}

 is the output after 



i


{\displaystyle i}

 steps of the SGD algorithm, then,





w

i


=

X

i



T




c

i




{\displaystyle w_{i}=X_{i}^{\mathsf {T}}c_{i}}

 
where 




c

i


=
(
(

c

i



)

1


,
(

c

i



)

2


,
.
.
.
,
(

c

i



)

i


)
∈


R


i




{\displaystyle c_{i}=((c_{i})_{1},(c_{i})_{2},...,(c_{i})_{i})\in \mathbb {R} ^{i}}

 and the sequence 




c

i




{\displaystyle c_{i}}

 satisfies the recursion:





c

0


=
0


{\displaystyle c_{0}=0}






(

c

i



)

j


=
(

c

i
−
1



)

j


,
j
=
1
,
2
,
.
.
.
,
i
−
1


{\displaystyle (c_{i})_{j}=(c_{i-1})_{j},j=1,2,...,i-1}

 and




(

c

i



)

i


=

γ

i




(



y

i


−

∑

j
=
1


i
−
1


(

c

i
−
1



)

j


⟨

x

j


,

x

i


⟩


)




{\displaystyle (c_{i})_{i}=\gamma _{i}{\Big (}y_{i}-\sum _{j=1}^{i-1}(c_{i-1})_{j}\langle x_{j},x_{i}\rangle {\Big )}}


Notice that here 



⟨

x

j


,

x

i


⟩


{\displaystyle \langle x_{j},x_{i}\rangle }

 is just the standard Kernel on 





R


d




{\displaystyle \mathbb {R} ^{d}}

, and the predictor is of the form 





f

i


(
x
)
=
⟨

w

i
−
1


,
x
⟩
=

∑

j
=
1


i
−
1


(

c

i
−
1



)

j


⟨

x

j


,
x
⟩
.


{\displaystyle f_{i}(x)=\langle w_{i-1},x\rangle =\sum _{j=1}^{i-1}(c_{i-1})_{j}\langle x_{j},x\rangle .}


 
Now, if a general kernel 



K


{\displaystyle K}

 is introduced instead and let the predictor be 





f

i


(
x
)
=

∑

j
=
1


i
−
1


(

c

i
−
1



)

j


K
(

x

j


,
x
)


{\displaystyle f_{i}(x)=\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x)}


then the same proof will also show that predictor minimising the least squares loss is obtained by changing the above recursion to




(

c

i



)

i


=

γ

i




(



y

i


−

∑

j
=
1


i
−
1


(

c

i
−
1



)

j


K
(

x

j


,

x

i


)


)




{\displaystyle (c_{i})_{i}=\gamma _{i}{\Big (}y_{i}-\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x_{i}){\Big )}}


The above expression requires storing all the data for updating 




c

i




{\displaystyle c_{i}}

. The total time complexity for the recursion when evaluating for the 



n


{\displaystyle n}

-th datapoint is 



O
(

n

2


d
k
)


{\displaystyle O(n^{2}dk)}

, where 



k


{\displaystyle k}

 is the cost of evaluating the kernel on a single pair of points.[1] Thus, the use of the kernel has allowed the movement from a finite dimensional parameter space 





w

i


∈


R


d





{\displaystyle \textstyle w_{i}\in \mathbb {R} ^{d}}

 to a possibly infinite dimensional feature represented by a kernel 



K


{\displaystyle K}

 by instead performing the recursion on the space of parameters 





c

i


∈


R


i





{\displaystyle \textstyle c_{i}\in \mathbb {R} ^{i}}

, whose dimension is the same as the size of the training dataset. In general, this is a consequence of the representer theorem.[1]
 Online convex optimization (OCO) [4] is a general framework for decision making which leverages convex optimization to allow for efficient algorithms. The framework is that of repeated game playing as follows:
 For 



t
=
1
,
2
,
.
.
.
,
T


{\displaystyle t=1,2,...,T}


 The goal is to minimize regret, or the difference between cumulative loss and the loss of the best fixed point 



u
∈
S


{\displaystyle u\in S}

 in hindsight. As an example, consider the case of online least squares linear regression. Here, the weight vectors come from the convex set 



S
=


R


d




{\displaystyle S=\mathbb {R} ^{d}}

, and nature sends back the convex loss function 




v

t


(
w
)
=
(
⟨
w
,

x

t


⟩
−

y

t



)

2




{\displaystyle v_{t}(w)=(\langle w,x_{t}\rangle -y_{t})^{2}}

. Note here that 




y

t




{\displaystyle y_{t}}

 is implicitly sent with 




v

t




{\displaystyle v_{t}}

.
 Some online prediction problems however cannot fit in the framework of OCO. For example, in online classification, the prediction domain and the loss functions are not convex. In such scenarios, two simple techniques for convexification are used: randomisation and surrogate loss functions.[citation needed]
 Some simple online convex optimisation algorithms are:
 The simplest learning rule to try is to select (at the current step) the hypothesis that has the least loss over all past rounds. This algorithm is called Follow the leader, and round 



t


{\displaystyle t}

 is simply given by:





w

t


=



a
r
g

m
i
n



w
∈
S


⁡

∑

i
=
1


t
−
1



v

i


(
w
)


{\displaystyle w_{t}=\mathop {\operatorname {arg\,min} } _{w\in S}\sum _{i=1}^{t-1}v_{i}(w)}


This method can thus be looked as a greedy algorithm. For the case of online quadratic optimization (where the loss function is 




v

t


(
w
)
=


‖

w
−

x

t



‖


2


2




{\displaystyle v_{t}(w)=\left\|w-x_{t}\right\|_{2}^{2}}

), one can show a regret bound that grows as 



log
⁡
(
T
)


{\displaystyle \log(T)}

. However, similar bounds cannot be obtained for the FTL algorithm for other important families of models like online linear optimization. To do so, one modifies FTL by adding regularisation.
 This is a natural modification of FTL that is used to stabilise the FTL solutions and obtain better regret bounds. A regularisation function 



R
:
S
→

R



{\displaystyle R:S\to \mathbb {R} }

 is chosen and learning performed in round t as follows:





w

t


=



a
r
g

m
i
n



w
∈
S


⁡

∑

i
=
1


t
−
1



v

i


(
w
)
+
R
(
w
)


{\displaystyle w_{t}=\mathop {\operatorname {arg\,min} } _{w\in S}\sum _{i=1}^{t-1}v_{i}(w)+R(w)}


As a special example, consider the case of online linear optimisation i.e. where nature sends back loss functions of the form 




v

t


(
w
)
=
⟨
w
,

z

t


⟩


{\displaystyle v_{t}(w)=\langle w,z_{t}\rangle }

. Also, let 



S
=


R


d




{\displaystyle S=\mathbb {R} ^{d}}

. Suppose the regularisation function 



R
(
w
)
=


1

2
η





‖
w
‖


2


2




{\textstyle R(w)={\frac {1}{2\eta }}\left\|w\right\|_{2}^{2}}

 is chosen for some positive number 



η


{\displaystyle \eta }

. Then, one can show that the regret minimising iteration becomes 





w

t
+
1


=
−
η

∑

i
=
1


t



z

i


=

w

t


−
η

z

t




{\displaystyle w_{t+1}=-\eta \sum _{i=1}^{t}z_{i}=w_{t}-\eta z_{t}}


Note that this can be rewritten as 




w

t
+
1


=

w

t


−
η
∇

v

t


(

w

t


)


{\displaystyle w_{t+1}=w_{t}-\eta \nabla v_{t}(w_{t})}

, which looks exactly like online gradient descent.
 If S is instead some convex subspace of 





R


d




{\displaystyle \mathbb {R} ^{d}}

, S would need to be projected onto, leading to the modified update rule





w

t
+
1


=

Π

S


(
−
η

∑

i
=
1


t



z

i


)
=

Π

S


(
η

θ

t
+
1


)


{\displaystyle w_{t+1}=\Pi _{S}(-\eta \sum _{i=1}^{t}z_{i})=\Pi _{S}(\eta \theta _{t+1})}


This algorithm is known as lazy projection, as the vector 




θ

t
+
1




{\displaystyle \theta _{t+1}}

 accumulates the gradients. It is also known as Nesterov's dual averaging algorithm. In this scenario of linear loss functions and quadratic regularisation, the regret is bounded by 



O
(


T


)


{\displaystyle O({\sqrt {T}})}

, and thus the average regret goes to 0 as desired.
 The above proved a regret bound for linear loss functions 




v

t


(
w
)
=
⟨
w
,

z

t


⟩


{\displaystyle v_{t}(w)=\langle w,z_{t}\rangle }

. To generalise the algorithm to any convex loss function, the subgradient 



∂

v

t


(

w

t


)


{\displaystyle \partial v_{t}(w_{t})}

 of 




v

t




{\displaystyle v_{t}}

 is used as a linear approximation to 




v

t




{\displaystyle v_{t}}

 near 




w

t




{\displaystyle w_{t}}

, leading to the online subgradient descent algorithm:
 Initialise parameter 



η
,

w

1


=
0


{\displaystyle \eta ,w_{1}=0}


 For 



t
=
1
,
2
,
.
.
.
,
T


{\displaystyle t=1,2,...,T}


 One can use the OSD algorithm to derive 



O
(


T


)


{\displaystyle O({\sqrt {T}})}

 regret bounds for the online version of SVM's for classification, which use the hinge loss




v

t


(
w
)
=
max
{
0
,
1
−

y

t


(
w
⋅

x

t


)
}


{\displaystyle v_{t}(w)=\max\{0,1-y_{t}(w\cdot x_{t})\}}


 Quadratically regularised FTRL algorithms lead to lazily projected gradient algorithms as described above. To use the above for arbitrary convex functions and regularisers, one uses online mirror descent. The optimal regularization in hindsight can be derived for linear loss functions, this leads to the AdaGrad algorithm. For the Euclidean regularisation, one can show a regret bound of 



O
(


T


)


{\displaystyle O({\sqrt {T}})}

, which can be improved further to a 



O
(
log
⁡
T
)


{\displaystyle O(\log T)}

 for strongly convex and exp-concave loss functions.
 Continual learning means constantly improving the learned model by processing continuous streams of information.[5] Continual learning capabilities are essential for software systems and autonomous agents interacting in an ever changing real world. However, continual learning is a challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting. 
 The paradigm of online learning has different interpretations depending on the choice of the learning model, each of which has distinct implications about the predictive quality of the sequence of functions 




f

1


,

f

2


,
…
,

f

n




{\displaystyle f_{1},f_{2},\ldots ,f_{n}}

. The prototypical stochastic gradient descent algorithm is used for this discussion. As noted above, its recursion is given by





w

t


=

w

t
−
1


−

γ

t


∇
V
(
⟨

w

t
−
1


,

x

t


⟩
,

y

t


)


{\displaystyle w_{t}=w_{t-1}-\gamma _{t}\nabla V(\langle w_{t-1},x_{t}\rangle ,y_{t})}


 The first interpretation consider the stochastic gradient descent method as applied to the problem of minimizing the expected risk 



I
[
w
]


{\displaystyle I[w]}

 defined above.[6] Indeed, in the case of an infinite stream of data, since the examples 



(

x

1


,

y

1


)
,
(

x

2


,

y

2


)
,
…


{\displaystyle (x_{1},y_{1}),(x_{2},y_{2}),\ldots }

 are assumed to be drawn i.i.d. from the distribution 



p
(
x
,
y
)


{\displaystyle p(x,y)}

, the sequence of gradients of 



V
(
⋅
,
⋅
)


{\displaystyle V(\cdot ,\cdot )}

 in the above iteration are an i.i.d. sample of stochastic estimates of the gradient of the expected risk 



I
[
w
]


{\displaystyle I[w]}

 and therefore one can apply complexity results for the stochastic gradient descent method to bound the deviation 



I
[

w

t


]
−
I
[

w

∗


]


{\displaystyle I[w_{t}]-I[w^{\ast }]}

, where 




w

∗




{\displaystyle w^{\ast }}

 is the minimizer of 



I
[
w
]


{\displaystyle I[w]}

.[7] This interpretation is also valid in the case of a finite training set; although with multiple passes through the data the gradients are no longer independent, still complexity results can be obtained in special cases.
 The second interpretation applies to the case of a finite training set and considers the SGD algorithm as an instance of incremental gradient descent method.[3] In this case, one instead looks at the empirical risk:





I

n


[
w
]
=


1
n



∑

i
=
1


n


V
(
⟨
w
,

x

i


⟩
,

y

i


)
 
.


{\displaystyle I_{n}[w]={\frac {1}{n}}\sum _{i=1}^{n}V(\langle w,x_{i}\rangle ,y_{i})\ .}


Since the gradients of 



V
(
⋅
,
⋅
)


{\displaystyle V(\cdot ,\cdot )}

 in the incremental gradient descent iterations are also stochastic estimates of the gradient of 




I

n


[
w
]


{\displaystyle I_{n}[w]}

, this interpretation is also related to the stochastic gradient descent method, but applied to minimize the empirical risk as opposed to the expected risk. Since this interpretation concerns the empirical risk and not the expected risk, multiple passes through the data are readily allowed and actually lead to tighter bounds on the deviations 




I

n


[

w

t


]
−

I

n


[

w

n


∗


]


{\displaystyle I_{n}[w_{t}]-I_{n}[w_{n}^{\ast }]}

, where 




w

n


∗




{\displaystyle w_{n}^{\ast }}

 is the minimizer of 




I

n


[
w
]


{\displaystyle I_{n}[w]}

.
 Learning paradigms
 General algorithms
 Learning models
",comput scienc onlin machin learn method machin learn data becom avail sequenti order use updat best predictor futur data step oppos batch learn techniqu gener best predictor learn entir train data set onlin learn common techniqu use area machin learn comput infeas train entir dataset requir need algorithm also use situat necessari algorithm dynam adapt new pattern data data gener function time predict price financi intern market onlin learn algorithm may prone catastroph interfer problem address increment learn approach set supervis learn function f x f learn x x thought space input space output predict well instanc drawn joint probabl distribut p x p x x realiti learner never know true distribut p x p x instanc instead learner usual access train set exampl x x n n n n set loss function given v r v r v f x v f x measur differ predict valu f x f x true valu ideal goal select function f h h h h space function call hypothesi space notion total loss minim depend type model statist adversari one devis differ notion loss lead differ learn algorithm statist learn model train sampl x assum drawn true distribut p x p x object minim expect risk f e v f x v f x p x f e v f x v f x dp x common paradigm situat estim function f f empir risk minim regular empir risk minim usual tikhonov regular choic loss function give rise sever learn algorithm regular least squar support vector machin pure onlin model categori would learn base new input x current best predictor f extra store inform usual expect storag requir independ train data size mani formul exampl nonlinear kernel method true onlin learn possibl though form hybrid onlin learn recurs algorithm use f permit depend f previou data point x x case space requir longer guarante constant sinc requir store previou data point solut may take less time comput addit new data point compar batch learn techniqu common strategi overcom issu learn use process small batch b data point time consid learn b b much smaller total number train point techniqu use repeat pass train data obtain optim version machin learn algorithm exampl stochast gradient descent combin backpropag current de facto train method train artifici neural network simpl exampl linear least squar use explain varieti idea onlin learn idea gener enough appli set exampl convex loss function consid set supervis learn f f linear function learn f x j w x j w x j f j w j j x j r j r vector input data point w r r linear filter vector goal comput filter vector w w end squar loss function v f x j j f x j j w x j j v f j j f j j w j j use comput vector w w minim empir loss n w j n v w x j j j n x j w j n w n v w j j n j j j r j r let x x data matrix r r column vector target valu arriv first data point assum covari matrix σ x x x invert otherwis preferenti proceed similar fashion tikhonov regular best solut f x w x x linear least squar problem given w x x x σ j x j j x j j calcul covari matrix σ j x j x j j j take time invert matrix take time rest multipl take time give total time n n total point dataset recomput solut arriv everi datapoint n n naiv approach total complex n n note store matrix σ updat step need ad x x take time reduc total time n n n addit storag space store σ recurs least squar rl algorithm consid onlin approach least squar problem shown initialis w r r γ r r solut linear least squar problem given previou section comput follow iter γ γ γ x x γ x γ x w w γ x x w iter algorithm prove use induct proof also show γ σ one look rl also context adapt filter see rl complex n n step algorithm n order magnitud faster correspond batch learn complex storag requir everi step store matrix γ constant case σ invert consid regularis version problem loss function j n x j w j λ w n j j easi show algorithm work γ λ iter proceed give γ σ λ w w γ x x w replac w w γ x x w w γ v w x v γ r r γ r r becom stochast gradient descent algorithm case complex n n step algorithm reduc n nd storag requir everi step constant howev stepsiz γ need chosen care solv expect risk minim problem detail choos decay step size γ one prove converg averag iter w n n n w w n n n set special case stochast optim well known problem optim practic one perform multipl stochast gradient pass also call cycl epoch data algorithm thu obtain call increment gradient method correspond iter w w γ v w x v main differ stochast gradient method sequenc chosen decid train point visit step sequenc stochast determinist number iter decoupl number point point consid increment gradient method shown provid minim empir risk increment techniqu advantag consid object function made sum mani term empir error correspond larg dataset kernel use extend algorithm model model paramet form infinit dimension space correspond procedur longer truli onlin instead involv store data point still faster brute forc method discuss restrict case squar loss though extend convex loss shown easi induct x data matrix w output step sgd algorithm w x c c c c c r r sequenc c satisfi recurs c c j c j j j j c γ j c j x j x j j notic x j x j standard kernel r r predictor form f x w x j c j x j x x j j gener kernel k k introduc instead let predictor f x j c j k x j x x j k j x proof also show predictor minimis least squar loss obtain chang recurs c γ j c j k x j x j k j express requir store data updat c total time complex recurs evalu n n datapoint n k dk k k cost evalu kernel singl pair point thu use kernel allow movement finit dimension paramet space w r r possibl infinit dimension featur repres kernel k k instead perform recurs space paramet c r r whose dimens size train dataset gener consequ represent theorem onlin convex optim oco gener framework decis make leverag convex optim allow effici algorithm framework repeat game play follow goal minim regret differ cumul loss loss best fix point u hindsight exampl consid case onlin least squar linear regress weight vector come convex set r r natur send back convex loss function v w w x w w note implicitli sent v onlin predict problem howev fit framework oco exampl onlin classif predict domain loss function convex scenario two simpl techniqu convexif use randomis surrog loss function citat need simpl onlin convex optimis algorithm simplest learn rule tri select current step hypothesi least loss past round algorithm call follow leader round simpli given w r g n w v w min w method thu look greedi algorithm case onlin quadrat optim loss function v w w x w one show regret bound grow log howev similar bound obtain ftl algorithm import famili model like onlin linear optim one modifi ftl ad regularis natur modif ftl use stabilis ftl solut obtain better regret bound regularis function r r r r chosen learn perform round follow w r g n w v w r w min w w special exampl consid case onlin linear optimis natur send back loss function form v w w z w w also let r r suppos regularis function r w η w r w chosen posit number η one show regret minimis iter becom w η z w η z note rewritten w w η v w look exactli like onlin gradient descent instead convex subspac r r would need project onto lead modifi updat rule w π η z π η θ algorithm known lazi project vector θ accumul gradient also known nesterov dual averag algorithm scenario linear loss function quadrat regularis regret bound thu averag regret goe desir prove regret bound linear loss function v w w z w w generalis algorithm convex loss function subgradi v w v use linear approxim v near w lead onlin subgradi descent algorithm initialis paramet η w one use osd algorithm deriv regret bound onlin version svm classif use hing loss v w max w x w quadrat regularis ftrl algorithm lead lazili project gradient algorithm describ use arbitrari convex function regularis one use onlin mirror descent optim regular hindsight deriv linear loss function lead adagrad algorithm euclidean regularis one show regret bound improv log strongli convex loss function continu learn mean constantli improv learn model process continu stream inform continu learn capabl essenti softwar system autonom agent interact ever chang real world howev continu learn challeng machin learn neural network model sinc continu acquisit increment avail inform data distribut gener lead catastroph forget paradigm onlin learn differ interpret depend choic learn model distinct implic predict qualiti sequenc function f f f n n prototyp stochast gradient descent algorithm use discuss note recurs given w w γ v w x v first interpret consid stochast gradient descent method appli problem minim expect risk w w defin inde case infinit stream data sinc exampl x x assum drawn distribut p x p x sequenc gradient v v iter sampl stochast estim gradient expect risk w w therefor one appli complex result stochast gradient descent method bound deviat w w w minim w w interpret also valid case finit train set although multipl pass data gradient longer independ still complex result obtain special case second interpret appli case finit train set consid sgd algorithm instanc increment gradient descent method case one instead look empir risk n w n n v w x n w n n v w sinc gradient v v increment gradient descent iter also stochast estim gradient n w n w interpret also relat stochast gradient descent method appli minim empir risk oppos expect risk sinc interpret concern empir risk expect risk multipl pass data readili allow actual lead tighter bound deviat n w n w n n n n w n n minim n w n w learn paradigm gener algorithm learn model
Curriculum learning,https://en.wikipedia.org/wiki/Curriculum_learning,"Curriculum learning is a technique in machine learning in which a model is trained on examples of increasing difficulty, where the definition of ""difficulty"" may be provided externally or discovered automatically as part of the training process. This is intended to attain good performance more quickly, or to converge to a better local optimum if the global optimum is not found.[1][2]
 Most generally, curriculum learning  is the technique of successively increasing the difficulty of examples in the training set that is presented to a model over multiple training iterations. This can produce better results than exposing the model to the full training set immediately under some circumstances; most typically, when the model is able to learn general principles from easier examples, and then gradually incorporate more complex and nuanced information as harder examples are introduced, such as edge cases. This has been shown to work in many domains, most likely as a form of regularization.[3]
 There are several major variations in how the technique is applied:
 Since curriculum learning only concerns the selection and ordering of training data, it can be combined with many other techniques in machine learning. The success of the method assumes that a model trained for an easier version of the problem can generalize to harder versions, so it can be seen as a form of transfer learning. Some authors also consider curriculum learning to include other forms of progressively increasing complexity, such as increasing the number of model parameters.[11] It is frequently combined with reinforcement learning, such as learning a simplified version of a game first.[12]
 Some domains have shown success with anti-curriculum learning: training on the most difficult examples first. One example is the ACCAN method for speech recognition, which trains on the examples with the lowest signal-to-noise ratio first.[13]
 The term ""curriculum learning"" was introduced by Yoshua Bengio et al in 2009,[14] with reference to the psychological technique of shaping in animals and structured education for humans: beginning with the simplest concepts and then building on them. The authors also note that the application of this technique in machine learning has its roots in the early study of neural networks such as Jeffrey Elman's 1993 paper Learning and development in neural networks: the importance of starting small. [15] Bengio et al showed good results for problems in image classification, such as identifying geometric shapes with progressively more complex forms, and language modeling, such as training with a gradually expanding vocabulary. They conclude that, for curriculum strategies, ""their beneficial effect is most pronounced on the test
set"", suggesting good generalization.
 The technique has since been applied to many other domains:
",curriculum learn techniqu machin learn model train exampl increas difficulti definit difficulti may provid extern discov automat part train process intend attain good perform quickli converg better local optimum global optimum found gener curriculum learn techniqu success increas difficulti exampl train set present model multipl train iter produc better result expos model full train set immedi circumst typic model abl learn gener principl easier exampl gradual incorpor complex nuanc inform harder exampl introduc edg case shown work mani domain like form regular sever major variat techniqu appli sinc curriculum learn concern select order train data combin mani techniqu machin learn success method assum model train easier version problem gener harder version seen form transfer learn author also consid curriculum learn includ form progress increas complex increas number model paramet frequent combin reinforc learn learn simplifi version game first domain shown success learn train difficult exampl first one exampl accan method speech recognit train exampl lowest ratio first term curriculum learn introduc yoshua bengio et al refer psycholog techniqu shape anim structur educ human begin simplest concept build author also note applic techniqu machin learn root earli studi neural network jeffrey elman paper learn develop neural network import start small bengio et al show good result problem imag classif identifi geometr shape progress complex form languag model train gradual expand vocabulari conclud curriculum strategi benefici effect pronounc test set suggest good gener techniqu sinc appli mani domain
Rule-based machine learning,https://en.wikipedia.org/wiki/Rule-based_machine_learning,"Rule-based machine learning (RBML) is a term in computer science intended to encompass any machine learning method that identifies, learns, or evolves 'rules' to store, manipulate or apply.[1][2][3] The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system.
 Rule-based machine learning approaches include learning classifier systems,[4] association rule learning,[5] artificial immune systems,[6] and any other method that relies on a set of rules, each covering contextual knowledge.
 While rule-based machine learning is conceptually a type of rule-based system, it is distinct from traditional rule-based systems, which are often hand-crafted, and other rule-based decision makers. This is because rule-based machine learning applies some form of learning algorithm to automatically identify useful rules, rather than a human needing to apply prior domain knowledge to manually construct rules and curate a rule set.
 Rules typically take the form of an '{IF:THEN} expression', (e.g. {IF 'condition' THEN 'result'}, or as a more specific example, {IF 'red' AND 'octagon' THEN 'stop-sign}). An individual rule is not in itself a model, since the rule is only applicable when its condition is satisfied. Therefore rule-based machine learning methods typically comprise a set of rules, or knowledge base, that collectively make up the prediction model.
",machin learn rbml term comput scienc intend encompass machin learn method identifi learn evolv store manipul appli defin characterist machin learner identif util set relat rule collect repres knowledg captur system machin learn approach includ learn classifi system associ rule learn artifici immun system method reli set rule cover contextu knowledg machin learn conceptu type system distinct tradit system often decis maker machin learn appli form learn algorithm automat identifi use rule rather human need appli prior domain knowledg manual construct rule curat rule set rule typic take form express specif exampl individu rule model sinc rule applic condit satisfi therefor machin learn method typic compris set rule knowledg base collect make predict model
Neuro-symbolic AI,https://en.wikipedia.org/wiki/Neuro-symbolic_AI,"Neuro-symbolic AI is a type of artificial intelligence that integrates neural and symbolic AI architectures to address the weaknesses of each, providing a robust AI capable of reasoning, learning, and cognitive modeling. As argued by Leslie Valiant[1] and others,[2][3] the effective construction of rich computational cognitive models demands the combination of symbolic reasoning and efficient machine learning. Gary Marcus argued, ""We cannot construct rich cognitive models in an adequate, automated way without the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated techniques for reasoning.""[4] Further, ""To build a robust, knowledge-driven approach to AI we must have the machinery of symbol manipulation in our toolkit. Too much useful knowledge is abstract to proceed without tools that represent and manipulate abstraction, and to date, the only known machinery that can manipulate such abstract knowledge reliably is the apparatus of symbol manipulation.""[5]
 Henry Kautz,[6] Francesca Rossi,[7] and Bart Selman[8] also argued for a synthesis. Their arguments attempt to address the two kinds of thinking, as discussed in Daniel Kahneman's book Thinking Fast and Slow. It describes cognition as encompassing two components: System 1 is fast, reflexive, intuitive, and unconscious. System 2 is slower, step-by-step, and explicit. System 1 is used for pattern recognition. System 2 handles planning, deduction, and deliberative thinking. In this view, deep learning best handles the first kind of cognition while symbolic reasoning best handles the second kind. Both are needed for a robust, reliable AI that can learn, reason, and interact with humans to accept advice and answer questions. Such dual-process models with explicit references to the two contrasting systems have been worked on since the 1990s, both in AI and in Cognitive Science, by multiple researchers.[9]
 Approaches for integration are diverse.[10] Henry Kautz's taxonomy of neuro-symbolic architectures[11] follows, along with some examples:
 These categories are not exhaustive, as they do not consider multi-agent systems. In 2005, Bader and Hitzler presented a more fine-grained categorization that considered, e.g., whether the use of symbols included logic and if it did, whether the logic was propositional or first-order logic.[15] The 2005 categorization and Kautz's taxonomy above are compared and contrasted in a 2021 article.[11] Recently, Sepp Hochreiter argued that Graph Neural Networks ""...are the predominant models of neural-symbolic computing""[16] since ""[t]hey describe the properties of molecules, simulate social networks, or predict future states in physical and engineering applications with particle-particle interactions.""[17]
 Gary Marcus argues that ""...hybrid architectures that combine learning and symbol manipulation are necessary for robust intelligence, but not sufficient"",[18] and that there are
 ...four cognitive prerequisites for building robust artificial intelligence: 
 This echoes earlier calls for hybrid models as early as the 1990s.[20][21]
 Garcez and Lamb described research in this area as ongoing at least since the 1990s.[22][23] At that time, the terms symbolic and sub-symbolic AI were popular.
 A series of workshops on neuro-symbolic AI has been held annually since 2005 Neuro-Symbolic Artificial Intelligence.[24] In the early 1990s, an initial set of workshops on this topic were organized.[20]
 Key research questions remain,[25] such as:
 Implementations of neuro-symbolic approaches include:
",ai type artifici intellig integr neural symbol ai architectur address weak provid robust ai capabl reason learn cognit model argu lesli valiant other effect construct rich comput cognit model demand combin symbol reason effici machin learn gari marcu argu construct rich cognit model adequ autom way without triumvir hybrid architectur rich prior knowledg sophist techniqu reason build robust approach ai must machineri symbol manipul toolkit much use knowledg abstract proceed without tool repres manipul abstract date known machineri manipul abstract knowledg reliabl apparatu symbol manipul henri kautz francesca rossi bart selman also argu synthesi argument attempt address two kind think discuss daniel kahneman book think fast slow describ cognit encompass two compon system fast reflex intuit unconsci system slower explicit system use pattern recognit system handl plan deduct delib think view deep learn best handl first kind cognit symbol reason best handl second kind need robust reliabl ai learn reason interact human accept advic answer question model explicit refer two contrast system work sinc ai cognit scienc multipl research approach integr divers henri kautz taxonomi architectur follow along exampl categori exhaust consid system bader hitzler present categor consid whether use symbol includ logic whether logic proposit logic categor kautz taxonomi compar contrast articl recent sepp hochreit argu graph neural network predomin model comput sinc hey describ properti molecul simul social network predict futur state physic engin applic interact gari marcu argu hybrid architectur combin learn symbol manipul necessari robust intellig suffici four cognit prerequisit build robust artifici intellig echo earlier call hybrid model earli garcez lamb describ research area ongo least sinc time term symbol ai popular seri workshop ai held annual sinc artifici intellig earli initi set workshop topic organ key research question remain implement approach includ
Neuromorphic computing,https://en.wikipedia.org/wiki/Neuromorphic_engineering,"
 Neuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.[1][2] A neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.[3][4] In recent times, the term neuromorphic has been used to describe analog, digital, mixed-mode analog/digital VLSI, and software systems that implement models of neural systems (for perception, motor control, or multisensory integration). Recent advances have even discovered ways to mimic the human nervous system through liquid solutions of chemical systems.[5]
 A key aspect of neuromorphic engineering is understanding how the morphology of individual neurons, circuits, applications, and overall architectures creates desirable computations, affects how information is represented, influences robustness to damage, incorporates learning and development, adapts to local change (plasticity), and facilitates evolutionary change.
 Neuromorphic engineering is an interdisciplinary subject that takes inspiration from biology, physics, mathematics, computer science, and electronic engineering[4] to design artificial neural systems, such as vision systems, head-eye systems, auditory processors, and autonomous robots, whose physical architecture and design principles are based on those of biological nervous systems.[6] One of the first applications for neuromorphic engineering was proposed by Carver Mead[7] in the late 1980s.
 Neuromorphic engineering is for now set apart by the inspiration it takes from what we know about the structure and operations of the brain. Neuromorphic engineering translates what we know about the brain's function into computer systems. Work has mostly focused on replicating the analog nature of biological computation and the role of neurons in cognition.
 The biological processes of neurons and their synapses are dauntingly complex, and thus very difficult to artificially simulate. A key feature of biological brains is that all of the processing in neurons uses analog chemical signals. This makes it hard to replicate brains in computers because the current generation of computers is completely digital. However, the characteristics of these  chemical signals can be abstracted into mathematical functions that closely capture the essence of the neuron's operations.
 The goal of neuromorphic computing is not to perfectly mimic the brain and all of its functions, but instead to extract what is known of its structure and operations to be used in a practical computing system. No neuromorphic system will claim nor attempt to reproduce every element of neurons and synapses, but all adhere to the idea that computation is highly distributed throughout a series of small computing elements analogous to a neuron. While this sentiment is standard, researchers chase this goal with different methods.[8]
 The implementation of neuromorphic computing on the hardware level can be realized by oxide-based memristors,[9] spintronic memories, threshold switches, transistors,[10][4] among others. The implementation details overlap with the concepts of Reservoir Computation. Training software-based neuromorphic systems of spiking neural networks can be achieved using error backpropagation, e.g. using Python-based frameworks such as snnTorch,[11] or using canonical learning rules from the biological learning literature, e.g. using BindsNet.[12]
 As early as 2006, researchers at Georgia Tech published a field programmable neural array.[13] This chip was the first in a line of increasingly complex arrays of floating gate transistors that allowed programmability of charge on the gates of MOSFETs to model the channel-ion characteristics of neurons in the brain and was one of the first cases of a silicon programmable array of neurons.
 In November 2011, a group of MIT researchers created a computer chip that mimics the analog, ion-based communication in a synapse between two neurons using 400 transistors and standard CMOS manufacturing techniques.[14][15]
 In June 2012, spintronic researchers at Purdue University presented a paper on the design of a neuromorphic chip using lateral spin valves and memristors. They argue that the architecture works similarly to neurons and can therefore be used to test methods of reproducing the brain's processing. In addition, these chips are significantly more energy-efficient than conventional ones.[16]
 Research at HP Labs on Mott memristors has shown that while they can be non-volatile, the volatile behavior exhibited at temperatures significantly below the phase transition temperature can be exploited to fabricate a neuristor,[17] a biologically-inspired device that mimics behavior found in neurons.[17] In September 2013, they presented models and simulations that show how the spiking behavior of these neuristors can be used to form the components required for a Turing machine.[18]
 Neurogrid, built by Brains in Silicon at Stanford University,[19] is an example of hardware designed using neuromorphic engineering principles. The circuit board is composed of 16 custom-designed chips, referred to as NeuroCores. Each NeuroCore's analog circuitry is designed to emulate neural elements for 65536 neurons, maximizing energy efficiency. The emulated neurons are connected using digital circuitry designed to maximize spiking throughput.[20][21]
 A research project with implications for neuromorphic engineering is the Human Brain Project that is attempting to simulate a complete human brain in a supercomputer using biological data. It is made up of a group of researchers in neuroscience, medicine, and computing.[22] Henry Markram, the project's co-director, has stated that the project proposes to establish a foundation to explore and understand the brain and its diseases, and to use that knowledge to build new computing technologies. The three primary goals of the project are to better understand how the pieces of the brain fit and work together, to understand how to objectively diagnose and treat brain diseases and to use the understanding of the human brain to develop neuromorphic computers. Since the simulation of a complete human brain will require a powerful supercomputer,  the current focus on neuromorphic computers is being encouraged.[23] $1.3 billion has been allocated to the project by The European Commission.[24]
 Other research with implications for neuromorphic engineering involve the BRAIN Initiative[25] and the TrueNorth chip from IBM.[26] Neuromorphic devices have also been demonstrated using nanocrystals, nanowires, and conducting polymers.[27] There also is development of a memristive device for quantum neuromorphic architectures.[28] In 2022, researchers at MIT have reported the development of brain-inspired artificial synapses, using the ion proton (H+), for 'analog deep learning'.[29][30]
 Intel unveiled its neuromorphic research chip, called ""Loihi"", in October 2017. The chip uses an asynchronous spiking neural network (SNN) to implement adaptive self-modifying event-driven fine-grained parallel computations used to implement learning and inference with high efficiency.[31][32]
 IMEC, a Belgium-based nanoelectronics research center, demonstrated the world's first self-learning neuromorphic chip. The brain-inspired chip, based on OxRAM technology, has the capability of self-learning and has been demonstrated to have the ability to compose music.[33] IMEC released the 30-second tune composed by the prototype. The chip was sequentially loaded with songs in the same time signature and style. The songs were old Belgian and French flute minuets, from which the chip learned the rules at play and then applied them.[34]
 The Blue Brain Project, led by Henry Markram, aims to build biologically detailed digital reconstructions and simulations of the mouse brain. The Blue Brain Project has created in silico models of rodent brains, while attempting to replicate as many details about its biology as possible. The supercomputer-based simulations offer new perspectives on understanding the structure and functions of the brain.
 The European Union funded a series of projects at the University of Heidelberg, which led to the development of BrainScaleS (brain-inspired multiscale computation in neuromorphic hybrid systems), a hybrid analog neuromorphic supercomputer located at Heidelberg University, Germany. It was developed as part of the Human Brain Project neuromorphic computing platform and is the complement to the SpiNNaker supercomputer (which is based on digital technology). The architecture used in BrainScaleS mimics biological neurons and their connections on a physical level; additionally, since the components are made of silicon, these model neurons operate on average 864 times (24 hours of real time is 100 seconds in the machine simulation) faster than that of their biological counterparts.[35]
 In 2019, the European Union funded the project ""Neuromorphic quantum computing""[36] exploring the use of neuromorphic computing to perform quantum operations. Neuromorphic quantum computing[37] (abbreviated as 'n.quantum computing') is an unconventional computing type of computing that uses neuromorphic computing to perform quantum operations.[38][39] It was suggested that quantum algorithms, which are algorithms that run on a realistic model of quantum computation, can be computed equally efficiently with neuromorphic quantum computing.[40][41][42][43][44] Both, traditional quantum computing and neuromorphic quantum computing are physics-based unconventional computing approaches to computations and do not follow the von Neumann architecture. They both construct a system (a circuit) that represents the physical problem at hand, and then leverage their respective physics properties of the system to seek the ""minimum"". Neuromorphic quantum computing and quantum computing share similar physical properties during computation.[44][45]
 Brainchip announced in October 2021 that it was taking orders for its Akida AI Processor Development Kits[46] and in January 2022 that it was taking orders for its Akida AI Processor PCIe boards,[47] making it the world's first commercially available neuromorphic processor.
 Neuromemristive systems are a subclass of neuromorphic computing systems that focuses on the use of memristors to implement neuroplasticity. While neuromorphic engineering focuses on mimicking biological behavior, neuromemristive systems focus on abstraction.[48] For example, a neuromemristive system may replace the details of a cortical microcircuit's behavior with an abstract neural network model.[49]
 There exist several neuron inspired threshold logic functions[9] implemented with memristors that have applications in high level pattern recognition applications. Some of the applications reported recently include speech recognition,[50] face recognition[51] and object recognition.[52] They also find applications in replacing conventional digital logic gates.[53][54]
 For (quasi)ideal passive memristive circuits, the evolution of the memristive memories can be written in a closed form (Caravelli–Traversa–Di Ventra equation):[55][56]
 as a function of the properties of the physical memristive network and the external sources. The equation is valid for the case of the Williams-Strukov original toy model, as  in the case of ideal memristors, 



α
=
0


{\displaystyle \alpha =0}

. However, the hypothesis of the existence of an ideal memristor is debatable.[57] In the equation above, 



α


{\displaystyle \alpha }

 is the ""forgetting"" time scale constant, typically associated to memory volatility, while 



χ
=




R

off


−

R

on




R

off






{\displaystyle \chi ={\frac {R_{\text{off}}-R_{\text{on}}}{R_{\text{off}}}}}

 is the ratio of off and on values of the limit resistances of the memristors, 






S
→





{\displaystyle {\vec {S}}}

 is the vector of the sources of the circuit and 



Ω


{\displaystyle \Omega }

 is a projector on the fundamental loops of the circuit. The constant 



β


{\displaystyle \beta }

 has the dimension of a voltage and is associated to the properties of the memristor; its physical origin is the charge mobility in the conductor. The diagonal matrix and vector 



X
=
diag
⁡
(



X
→



)


{\displaystyle X=\operatorname {diag} ({\vec {X}})}

 and 






X
→





{\displaystyle {\vec {X}}}

 respectively, are instead the internal value of the memristors, with values between 0 and 1. This equation thus requires adding extra constraints on the memory values in order to be reliable.
 It has been recently shown that the equation above exhibits tunneling phenomena and used to study Lyapunov functions.[58][56]
 The concept of neuromorphic systems can be extended to sensors (not just to computation). An example of this applied to detecting light is the retinomorphic sensor or, when employed in an array, the event camera. An event camera's pixels all register changes in brightness levels individually, which makes these cameras comparable to human eyesight in their theoretical power consumption.[59] In 2022, researchers from the Max Planck Institute for Polymer Research reported an organic artificial spiking neuron that exhibits the signal diversity of biological neurons while operating in the biological wetware, thus enabling in-situ neuromorphic sensing and biointerfacing applications.[60][61]
 The Joint Artificial Intelligence Center, a branch of the U.S. military, is a center dedicated to the procurement and implementation of AI software and neuromorphic hardware for combat use. Specific applications include smart headsets/goggles and robots. JAIC intends to rely heavily on neuromorphic technology to connect ""every sensor (to) every shooter"" within a network of neuromorphic-enabled units.
 While the interdisciplinary concept of neuromorphic engineering is relatively new, many of the same ethical considerations apply to neuromorphic systems as apply to human-like machines and artificial intelligence in general. However, the fact that neuromorphic systems are designed to mimic a human brain gives rise to unique ethical questions surrounding their usage.
 However, the practical debate is that neuromorphic hardware as well as artificial ""neural networks"" are immensely simplified models of how the brain operates or processes information at a much lower complexity in terms of size and functional technology and a much more regular structure in terms of connectivity. Comparing neuromorphic chips to the brain is a very crude comparison similar to comparing a plane to a bird just because they both have wings and a tail. The fact is that biological neural cognitive systems are many orders of magnitude more energy- and compute-efficient than current state-of-the-art AI and neuromorphic engineering is an attempt to narrow this gap by inspiring from the brain's mechanism just like many engineering designs have bio-inspired features.
 Significant ethical limitations may be placed on neuromorphic engineering due to public perception.[62] Special Eurobarometer 382: Public Attitudes Towards Robots, a survey conducted by the European Commission, found that 60% of European Union citizens wanted a ban of robots in the care of children, the elderly, or the disabled. Furthermore, 34% were in favor of a ban on robots in education, 27% in healthcare, and 20% in leisure. The European Commission classifies these areas as notably “human.” The report cites increased public concern with robots that are able to mimic or replicate human functions. Neuromorphic engineering, by definition, is designed to replicate the function of the human brain.[63]
 The social concerns surrounding neuromorphic engineering are likely to become even more profound in the future. The European Commission found that EU citizens between the ages of 15 and 24 are more likely to think of robots as human-like (as opposed to instrument-like) than EU citizens over the age of 55. When presented an image of a robot that had been defined as human-like, 75% of EU citizens aged 15–24 said it corresponded with the idea they had of robots while only 57% of EU citizens over the age of 55 responded the same way. The human-like nature of neuromorphic systems, therefore, could place them in the categories of robots many EU citizens would like to see banned in the future.[63]
 As neuromorphic systems have become increasingly advanced, some scholars[who?] have advocated for granting personhood rights to these systems.  Daniel Lim, a critic of technology development in the Human Brain Project, which aims to advance brain-inspired computing, has argued that advancement in neuromorphic computing could lead to machine consciousness or personhood.[64]  If these systems are to be treated as people, then many tasks humans perform using neuromorphic systems, including their termination, may be morally impermissible as these acts would violate their autonomy.[64]
 There is significant legal debate around property rights and artificial intelligence. In Acohs Pty Ltd v. Ucorp Pty Ltd, Justice Christopher Jessup of the Federal Court of Australia found that the source code for Material Safety Data Sheets could not be copyrighted as it was generated by a software interface rather than a human author.[65] The same question may apply to neuromorphic systems: if a neuromorphic system successfully mimics a human brain and produces a piece of original work, who, if anyone, should be able to claim ownership of the work?[66]
",neuromorph comput approach comput inspir structur function human brain neuromorph devic use physic artifici neuron comput recent time term neuromorph use describ analog digit vlsi softwar system implement model neural system percept motor control multisensori integr recent advanc even discov way mimic human nervou system liquid solut chemic system key aspect neuromorph engin understand morpholog individu neuron circuit applic overal architectur creat desir comput affect inform repres influenc robust damag incorpor learn develop adapt local chang plastic facilit evolutionari chang neuromorph engin interdisciplinari subject take inspir biolog physic mathemat comput scienc electron engin design artifici neural system vision system system auditori processor autonom robot whose physic architectur design principl base biolog nervou system one first applic neuromorph engin propos carver mead late neuromorph engin set apart inspir take know structur oper brain neuromorph engin translat know brain function comput system work mostli focus replic analog natur biolog comput role neuron cognit biolog process neuron synaps dauntingli complex thu difficult artifici simul key featur biolog brain process neuron use analog chemic signal make hard replic brain comput current gener comput complet digit howev characterist chemic signal abstract mathemat function close captur essenc neuron oper goal neuromorph comput perfectli mimic brain function instead extract known structur oper use practic comput system neuromorph system claim attempt reproduc everi element neuron synaps adher idea comput highli distribut throughout seri small comput element analog neuron sentiment standard research chase goal differ method implement neuromorph comput hardwar level realiz memristor spintron memori threshold switch transistor among other implement detail overlap concept reservoir comput train neuromorph system spike neural network achiev use error backpropag use framework snntorch use canon learn rule biolog learn literatur use bindsnet earli research georgia tech publish field programm neural array chip first line increasingli complex array float gate transistor allow programm charg gate mosfet model characterist neuron brain one first case silicon programm array neuron novemb group mit research creat comput chip mimic analog commun synaps two neuron use transistor standard cmo manufactur techniqu june spintron research purdu univers present paper design neuromorph chip use later spin valv memristor argu architectur work similarli neuron therefor use test method reproduc brain process addit chip significantli convent one research hp lab mott memristor shown volatil behavior exhibit temperatur significantli phase transit temperatur exploit fabric neuristor devic mimic behavior found neuron septemb present model simul show spike behavior neuristor use form compon requir ture machin neurogrid built brain silicon stanford univers exampl hardwar design use neuromorph engin principl circuit board compos chip refer neurocor neurocor analog circuitri design emul neural element neuron maxim energi effici emul neuron connect use digit circuitri design maxim spike throughput research project implic neuromorph engin human brain project attempt simul complet human brain supercomput use biolog data made group research neurosci medicin comput henri markram project state project propos establish foundat explor understand brain diseas use knowledg build new comput technolog three primari goal project better understand piec brain fit work togeth understand object diagnos treat brain diseas use understand human brain develop neuromorph comput sinc simul complet human brain requir power supercomput current focu neuromorph comput encourag billion alloc project european commiss research implic neuromorph engin involv brain initi truenorth chip ibm neuromorph devic also demonstr use nanocryst nanowir conduct polym also develop memrist devic quantum neuromorph architectur research mit report develop artifici synaps use ion proton deep learn intel unveil neuromorph research chip call loihi octob chip use asynchron spike neural network snn implement adapt parallel comput use implement learn infer high effici imec nanoelectron research center demonstr world first neuromorph chip chip base oxram technolog capabl demonstr abil compos music imec releas tune compos prototyp chip sequenti load song time signatur style song old belgian french flute minuet chip learn rule play appli blue brain project led henri markram aim build biolog detail digit reconstruct simul mous brain blue brain project creat silico model rodent brain attempt replic mani detail biolog possibl simul offer new perspect understand structur function brain european union fund seri project univers heidelberg led develop brainscal multiscal comput neuromorph hybrid system hybrid analog neuromorph supercomput locat heidelberg univers germani develop part human brain project neuromorph comput platform complement spinnak supercomput base digit technolog architectur use brainscal mimic biolog neuron connect physic level addit sinc compon made silicon model neuron oper averag time hour real time second machin simul faster biolog counterpart european union fund project neuromorph quantum comput explor use neuromorph comput perform quantum oper neuromorph quantum comput abbrevi comput unconvent comput type comput use neuromorph comput perform quantum oper suggest quantum algorithm algorithm run realist model quantum comput comput equal effici neuromorph quantum comput tradit quantum comput neuromorph quantum comput unconvent comput approach comput follow von neumann architectur construct system circuit repres physic problem hand leverag respect physic properti system seek minimum neuromorph quantum comput quantum comput share similar physic properti comput brainchip announc octob take order akida ai processor develop kit januari take order akida ai processor pcie board make world first commerci avail neuromorph processor neuromemrist system subclass neuromorph comput system focus use memristor implement neuroplast neuromorph engin focus mimick biolog behavior neuromemrist system focu abstract exampl neuromemrist system may replac detail cortic microcircuit behavior abstract neural network model exist sever neuron inspir threshold logic function implement memristor applic high level pattern recognit applic applic report recent includ speech recognit face recognit object recognit also find applic replac convent digit logic gate quasi ideal passiv memrist circuit evolut memrist memori written close form ventra equat function properti physic memrist network extern sourc equat valid case origin toy model case ideal memristor α howev hypothesi exist ideal memristor debat equat α forget time scale constant typic associ memori volatil χ r r r ratio valu limit resist memristor vector sourc circuit ω projector fundament loop circuit constant β dimens voltag associ properti memristor physic origin charg mobil conductor diagon matrix vector x diag x diag x x x respect instead intern valu memristor valu equat thu requir ad extra constraint memori valu order reliabl recent shown equat exhibit tunnel phenomena use studi lyapunov function concept neuromorph system extend sensor comput exampl appli detect light retinomorph sensor employ array event camera event camera pixel regist chang bright level individu make camera compar human eyesight theoret power consumpt research max planck institut polym research report organ artifici spike neuron exhibit signal divers biolog neuron oper biolog wetwar thu enabl neuromorph sens biointerfac applic joint artifici intellig center branch militari center dedic procur implement ai softwar neuromorph hardwar combat use specif applic includ smart robot jaic intend reli heavili neuromorph technolog connect everi sensor everi shooter within network unit interdisciplinari concept neuromorph engin rel new mani ethic consider appli neuromorph system appli machin artifici intellig gener howev fact neuromorph system design mimic human brain give rise uniqu ethic question surround usag howev practic debat neuromorph hardwar well artifici neural network immens simplifi model brain oper process inform much lower complex term size function technolog much regular structur term connect compar neuromorph chip brain crude comparison similar compar plane bird wing tail fact biolog neural cognit system mani order magnitud current ai neuromorph engin attempt narrow gap inspir brain mechan like mani engin design featur signific ethic limit may place neuromorph engin due public percept special eurobaromet public attitud toward robot survey conduct european commiss found european union citizen want ban robot care children elderli disabl furthermor favor ban robot educ healthcar leisur european commiss classifi area notabl report cite increas public concern robot abl mimic replic human function neuromorph engin definit design replic function human brain social concern surround neuromorph engin like becom even profound futur european commiss found eu citizen age like think robot oppos eu citizen age present imag robot defin eu citizen age said correspond idea robot eu citizen age respond way natur neuromorph system therefor could place categori robot mani eu citizen would like see ban futur neuromorph system becom increasingli advanc scholar advoc grant personhood right system daniel lim critic technolog develop human brain project aim advanc comput argu advanc neuromorph comput could lead machin conscious personhood system treat peopl mani task human perform use neuromorph system includ termin may moral impermiss act would violat autonomi signific legal debat around properti right artifici intellig acoh pti ltd ucorp pti ltd justic christoph jessup feder court australia found sourc code materi safeti data sheet could copyright gener softwar interfac rather human author question may appli neuromorph system neuromorph system success mimic human brain produc piec origin work anyon abl claim ownership work
Quantum machine learning,https://en.wikipedia.org/wiki/Quantum_machine_learning,"Quantum machine learning is the integration of quantum algorithms within machine learning programs.[1][2][3][4][5][6][7][8]
 The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning.[9][10][11] While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program.[12] This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device.[13][14][15] These routines can be more complex in nature and executed faster on a quantum computer.[7] Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data.[16][17]
 Beyond quantum computing, the term ""quantum machine learning"" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system[18][19] or creating new quantum experiments.[20][21][22]
 Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa.[23][24][25]
 Furthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as ""quantum learning theory"".[26][27]
 Quantum-enhanced machine learning refers to quantum algorithms that solve tasks in machine learning, thereby improving and often expediting classical machine learning techniques. Such algorithms typically require one to encode the given classical data set into a quantum computer to make it accessible for quantum information processing. Subsequently, quantum information processing routines are applied and the result of the quantum computation is read out by measuring the quantum system. For example, the outcome of the measurement of a qubit reveals the result of a binary classification task. While many proposals of quantum machine learning algorithms are still purely theoretical and require a full-scale universal quantum computer to be tested, others have been implemented on small-scale or special purpose quantum devices.
 Associative (or content-addressable memories) are able to recognize stored content on the basis of a similarity measure, rather than fixed addresses, like in random access memories. As such they must be able to retrieve both incomplete and corrupted patterns, the essential machine learning task of pattern recognition.
 Typical classical associative memories store p patterns in the 



O
(

n

2


)


{\displaystyle O(n^{2})}

 interactions (synapses) of a real,  symmetric energy matrix over a network of n artificial neurons. The encoding is such that the desired patterns are local minima of the energy functional and retrieval is done by minimizing the total energy, starting from an initial configuration.
 Unfortunately, classical associative memories are severely limited by the phenomenon of cross-talk. When too many patterns are stored, spurious memories appear which quickly proliferate, so that the energy landscape becomes disordered and no retrieval is anymore possible. The number of storable patterns is typically limited by a linear function of the number of neurons, 



p
≤
O
(
n
)


{\displaystyle p\leq O(n)}

.
 Quantum associative memories[2][3][4] (in their simplest realization) store patterns in a unitary matrix U acting on the Hilbert space of n qubits. Retrieval is realized by the unitary evolution of a fixed initial state to a quantum superposition of the desired patterns with probability distribution peaked on the most similar pattern to an input. By its very quantum nature, the retrieval process is thus probabilistic. Because quantum associative memories are free from cross-talk, however, spurious memories are never generated. Correspondingly, they have a superior capacity than classical ones. The number of parameters in the unitary matrix U is 



O
(
p
n
)


{\displaystyle O(pn)}

. One can thus have efficient, spurious-memory-free quantum associative memories for any polynomial number of patterns.
 A number of quantum algorithms for machine learning are based on the idea of amplitude encoding, that is, to associate the amplitudes of a quantum state with the inputs and outputs of computations.[30][31][32] Since a state of 



n


{\displaystyle n}

 qubits is described by 




2

n




{\displaystyle 2^{n}}

 complex amplitudes, this information encoding can allow for an exponentially compact representation. Intuitively, this corresponds to associating a discrete probability distribution over binary random variables with a classical vector. The goal of algorithms based on amplitude encoding is to formulate quantum algorithms whose resources grow polynomially in the number of qubits 



n


{\displaystyle n}

, which amounts to a logarithmic time complexity in the number of amplitudes and thereby the dimension of the input.
 Many quantum machine learning algorithms in this category are based on variations of the quantum algorithm for linear systems of equations[33] (colloquially called HHL, after the paper's authors) which, under specific conditions, performs a matrix inversion using an amount of physical resources growing only logarithmically in the dimensions of the matrix. One of these conditions is that a Hamiltonian which entry wise corresponds to the matrix can be simulated efficiently, which is known to be possible if the matrix is sparse[34] or low rank.[35] For reference, any known classical algorithm for matrix inversion requires a number of operations that grows more than quadratically in the dimension of the matrix (e.g. 



O



(

n

2.373


)





{\displaystyle O{\mathord {\left(n^{2.373}\right)}}}

), but they are not restricted to sparse matrices.
 Quantum matrix inversion can be applied to machine learning methods in which the training reduces to solving a linear system of equations, for example in least-squares linear regression,[31][32] the least-squares version of support vector machines,[30] and Gaussian processes.[36]
 A crucial bottleneck of methods that simulate linear algebra computations with the amplitudes of quantum states is state preparation, which often requires one to initialise a quantum system in a state whose amplitudes reflect the features of the entire dataset. Although efficient methods for state preparation are known for specific cases,[37][38] this step easily hides the complexity of the task.[39][40]
 VQAs are one of the most studied classes of quantum algorithms, as modern research demonstrates their applicability to the vast majority of known major applications of the quantum computer, and they appear to be a leading hope for gaining quantum supremacy.[41]  VQAs are a mixed quantum-classical approach where the quantum processor prepares quantum states and measurement is made and the optimization is done by a classical computer. VQAs are considered best for NISQ as VQAs are noise tolerant compared to other algorithms and give quantum superiority with only a few hundred qubits. Researchers have studied circuit-based algorithms to solve optimization problems and find the ground state energy of complex systems, which were difficult to solve or required a large time to perform the computation using a classical computer.[42][43]
 Variational Quantum Circuits also known as Parametrized Quantum Circuits (PQCs) are based on Variational Quantum Algorithms (VQAs). VQCs consist of three parts: preparation of initial states, quantum circuit, and measurement. Researchers are extensively studying VQCs, as it uses the power of quantum computation to learn in a short time and also use fewer parameters than its classical counterparts. It is theoretically and numerically proven that we can approximate non-linear functions, like those used in neural networks, on quantum circuits. Due to VQCs superiority, neural network has been replaced by VQCs in Reinforcement Learning tasks and Generative Algorithms. The intrinsic nature of quantum devices towards decoherence, random gate error and measurement errors caused to have high potential to limit the training of the variation circuits. Training the VQCs on the classical devices before employing them on quantum devices helps to overcome the problem of decoherence noise that came through the number of repetitions for training.[44][45][46]
 Pattern reorganization is one of the important tasks of machine learning, binary classification is one of the tools or algorithms to find patterns. Binary classification is used in supervised learning and in unsupervised learning. In quantum machine learning, classical bits are converted to qubits and they are mapped to Hilbert space; complex value data are used in a quantum binary classifier to use the advantage of Hilbert space.[47][48] By exploiting the quantum mechanic properties such as superposition, entanglement, interference the quantum binary classifier produces the accurate result in short period of time.[49]
 Another approach to improving classical machine learning with quantum information processing uses amplitude amplification methods based on Grover's search algorithm, which has been shown to solve unstructured search problems with a quadratic speedup compared to classical algorithms. These quantum routines can be employed for learning algorithms that translate into an unstructured search task, as can be done, for instance, in the case of the k-medians[50] and the k-nearest neighbors algorithms.[9] Other applications include quadratic speedups in the training of perceptron[51] and the computation of attention.[52]
 An example of amplitude amplification being used in a machine learning algorithm is Grover's search algorithm minimization. In which a subroutine uses Grover's search algorithm to find an element less than some previously defined element. This can be done with an oracle that determines whether or not a state with a corresponding element is less than the predefined one. Grover's algorithm can then find an element such that our condition is met. The minimization is initialized by some random element in our data set, and iteratively does this subroutine to find the minimum element in the data set. This minimization is notably used in quantum k-medians, and it has a speed up of at least 





O



(



n
k



)



{\displaystyle {\mathcal {O}}\left({\sqrt {\frac {n}{k}}}\right)}

 compared to classical versions of k-medians, where 



n


{\displaystyle n}

 is the number of data points and 



k


{\displaystyle k}

 is the number of clusters.[50]
 Amplitude amplification is often combined with quantum walks to achieve the same quadratic speedup. Quantum walks have been proposed to enhance Google's PageRank algorithm[53] as well as the performance of reinforcement learning agents in the projective simulation framework.[54]
 Reinforcement learning is a branch of machine learning distinct from supervised and unsupervised learning, which also admits quantum enhancements.[55][54][56] In quantum-enhanced reinforcement learning, a quantum agent interacts with a classical or quantum environment and occasionally receives rewards for its actions, which allows the agent to adapt its behavior—in other words, to learn what to do in order to gain more rewards. In some situations, either because of the quantum processing capability of the agent,[54] or due to the possibility to probe the environment in superpositions,[29] a quantum speedup may be achieved. Implementations of these kinds of protocols have been proposed for systems of trapped ions[57] and superconducting circuits.[58] A quantum speedup of the agent's internal decision-making time[54] has been experimentally demonstrated in trapped ions,[59] while a quantum speedup of the learning time in a fully coherent (`quantum') interaction between agent and environment has been experimentally realized in a photonic setup.[60]
 Quantum annealing is an optimization technique used to determine the local minima and maxima of a function over a given set of candidate functions. This is a method of discretizing a function with many local minima or maxima in order to determine the observables of the function. The process can be distinguished from Simulated annealing by the Quantum tunneling process, by which particles tunnel through kinetic or potential barriers from a high state to a low state. Quantum annealing starts from a superposition of all possible states of a system, weighted equally. Then the time-dependent Schrödinger equation guides the time evolution of the system, serving to affect the amplitude of each state as time increases. Eventually, the ground state can be reached to yield the instantaneous Hamiltonian of the system.
 As the depth of the quantum circuit advances on NISQ devices, the noise level rises, posing a significant challenge to accurately computing costs and gradients on training models. The noise tolerance will be improved by using the quantum perceptron and the quantum algorithm on the currently accessible quantum hardware.[citation needed]
 A regular connection of similar components known as neurons forms the basis of even the most complex brain networks. Typically, a neuron has two operations: the inner product and an activation function. As opposed to the activation function, which is typically nonlinear, the inner product is a linear process. With quantum computing, linear processes may be easily accomplished additionally,  due to the simplicity of implementation, the threshold function is preferred by the majority of quantum neurons for activation functions.[citation needed]
 Sampling from high-dimensional probability distributions is at the core of a wide spectrum of computational techniques with important applications across science, engineering, and society. Examples include deep learning, probabilistic programming, and other machine learning and artificial intelligence applications.
 A computationally hard problem, which is key for some relevant machine learning tasks, is the estimation of averages over probabilistic models defined in terms of a Boltzmann distribution. Sampling from generic probabilistic models is hard: algorithms relying heavily on sampling are expected to remain intractable no matter how large and powerful classical computing resources become. Even though quantum annealers, like those produced by D-Wave Systems, were designed for challenging combinatorial optimization problems, it has been recently recognized as a potential candidate to speed up computations that rely on sampling by exploiting quantum effects.[61]
 Some research groups have recently explored the use of quantum annealing hardware for training Boltzmann machines and deep neural networks.[62][63][64] The standard approach to training Boltzmann machines relies on the computation of certain averages that can be estimated by standard sampling techniques, such as Markov chain Monte Carlo algorithms. Another possibility is to rely on a physical process, like quantum annealing, that naturally generates samples from a Boltzmann distribution. The objective is to find the optimal control parameters that best represent the empirical distribution of a given dataset.
 The D-Wave 2X system hosted at NASA Ames Research Center has been recently used for the learning of a special class of restricted Boltzmann machines that can serve as a building block for deep learning architectures.[63] Complementary work that appeared roughly simultaneously showed that quantum annealing can be used for supervised learning in classification tasks.[62] The same device was later used to train a fully connected Boltzmann machine to generate, reconstruct, and classify down-scaled, low-resolution handwritten digits, among other synthetic datasets.[65] In both cases, the models trained by quantum annealing had a similar or better performance in terms of quality. The ultimate question that drives this endeavour is whether there is quantum speedup in sampling applications. Experience with the use of quantum annealers for combinatorial optimization suggests the answer is not straightforward. Reverse annealing has been used as well to solve a fully connected quantum restricted Boltzmann machine.[66]
 Inspired by the success of Boltzmann machines based on classical Boltzmann distribution, a new machine learning approach based on quantum Boltzmann distribution of a transverse-field Ising Hamiltonian was recently proposed.[67] Due to the non-commutative nature of quantum mechanics, the training process of the quantum Boltzmann machine can become nontrivial. This problem was, to some extent, circumvented by introducing bounds on the quantum probabilities, allowing the authors to train the model efficiently by sampling. It is possible that a specific type of quantum Boltzmann machine has been trained in the D-Wave 2X by using a learning rule analogous to that of classical Boltzmann machines.[65][64][68]
 Quantum annealing is not the only technology for sampling. In a prepare-and-measure scenario, a universal quantum computer prepares a thermal state, which is then sampled by measurements. This can reduce the time required to train a deep restricted Boltzmann machine, and provide a richer and more comprehensive framework for deep learning than classical computing.[69] The same quantum methods also permit efficient training of full Boltzmann machines and multi-layer, fully connected models and do not have well-known classical counterparts. Relying on an efficient thermal state preparation protocol starting from an arbitrary state, quantum-enhanced Markov logic networks exploit the symmetries and the locality structure of the probabilistic graphical model generated by a first-order logic template.[70][19] This provides an exponential reduction in computational complexity in probabilistic inference, and, while the protocol relies on a universal quantum computer, under mild assumptions it can be embedded on contemporary quantum annealing hardware.
 Quantum analogues or generalizations of classical neural nets are often referred to as quantum neural networks. The term is claimed by a wide range of approaches, including the implementation and extension of neural networks using photons, layered variational circuits or quantum Ising-type models. Quantum neural networks are often defined as an expansion on Deutsch's model of a quantum computational network.[71] Within this model, nonlinear and irreversible gates, dissimilar to the Hamiltonian operator, are deployed to speculate the given data set.[71] Such gates make certain phases unable to be observed and generate specific oscillations.[71] Quantum neural networks apply the principals quantum information and quantum computation to classical neurocomputing.[72] Current research shows that QNN can exponentially increase the amount of computing power and the degrees of freedom for a computer, which is limited for a classical computer to its size.[72] A quantum neural network has computational capabilities to decrease the number of steps, qubits used, and computation time.[71] The wave function to quantum mechanics is the neuron for Neural networks. To test quantum applications in a neural network, quantum dot molecules are deposited on a substrate of GaAs or similar to record how they communicate with one another. Each quantum dot can be referred as an island of electric activity, and when such dots are close enough (approximately 10 - 20 nm)[73] electrons can tunnel underneath the islands. An even distribution across the substrate in sets of two create dipoles and ultimately two spin states, up or down. These states are commonly known as qubits with corresponding states of 




|

0
⟩


{\displaystyle |0\rangle }

  and 




|

1
⟩


{\displaystyle |1\rangle }

 in Dirac notation.[73]
 A novel design for multi-dimensional vectors that uses circuits as convolution filters[74] is QCNN. It was inspired by the advantages of CNNs[75][76] and the power of QML. It is made using a combination of a variational quantum circuit(VQC)[77] and a deep neural network[78](DNN), fully utilizing the power of extremely parallel processing on a superposition of a quantum state with a finite number of qubits. The main strategy is to carry out an iterative optimization process in the NISQ[79] devices, without the negative impact of noise, which is possibly incorporated into the circuit parameter, and without the need for quantum error correction.[80]
 The quantum circuit must effectively handle spatial information in order for QCNN to function as CNN. The convolution filter is the most basic technique for making use of spatial information. One or more quantum convolutional filters make up a quantum convolutional neural network (QCNN), and each of these filters transforms input data using a quantum circuit that can be created in an organized or randomized way. Three parts  that make up the quantum convolutional filter are:  the encoder, the parameterized quantum circuit (PQC),[81] and the measurement. The quantum convolutional filter can be seen as an extension of the filter in the traditional CNN because it was designed with trainable parameters.
 Quantum neural networks take advantage of the hierarchical structures,[82] and for each subsequent layer, the number of qubits from the preceding layer is decreased by a factor of two. For n input qubits, these structure have O(log(n)) layers, allowing for shallow circuit depth. Additionally, they are able to avoid ""barren plateau,"" one of the most significant issues with PQC-based algorithms, ensuring trainability.[83] Despite the fact that the QCNN model does not include the corresponding quantum operation, the fundamental idea of the pooling layer is also offered to assure validity. In QCNN architecture, the pooling layer is typically placed between succeeding convolutional layers. Its function is to shrink the representation's spatial size while preserving crucial features, which allows it to reduce the number of parameters, streamline network computing, and manage over-fitting.  Such process can be accomplished applying full Tomography on the state to reduce it all the way down to one qubit and then processed it in subway. The most frequently used unit type in the pooling layer is max pooling, although there are other types as well. Similar to conventional feed-forward neural networks, the last module is a fully connected layer with full connections to all activations in the preceding layer. Translational invariance, which requires identical blocks of parameterized quantum gates within a layer, is a distinctive feature of the QCNN architecture.[84]
 Dissipative QNNs (DQNNs) are constructed from layers of qubits coupled by perceptron called building blocks, which have an arbitrary unitary design. Each node in the network layer of a DQNN is given a distinct collection of qubits, and each qubit is also given a unique quantum perceptron unitary to characterize it.[85][86] The input states information are transported through the network in a feed-forward fashion, layer-to-layer transition mapping on the qubits of the two adjacent layers, as the name implies. Dissipative term also refers to the fact that the output layer is formed by the ancillary qubits while the input layers are dropped while tracing out the final layer.[87] When performing a broad supervised learning task, DQNN are used to learn a unitary matrix connecting the input and output quantum states. The training data for this task consists of the quantum state and the corresponding classical labels.
 Inspired by the extremely successful classical Generative adversarial network(GAN),[88] dissipative quantum generative adversarial network (DQGAN) is introduced for unsupervised learning of the unlabeled training data . The generator and the discriminator are the two DQNNs that make up a single DQGAN.[86] The generator's goal is to create false training states that the discriminator cannot differentiate from the genuine ones, while the discriminator's objective is to separate the real training states from the fake states created by the generator. The relevant features of the training set are learned by the generator by alternate and adversarial training of the networks that aid in the production of sets that extend the training set. DQGAN has a fully quantum architecture and is trained in quantum data.
 Hidden quantum Markov models[89] (HQMMs) are a quantum-enhanced version of classical Hidden Markov Models (HMMs), which are typically used to model sequential data in various fields like robotics and natural language processing. Unlike the approach taken by other quantum-enhanced machine learning algorithms, HQMMs can be viewed as models inspired by quantum mechanics that can be run on classical computers as well.[90] Where classical HMMs use probability vectors to represent hidden 'belief' states, HQMMs use the quantum analogue: density matrices. Recent work has shown that these models can be successfully learned by maximizing the log-likelihood of the given data via classical optimization, and there is some empirical evidence that these models can better model sequential data compared to classical HMMs in practice, although further work is needed to determine exactly when and how these benefits are derived.[90] Additionally, since classical HMMs are a particular kind of Bayes net, an exciting aspect of HQMMs is that the techniques used show how we can perform quantum-analogous Bayesian inference, which should allow for the general construction of the quantum versions of probabilistic graphical models.[90]
 In the most general case of quantum machine learning, both the learning device and the system under study, as well as their interaction, are fully quantum. This section gives a few examples of results on this topic.
 One class of problem that can benefit from the fully quantum approach is that of 'learning' unknown quantum states, processes or measurements, in the sense that one can subsequently reproduce them on another quantum system. For example, one may wish to learn a measurement that discriminates between two coherent states, given not a classical description of the states to be discriminated, but instead a set of example quantum systems prepared in these states. The naive approach would be to first extract a classical description of the states and then implement an ideal discriminating measurement based on this information. This would only require classical learning. However, one can show that a fully quantum approach is strictly superior in this case.[91] (This also relates to work on quantum pattern matching.[92]) The problem of learning unitary transformations can be approached in a similar way.[93]
 Going beyond the specific problem of learning states and transformations, the task of clustering also admits a fully quantum version, wherein both the oracle which returns the distance between data-points and the information processing device which runs the algorithm are quantum.[94] Finally, a general framework spanning supervised, unsupervised and reinforcement learning in the fully quantum setting was introduced in,[29] where it was also shown that the possibility of probing the environment in superpositions permits a quantum speedup in reinforcement learning. Such a speedup in the reinforcement-learning paradigm has been experimentally demonstrated in a photonic setup.[60]
 The need for models that can be understood by humans emerges in quantum machine learning in analogy to classical machine learning and drives the research field of explainable quantum machine learning (or XQML[95] in analogy to XAI/XML). These efforts are often also referred to as Interpretable Machine Learning (IML, and by extension IQML).[96] XQML/IQML can be considered as an alternative research direction instead of finding a quantum advantage.[97] For example, XQML has been used in the context of mobile malware detection and classification.[98] Quantum Shapley values have also been proposed to interpret gates within a circuit based on a game-theoretic approach.[95] For this purpose, gates instead of features act as players in a coalitional game with a value function that depends on measurements of the quantum circuit of interest. Additionally, a quantum version of the classical technique known as LIME (Linear Interpretable Model-Agnostic Explanations)[99] has also been proposed, known as Q-LIME.[100]
 The term ""quantum machine learning"" sometimes refers to classical machine learning performed on data from quantum systems. A basic example of this is quantum state tomography, where a quantum state is learned from measurement. Other applications include learning Hamiltonians[101] and automatically generating quantum experiments.[20]
 Quantum learning theory pursues a mathematical analysis of the quantum generalizations of classical learning models and of the possible speed-ups or other improvements that they may provide. The framework is very similar to that of classical computational learning theory, but the learner in this case is a quantum information processing device, while the data may be either classical or quantum. Quantum learning theory should be contrasted with the quantum-enhanced machine learning discussed above, where the goal was to consider specific problems and to use quantum protocols to improve the time complexity of classical algorithms for these problems. Although quantum learning theory is still under development, partial results in this direction have been obtained.[102]
 The starting point in learning theory is typically a concept class, a set of possible concepts. Usually a concept is a function on some domain, such as 



{
0
,
1

}

n




{\displaystyle \{0,1\}^{n}}

. For example, the concept class could be the set of disjunctive normal form (DNF) formulas on n bits or the set of Boolean circuits of some constant depth. The goal for the learner is to learn (exactly or approximately) an unknown target concept from this concept class. The learner may be actively interacting with the target concept, or passively receiving samples from it.
 In active learning, a learner can make membership queries to the target concept c, asking for its value c(x) on inputs x chosen by the learner. The learner then has to reconstruct the exact target concept, with high probability. In the model of quantum exact learning, the learner can make membership queries in quantum superposition. If the complexity of the learner is measured by the number of membership queries it makes, then quantum exact learners can be polynomially more efficient than classical learners for some concept classes, but not more.[103] If complexity is measured by the amount of time the learner uses, then there are concept classes that can be learned efficiently by quantum learners but not by classical learners (under plausible complexity-theoretic assumptions).[103]
 A natural model of passive learning is Valiant's probably approximately correct (PAC) learning. Here the learner receives random examples (x,c(x)), where x is distributed according to some unknown distribution D. The learner's goal is to output a hypothesis function h such that h(x)=c(x) with high probability when x is drawn according to D. The learner has to be able to produce such an 'approximately correct' h for every D and every target concept c in its concept class. We can consider replacing the random examples by potentially more powerful quantum examples 




∑

x




D
(
x
)



|

x
,
c
(
x
)
⟩


{\displaystyle \sum _{x}{\sqrt {D(x)}}|x,c(x)\rangle }

. In the PAC model (and the related agnostic model), this doesn't significantly reduce the number of examples needed: for every concept class, classical and quantum sample complexity are the same up to constant factors.[104] However, for learning under some fixed distribution D, quantum examples can be very helpful, for example for learning DNF under the uniform distribution.[105] When considering time complexity, there exist concept classes that can be PAC-learned efficiently by quantum learners, even from classical examples, but not by classical learners (again, under plausible complexity-theoretic assumptions).[103]
 This passive learning type is also the most common scheme in supervised learning: a learning algorithm typically takes the training examples fixed, without the ability to query the label of unlabelled examples. Outputting a hypothesis h is a step of induction. Classically, an inductive model splits into a training and an application phase: the model parameters are estimated in the training phase, and the learned model is applied an arbitrary many times in the application phase. In the asymptotic limit of the number of applications, this splitting of phases is also present with quantum resources.[106]
 The earliest experiments were conducted using the adiabatic D-Wave quantum computer, for instance, to detect cars in digital images using regularized boosting with a nonconvex objective function in a demonstration in 2009.[107] Many experiments followed on the same architecture, and leading tech companies have shown interest in the potential of quantum machine learning for future technological implementations. In 2013, Google Research, NASA, and the Universities Space Research Association launched the Quantum Artificial Intelligence Lab which explores the use of the adiabatic D-Wave quantum computer.[108][109] A more recent example trained a probabilistic generative models with arbitrary pairwise connectivity, showing that their model is capable of generating handwritten digits as well as reconstructing noisy images of bars and stripes and handwritten digits.[65]
 Using a different annealing technology based on nuclear magnetic resonance (NMR), a quantum Hopfield network was implemented in 2009 that mapped the input data and memorized data to Hamiltonians, allowing the use of adiabatic quantum computation.[110] NMR technology also enables universal quantum computing,[citation needed] and it was used for the first experimental implementation of a quantum support vector machine to distinguish hand written number ‘6’ and ‘9’ on a liquid-state  quantum computer in 2015.[111] The training data involved the pre-processing of the image which maps them to normalized 2-dimensional vectors to represent the images as the states of a qubit. The two entries of the vector are the vertical and horizontal ratio of the pixel intensity of the image. Once the vectors are defined on the feature space, the quantum support vector machine was implemented to classify the unknown input vector. The readout avoids costly quantum tomography by reading out the final state in terms of direction (up/down) of the NMR signal.
 Photonic implementations are attracting more attention,[112] not the least because they do not require extensive cooling. Simultaneous spoken digit and speaker recognition and chaotic time-series prediction were demonstrated at data rates beyond 1 gigabyte per second in 2013.[113] Using non-linear photonics to implement an all-optical linear classifier, a perceptron model was capable of learning the classification boundary iteratively from training data through a feedback rule.[114] A core building block in many learning algorithms is to calculate the distance between two vectors: this was first experimentally demonstrated for up to eight dimensions using entangled qubits in a photonic quantum computer in 2015.[115]
 Recently, based on a neuromimetic approach, a novel ingredient has been added to the field of quantum machine learning, in the form of a so-called quantum memristor, a quantized model of the standard classical memristor.[116] This device can be constructed by means of a tunable resistor, weak measurements on the system, and a classical feed-forward mechanism. An implementation of a quantum memristor in superconducting circuits has been proposed,[117] and an experiment with quantum dots performed.[118] A quantum memristor would implement nonlinear interactions in the quantum dynamics which would aid the search for a fully functional quantum neural network.
 Since 2016, IBM has launched an online cloud-based platform for quantum software developers, called the IBM Q Experience. This platform consists of several fully operational quantum processors accessible via the IBM Web API. In doing so, the company is encouraging software developers to pursue new algorithms through a development environment with quantum capabilities. New architectures are being explored on an experimental basis, up to 32 qubits, using both trapped-ion and superconductive quantum computing methods.
 In October 2019, it was noted that the introduction of Quantum Random Number Generators (QRNGs) to machine learning models including Neural Networks and Convolutional Neural Networks for random initial weight distribution and Random Forests for splitting processes had a profound effect on their ability when compared to the classical method of Pseudorandom Number Generators (PRNGs).[119] However, in a more recent publication from 2021, these claims could not be reproduced for Neural Network weight initialization and no significant advantage of using QRNGs over PRNGs was found.[120] The work also demonstrated that the generation of fair random numbers with a gate quantum computer is a non-trivial task on NISQ devices, and QRNGs are therefore typically much more difficult to use in practice than PRNGs.
 A paper published in December 2018 reported on an experiment using a trapped-ion system demonstrating a quantum speedup of the deliberation time of reinforcement learning agents employing internal quantum hardware.[59]
 In March 2021, a team of researchers from Austria, The Netherlands, the US and Germany reported the experimental demonstration of a quantum speedup of the learning time of reinforcement learning agents interacting fully quantumly with the environment.[121][60] The relevant degrees of freedom of both agent and environment were realized on a compact and fully tunable integrated nanophotonic processor.
 While machine learning itself is now not only a research field but an economically significant and fast growing industry and quantum computing is a well established field of both theoretical and experimental research, quantum machine learning remains a purely theoretical field of studies. Attempts to experimentally demonstrate concepts of quantum machine learning remain insufficient.[citation needed] Further, another obstacle exists at the prediction stage because the outputs of quantum learning models are inherently random.[122] This creates an often considerable overhead, as many executions of a quantum learning model have to be aggregated to obtain an actual prediction.
 Many of the leading scientists that extensively publish in the field of quantum machine learning warn about the extensive hype around the topic and are very restrained if asked about its practical uses in the foreseeable future. Sophia Chen[123] collected some of the statements made by well known scientists in the field:
",quantum machin learn integr quantum algorithm within machin learn program common use term refer machin learn algorithm analysi classic data execut quantum comput machin learn machin learn algorithm use comput immens quantiti data quantum machin learn util qubit quantum oper special quantum system improv comput speed data storag done algorithm program includ hybrid method involv classic quantum process comput difficult subroutin outsourc quantum devic routin complex natur execut faster quantum comput furthermor quantum algorithm use analyz quantum state instead classic data beyond quantum comput term quantum machin learn also associ classic machin learn method appli data gener quantum experi machin learn quantum system learn phase transit quantum system creat new quantum experi quantum machin learn also extend branch research explor methodolog structur similar certain physic system learn system particular neural network exampl mathemat numer techniqu quantum physic applic classic deep learn vice versa furthermor research investig abstract notion learn theori respect quantum inform sometim refer quantum learn theori machin learn refer quantum algorithm solv task machin learn therebi improv often expedit classic machin learn techniqu algorithm typic requir one encod given classic data set quantum comput make access quantum inform process subsequ quantum inform process routin appli result quantum comput read measur quantum system exampl outcom measur qubit reveal result binari classif task mani propos quantum machin learn algorithm still pure theoret requir univers quantum comput test other implement special purpos quantum devic associ memori abl recogn store content basi similar measur rather fix address like random access memori must abl retriev incomplet corrupt pattern essenti machin learn task pattern recognit typic classic associ memori store p pattern n interact synaps real symmetr energi matrix network n artifici neuron encod desir pattern local minima energi function retriev done minim total energi start initi configur unfortun classic associ memori sever limit phenomenon mani pattern store spuriou memori appear quickli prolifer energi landscap becom disord retriev anymor possibl number storabl pattern typic limit linear function number neuron p n n quantum associ memori simplest realiz store pattern unitari matrix u act hilbert space n qubit retriev realiz unitari evolut fix initi state quantum superposit desir pattern probabl distribut peak similar pattern input quantum natur retriev process thu probabilist quantum associ memori free howev spuriou memori never gener correspondingli superior capac classic one number paramet unitari matrix u p n pn one thu effici quantum associ memori polynomi number pattern number quantum algorithm machin learn base idea amplitud encod associ amplitud quantum state input output comput sinc state n n qubit describ n n complex amplitud inform encod allow exponenti compact represent intuit correspond associ discret probabl distribut binari random variabl classic vector goal algorithm base amplitud encod formul quantum algorithm whose resourc grow polynomi number qubit n n amount logarithm time complex number amplitud therebi dimens input mani quantum machin learn algorithm categori base variat quantum algorithm linear system equat colloqui call hhl paper author specif condit perform matrix invers use amount physic resourc grow logarithm dimens matrix one condit hamiltonian entri wise correspond matrix simul effici known possibl matrix spars low rank refer known classic algorithm matrix invers requir number oper grow quadrat dimens matrix n restrict spars matric quantum matrix invers appli machin learn method train reduc solv linear system equat exampl linear regress version support vector machin gaussian process crucial bottleneck method simul linear algebra comput amplitud quantum state state prepar often requir one initialis quantum system state whose amplitud reflect featur entir dataset although effici method state prepar known specif case step easili hide complex task vqa one studi class quantum algorithm modern research demonstr applic vast major known major applic quantum comput appear lead hope gain quantum supremaci vqa mix approach quantum processor prepar quantum state measur made optim done classic comput vqa consid best nisq vqa nois toler compar algorithm give quantum superior hundr qubit research studi algorithm solv optim problem find ground state energi complex system difficult solv requir larg time perform comput use classic comput variat quantum circuit also known parametr quantum circuit pqc base variat quantum algorithm vqa vqc consist three part prepar initi state quantum circuit measur research extens studi vqc use power quantum comput learn short time also use fewer paramet classic counterpart theoret numer proven approxim function like use neural network quantum circuit due vqc superior neural network replac vqc reinforc learn task gener algorithm intrins natur quantum devic toward decoher random gate error measur error caus high potenti limit train variat circuit train vqc classic devic employ quantum devic help overcom problem decoher nois came number repetit train pattern reorgan one import task machin learn binari classif one tool algorithm find pattern binari classif use supervis learn unsupervis learn quantum machin learn classic bit convert qubit map hilbert space complex valu data use quantum binari classifi use advantag hilbert space exploit quantum mechan properti superposit entangl interfer quantum binari classifi produc accur result short period time anoth approach improv classic machin learn quantum inform process use amplitud amplif method base grover search algorithm shown solv unstructur search problem quadrat speedup compar classic algorithm quantum routin employ learn algorithm translat unstructur search task done instanc case neighbor algorithm applic includ quadrat speedup train perceptron comput attent exampl amplitud amplif use machin learn algorithm grover search algorithm minim subroutin use grover search algorithm find element less previous defin element done oracl determin whether state correspond element less predefin one grover algorithm find element condit met minim initi random element data set iter subroutin find minimum element data set minim notabl use quantum speed least n k n k compar classic version n n number data point k k number cluster amplitud amplif often combin quantum walk achiev quadrat speedup quantum walk propos enhanc googl pagerank algorithm well perform reinforc learn agent project simul framework reinforc learn branch machin learn distinct supervis unsupervis learn also admit quantum enhanc reinforc learn quantum agent interact classic quantum environ occasion receiv reward action allow agent adapt word learn order gain reward situat either quantum process capabl agent due possibl probe environ superposit quantum speedup may achiev implement kind protocol propos system trap ion superconduct circuit quantum speedup agent intern time experiment demonstr trap ion quantum speedup learn time fulli coher quantum interact agent environ experiment realiz photon setup quantum anneal optim techniqu use determin local minima maxima function given set candid function method discret function mani local minima maxima order determin observ function process distinguish simul anneal quantum tunnel process particl tunnel kinet potenti barrier high state low state quantum anneal start superposit possibl state system weight equal schrödinger equat guid time evolut system serv affect amplitud state time increas eventu ground state reach yield instantan hamiltonian system depth quantum circuit advanc nisq devic nois level rise pose signific challeng accur comput cost gradient train model nois toler improv use quantum perceptron quantum algorithm current access quantum hardwar citat need regular connect similar compon known neuron form basi even complex brain network typic neuron two oper inner product activ function oppos activ function typic nonlinear inner product linear process quantum comput linear process may easili accomplish addit due simplic implement threshold function prefer major quantum neuron activ function citat need sampl probabl distribut core wide spectrum comput techniqu import applic across scienc engin societi exampl includ deep learn probabilist program machin learn artifici intellig applic comput hard problem key relev machin learn task estim averag probabilist model defin term boltzmann distribut sampl gener probabilist model hard algorithm reli heavili sampl expect remain intract matter larg power classic comput resourc becom even though quantum anneal like produc system design challeng combinatori optim problem recent recogn potenti candid speed comput reli sampl exploit quantum effect research group recent explor use quantum anneal hardwar train boltzmann machin deep neural network standard approach train boltzmann machin reli comput certain averag estim standard sampl techniqu markov chain mont carlo algorithm anoth possibl reli physic process like quantum anneal natur gener sampl boltzmann distribut object find optim control paramet best repres empir distribut given dataset system host nasa ame research center recent use learn special class restrict boltzmann machin serv build block deep learn architectur complementari work appear roughli simultan show quantum anneal use supervis learn classif task devic later use train fulli connect boltzmann machin gener reconstruct classifi handwritten digit among synthet dataset case model train quantum anneal similar better perform term qualiti ultim question drive endeavour whether quantum speedup sampl applic experi use quantum anneal combinatori optim suggest answer straightforward revers anneal use well solv fulli connect quantum restrict boltzmann machin inspir success boltzmann machin base classic boltzmann distribut new machin learn approach base quantum boltzmann distribut ise hamiltonian recent propos due natur quantum mechan train process quantum boltzmann machin becom nontrivi problem extent circumv introduc bound quantum probabl allow author train model effici sampl possibl specif type quantum boltzmann machin train use learn rule analog classic boltzmann machin quantum anneal technolog sampl scenario univers quantum comput prepar thermal state sampl measur reduc time requir train deep restrict boltzmann machin provid richer comprehens framework deep learn classic comput quantum method also permit effici train full boltzmann machin fulli connect model classic counterpart reli effici thermal state prepar protocol start arbitrari state markov logic network exploit symmetri local structur probabilist graphic model gener logic templat provid exponenti reduct comput complex probabilist infer protocol reli univers quantum comput mild assumpt embed contemporari quantum anneal hardwar quantum analogu gener classic neural net often refer quantum neural network term claim wide rang approach includ implement extens neural network use photon layer variat circuit quantum model quantum neural network often defin expans deutsch model quantum comput network within model nonlinear irrevers gate dissimilar hamiltonian oper deploy specul given data set gate make certain phase unabl observ gener specif oscil quantum neural network appli princip quantum inform quantum comput classic neurocomput current research show qnn exponenti increas amount comput power degre freedom comput limit classic comput size quantum neural network comput capabl decreas number step qubit use comput time wave function quantum mechan neuron neural network test quantum applic neural network quantum dot molecul deposit substrat gaa similar record commun one anoth quantum dot refer island electr activ dot close enough approxim nm electron tunnel underneath island even distribut across substrat set two creat dipol ultim two spin state state commonli known qubit correspond state dirac notat novel design vector use circuit convolut filter qcnn inspir advantag cnn power qml made use combin variat quantum circuit vqc deep neural network dnn fulli util power extrem parallel process superposit quantum state finit number qubit main strategi carri iter optim process nisq devic without neg impact nois possibl incorpor circuit paramet without need quantum error correct quantum circuit must effect handl spatial inform order qcnn function cnn convolut filter basic techniqu make use spatial inform one quantum convolut filter make quantum convolut neural network qcnn filter transform input data use quantum circuit creat organ random way three part make quantum convolut filter encod parameter quantum circuit pqc measur quantum convolut filter seen extens filter tradit cnn design trainabl paramet quantum neural network take advantag hierarch structur subsequ layer number qubit preced layer decreas factor two n input qubit structur log n layer allow shallow circuit depth addit abl avoid barren plateau one signific issu algorithm ensur trainabl despit fact qcnn model includ correspond quantum oper fundament idea pool layer also offer assur valid qcnn architectur pool layer typic place succeed convolut layer function shrink represent spatial size preserv crucial featur allow reduc number paramet streamlin network comput manag process accomplish appli full tomographi state reduc way one qubit process subway frequent use unit type pool layer max pool although type well similar convent neural network last modul fulli connect layer full connect activ preced layer translat invari requir ident block parameter quantum gate within layer distinct featur qcnn architectur dissip qnn dqnn construct layer qubit coupl perceptron call build block arbitrari unitari design node network layer dqnn given distinct collect qubit qubit also given uniqu quantum perceptron unitari character input state inform transport network fashion transit map qubit two adjac layer name impli dissip term also refer fact output layer form ancillari qubit input layer drop trace final layer perform broad supervis learn task dqnn use learn unitari matrix connect input output quantum state train data task consist quantum state correspond classic label inspir extrem success classic gener adversari network gan dissip quantum gener adversari network dqgan introduc unsupervis learn unlabel train data gener discrimin two dqnn make singl dqgan gener goal creat fals train state discrimin differenti genuin one discrimin object separ real train state fake state creat gener relev featur train set learn gener altern adversari train network aid product set extend train set dqgan fulli quantum architectur train quantum data hidden quantum markov model hqmm version classic hidden markov model hmm typic use model sequenti data variou field like robot natur languag process unlik approach taken machin learn algorithm hqmm view model inspir quantum mechan run classic comput well classic hmm use probabl vector repres hidden state hqmm use quantum analogu densiti matric recent work shown model success learn maxim given data via classic optim empir evid model better model sequenti data compar classic hmm practic although work need determin exactli benefit deriv addit sinc classic hmm particular kind bay net excit aspect hqmm techniqu use show perform bayesian infer allow gener construct quantum version probabilist graphic model gener case quantum machin learn learn devic system studi well interact fulli quantum section give exampl result topic one class problem benefit fulli quantum approach unknown quantum state process measur sens one subsequ reproduc anoth quantum system exampl one may wish learn measur discrimin two coher state given classic descript state discrimin instead set exampl quantum system prepar state naiv approach would first extract classic descript state implement ideal discrimin measur base inform would requir classic learn howev one show fulli quantum approach strictli superior case also relat work quantum pattern match problem learn unitari transform approach similar way go beyond specif problem learn state transform task cluster also admit fulli quantum version wherein oracl return distanc inform process devic run algorithm quantum final gener framework span supervis unsupervis reinforc learn fulli quantum set introduc also shown possibl probe environ superposit permit quantum speedup reinforc learn speedup paradigm experiment demonstr photon setup need model understood human emerg quantum machin learn analog classic machin learn drive research field explain quantum machin learn xqml analog effort often also refer interpret machin learn iml extens iqml consid altern research direct instead find quantum advantag exampl xqml use context mobil malwar detect classif quantum shapley valu also propos interpret gate within circuit base approach purpos gate instead featur act player coalit game valu function depend measur quantum circuit interest addit quantum version classic techniqu known lime linear interpret explan also propos known term quantum machin learn sometim refer classic machin learn perform data quantum system basic exampl quantum state tomographi quantum state learn measur applic includ learn hamiltonian automat gener quantum experi quantum learn theori pursu mathemat analysi quantum gener classic learn model possibl improv may provid framework similar classic comput learn theori learner case quantum inform process devic data may either classic quantum quantum learn theori contrast machin learn discuss goal consid specif problem use quantum protocol improv time complex classic algorithm problem although quantum learn theori still develop partial result direct obtain start point learn theori typic concept class set possibl concept usual concept function domain n n exampl concept class could set disjunct normal form dnf formula n bit set boolean circuit constant depth goal learner learn exactli approxim unknown target concept concept class learner may activ interact target concept passiv receiv sampl activ learn learner make membership queri target concept c ask valu c x input x chosen learner learner reconstruct exact target concept high probabl model quantum exact learn learner make membership queri quantum superposit complex learner measur number membership queri make quantum exact learner polynomi effici classic learner concept class complex measur amount time learner use concept class learn effici quantum learner classic learner plausibl assumpt natur model passiv learn valiant probabl approxim correct pac learn learner receiv random exampl x c x x distribut accord unknown distribut learner goal output hypothesi function h h x x high probabl x drawn accord learner abl produc correct h everi everi target concept c concept class consid replac random exampl potenti power quantum exampl x x x c x x x c x pac model relat agnost model significantli reduc number exampl need everi concept class classic quantum sampl complex constant factor howev learn fix distribut quantum exampl help exampl learn dnf uniform distribut consid time complex exist concept class effici quantum learner even classic exampl classic learner plausibl assumpt passiv learn type also common scheme supervis learn learn algorithm typic take train exampl fix without abil queri label unlabel exampl output hypothesi h step induct classic induct model split train applic phase model paramet estim train phase learn model appli arbitrari mani time applic phase asymptot limit number applic split phase also present quantum resourc earliest experi conduct use adiabat quantum comput instanc detect car digit imag use regular boost nonconvex object function demonstr mani experi follow architectur lead tech compani shown interest potenti quantum machin learn futur technolog implement googl research nasa univers space research associ launch quantum artifici intellig lab explor use adiabat quantum comput recent exampl train probabilist gener model arbitrari pairwis connect show model capabl gener handwritten digit well reconstruct noisi imag bar stripe handwritten digit use differ anneal technolog base nuclear magnet reson nmr quantum hopfield network implement map input data memor data hamiltonian allow use adiabat quantum comput nmr technolog also enabl univers quantum comput citat need use first experiment implement quantum support vector machin distinguish hand written number quantum comput train data involv imag map normal vector repres imag state qubit two entri vector vertic horizont ratio pixel intens imag vector defin featur space quantum support vector machin implement classifi unknown input vector readout avoid costli quantum tomographi read final state term direct nmr signal photon implement attract attent least requir extens cool simultan spoken digit speaker recognit chaotic predict demonstr data rate beyond gigabyt per second use photon implement linear classifi perceptron model capabl learn classif boundari iter train data feedback rule core build block mani learn algorithm calcul distanc two vector first experiment demonstr eight dimens use entangl qubit photon quantum comput recent base neuromimet approach novel ingredi ad field quantum machin learn form quantum memristor quantiz model standard classic memristor devic construct mean tunabl resistor weak measur system classic mechan implement quantum memristor superconduct circuit propos experi quantum dot perform quantum memristor would implement nonlinear interact quantum dynam would aid search fulli function quantum neural network sinc ibm launch onlin platform quantum softwar develop call ibm q experi platform consist sever fulli oper quantum processor access via ibm web api compani encourag softwar develop pursu new algorithm develop environ quantum capabl new architectur explor experiment basi qubit use superconduct quantum comput method octob note introduct quantum random number gener qrng machin learn model includ neural network convolut neural network random initi weight distribut random forest split process profound effect abil compar classic method pseudorandom number gener prng howev recent public claim could reproduc neural network weight initi signific advantag use qrng prng found work also demonstr gener fair random number gate quantum comput task nisq devic qrng therefor typic much difficult use practic prng paper publish decemb report experi use system demonstr quantum speedup deliber time reinforc learn agent employ intern quantum hardwar march team research austria netherland us germani report experiment demonstr quantum speedup learn time reinforc learn agent interact fulli quantumli environ relev degre freedom agent environ realiz compact fulli tunabl integr nanophoton processor machin learn research field econom signific fast grow industri quantum comput well establish field theoret experiment research quantum machin learn remain pure theoret field studi attempt experiment demonstr concept quantum machin learn remain insuffici citat need anoth obstacl exist predict stage output quantum learn model inher random creat often consider overhead mani execut quantum learn model aggreg obtain actual predict mani lead scientist extens publish field quantum machin learn warn extens hype around topic restrain ask practic use forese futur sophia chen collect statement made well known scientist field
Statistical classification,https://en.wikipedia.org/wiki/Statistical_classification,"When classification is performed by a computer, statistical methods are normally used to develop the algorithm.
 Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features.  These properties may variously be categorical (e.g. ""A"", ""B"", ""AB"" or ""O"", for blood type), ordinal (e.g. ""large"", ""medium"" or ""small""), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.
 An algorithm that implements classification, especially in a concrete implementation, is known as a classifier.  The term ""classifier"" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.
 Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable.  In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes.  Other fields may use different terminology: e.g. in community ecology, the term ""classification"" normally refers to cluster analysis.
 Classification and clustering are examples of the more general problem of pattern recognition, which is the assignment of some sort of output value to a given input value.  Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence; etc.
 A common subclass of classification is probabilistic classification.  Algorithms of this nature use statistical inference to find the best class for a given instance.  Unlike other algorithms, which simply output a ""best"" class, probabilistic algorithms output a probability of the instance being a member of each of the possible classes.  The best class is normally then selected as the one with the highest probability.  However, such an algorithm has numerous advantages over non-probabilistic classifiers:
 Early work on statistical classification was undertaken by Fisher,[1][2] in the context of two-group problems, leading to Fisher's linear discriminant function as the rule for assigning a group to a new observation.[3] This early work assumed that data-values within each of the two groups had a multivariate normal distribution. The extension of this same context to more than two groups has also been considered with a restriction imposed that the classification rule should be linear.[3][4] Later work for the multivariate normal distribution allowed the classifier to be nonlinear:[5] several classification rules can be derived based on different adjustments of the Mahalanobis distance, with a new observation being assigned to the group whose centre has the lowest adjusted distance from the observation.
 Unlike frequentist procedures, Bayesian classification procedures provide a natural way of taking into account any available information about the relative sizes of the different groups within the overall population.[6] Bayesian procedures tend to be computationally expensive and, in the days before Markov chain Monte Carlo computations were developed, approximations for Bayesian clustering rules were devised.[7]
 Some Bayesian procedures involve the calculation of  group-membership probabilities: these provide a more informative outcome than a simple attribution of a single group-label to each new observation.
 Classification can be thought of as two separate problems – binary classification and multiclass classification. In binary classification, a better understood task, only two classes are involved, whereas multiclass classification involves assigning an object to one of several classes.[8] Since many classification methods have been developed specifically for binary classification, multiclass classification often requires the combined use of multiple binary classifiers.
 Most algorithms describe an individual instance whose category is to be predicted using a feature vector of individual, measurable properties of the instance.  Each property is termed a feature, also known in statistics as an explanatory variable (or independent variable, although features may or may not be statistically independent).  Features may variously be binary (e.g. ""on"" or ""off""); categorical (e.g. ""A"", ""B"", ""AB"" or ""O"", for blood type); ordinal (e.g. ""large"", ""medium"" or ""small""); integer-valued (e.g. the number of occurrences of a particular word in an email); or real-valued (e.g. a measurement of blood pressure).  If the instance is an image, the feature values might correspond to the pixels of an image; if the instance is a piece of text, the feature values might be occurrence frequencies of different words.  Some algorithms work only in terms of discrete data and require that real-valued or integer-valued data be discretized into groups (e.g. less than 5, between 5 and 10, or greater than 10).
 A large number of algorithms for classification can be phrased in terms of a linear function that assigns a score to each possible category k by combining the feature vector of an instance with a vector of weights, using a dot product.  The predicted category is the one with the highest score.  This type of score function is known as a linear predictor function and has the following general form:




score
⁡
(


X


i


,
k
)
=


β


k


⋅


X


i


,


{\displaystyle \operatorname {score} (\mathbf {X} _{i},k)={\boldsymbol {\beta }}_{k}\cdot \mathbf {X} _{i},}


where Xi is the feature vector for instance i, βk is the vector of weights corresponding to category k, and score(Xi, k) is the score associated with assigning instance i to category k.  In discrete choice theory, where instances represent people and categories represent choices, the score is considered the utility associated with person i choosing category k.
 Algorithms with this basic setup are known as linear classifiers.  What distinguishes them is the procedure for determining (training) the optimal weights/coefficients and the way that the score is interpreted.
 Examples of such algorithms include
 Since no single form of classification is appropriate for all data sets, a large toolkit of classification algorithms has been developed. The most commonly used include:[9]
 Choices between different possible algorithms are frequently made on the basis of quantitative evaluation of accuracy.
 Classification has many applications. In some of these, it is employed as a data mining procedure, while in others more detailed statistical modeling is undertaken.
",classif perform comput statist method normal use develop algorithm often individu observ analyz set quantifi properti known various explanatori variabl featur properti may various categor b ab blood type ordin larg medium small number occurr particular word email measur blood pressur classifi work compar observ previou observ mean similar distanc function algorithm implement classif especi concret implement known classifi term classifi sometim also refer mathemat function implement classif algorithm map input data categori terminolog across field quit vari statist classif often done logist regress similar procedur properti observ term explanatori variabl independ variabl regressor etc categori predict known outcom consid possibl valu depend variabl machin learn observ often known instanc explanatori variabl term featur group featur vector possibl categori predict class field may use differ terminolog commun ecolog term classif normal refer cluster analysi classif cluster exampl gener problem pattern recognit assign sort output valu given input valu exampl regress assign output input sequenc label assign class member sequenc valu exampl part speech tag assign part speech word input sentenc pars assign pars tree input sentenc describ syntact structur sentenc etc common subclass classif probabilist classif algorithm natur use statist infer find best class given instanc unlik algorithm simpli output best class probabilist algorithm output probabl instanc member possibl class best class normal select one highest probabl howev algorithm numer advantag classifi earli work statist classif undertaken fisher context problem lead fisher linear discrimin function rule assign group new observ earli work assum within two group multivari normal distribut extens context two group also consid restrict impos classif rule linear later work multivari normal distribut allow classifi nonlinear sever classif rule deriv base differ adjust mahalanobi distanc new observ assign group whose centr lowest adjust distanc observ unlik frequentist procedur bayesian classif procedur provid natur way take account avail inform rel size differ group within overal popul bayesian procedur tend comput expens day markov chain mont carlo comput develop approxim bayesian cluster rule devis bayesian procedur involv calcul probabl provid inform outcom simpl attribut singl new observ classif thought two separ problem binari classif multiclass classif binari classif better understood task two class involv wherea multiclass classif involv assign object one sever class sinc mani classif method develop specif binari classif multiclass classif often requir combin use multipl binari classifi algorithm describ individu instanc whose categori predict use featur vector individu measur properti instanc properti term featur also known statist explanatori variabl independ variabl although featur may may statist independ featur may various binari categor b ab blood type ordin larg medium small number occurr particular word email measur blood pressur instanc imag featur valu might correspond pixel imag instanc piec text featur valu might occurr frequenc differ word algorithm work term discret data requir data discret group less greater larg number algorithm classif phrase term linear function assign score possibl categori k combin featur vector instanc vector weight use dot product predict categori one highest score type score function known linear predictor function follow gener form score x k β k x score x k k x xi featur vector instanc βk vector weight correspond categori k score xi k score associ assign instanc categori discret choic theori instanc repres peopl categori repres choic score consid util associ person choos categori algorithm basic setup known linear classifi distinguish procedur determin train optim way score interpret exampl algorithm includ sinc singl form classif appropri data set larg toolkit classif algorithm develop commonli use includ choic differ possibl algorithm frequent made basi quantit evalu accuraci classif mani applic employ data mine procedur other detail statist model undertaken
Generative model,https://en.wikipedia.org/wiki/Generative_model,"In statistical classification, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent,[a] but three major types can be distinguished, following Jebara (2004):
 The distinction between these last two classes is not consistently made;[4] Jebara (2004) refers to these three classes as generative learning, conditional learning, and discriminative learning, but Ng & Jordan (2002) only distinguish two classes, calling them generative classifiers (joint distribution) and discriminative classifiers (conditional distribution or no distribution), not distinguishing between the latter two classes.[5] Analogously, a classifier based on a generative model is a generative classifier, while a classifier based on a discriminative model is a discriminative classifier, though this term also refers to classifiers that are not based on a model.
 Standard examples of each, all of which are linear classifiers, are:
 In application to classification, one wishes to go from an observation x to a label y (or probability distribution on labels). One can compute this directly, without using a probability distribution (distribution-free classifier); one can estimate the probability of a label given an observation, 



P
(
Y

|

X
=
x
)


{\displaystyle P(Y|X=x)}

 (discriminative model), and base classification on that; or one can estimate the joint distribution 



P
(
X
,
Y
)


{\displaystyle P(X,Y)}

 (generative model), from that compute the conditional probability 



P
(
Y

|

X
=
x
)


{\displaystyle P(Y|X=x)}

, and then base classification on that. These are increasingly indirect, but increasingly probabilistic, allowing more domain knowledge and probability theory to be applied. In practice different approaches are used, depending on the particular problem, and hybrids can combine strengths of multiple approaches.
 An alternative division defines these symmetrically as:
 Regardless of precise definition, the terminology is constitutional because a generative model can be used to ""generate"" random instances (outcomes), either of an observation and target 



(
x
,
y
)


{\displaystyle (x,y)}

, or of an observation x given a target value y,[2] while a discriminative model or discriminative classifier (without a model) can be used to ""discriminate"" the value of the target variable Y, given an observation x.[3] The difference between ""discriminate"" (distinguish) and ""classify"" is subtle, and these are not consistently distinguished. (The term ""discriminative classifier"" becomes a pleonasm when ""discrimination"" is equivalent to ""classification"".)
 The term ""generative model"" is also used to describe models that generate instances of output variables in a way that has no clear relationship to probability distributions over potential samples of input variables. Generative adversarial networks are examples of this class of generative models, and are judged primarily by the similarity of particular outputs to potential inputs. Such models are not classifiers.
 In application to classification, the observable X is frequently a continuous variable, the target Y is generally a discrete variable consisting of a finite set of labels, and the conditional probability 



P
(
Y
∣
X
)


{\displaystyle P(Y\mid X)}

 can also be interpreted as a (non-deterministic) target function 



f
:
X
→
Y


{\displaystyle f\colon X\to Y}

, considering X as inputs and Y as outputs.
 Given a finite set of labels, the two definitions of ""generative model"" are closely related. A model of the conditional distribution 



P
(
X
∣
Y
=
y
)


{\displaystyle P(X\mid Y=y)}

 is a model of the distribution of each label, and a model of the joint distribution is equivalent to a model of the distribution of label values 



P
(
Y
)


{\displaystyle P(Y)}

, together with the distribution of observations given a label, 



P
(
X
∣
Y
)


{\displaystyle P(X\mid Y)}

; symbolically, 



P
(
X
,
Y
)
=
P
(
X
∣
Y
)
P
(
Y
)
.


{\displaystyle P(X,Y)=P(X\mid Y)P(Y).}

 Thus, while a model of the joint probability distribution is more informative than a model of the distribution of label (but without their relative frequencies), it is a relatively small step, hence these are not always distinguished.
 Given a model of the joint distribution, 



P
(
X
,
Y
)


{\displaystyle P(X,Y)}

, the distribution of the individual variables can be computed as the marginal distributions 



P
(
X
)
=

∑

y


P
(
X
,
Y
=
y
)


{\displaystyle P(X)=\sum _{y}P(X,Y=y)}

 and 



P
(
Y
)
=

∫

x


P
(
Y
,
X
=
x
)


{\displaystyle P(Y)=\int _{x}P(Y,X=x)}

 (considering X as continuous, hence integrating over it, and Y as discrete, hence summing over it), and either conditional distribution can be computed from the definition of conditional probability: 



P
(
X
∣
Y
)
=
P
(
X
,
Y
)

/

P
(
Y
)


{\displaystyle P(X\mid Y)=P(X,Y)/P(Y)}

 and 



P
(
Y
∣
X
)
=
P
(
X
,
Y
)

/

P
(
X
)


{\displaystyle P(Y\mid X)=P(X,Y)/P(X)}

.
 Given a model of one conditional probability, and estimated probability distributions for the variables X and Y, denoted 



P
(
X
)


{\displaystyle P(X)}

 and 



P
(
Y
)


{\displaystyle P(Y)}

, one can estimate the opposite conditional probability using Bayes' rule:
 For example, given a generative model for 



P
(
X
∣
Y
)


{\displaystyle P(X\mid Y)}

, one can estimate:
 and given a discriminative model for 



P
(
Y
∣
X
)


{\displaystyle P(Y\mid X)}

, one can estimate:
 Note that Bayes' rule (computing one conditional probability in terms of the other) and the definition of conditional probability (computing conditional probability in terms of the joint distribution) are frequently conflated as well.
 A generative algorithm models how the data was generated in order to categorize a signal. It asks the question: based on my generation assumptions, which category is most likely to generate this signal? A discriminative algorithm does not care about how the data was generated, it simply categorizes a given signal. So, discriminative algorithms try to learn 



p
(
y

|

x
)


{\displaystyle p(y|x)}

 directly from the data and then try to classify data. On the other hand, generative algorithms try to learn 



p
(
x
,
y
)


{\displaystyle p(x,y)}

 which can be transformed into 



p
(
y

|

x
)


{\displaystyle p(y|x)}

 later to classify the data. One of the advantages of generative algorithms is that you can use 



p
(
x
,
y
)


{\displaystyle p(x,y)}

 to generate new data similar to existing data. On the other hand, it has been proved that some discriminative algorithms give better performance than some generative algorithms in classification tasks.[6]
 Despite the fact that discriminative models do not need to model the distribution of the observed variables, they cannot generally express complex relationships between the observed and target variables. But in general, they don't necessarily perform better than generative models at classification and regression tasks. The two classes are seen as complementary or as different views of the same procedure.[7]
 With the rise of deep learning, a new family of methods, called deep generative models (DGMs),[8][9] is formed through the combination of generative models and deep neural networks. An increase in the scale of the neural networks is typically accompanied by an increase in the scale of the training data, both of which are required for good performance.[10]
 Popular DGMs include variational autoencoders (VAEs), generative adversarial networks (GANs), and auto-regressive models. Recently, there has been a trend to build very large deep generative models.[8] For example, GPT-3, and its precursor GPT-2,[11] are auto-regressive neural language models that contain billions of parameters, BigGAN[12] and VQ-VAE[13] which are used for image generation that can have hundreds of millions of parameters, and Jukebox is a very large generative model for musical audio that contains billions of parameters.[14]
 Types of generative models are:
 If the observed data are truly sampled from the generative model, then fitting the parameters of the generative model to maximize the data likelihood is a common method. However, since most statistical models are only approximations to the true distribution, if the model's application is to infer about a subset of variables conditional on known values of others, then it can be argued that the approximation makes more assumptions than are necessary to solve the problem at hand. In such cases, it can be more accurate to model the conditional density functions directly using a discriminative model (see below), although application-specific details will ultimately dictate which approach is most suitable in any particular case.
 Suppose the input data is 



x
∈
{
1
,
2
}


{\displaystyle x\in \{1,2\}}

, the set of labels for 



x


{\displaystyle x}

 is 



y
∈
{
0
,
1
}


{\displaystyle y\in \{0,1\}}

, and there are the following 4 data points:




(
x
,
y
)
=
{
(
1
,
0
)
,
(
1
,
1
)
,
(
2
,
0
)
,
(
2
,
1
)
}


{\displaystyle (x,y)=\{(1,0),(1,1),(2,0),(2,1)\}}


 For the above data, estimating the joint probability distribution 



p
(
x
,
y
)


{\displaystyle p(x,y)}

 from the empirical measure will be the following:
 while 



p
(
y

|

x
)


{\displaystyle p(y|x)}

 will be following:
 Shannon (1948) gives an example in which a table of frequencies of English word pairs is used to generate a sentence beginning with ""representing and speedily is an good""; which is not proper English but which will increasingly approximate it as the table is moved from word pairs to word triplets etc.
",statist classif two main approach call gener approach discrimin approach comput classifi differ approach differ degre statist model terminolog inconsist three major type distinguish follow jebara distinct last two class consist made jebara refer three class gener learn condit learn discrimin learn ng jordan distinguish two class call gener classifi joint distribut discrimin classifi condit distribut distribut distinguish latter two class analog classifi base gener model gener classifi classifi base discrimin model discrimin classifi though term also refer classifi base model standard exampl linear classifi applic classif one wish go observ x label probabl distribut label one comput directli without use probabl distribut classifi one estim probabl label given observ p x x p discrimin model base classif one estim joint distribut p x p x gener model comput condit probabl p x x p base classif increasingli indirect increasingli probabilist allow domain knowledg probabl theori appli practic differ approach use depend particular problem hybrid combin strength multipl approach altern divis defin symmetr regardless precis definit terminolog constitut gener model use gener random instanc outcom either observ target x x observ x given target valu discrimin model discrimin classifi without model use discrimin valu target variabl given observ x differ discrimin distinguish classifi subtl consist distinguish term discrimin classifi becom pleonasm discrimin equival classif term gener model also use describ model gener instanc output variabl way clear relationship probabl distribut potenti sampl input variabl gener adversari network exampl class gener model judg primarili similar particular output potenti input model classifi applic classif observ x frequent continu variabl target gener discret variabl consist finit set label condit probabl p x p x also interpret target function f x consid x input output given finit set label two definit gener model close relat model condit distribut p x p model distribut label model joint distribut equival model distribut label valu p p togeth distribut observ given label p x p symbol p x p x p p x p thu model joint probabl distribut inform model distribut label without rel frequenc rel small step henc alway distinguish given model joint distribut p x p x distribut individu variabl comput margin distribut p x p x p x p x p x p x x p x p consid x continu henc integr discret henc sum either condit distribut comput definit condit probabl p x p x p p x p x p x p x p x x x given model one condit probabl estim probabl distribut variabl x denot p x p x p p one estim opposit condit probabl use bay rule exampl given gener model p x p one estim given discrimin model p x p x one estim note bay rule comput one condit probabl term definit condit probabl comput condit probabl term joint distribut frequent conflat well gener algorithm model data gener order categor signal ask question base gener assumpt categori like gener signal discrimin algorithm care data gener simpli categor given signal discrimin algorithm tri learn p x p directli data tri classifi data hand gener algorithm tri learn p x p x transform p x p later classifi data one advantag gener algorithm use p x p x gener new data similar exist data hand prove discrimin algorithm give better perform gener algorithm classif task despit fact discrimin model need model distribut observ variabl gener express complex relationship observ target variabl gener necessarili perform better gener model classif regress task two class seen complementari differ view procedur rise deep learn new famili method call deep gener model dgm form combin gener model deep neural network increas scale neural network typic accompani increas scale train data requir good perform popular dgm includ variat autoencod vae gener adversari network gan model recent trend build larg deep gener model exampl precursor neural languag model contain billion paramet biggan use imag gener hundr million paramet jukebox larg gener model music audio contain billion paramet type gener model observ data truli sampl gener model fit paramet gener model maxim data likelihood common method howev sinc statist model approxim true distribut model applic infer subset variabl condit known valu other argu approxim make assumpt necessari solv problem hand case accur model condit densiti function directli use discrimin model see although detail ultim dictat approach suitabl particular case suppos input data x set label x x follow data point x x data estim joint probabl distribut p x p x empir measur follow p x p follow shannon give exampl tabl frequenc english word pair use gener sentenc begin repres speedili good proper english increasingli approxim tabl move word pair word triplet etc
Regression analysis,https://en.wikipedia.org/wiki/Regression_analysis,"In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the outcome or response variable, or a label in machine learning parlance) and one or more error-free independent variables (often called regressors, predictors, covariates, explanatory variables or features). The most common form of regression analysis is linear regression, in which one finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes the unique line (or hyperplane) that minimizes the sum of squared differences between the true data and that line (or hyperplane). For specific mathematical reasons (see linear regression), this allows the researcher to estimate the conditional expectation (or population average value) of the dependent variable when the independent variables take on a given set of values. Less common forms of regression use slightly different procedures to estimate alternative location parameters (e.g., quantile regression or Necessary Condition Analysis[1]) or estimate the conditional expectation across a broader collection of non-linear models (e.g., nonparametric regression).
 Regression analysis is primarily used for two conceptually distinct purposes. First, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. Second, in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables. Importantly, regressions by themselves only reveal relationships between a dependent variable and a collection of independent variables in a fixed dataset. To use regressions for prediction or to infer causal relationships, respectively, a researcher must carefully justify why existing relationships have predictive power for a new context or why a relationship between two variables has a causal interpretation. The latter is especially important when researchers hope to estimate causal relationships using  observational data.[2][3]
 The earliest regression form was seen in Isaac Newton's work in 1700 while studying equinoxes, being credited with introducing ""an embryonic linear aggression analysis"" as ""Not only did he perform the averaging of a set of data, 50 years before Tobias Mayer, but summing the residuals to zero he forced the regression line to pass through the average point. He also distinguished between two inhomogeneous sets of data and might have thought of an optimal solution in terms of bias, though not in terms of effectiveness."" He previously used an averaging method in his 1671 work on Newton's rings, which was unprecedented at the time.[4][5]
 The method of least squares was published by Legendre in 1805,[6] and by Gauss in 1809.[7] Legendre and Gauss both applied the method to the problem of determining, from astronomical observations, the orbits of bodies about the Sun (mostly comets, but also later the then newly discovered minor planets). Gauss published a further development of the theory of least squares in 1821,[8] including a version of the Gauss–Markov theorem.
 The term ""regression"" was coined by Francis Galton in the 19th century to describe a biological phenomenon. The phenomenon was that the heights of descendants of tall ancestors tend to regress down towards a normal average (a phenomenon also known as regression toward the mean).[9][10]
For Galton, regression had only this biological meaning,[11][12] but his work was later extended by Udny Yule and Karl Pearson to a more general statistical context.[13][14] In the work of Yule and Pearson, the joint distribution of the response and explanatory variables is assumed to be Gaussian. This assumption was weakened by R.A. Fisher in his works of 1922 and 1925.[15][16][17] Fisher assumed that the conditional distribution of the response variable is Gaussian, but the joint distribution need not be. In this respect, Fisher's assumption is closer to Gauss's formulation of 1821.
 In the 1950s and 1960s, economists used electromechanical desk calculators to calculate regressions. Before 1970, it sometimes took up to 24 hours to receive the result from one regression.[18]
 Regression methods continue to be an area of active research. In recent decades, new methods have been developed for robust regression, regression involving correlated responses such as time series and growth curves, regression in which the predictor (independent variable) or response variables are curves, images, graphs, or other complex data objects, regression methods accommodating various types of missing data, nonparametric regression, Bayesian methods for regression, regression in which the predictor variables are measured with error, regression with more predictor variables than observations, and causal inference with regression. Modern regression analysis is typically done with statistical and spreadsheet software packages on computers as well as on handheld scientific and graphing calculators.
 In practice, researchers first select a model they would like to estimate and then use their chosen method (e.g., ordinary least squares) to estimate the parameters of that model. Regression models involve the following components:
 In various fields of application, different terminologies are used in place of dependent and independent variables.
 Most regression models propose that 




Y

i




{\displaystyle Y_{i}}

 is a function (regression function) of 




X

i




{\displaystyle X_{i}}

 and 



β


{\displaystyle \beta }

, with 




e

i




{\displaystyle e_{i}}

 representing an additive error term that may stand in for un-modeled determinants of 




Y

i




{\displaystyle Y_{i}}

 or random statistical noise:
 Note that the independent variables 




X

i




{\displaystyle X_{i}}

 are assumed to be free of error. This important assumption is often overlooked, although errors-in-variables models can be used when the independent variables are assumed to contain errors.
 The researchers' goal is to estimate the function 



f
(

X

i


,
β
)


{\displaystyle f(X_{i},\beta )}

 that most closely fits the data. To carry out regression analysis, the form of the function 



f


{\displaystyle f}

 must be specified. Sometimes the form of this function is based on knowledge about the relationship between 




Y

i




{\displaystyle Y_{i}}

 and 




X

i




{\displaystyle X_{i}}

 that does not rely on the data. If no such knowledge is available, a flexible or convenient form for 



f


{\displaystyle f}

 is chosen. For example, a simple univariate regression may propose 



f
(

X

i


,
β
)
=

β

0


+

β

1



X

i




{\displaystyle f(X_{i},\beta )=\beta _{0}+\beta _{1}X_{i}}

, suggesting that the researcher believes 




Y

i


=

β

0


+

β

1



X

i


+

e

i




{\displaystyle Y_{i}=\beta _{0}+\beta _{1}X_{i}+e_{i}}

 to be a reasonable approximation for the statistical process generating the data.
 Once researchers determine their preferred statistical model, different forms of regression analysis provide tools to estimate the parameters 



β


{\displaystyle \beta }

. For example, least squares (including its most common variant, ordinary least squares) finds the value of 



β


{\displaystyle \beta }

 that minimizes the sum of squared errors 




∑

i


(

Y

i


−
f
(

X

i


,
β
)

)

2




{\displaystyle \sum _{i}(Y_{i}-f(X_{i},\beta ))^{2}}

. A given regression method will ultimately provide an estimate of 



β


{\displaystyle \beta }

, usually denoted 






β
^





{\displaystyle {\hat {\beta }}}

 to distinguish the estimate from the true (unknown) parameter value that generated the data. Using this estimate, the researcher can then use the fitted value 







Y

i


^



=
f
(

X

i


,



β
^



)


{\displaystyle {\hat {Y_{i}}}=f(X_{i},{\hat {\beta }})}

 for prediction or to assess the accuracy of the model in explaining the data. Whether the researcher is intrinsically interested in the estimate 






β
^





{\displaystyle {\hat {\beta }}}

 or the predicted value 







Y

i


^





{\displaystyle {\hat {Y_{i}}}}

 will depend on context and their goals. As described in ordinary least squares, least squares is widely used because the estimated function 



f
(

X

i


,



β
^



)


{\displaystyle f(X_{i},{\hat {\beta }})}

 approximates the conditional expectation 



E
(

Y

i



|


X

i


)


{\displaystyle E(Y_{i}|X_{i})}

.[7] However, alternative variants (e.g., least absolute deviations or quantile regression) are useful when researchers want to model other functions 



f
(

X

i


,
β
)


{\displaystyle f(X_{i},\beta )}

.
 It is important to note that there must be sufficient data to estimate a regression model. For example, suppose that a researcher has access to 



N


{\displaystyle N}

 rows of data with one dependent and two independent variables: 



(

Y

i


,

X

1
i


,

X

2
i


)


{\displaystyle (Y_{i},X_{1i},X_{2i})}

. Suppose further that the researcher wants to estimate a bivariate linear model via least squares: 




Y

i


=

β

0


+

β

1



X

1
i


+

β

2



X

2
i


+

e

i




{\displaystyle Y_{i}=\beta _{0}+\beta _{1}X_{1i}+\beta _{2}X_{2i}+e_{i}}

. If the researcher only has access to 



N
=
2


{\displaystyle N=2}

 data points, then they could find infinitely many combinations 



(




β
^




0


,




β
^




1


,




β
^




2


)


{\displaystyle ({\hat {\beta }}_{0},{\hat {\beta }}_{1},{\hat {\beta }}_{2})}

 that explain the data equally well: any combination can be chosen that satisfies 







Y
^




i


=




β
^




0


+




β
^




1



X

1
i


+




β
^




2



X

2
i




{\displaystyle {\hat {Y}}_{i}={\hat {\beta }}_{0}+{\hat {\beta }}_{1}X_{1i}+{\hat {\beta }}_{2}X_{2i}}

, all of which lead to 




∑

i






e
^




i


2


=

∑

i


(




Y
^




i


−
(




β
^




0


+




β
^




1



X

1
i


+




β
^




2



X

2
i


)

)

2


=
0


{\displaystyle \sum _{i}{\hat {e}}_{i}^{2}=\sum _{i}({\hat {Y}}_{i}-({\hat {\beta }}_{0}+{\hat {\beta }}_{1}X_{1i}+{\hat {\beta }}_{2}X_{2i}))^{2}=0}

 and are therefore valid solutions that minimize the sum of squared residuals. To understand why there are infinitely many options, note that the system of 



N
=
2


{\displaystyle N=2}

 equations is to be solved for 3 unknowns, which makes the system underdetermined. Alternatively, one can visualize infinitely many 3-dimensional planes that go through 



N
=
2


{\displaystyle N=2}

 fixed points.
 More generally, to estimate a least squares model with 



k


{\displaystyle k}

 distinct parameters, one must have 



N
≥
k


{\displaystyle N\geq k}

 distinct data points. If 



N
>
k


{\displaystyle N>k}

, then there does not generally exist a set of parameters that will perfectly fit the data. The quantity 



N
−
k


{\displaystyle N-k}

 appears often in regression analysis, and is referred to as the degrees of freedom in the model. Moreover, to estimate a least squares model, the independent variables 



(

X

1
i


,

X

2
i


,
.
.
.
,

X

k
i


)


{\displaystyle (X_{1i},X_{2i},...,X_{ki})}

 must be linearly independent: one must not be able to reconstruct any of the independent variables by adding and multiplying the remaining independent variables. As discussed in ordinary least squares, this condition ensures that 




X

T


X


{\displaystyle X^{T}X}

 is an invertible matrix and therefore that a unique solution 






β
^





{\displaystyle {\hat {\beta }}}

 exists.
 By itself, a regression is simply a calculation using the data. In order to interpret the output of regression as a meaningful statistical quantity that measures real-world relationships, researchers often rely on a number of classical assumptions. These assumptions often include:
 A handful of conditions are sufficient for the least-squares estimator to possess desirable properties: in particular, the Gauss–Markov assumptions imply that the parameter estimates will be unbiased, consistent, and efficient in the class of linear unbiased estimators. Practitioners have developed a variety of methods to maintain some or all of these desirable properties in real-world settings, because these classical assumptions are unlikely to hold exactly. For example, modeling errors-in-variables can lead to reasonable estimates independent variables are measured with errors. Heteroscedasticity-consistent standard errors allow the variance of 




e

i




{\displaystyle e_{i}}

 to change across values of 




X

i




{\displaystyle X_{i}}

. Correlated errors that exist within subsets of the data or follow specific patterns can be handled using clustered standard errors, geographic weighted regression, or Newey–West standard errors, among other techniques. When rows of data correspond to locations in space, the choice of how to model 




e

i




{\displaystyle e_{i}}

 within geographic units can have important consequences.[19][20] The subfield of econometrics is largely focused on developing techniques that allow researchers to make reasonable real-world conclusions in real-world settings, where classical assumptions do not hold exactly.
 In linear regression, the model specification is that the dependent variable, 




y

i




{\displaystyle y_{i}}

 is a linear combination of the parameters (but need not be linear in the independent variables). For example, in simple linear regression for modeling 



n


{\displaystyle n}

 data points there is one independent variable: 




x

i




{\displaystyle x_{i}}

, and two parameters, 




β

0




{\displaystyle \beta _{0}}

 and 




β

1




{\displaystyle \beta _{1}}

:
 In multiple linear regression, there are several independent variables or functions of independent variables.
 Adding a term in 




x

i


2




{\displaystyle x_{i}^{2}}

 to the preceding regression gives:
 This is still linear regression; although the expression on the right hand side is quadratic in the independent variable 




x

i




{\displaystyle x_{i}}

, it is linear in the parameters 




β

0




{\displaystyle \beta _{0}}

, 




β

1




{\displaystyle \beta _{1}}

 and 




β

2


.


{\displaystyle \beta _{2}.}


 In both cases, 




ε

i




{\displaystyle \varepsilon _{i}}

 is an error term and the subscript 



i


{\displaystyle i}

 indexes a particular observation.
 Returning our attention to the straight line case: Given a random sample from the population, we estimate the population parameters and obtain the sample linear regression model:
 The residual, 




e

i


=

y

i


−




y
^




i




{\displaystyle e_{i}=y_{i}-{\widehat {y}}_{i}}

, is the difference between the value of the dependent variable predicted by the model, 







y
^




i




{\displaystyle {\widehat {y}}_{i}}

, and the true value of the dependent variable, 




y

i




{\displaystyle y_{i}}

. One method of estimation is ordinary least squares. This method obtains parameter estimates that minimize the sum of squared residuals, SSR:
 Minimization of this function results in a set of normal equations, a set of simultaneous linear equations in the parameters, which are solved to yield the parameter estimators, 







β
^




0


,




β
^




1




{\displaystyle {\widehat {\beta }}_{0},{\widehat {\beta }}_{1}}

.
 In the case of simple regression, the formulas for the least squares estimates are
 where 






x
¯





{\displaystyle {\bar {x}}}

 is the mean (average) of the 



x


{\displaystyle x}

 values and 






y
¯





{\displaystyle {\bar {y}}}

 is the mean of the 



y


{\displaystyle y}

 values.
 Under the assumption that the population error term has a constant variance, the estimate of that variance is given by:
 This is called the mean square error (MSE) of the regression. The denominator is the sample size reduced by the number of model parameters estimated from the same data, 



(
n
−
p
)


{\displaystyle (n-p)}

 for 



p


{\displaystyle p}

 regressors or 



(
n
−
p
−
1
)


{\displaystyle (n-p-1)}

 if an intercept is used.[21] In this case, 



p
=
1


{\displaystyle p=1}

 so the denominator is 



n
−
2


{\displaystyle n-2}

.
 The standard errors of the parameter estimates are given by
 Under the further assumption that the population error term is normally distributed, the researcher can use these estimated standard errors to create confidence intervals and conduct hypothesis tests about the population parameters.
 In the more general multiple regression model, there are 



p


{\displaystyle p}

 independent variables:
 where 




x

i
j




{\displaystyle x_{ij}}

 is the 



i


{\displaystyle i}

-th observation on the 



j


{\displaystyle j}

-th independent variable.
If the first independent variable takes the value 1 for all 



i


{\displaystyle i}

, 




x

i
1


=
1


{\displaystyle x_{i1}=1}

, then 




β

1




{\displaystyle \beta _{1}}

 is called the regression intercept.
 The least squares parameter estimates are obtained from 



p


{\displaystyle p}

 normal equations. The residual can be written as
 The normal equations are
 In matrix notation, the normal equations are written as
 where the 



i
j


{\displaystyle ij}

 element of 




X



{\displaystyle \mathbf {X} }

 is 




x

i
j




{\displaystyle x_{ij}}

, the 



i


{\displaystyle i}

 element of the column vector 



Y


{\displaystyle Y}

 is 




y

i




{\displaystyle y_{i}}

, and the 



j


{\displaystyle j}

 element of 






β
^





{\displaystyle {\hat {\boldsymbol {\beta }}}}

 is 







β
^




j




{\displaystyle {\hat {\beta }}_{j}}

. Thus 




X



{\displaystyle \mathbf {X} }

 is 



n
×
p


{\displaystyle n\times p}

, 



Y


{\displaystyle Y}

 is 



n
×
1


{\displaystyle n\times 1}

, and 






β
^





{\displaystyle {\hat {\boldsymbol {\beta }}}}

 is 



p
×
1


{\displaystyle p\times 1}

. The solution is
 Once a regression model has been constructed, it may be important to confirm the goodness of fit of the model and the statistical significance of the estimated parameters. Commonly used checks of goodness of fit include the R-squared, analyses of the pattern of residuals and hypothesis testing. Statistical significance can be checked by an F-test of the overall fit, followed by t-tests of individual parameters.
 Interpretations of these diagnostic tests rest heavily on the model's assumptions. Although examination of the residuals can be used to invalidate a model, the results of a t-test or F-test are sometimes more difficult to interpret if the model's assumptions are violated. For example, if the error term does not have a normal distribution, in small samples the estimated parameters will not follow normal distributions and complicate inference. With relatively large samples, however, a central limit theorem can be invoked such that hypothesis testing may proceed using asymptotic approximations.
 Limited dependent variables, which are response variables that are categorical variables or are variables constrained to fall only in a certain range, often arise in econometrics.
 The response variable may be non-continuous (""limited"" to lie on some subset of the real line). For binary (zero or one) variables, if analysis proceeds with least-squares linear regression, the model is called the linear probability model. Nonlinear models for binary dependent variables include the probit and logit model. The multivariate probit model is a standard method of estimating a joint relationship between several binary dependent variables and some independent variables. For categorical variables with more than two values there is the multinomial logit. For ordinal variables with more than two values, there are the ordered logit and ordered probit models. Censored regression models may be used when the dependent variable is only sometimes observed, and Heckman correction type models may be used when the sample is not randomly selected from the population of interest. An alternative to such procedures is linear regression based on polychoric correlation (or polyserial correlations) between the categorical variables. Such procedures differ in the assumptions made about the distribution of the variables in the population. If the variable is positive with low values and represents the repetition of the occurrence of an event, then count models like the Poisson regression or the negative binomial model may be used.
 When the model function is not linear in the parameters, the sum of squares must be minimized by an iterative procedure. This introduces many complications which are summarized in Differences between linear and non-linear least squares.
 Regression models predict a value of the Y variable given known values of the X variables. Prediction within the range of values in the dataset used for model-fitting is known informally as interpolation. Prediction outside this range of the data is known as extrapolation. Performing extrapolation relies strongly on the regression assumptions. The further the extrapolation goes outside the data, the more room there is for the model to fail due to differences between the assumptions and the sample data or the true values.
 A prediction interval that represents the uncertainty may accompany the point prediction. Such intervals tend to expand rapidly as the values of the independent variable(s) moved outside the range covered by the observed data.
 For such reasons and others, some tend to say that it might be unwise to undertake extrapolation.[23]
 The assumption of a particular form for the relation between Y and X is another source of uncertainty. A properly conducted regression analysis will include an assessment of how well the assumed form is matched by the observed data, but it can only do so within the range of values of the independent variables actually available. This means that any extrapolation is particularly reliant on the assumptions being made about the structural form of the regression relationship. If this knowledge includes the fact that the dependent variable cannot go outside a certain range of values, this can be made use of in selecting the model – even if the observed dataset has no values particularly near such bounds. The implications of this step of choosing an appropriate functional form for the regression can be great when extrapolation is considered. At a minimum, it can ensure that any extrapolation arising from a fitted model is ""realistic"" (or in accord with what is known).
 There are no generally agreed methods for relating the number of observations versus the number of independent variables in the model. One method conjectured by Good and Hardin is 



N
=

m

n




{\displaystyle N=m^{n}}

, where 



N


{\displaystyle N}

 is the sample size, 



n


{\displaystyle n}

 is the number of independent variables and 



m


{\displaystyle m}

 is the number of observations needed to reach the desired precision if the model had only one independent variable.[24] For example, a researcher is building a linear regression model using a dataset that contains 1000 patients (



N


{\displaystyle N}

). If the researcher decides that five observations are needed to precisely define a straight line (



m


{\displaystyle m}

), then the maximum number of independent variables (



n


{\displaystyle n}

) the model can support is 4, because
 Although the parameters of a regression model are usually estimated using the method of least squares, other methods which have been used include:
 All major statistical software packages perform least squares regression analysis and inference. Simple linear regression and multiple regression using least squares can be done in some spreadsheet applications and on some calculators. While many statistical software packages can perform various types of nonparametric and robust regression, these methods are less standardized. Different software packages implement different methods, and a method with a given name may be implemented differently in different packages. Specialized regression software has been developed for use in fields such as survey analysis and neuroimaging.
",statist model regress analysi set statist process estim relationship depend variabl often call outcom respons variabl label machin learn parlanc one independ variabl often call regressor predictor covari explanatori variabl featur common form regress analysi linear regress one find line complex linear combin close fit data accord specif mathemat criterion exampl method ordinari least squar comput uniqu line hyperplan minim sum squar differ true data line hyperplan specif mathemat reason see linear regress allow research estim condit expect popul averag valu depend variabl independ variabl take given set valu less common form regress use slightli differ procedur estim altern locat paramet quantil regress necessari condit analysi estim condit expect across broader collect model nonparametr regress regress analysi primarili use two conceptu distinct purpos first regress analysi wide use predict forecast use substanti overlap field machin learn second situat regress analysi use infer causal relationship independ depend variabl importantli regress reveal relationship depend variabl collect independ variabl fix dataset use regress predict infer causal relationship respect research must care justifi exist relationship predict power new context relationship two variabl causal interpret latter especi import research hope estim causal relationship use observ data earliest regress form seen isaac newton work studi equinox credit introduc embryon linear aggress analysi perform averag set data year tobia mayer sum residu zero forc regress line pass averag point also distinguish two inhomogen set data might thought optim solut term bia though term effect previous use averag method work newton ring unpreced time method least squar publish legendr gauss legendr gauss appli method problem determin astronom observ orbit bodi sun mostli comet also later newli discov minor planet gauss publish develop theori least squar includ version theorem term regress coin franci galton centuri describ biolog phenomenon phenomenon height descend tall ancestor tend regress toward normal averag phenomenon also known regress toward mean galton regress biolog mean work later extend udni yule karl pearson gener statist context work yule pearson joint distribut respons explanatori variabl assum gaussian assumpt weaken fisher work fisher assum condit distribut respons variabl gaussian joint distribut need respect fisher assumpt closer gauss formul economist use electromechan desk calcul calcul regress sometim took hour receiv result one regress regress method continu area activ research recent decad new method develop robust regress regress involv correl respons time seri growth curv regress predictor independ variabl respons variabl curv imag graph complex data object regress method accommod variou type miss data nonparametr regress bayesian method regress regress predictor variabl measur error regress predictor variabl observ causal infer regress modern regress analysi typic done statist spreadsheet softwar packag comput well handheld scientif graph calcul practic research first select model would like estim use chosen method ordinari least squar estim paramet model regress model involv follow compon variou field applic differ terminolog use place depend independ variabl regress model propos function regress function x β e repres addit error term may stand determin random statist nois note independ variabl x assum free error import assumpt often overlook although model use independ variabl assum contain error research goal estim function f x β f close fit data carri regress analysi form function f f must specifi sometim form function base knowledg relationship x reli data knowledg avail flexibl conveni form f f chosen exampl simpl univari regress may propos f x β β β x f suggest research believ β β x e reason approxim statist process gener data research determin prefer statist model differ form regress analysi provid tool estim paramet β exampl least squar includ common variant ordinari least squar find valu β minim sum squar error f x β given regress method ultim provid estim β usual denot β distinguish estim true unknown paramet valu gener data use estim research use fit valu f x β predict assess accuraci model explain data whether research intrins interest estim β predict valu depend context goal describ ordinari least squar least squar wide use estim function f x β f approxim condit expect e x e howev altern variant least absolut deviat quantil regress use research want model function f x β f import note must suffici data estim regress model exampl suppos research access n n row data one depend two independ variabl x x suppos research want estim bivari linear model via least squar β β x β x e research access n data point could find infinit mani combin β β β explain data equal well combin chosen satisfi β β x β x lead e β β x β x e therefor valid solut minim sum squar residu understand infinit mani option note system n equat solv unknown make system underdetermin altern one visual infinit mani plane go n fix point gener estim least squar model k k distinct paramet one must n k k distinct data point n k n k gener exist set paramet perfectli fit data quantiti n k appear often regress analysi refer degre freedom model moreov estim least squar model independ variabl x x x k ki must linearli independ one must abl reconstruct independ variabl ad multipli remain independ variabl discuss ordinari least squar condit ensur x x x invert matrix therefor uniqu solut β exist regress simpli calcul use data order interpret output regress meaning statist quantiti measur relationship research often reli number classic assumpt assumpt often includ hand condit suffici estim possess desir properti particular assumpt impli paramet estim unbias consist effici class linear unbias estim practition develop varieti method maintain desir properti set classic assumpt unlik hold exactli exampl model lead reason estim independ variabl measur error standard error allow varianc e chang across valu x correl error exist within subset data follow specif pattern handl use cluster standard error geograph weight regress standard error among techniqu row data correspond locat space choic model e within geograph unit import consequ subfield econometr larg focus develop techniqu allow research make reason conclus set classic assumpt hold exactli linear regress model specif depend variabl linear combin paramet need linear independ variabl exampl simpl linear regress model n n data point one independ variabl x two paramet β β multipl linear regress sever independ variabl function independ variabl ad term x preced regress give still linear regress although express right hand side quadrat independ variabl x linear paramet β β β case ε error term subscript index particular observ return attent straight line case given random sampl popul estim popul paramet obtain sampl linear regress model residu e differ valu depend variabl predict model true valu depend variabl one method estim ordinari least squar method obtain paramet estim minim sum squar residu ssr minim function result set normal equat set simultan linear equat paramet solv yield paramet estim β β case simpl regress formula least squar estim x x mean averag x x valu mean valu assumpt popul error term constant varianc estim varianc given call mean squar error mse regress denomin sampl size reduc number model paramet estim data n p p p regressor n p intercept use case p denomin n standard error paramet estim given assumpt popul error term normal distribut research use estim standard error creat confid interv conduct hypothesi test popul paramet gener multipl regress model p p independ variabl x j ij observ j j independ variabl first independ variabl take valu x β call regress intercept least squar paramet estim obtain p p normal equat residu written normal equat matrix notat normal equat written j ij element x x x j ij element column vector j j element β β j j thu x x n p p n β p solut regress model construct may import confirm good fit model statist signific estim paramet commonli use check good fit includ analys pattern residu hypothesi test statist signific check overal fit follow individu paramet interpret diagnost test rest heavili model assumpt although examin residu use invalid model result sometim difficult interpret model assumpt violat exampl error term normal distribut small sampl estim paramet follow normal distribut complic infer rel larg sampl howev central limit theorem invok hypothesi test may proceed use asymptot approxim limit depend variabl respons variabl categor variabl variabl constrain fall certain rang often aris econometr respons variabl may limit lie subset real line binari zero one variabl analysi proce linear regress model call linear probabl model nonlinear model binari depend variabl includ probit logit model multivari probit model standard method estim joint relationship sever binari depend variabl independ variabl categor variabl two valu multinomi logit ordin variabl two valu order logit order probit model censor regress model may use depend variabl sometim observ heckman correct type model may use sampl randomli select popul interest altern procedur linear regress base polychor correl polyseri correl categor variabl procedur differ assumpt made distribut variabl popul variabl posit low valu repres repetit occurr event count model like poisson regress neg binomi model may use model function linear paramet sum squar must minim iter procedur introduc mani complic summar differ linear least squar regress model predict valu variabl given known valu x variabl predict within rang valu dataset use known inform interpol predict outsid rang data known extrapol perform extrapol reli strongli regress assumpt extrapol goe outsid data room model fail due differ assumpt sampl data true valu predict interv repres uncertainti may accompani point predict interv tend expand rapidli valu independ variabl move outsid rang cover observ data reason other tend say might unwis undertak extrapol assumpt particular form relat x anoth sourc uncertainti properli conduct regress analysi includ assess well assum form match observ data within rang valu independ variabl actual avail mean extrapol particularli reliant assumpt made structur form regress relationship knowledg includ fact depend variabl go outsid certain rang valu made use select model even observ dataset valu particularli near bound implic step choos appropri function form regress great extrapol consid minimum ensur extrapol aris fit model realist accord known gener agre method relat number observ versu number independ variabl model one method conjectur good hardin n n n n n sampl size n n number independ variabl number observ need reach desir precis model one independ variabl exampl research build linear regress model use dataset contain patient n n research decid five observ need precis defin straight line maximum number independ variabl n n model support although paramet regress model usual estim use method least squar method use includ major statist softwar packag perform least squar regress analysi infer simpl linear regress multipl regress use least squar done spreadsheet applic calcul mani statist softwar packag perform variou type nonparametr robust regress method less standard differ softwar packag implement differ method method given name may implement differ differ packag special regress softwar develop use field survey analysi neuroimag
Cluster analysis,https://en.wikipedia.org/wiki/Cluster_analysis,"Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some specific sense defined by the analyst) to each other than to those in other groups (clusters). It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.
 Cluster analysis refers to a family of algorithms and tasks rather than one specific algorithm. It can be achieved by various algorithms that differ significantly in their understanding of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances between cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including parameters such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties.
 Besides the term clustering, there is a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek: βότρυς 'grape'), typological analysis, and community detection. The subtle differences are often in the use of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest.
 Cluster analysis originated in anthropology by Driver and Kroeber in 1932[1] and introduced to psychology by Joseph Zubin in 1938[2] and Robert Tryon in 1939[3] and famously used by Cattell beginning in 1943[4] for trait theory classification in personality psychology.
 The notion of a ""cluster"" cannot be precisely defined, which is one of the reasons why there are so many clustering algorithms.[5] There is a common denominator: a group of data objects. However, different researchers employ different cluster models, and for each of these cluster models again different algorithms can be given. The notion of a cluster, as found by different algorithms, varies significantly in its properties. Understanding these ""cluster models"" is key to understanding the differences between the various algorithms. Typical cluster models include:
 A ""clustering"" is essentially a set of such clusters, usually containing all objects in the data set. Additionally, it may specify the relationship of the clusters to each other, for example, a hierarchy of clusters embedded in each other. Clusterings can be roughly distinguished as:
 There are also finer distinctions possible, for example:
 As listed above, clustering algorithms can be categorized based on their cluster model. The following overview will only list the most prominent examples of clustering algorithms, as there are possibly over 100 published clustering algorithms. Not all provide models for their clusters and can thus not easily be categorized. An overview of algorithms explained in Wikipedia can be found in the list of statistics algorithms.
 There is no objectively ""correct"" clustering algorithm, but as it was noted, ""clustering is in the eye of the beholder.""[5] In fact, an axiomatic approach to clustering demonstrates that it is impossible for any clustering method to meet three fundamental properties simultaneously: scale invariance (results remain unchanged under proportional scaling of distances), richness (all possible partitions of the data can be achieved), and consistency between distances and the clustering structure.[7] The most appropriate clustering algorithm for a particular problem often needs to be chosen experimentally, unless there is a mathematical reason to prefer one cluster model over another. An algorithm that is designed for one kind of model will generally fail on a data set that contains a radically different kind of model.[5] For example, k-means cannot find non-convex clusters.[5] Most traditional clustering methods assume the clusters exhibit a spherical, elliptical or convex shape.[8]
 Connectivity-based clustering, also known as hierarchical clustering, is based on the core idea of objects being more related to nearby objects than to objects farther away. These algorithms connect ""objects"" to form ""clusters"" based on their distance. A cluster can be described largely by the maximum distance needed to connect parts of the cluster. At different distances, different clusters will form, which can be represented using a dendrogram, which explains where the common name ""hierarchical clustering"" comes from: these algorithms do not provide a single partitioning of the data set, but instead provide an extensive hierarchy of clusters that merge with each other at certain distances. In a dendrogram, the y-axis marks the distance at which the clusters merge, while the objects are placed along the x-axis such that the clusters don't mix.
 Connectivity-based clustering is a whole family of methods that differ by the way distances are computed. Apart from the usual choice of distance functions, the user also needs to decide on the linkage criterion (since a cluster consists of multiple objects, there are multiple candidates to compute the distance) to use. Popular choices are known as single-linkage clustering (the minimum of object distances), complete linkage clustering (the maximum of object distances), and UPGMA or WPGMA (""Unweighted or Weighted Pair Group Method with Arithmetic Mean"", also known as average linkage clustering). Furthermore, hierarchical clustering can be agglomerative (starting with single elements and aggregating them into clusters) or divisive (starting with the complete data set and dividing it into partitions).
 These methods will not produce a unique partitioning of the data set, but a hierarchy from which the user still needs to choose appropriate clusters. They are not very robust towards outliers, which will either show up as additional clusters or even cause other clusters to merge (known as ""chaining phenomenon"", in particular with single-linkage clustering). In the general case, the complexity is 





O


(

n

3


)


{\displaystyle {\mathcal {O}}(n^{3})}

 for agglomerative clustering and 





O


(

2

n
−
1


)


{\displaystyle {\mathcal {O}}(2^{n-1})}

 for divisive clustering,[9] which makes them too slow for large data sets. For some special cases, optimal efficient methods (of complexity 





O


(

n

2


)


{\displaystyle {\mathcal {O}}(n^{2})}

) are known: SLINK[10] for single-linkage and CLINK[11] for complete-linkage clustering.
 In centroid-based clustering, each cluster is represented by a central vector, which is not necessarily a member of the data set. When the number of clusters is fixed to k, k-means clustering gives a formal definition as an optimization problem: find the k cluster centers and assign the objects to the nearest cluster center, such that the squared distances from the cluster are minimized.
 The optimization problem itself is known to be NP-hard, and thus the common approach is to search only for approximate solutions. A particularly well-known approximate method is Lloyd's algorithm,[12] often just referred to as ""k-means algorithm"" (although another algorithm introduced this name). It does however only find a local optimum, and is commonly run multiple times with different random initializations. Variations of k-means often include such optimizations as choosing the best of multiple runs, but also restricting the centroids to members of the data set (k-medoids), choosing medians (k-medians clustering), choosing the initial centers less randomly (k-means++) or allowing a fuzzy cluster assignment (fuzzy c-means).
 Most k-means-type algorithms require the number of clusters – k – to be specified in advance, which is considered to be one of the biggest drawbacks of these algorithms. Furthermore, the algorithms prefer clusters of approximately similar size, as they will always assign an object to the nearest centroid. This often leads to incorrectly cut borders of clusters (which is not surprising since the algorithm optimizes cluster centers, not cluster borders).
 K-means has a number of interesting theoretical properties. First, it partitions the data space into a structure known as a Voronoi diagram. Second, it is conceptually close to nearest neighbor classification, and as such is popular in machine learning. Third, it can be seen as a variation of model-based clustering, and Lloyd's algorithm as a variation of the Expectation-maximization algorithm for this model discussed below.
 Centroid-based clustering problems such as k-means and k-medoids are special cases of the uncapacitated, metric facility location problem, a canonical problem in the operations research and computational geometry communities. In a basic facility location problem (of which there are numerous variants that model more elaborate settings), the task is to find the best warehouse locations to optimally service a given set of consumers. One may view ""warehouses"" as cluster centroids and ""consumer locations"" as the data to be clustered. This makes it possible to apply the well-developed algorithmic solutions from the facility location literature to the presently considered centroid-based clustering problem.
 The clustering framework most closely related to statistics is model-based clustering, which is based on distribution models. This approach models the data as arising from a mixture of probability distributions. It has the advantages of providing principled statistical answers to questions such as how many clusters there are, what clustering method or model to use, and how to detect and deal with outliers.
 While the theoretical foundation of these methods is excellent, they suffer from overfitting unless constraints are put on the model complexity. A more complex model will usually be able to explain the data better, which makes choosing the appropriate model complexity inherently difficult. Standard model-based clustering methods include more parsimonious models based on the eigenvalue decomposition of the covariance matrices, that provide a balance between overfitting and fidelity to the data.
 One prominent method is known as Gaussian mixture models (using the expectation-maximization algorithm). Here, the data set is usually modeled with a fixed (to avoid overfitting) number of Gaussian distributions that are initialized randomly and whose parameters are iteratively optimized to better fit the data set. This will converge to a local optimum, so multiple runs may produce different results. In order to obtain a hard clustering, objects are often then assigned to the Gaussian distribution they most likely belong to; for soft clusterings, this is not necessary.
 Distribution-based clustering produces complex models for clusters that can capture correlation and dependence between attributes. However, these algorithms put an extra burden on the user: for many real data sets, there may be no concisely defined mathematical model (e.g. assuming Gaussian distributions is a rather strong assumption on the data).
 In density-based clustering,[13] clusters are defined as areas of higher density than the remainder of the data set. Objects in sparse areas – that are required to separate clusters – are usually considered to be noise and border points.
 The most popular[14] density-based clustering method is DBSCAN.[15] In contrast to many newer methods, it features a well-defined cluster model called ""density-reachability"". Similar to linkage-based clustering, it is based on connecting points within certain distance thresholds. However, it only connects points that satisfy a density criterion, in the original variant defined as a minimum number of other objects within this radius. A cluster consists of all density-connected objects (which can form a cluster of an arbitrary shape, in contrast to many other methods) plus all objects that are within these objects' range. Another interesting property of DBSCAN is that its complexity is fairly low – it requires a linear number of range queries on the database – and that it will discover essentially the same results (it is deterministic for core and noise points, but not for border points) in each run, therefore there is no need to run it multiple times. OPTICS[16] is a generalization of DBSCAN that removes the need to choose an appropriate value for the range parameter 



ε


{\displaystyle \varepsilon }

, and produces a hierarchical result related to that of linkage clustering. DeLi-Clu,[17] Density-Link-Clustering combines ideas from single-linkage clustering and OPTICS, eliminating the 



ε


{\displaystyle \varepsilon }

 parameter entirely and offering performance improvements over OPTICS by using an R-tree index.
 The key drawback of DBSCAN and OPTICS is that they expect some kind of density drop to detect cluster borders. On data sets with, for example, overlapping Gaussian distributions – a common use case in artificial data – the cluster borders produced by these algorithms will often look arbitrary, because the cluster density decreases continuously. On a data set consisting of mixtures of Gaussians, these algorithms are nearly always outperformed by methods such as EM clustering that are able to precisely model this kind of data.
 Mean-shift is a clustering approach where each object is moved to the densest area in its vicinity, based on kernel density estimation. Eventually, objects converge to local maxima of density. Similar to k-means clustering, these ""density attractors"" can serve as representatives for the data set, but mean-shift can detect arbitrary-shaped clusters similar to DBSCAN. Due to the expensive iterative procedure and density estimation, mean-shift is usually slower than DBSCAN or k-Means. Besides that, the applicability of the mean-shift algorithm to multidimensional data is hindered by the unsmooth behaviour of the kernel density estimate, which results in over-fragmentation of cluster tails.[17]
 The grid-based technique is used for a multi-dimensional data set.[18] In this technique, we create a grid structure, and the comparison is performed on grids (also known as cells). The grid-based technique is fast and has low computational complexity. There are two types of grid-based clustering methods: STING and CLIQUE. Steps involved in grid-based clustering algorithm are:
 In recent years, considerable effort has been put into improving the performance of existing algorithms.[19][20] Among them are CLARANS,[21] and BIRCH.[22] With the recent need to process larger and larger data sets (also known as big data), the willingness to trade semantic meaning of the generated clusters for performance has been increasing. This led to the development of pre-clustering methods such as canopy clustering, which can process huge data sets efficiently, but the resulting ""clusters"" are merely a rough pre-partitioning of the data set to then analyze the partitions with existing slower methods such as k-means clustering.
 For high-dimensional data, many of the existing methods fail due to the curse of dimensionality, which renders particular distance functions problematic in high-dimensional spaces. This led to new clustering algorithms for high-dimensional data that focus on subspace clustering (where only some attributes are used, and cluster models include the relevant attributes for the cluster) and correlation clustering that also looks for arbitrary rotated (""correlated"") subspace clusters that can be modeled by giving a correlation of their attributes.[23] Examples for such clustering algorithms are CLIQUE[24] and SUBCLU.[25]
 Ideas from density-based clustering methods (in particular the DBSCAN/OPTICS family of algorithms) have been adapted to subspace clustering (HiSC,[26] hierarchical subspace clustering and DiSH[27]) and correlation clustering (HiCO,[28] hierarchical correlation clustering, 4C[29] using ""correlation connectivity"" and ERiC[30] exploring hierarchical density-based correlation clusters).
 Several different clustering systems based on mutual information have been proposed. One is Marina Meilă's variation of information metric;[31] another provides hierarchical clustering.[32] Using genetic algorithms, a wide range of different fit-functions can be optimized, including mutual information.[33] Also belief propagation, a recent development in computer science and statistical physics, has led to the creation of new types of clustering algorithms.[34]
 Evaluation (or ""validation"") of clustering results is as difficult as the clustering itself.[35] Popular approaches involve ""internal"" evaluation, where the clustering is summarized to a single quality score, ""external"" evaluation, where the clustering is compared to an existing ""ground truth"" classification, ""manual"" evaluation by a human expert, and ""indirect"" evaluation by evaluating the utility of the clustering in its intended application.[36]
 Internal evaluation measures suffer from the problem that they represent functions that themselves can be seen as a clustering objective. For example, one could cluster the data set by the Silhouette coefficient; except that there is no known efficient algorithm for this. By using such an internal measure for evaluation, one rather compares the similarity of the optimization problems,[36] and not necessarily how useful the clustering is.
 External evaluation has similar problems: if we have such ""ground truth"" labels, then we would not need to cluster; and in practical applications we usually do not have such labels. On the other hand, the labels only reflect one possible partitioning of the data set, which does not imply that there does not exist a different, and maybe even better, clustering.
 Neither of these approaches can therefore ultimately judge the actual quality of a clustering, but this needs human evaluation,[36] which is highly subjective. Nevertheless, such statistics can be quite informative in identifying bad clusterings,[37] but one should not dismiss subjective human evaluation.[37]
 When a clustering result is evaluated based on the data that was clustered itself, this is called internal evaluation. These methods usually assign the best score to the algorithm that produces clusters with high similarity within a cluster and low similarity between clusters. One drawback of using internal criteria in cluster evaluation is that high scores on an internal measure do not necessarily result in effective information retrieval applications.[38] Additionally, this evaluation is biased towards algorithms that use the same cluster model. For example, k-means clustering naturally optimizes object distances, and a distance-based internal criterion will likely overrate the resulting clustering.
 Therefore, the internal evaluation measures are best suited to get some insight into situations where one algorithm performs better than another, but this shall not imply that one algorithm produces more valid results than another.[5] Validity as measured by such an index depends on the claim that this kind of structure exists in the data set. An algorithm designed for some kind of models has no chance if the data set contains a radically different set of models, or if the evaluation measures a radically different criterion.[5] For example, k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. On a data set with non-convex clusters neither the use of k-means, nor of an evaluation criterion that assumes convexity, is sound.
 More than a dozen of internal evaluation measures exist, usually based on the intuition that items in the same cluster should be more similar than items in different clusters.[39]: 115–121  For example, the following methods can be used to assess the quality of clustering algorithms based on internal criterion:
 In external evaluation, clustering results are evaluated based on data that was not used for clustering, such as known class labels and external benchmarks. Such benchmarks consist of a set of pre-classified items, and these sets are often created by (expert) humans. Thus, the benchmark sets can be thought of as a gold standard for evaluation.[35] These types of evaluation methods measure how close the clustering is to the predetermined benchmark classes. However, it has recently been discussed whether this is adequate for real data, or only on synthetic data sets with a factual ground truth, since classes can contain internal structure, the attributes present may not allow separation of clusters or the classes may contain anomalies.[42] Additionally, from a knowledge discovery point of view, the reproduction of known knowledge may not necessarily be the intended result.[42] In the special scenario of constrained clustering, where meta information (such as class labels) is used already in the clustering process, the hold-out of information for evaluation purposes is non-trivial.[43]
 A number of measures are adapted from variants used to evaluate classification tasks. In place of counting the number of times a class was correctly assigned to a single data point (known as true positives), such pair counting metrics assess whether each pair of data points that is truly in the same cluster is predicted to be in the same cluster.[35]
 As with internal evaluation, several external evaluation measures exist,[39]: 125–129  for example:
 One issue with the Rand index is that false positives and false negatives are equally weighted. This may be an undesirable characteristic for some clustering applications. The F-measure addresses this concern,[citation needed] as does the chance-corrected adjusted Rand index.
 To measure cluster tendency is to measure to what degree clusters exist in the data to be clustered, and may be performed as an initial test, before attempting clustering. One way to do this is to compare the data against random data. On average, random data should not have clusters.
",cluster analysi cluster task group set object way object group call cluster similar specif sens defin analyst group cluster main task exploratori data analysi common techniqu statist data analysi use mani field includ pattern recognit imag analysi inform retriev bioinformat data compress comput graphic machin learn cluster analysi refer famili algorithm task rather one specif algorithm achiev variou algorithm differ significantli understand constitut cluster effici find popular notion cluster includ group small distanc cluster member dens area data space interv particular statist distribut cluster therefor formul optim problem appropri cluster algorithm paramet set includ paramet distanc function use densiti threshold number expect cluster depend individu data set intend use result cluster analysi automat task iter process knowledg discoveri interact optim involv trial failur often necessari modifi data preprocess model paramet result achiev desir properti besid term cluster number term similar mean includ automat classif numer taxonomi botryolog greek βότρυς typolog analysi commun detect subtl differ often use result data mine result group matter interest automat classif result discrimin power interest cluster analysi origin anthropolog driver kroeber introduc psycholog joseph zubin robert tryon famous use cattel begin trait theori classif person psycholog notion cluster precis defin one reason mani cluster algorithm common denomin group data object howev differ research employ differ cluster model cluster model differ algorithm given notion cluster found differ algorithm vari significantli properti understand cluster model key understand differ variou algorithm typic cluster model includ cluster essenti set cluster usual contain object data set addit may specifi relationship cluster exampl hierarchi cluster embed cluster roughli distinguish also finer distinct possibl exampl list cluster algorithm categor base cluster model follow overview list promin exampl cluster algorithm possibl publish cluster algorithm provid model cluster thu easili categor overview algorithm explain wikipedia found list statist algorithm object correct cluster algorithm note cluster eye behold fact axiomat approach cluster demonstr imposs cluster method meet three fundament properti simultan scale invari result remain unchang proport scale distanc rich possibl partit data achiev consist distanc cluster structur appropri cluster algorithm particular problem often need chosen experiment unless mathemat reason prefer one cluster model anoth algorithm design one kind model gener fail data set contain radic differ kind model exampl find cluster tradit cluster method assum cluster exhibit spheric ellipt convex shape cluster also known hierarch cluster base core idea object relat nearbi object object farther away algorithm connect object form cluster base distanc cluster describ larg maximum distanc need connect part cluster differ distanc differ cluster form repres use dendrogram explain common name hierarch cluster come algorithm provid singl partit data set instead provid extens hierarchi cluster merg certain distanc dendrogram mark distanc cluster merg object place along cluster mix cluster whole famili method differ way distanc comput apart usual choic distanc function user also need decid linkag criterion sinc cluster consist multipl object multipl candid comput distanc use popular choic known cluster minimum object distanc complet linkag cluster maximum object distanc upgma wpgma unweight weight pair group method arithmet mean also known averag linkag cluster furthermor hierarch cluster agglom start singl element aggreg cluster divis start complet data set divid partit method produc uniqu partit data set hierarchi user still need choos appropri cluster robust toward outlier either show addit cluster even caus cluster merg known chain phenomenon particular cluster gener case complex n agglom cluster n divis cluster make slow larg data set special case optim effici method complex n known slink clink cluster cluster cluster repres central vector necessarili member data set number cluster fix k cluster give formal definit optim problem find k cluster center assign object nearest cluster center squar distanc cluster minim optim problem known thu common approach search approxim solut particularli approxim method lloyd algorithm often refer algorithm although anoth algorithm introduc name howev find local optimum commonli run multipl time differ random initi variat often includ optim choos best multipl run also restrict centroid member data set choos median cluster choos initi center less randomli allow fuzzi cluster assign fuzzi algorithm requir number cluster k specifi advanc consid one biggest drawback algorithm furthermor algorithm prefer cluster approxim similar size alway assign object nearest centroid often lead incorrectli cut border cluster surpris sinc algorithm optim cluster center cluster border number interest theoret properti first partit data space structur known voronoi diagram second conceptu close nearest neighbor classif popular machin learn third seen variat cluster lloyd algorithm variat algorithm model discuss cluster problem special case uncapacit metric facil locat problem canon problem oper research comput geometri commun basic facil locat problem numer variant model elabor set task find best warehous locat optim servic given set consum one may view warehous cluster centroid consum locat data cluster make possibl appli algorithm solut facil locat literatur present consid cluster problem cluster framework close relat statist cluster base distribut model approach model data aris mixtur probabl distribut advantag provid principl statist answer question mani cluster cluster method model use detect deal outlier theoret foundat method excel suffer overfit unless constraint put model complex complex model usual abl explain data better make choos appropri model complex inher difficult standard cluster method includ parsimoni model base eigenvalu decomposit covari matric provid balanc overfit fidel data one promin method known gaussian mixtur model use algorithm data set usual model fix avoid overfit number gaussian distribut initi randomli whose paramet iter optim better fit data set converg local optimum multipl run may produc differ result order obtain hard cluster object often assign gaussian distribut like belong soft cluster necessari cluster produc complex model cluster captur correl depend attribut howev algorithm put extra burden user mani real data set may concis defin mathemat model assum gaussian distribut rather strong assumpt data cluster cluster defin area higher densiti remaind data set object spars area requir separ cluster usual consid nois border point popular cluster method dbscan contrast mani newer method featur cluster model call similar cluster base connect point within certain distanc threshold howev connect point satisfi densiti criterion origin variant defin minimum number object within radiu cluster consist object form cluster arbitrari shape contrast mani method plu object within object rang anoth interest properti dbscan complex fairli low requir linear number rang queri databas discov essenti result determinist core nois point border point run therefor need run multipl time optic gener dbscan remov need choos appropri valu rang paramet ε produc hierarch result relat linkag cluster combin idea cluster optic elimin ε paramet entir offer perform improv optic use index key drawback dbscan optic expect kind densiti drop detect cluster border data set exampl overlap gaussian distribut common use case artifici data cluster border produc algorithm often look arbitrari cluster densiti decreas continu data set consist mixtur gaussian algorithm nearli alway outperform method em cluster abl precis model kind data cluster approach object move densest area vicin base kernel densiti estim eventu object converg local maxima densiti similar cluster densiti attractor serv repres data set detect cluster similar dbscan due expens iter procedur densiti estim usual slower dbscan besid applic algorithm multidimension data hinder unsmooth behaviour kernel densiti estim result cluster tail techniqu use data set techniqu creat grid structur comparison perform grid also known cell techniqu fast low comput complex two type cluster method sting cliqu step involv cluster algorithm recent year consider effort put improv perform exist algorithm among claran birch recent need process larger larger data set also known big data willing trade semant mean gener cluster perform increas led develop method canopi cluster process huge data set effici result cluster mere rough data set analyz partit exist slower method cluster data mani exist method fail due curs dimension render particular distanc function problemat space led new cluster algorithm data focu subspac cluster attribut use cluster model includ relev attribut cluster correl cluster also look arbitrari rotat correl subspac cluster model give correl attribut exampl cluster algorithm cliqu subclu idea cluster method particular famili algorithm adapt subspac cluster hisc hierarch subspac cluster dish correl cluster hico hierarch correl cluster use correl connect eric explor hierarch correl cluster sever differ cluster system base mutual inform propos one marina meilă variat inform metric anoth provid hierarch cluster use genet algorithm wide rang differ optim includ mutual inform also belief propag recent develop comput scienc statist physic led creation new type cluster algorithm evalu valid cluster result difficult cluster popular approach involv intern evalu cluster summar singl qualiti score extern evalu cluster compar exist ground truth classif manual evalu human expert indirect evalu evalu util cluster intend applic intern evalu measur suffer problem repres function seen cluster object exampl one could cluster data set silhouett coeffici except known effici algorithm use intern measur evalu one rather compar similar optim problem necessarili use cluster extern evalu similar problem ground truth label would need cluster practic applic usual label hand label reflect one possibl partit data set impli exist differ mayb even better cluster neither approach therefor ultim judg actual qualiti cluster need human evalu highli subject nevertheless statist quit inform identifi bad cluster one dismiss subject human evalu cluster result evalu base data cluster call intern evalu method usual assign best score algorithm produc cluster high similar within cluster low similar cluster one drawback use intern criteria cluster evalu high score intern measur necessarili result effect inform retriev applic addit evalu bias toward algorithm use cluster model exampl cluster natur optim object distanc intern criterion like overr result cluster therefor intern evalu measur best suit get insight situat one algorithm perform better anoth shall impli one algorithm produc valid result anoth valid measur index depend claim kind structur exist data set algorithm design kind model chanc data set contain radic differ set model evalu measur radic differ criterion exampl cluster find convex cluster mani evalu index assum convex cluster data set cluster neither use evalu criterion assum convex sound dozen intern evalu measur exist usual base intuit item cluster similar item differ cluster exampl follow method use assess qualiti cluster algorithm base intern criterion extern evalu cluster result evalu base data use cluster known class label extern benchmark benchmark consist set item set often creat expert human thu benchmark set thought gold standard evalu type evalu method measur close cluster predetermin benchmark class howev recent discuss whether adequ real data synthet data set factual ground truth sinc class contain intern structur attribut present may allow separ cluster class may contain anomali addit knowledg discoveri point view reproduct known knowledg may necessarili intend result special scenario constrain cluster meta inform class label use alreadi cluster process inform evalu purpos number measur adapt variant use evalu classif task place count number time class correctli assign singl data point known true posit pair count metric assess whether pair data point truli cluster predict cluster intern evalu sever extern evalu measur exist exampl one issu rand index fals posit fals neg equal weight may undesir characterist cluster applic address concern citat need adjust rand index measur cluster tendenc measur degre cluster exist data cluster may perform initi test attempt cluster one way compar data random data averag random data cluster
Dimensionality reduction,https://en.wikipedia.org/wiki/Dimensionality_reduction,"Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable. Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics.[1]
 Methods are commonly divided into linear and nonlinear approaches.[1] Approaches can also be divided into feature selection and feature extraction.[2] Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses.
 The process of feature selection aims to find a suitable subset of the input variables (features, or attributes) for the task at hand. The three strategies are: the filter strategy (e.g., information gain), the wrapper strategy (e.g., accuracy-guided search), and the embedded strategy (features are added or removed while building the model based on prediction errors).
 Data analysis such as regression or classification can be done in the reduced space more accurately than in the original space.[3]
 Feature projection (also called feature extraction) transforms the data from the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist.[4][5] For multidimensional data, tensor representation can be used in dimensionality reduction through multilinear subspace learning.[6]
 The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the covariance (and sometimes the correlation) matrix of the data is constructed and the eigenvectors on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system, because they often contribute the vast majority of the system's energy, especially in low-dimensional systems. Still, this must be proved on a case-by-case basis as not all systems exhibit this behavior. The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors. [citation needed]
 NMF decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist,[7][8] such as astronomy.[9][10] NMF is well known since the multiplicative update rule by Lee & Seung,[7] which has been continuously developed: the inclusion of uncertainties,[9] the consideration of missing data and parallel computation,[11] sequential construction[11] which leads to the stability and linearity of NMF,[10] as well as other updates including handling missing data in digital image processing.[12]
 With a stable component basis during construction, and a linear modeling process, sequential NMF[11] is able to preserve the flux in direct imaging of circumstellar structures in astronomy,[10] as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar discs. In comparison with PCA, NMF does not remove the mean of the matrices, which leads to physical non-negative fluxes; therefore NMF is able to preserve more information than PCA as demonstrated by Ren et al.[10]
 Principal component analysis can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is called kernel PCA.
 Other prominent nonlinear techniques include manifold learning techniques such as Isomap, locally linear embedding (LLE),[13] Hessian LLE, Laplacian eigenmaps, and methods based on tangent space analysis.[14] These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA.
 More recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using semidefinite programming. The most prominent example of such a technique is maximum variance unfolding (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space) while maximizing the distances between points that are not nearest neighbors.
 An alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include: classical multidimensional scaling, which is identical to PCA; Isomap, which uses geodesic distances in the data space; diffusion maps, which use diffusion distances in the data space; t-distributed stochastic neighbor embedding (t-SNE), which minimizes the divergence between distributions over pairs of points; and curvilinear component analysis.
 A different approach to nonlinear dimensionality reduction is through the use of autoencoders, a special kind of feedforward neural networks with a bottleneck hidden layer.[15] The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of restricted Boltzmann machines) that is followed by a finetuning stage based on backpropagation.
 Linear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events.
 GDA deals with nonlinear discriminant analysis using kernel function operator. The underlying theory is close to the support-vector machines (SVM) insofar as the GDA method provides a mapping of the input vectors into high-dimensional feature space.[16][17] Similar to LDA, the objective of GDA is to find a projection for the features into a lower dimensional space by maximizing the ratio of between-class scatter to within-class scatter.
 Autoencoders can be used to learn nonlinear dimension reduction functions and codings together with an inverse function from the coding to the original representation.
 T-distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimensionality reduction technique useful for the visualization of high-dimensional datasets. It is not recommended for use in analysis such as clustering or outlier detection since it does not necessarily preserve densities or distances well.[18]
 Uniform manifold approximation and projection (UMAP) is a nonlinear dimensionality reduction technique. Visually, it is similar to t-SNE, but it assumes that the data is uniformly distributed on a locally connected Riemannian manifold and that the Riemannian metric is locally constant or approximately locally constant.
 For high-dimensional datasets, dimension reduction is usually performed prior to applying a k-nearest neighbors (k-NN) algorithm in order to mitigate the curse of dimensionality.[19]
 Feature extraction and dimension reduction can be combined in one step, using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA), or non-negative matrix factorization (NMF) techniques to pre-process the data, followed by clustering via k-NN on feature vectors in a reduced-dimension space. In machine learning, this process is also called low-dimensional embedding.[20]
 For high-dimensional datasets (e.g., when performing similarity search on live video streams, DNA data, or high-dimensional time series), running a fast approximate k-NN search using locality-sensitive hashing, random projection,[21] ""sketches"",[22] or other high-dimensional similarity search techniques from the VLDB conference toolbox may be the only feasible option.
 A dimensionality reduction technique that is sometimes used in neuroscience is maximally informative dimensions,[23] which finds a lower-dimensional representation of a dataset such that as much information as possible about the original data is preserved.
",dimension reduct dimens reduct transform data space space represent retain meaning properti origin data ideal close intrins dimens work space undesir mani reason raw data often spars consequ curs dimension analyz data usual comput intract dimension reduct common field deal larg number observ larg number variabl signal process speech recognit neuroinformat bioinformat method commonli divid linear nonlinear approach approach also divid featur select featur extract dimension reduct use nois reduct data visual cluster analysi intermedi step facilit analys process featur select aim find suitabl subset input variabl featur attribut task hand three strategi filter strategi inform gain wrapper strategi search embed strategi featur ad remov build model base predict error data analysi regress classif done reduc space accur origin space featur project also call featur extract transform data space space fewer dimens data transform may linear princip compon analysi pca mani nonlinear dimension reduct techniqu also exist multidimension data tensor represent use dimension reduct multilinear subspac learn main linear techniqu dimension reduct princip compon analysi perform linear map data space way varianc data represent maxim practic covari sometim correl matrix data construct eigenvector matrix comput eigenvector correspond largest eigenvalu princip compon use reconstruct larg fraction varianc origin data moreov first eigenvector often interpret term physic behavior system often contribut vast major system energi especi system still must prove basi system exhibit behavior origin space dimens number point reduc data loss hope retain import varianc space span eigenvector citat need nmf decompos matrix product two one promis tool field signal exist astronomi nmf well known sinc multipl updat rule lee seung continu develop inclus uncertainti consider miss data parallel comput sequenti construct lead stabil linear nmf well updat includ handl miss data digit imag process stabl compon basi construct linear model process sequenti nmf abl preserv flux direct imag circumstellar structur astronomi one method detect exoplanet especi direct imag circumstellar disc comparison pca nmf remov mean matric lead physic flux therefor nmf abl preserv inform pca demonstr ren et al princip compon analysi employ nonlinear way mean kernel trick result techniqu capabl construct nonlinear map maxim varianc data result techniqu call kernel pca promin nonlinear techniqu includ manifold learn techniqu isomap local linear embed lle hessian lle laplacian eigenmap method base tangent space analysi techniqu construct data represent use cost function retain local properti data view defin kernel kernel pca recent techniqu propos instead defin fix kernel tri learn kernel use semidefinit program promin exampl techniqu maximum varianc unfold mvu central idea mvu exactli preserv pairwis distanc nearest neighbor inner product space maxim distanc point nearest neighbor altern approach neighborhood preserv minim cost function measur differ distanc input output space import exampl techniqu includ classic multidimension scale ident pca isomap use geodes distanc data space diffus map use diffus distanc data space stochast neighbor embed minim diverg distribut pair point curvilinear compon analysi differ approach nonlinear dimension reduct use autoencod special kind feedforward neural network bottleneck hidden layer train deep encod typic perform use greedi use stack restrict boltzmann machin follow finetun stage base backpropag linear discrimin analysi lda gener fisher linear discrimin method use statist pattern recognit machin learn find linear combin featur character separ two class object event gda deal nonlinear discrimin analysi use kernel function oper underli theori close machin svm insofar gda method provid map input vector featur space similar lda object gda find project featur lower dimension space maxim ratio scatter scatter autoencod use learn nonlinear dimens reduct function code togeth invers function code origin represent stochast neighbor embed nonlinear dimension reduct techniqu use visual dataset recommend use analysi cluster outlier detect sinc necessarili preserv densiti distanc well uniform manifold approxim project umap nonlinear dimension reduct techniqu visual similar assum data uniformli distribut local connect riemannian manifold riemannian metric local constant approxim local constant dataset dimens reduct usual perform prior appli neighbor algorithm order mitig curs dimension featur extract dimens reduct combin one step use princip compon analysi pca linear discrimin analysi lda canon correl analysi cca matrix factor nmf techniqu data follow cluster via featur vector space machin learn process also call embed dataset perform similar search live video stream dna data time seri run fast approxim search use hash random project sketch similar search techniqu vldb confer toolbox may feasibl option dimension reduct techniqu sometim use neurosci maxim inform dimens find represent dataset much inform possibl origin data preserv
Density estimation,https://en.wikipedia.org/wiki/Density_estimation,"In statistics, probability density estimation or simply density estimation is the construction of an estimate, based on observed data, of an unobservable underlying probability density function.  The unobservable density function is thought of as the density according to which a large population is distributed; the data are usually thought of as a random sample from that population.[1]
 A variety of approaches to density estimation are used, including Parzen windows and a range of data clustering techniques, including vector quantization. The most basic form of density estimation is a rescaled histogram.
 We will consider records of the incidence of diabetes. The following is quoted verbatim from the data set description:
 In this example, we construct three density estimates for ""glu"" (plasma glucose concentration), one conditional on the presence of diabetes,
the second conditional on the absence of diabetes, and the third not conditional on diabetes.
The conditional density estimates are then used to construct the probability of diabetes conditional on ""glu"".
 The ""glu"" data were obtained from the MASS package[4] of the R programming language. Within R, ?Pima.tr and ?Pima.te give a fuller account of the data.
 The mean of ""glu"" in the diabetes cases is 143.1 and the standard deviation is 31.26.
The mean of ""glu"" in the non-diabetes cases is 110.0 and the standard deviation is 24.29.
From this we see that, in this data set, diabetes cases are associated with greater levels of ""glu"".
This will be made clearer by plots of the estimated density functions.
 The first figure shows density estimates of p(glu | diabetes=1), p(glu | diabetes=0), and p(glu).
The density estimates are kernel density estimates using a Gaussian kernel. That is, a Gaussian density function is placed at each data point, and the sum of the density functions is computed over the range of the data.
 From the density of ""glu"" conditional on diabetes, we can obtain the probability of diabetes conditional on ""glu"" via Bayes' rule. For brevity, ""diabetes"" is abbreviated ""db."" in this formula.
 The second figure shows the estimated posterior probability p(diabetes=1 | glu). From these data, it appears that an increased level of ""glu"" is associated with diabetes.
 A very natural use of density estimates is in the informal investigation of the properties of a given set of data. Density estimates can give a valuable indication of such features as skewness and multimodality in the data. In some cases they will yield conclusions that may then be regarded as self-evidently true, while in others all they will do is to point the way to further analysis and/or data collection.[5]
 An important aspect of statistics is often the presentation of data back to the client in order to provide explanation and illustration of conclusions that may possibly have been obtained by other means. Density estimates are ideal for this purpose, for the simple reason that they are fairly easily comprehensible to non-mathematicians.
 More examples illustrating the use of density estimates for exploratory and presentational purposes, including the important case of bivariate data.[7]
 Density estimation is also frequently used in anomaly detection or novelty detection:[8] if an observation lies in a very low-density region, it is likely to be an anomaly or a novelty.
 Sources
",statist probabl densiti estim simpli densiti estim construct estim base observ data unobserv underli probabl densiti function unobserv densiti function thought densiti accord larg popul distribut data usual thought random sampl popul varieti approach densiti estim use includ parzen window rang data cluster techniqu includ vector quantiz basic form densiti estim rescal histogram consid record incid diabet follow quot verbatim data set descript exampl construct three densiti estim glu plasma glucos concentr one condit presenc diabet second condit absenc diabet third condit diabet condit densiti estim use construct probabl diabet condit glu glu data obtain mass packag r program languag within r give fuller account data mean glu diabet case standard deviat mean glu case standard deviat see data set diabet case associ greater level glu made clearer plot estim densiti function first figur show densiti estim p glu p glu p glu densiti estim kernel densiti estim use gaussian kernel gaussian densiti function place data point sum densiti function comput rang data densiti glu condit diabet obtain probabl diabet condit glu via bay rule breviti diabet abbrevi db formula second figur show estim posterior probabl p glu data appear increas level glu associ diabet natur use densiti estim inform investig properti given set data densiti estim give valuabl indic featur skew multimod data case yield conclus may regard true other point way analysi data collect import aspect statist often present data back client order provid explan illustr conclus may possibl obtain mean densiti estim ideal purpos simpl reason fairli easili comprehens exampl illustr use densiti estim exploratori present purpos includ import case bivari data densiti estim also frequent use anomali detect novelti detect observ lie region like anomali novelti sourc
Anomaly detection,https://en.wikipedia.org/wiki/Anomaly_detection,"In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behavior.[1] Such examples may arouse suspicions of being generated by a different mechanism,[2] or appear inconsistent with the remainder of that set of data.[3]
 Anomaly detection finds application in many domains including cybersecurity, medicine, machine vision, statistics, neuroscience, law enforcement and financial fraud to name only a few. Anomalies were initially searched for clear rejection or omission from the data to aid statistical analysis, for example to compute the mean or standard deviation. They were also removed to better predictions from models such as linear regression, and more recently their removal aids the performance of machine learning algorithms. However, in many applications anomalies themselves are of interest and are the observations most desirous in the entire data set, which need to be identified and separated from noise or irrelevant outliers.
 Three broad categories of anomaly detection techniques exist.[1] Supervised anomaly detection techniques require a data set that has been labeled as ""normal"" and ""abnormal"" and involves training a classifier. However, this approach is rarely used in anomaly detection due to the general unavailability of labelled data and the inherent unbalanced nature of the classes. Semi-supervised anomaly detection techniques assume that some portion of the data is labelled. This may be any combination of the normal or anomalous data, but more often than not, the techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the model. Unsupervised anomaly detection techniques assume the data is unlabelled and are by far the most commonly used due to their wider and relevant application.
 Many attempts have been made in the statistical and computer science communities to define an anomaly. The most prevalent ones include the following, and can be categorised into three groups: those that are ambiguous, those that are specific to a method with pre-defined thresholds usually chosen empirically, and those that are formally defined:
 The concept of intrusion detection, a critical component of anomaly detection, has evolved significantly over time. Initially, it was a manual process where system administrators would monitor for unusual activities, such as a vacationing user's account being accessed or unexpected printer activity. This approach was not scalable and was soon superseded by the analysis of audit logs and system logs for signs of malicious behavior.[4]
 By the late 1970s and early 1980s, the analysis of these logs was primarily used retrospectively to investigate incidents, as the volume of data made it impractical for real-time monitoring. The affordability of digital storage eventually led to audit logs being analyzed online, with specialized programs being developed to sift through the data. These programs, however, were typically run during off-peak hours due to their computational intensity.[4]
 The 1990s brought the advent of real-time intrusion detection systems capable of analyzing audit data as it was generated, allowing for immediate detection of and response to attacks. This marked a significant shift towards proactive intrusion detection.[4]
 As the field has continued to develop, the focus has shifted to creating solutions that can be efficiently implemented across large and complex network environments, adapting to the ever-growing variety of security threats and the dynamic nature of modern computing infrastructures.[4]
 Anomaly detection is applicable in a very large number and variety of domains, and is an important subarea of unsupervised machine learning. As such it has applications in cyber-security, intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, detecting ecosystem disturbances, defect detection in images using machine vision, medical diagnosis and law enforcement.[5]
 Anomaly detection was proposed for intrusion detection systems (IDS) by Dorothy Denning in 1986.[6] Anomaly detection for IDS is normally accomplished with thresholds and statistics, but can also be done with soft computing, and inductive learning.[7] Types of features proposed by 1999 included profiles of users, workstations, networks, remote hosts, groups of users, and programs based on frequencies, means, variances, covariances, and standard deviations.[8]  The counterpart of anomaly detection in intrusion detection is misuse detection.
 Anomaly detection is vital in fintech for fraud prevention.[9][10]
 Preprocessing data to remove anomalies can be an important step in data analysis, and is done for a number of reasons. Statistics such as the mean and standard deviation are more accurate after the removal of anomalies, and the visualisation of data can also be improved. In supervised learning, removing the anomalous data from the dataset often results in a statistically significant increase in accuracy.[11][12]
 Anomaly detection has become increasingly vital in video surveillance to enhance security and safety.[13][14] With the advent of deep learning technologies, methods using Convolutional Neural Networks (CNNs) and Simple Recurrent Units (SRUs) have shown significant promise in identifying unusual activities or behaviors in video data.[13] These models can process and analyze extensive video feeds in real-time, recognizing patterns that deviate from the norm, which may indicate potential security threats or safety violations.[13]
 In IT infrastructure management, anomaly detection is crucial for ensuring the smooth operation and reliability of services.[15] Techniques like the IT Infrastructure Library (ITIL) and monitoring frameworks are employed to track and manage system performance and user experience.[15] Detection anomalies can help identify and pre-empt potential performance degradations or system failures, thus maintaining productivity and business process effectiveness.[15]
 Anomaly detection is critical for the security and efficiency of Internet of Things (IoT) systems.[16] It helps in identifying system failures and security breaches in complex networks of IoT devices.[16] The methods must manage real-time data, diverse device types, and scale effectively. Garbe et al.[17] have introduced a multi-stage anomaly detection framework that improves upon traditional methods by incorporating spatial clustering, density-based clustering, and locality-sensitive hashing. This tailored approach is designed to better handle the vast and varied nature of IoT data, thereby enhancing security and operational reliability in smart infrastructure and industrial IoT systems.[17]
 Anomaly detection is crucial in the petroleum industry for monitoring critical machinery.[18] Martí et al. used a novel segmentation algorithm to analyze sensor data for real-time anomaly detection.[18] This approach helps promptly identify and address any irregularities in sensor readings, ensuring the reliability and safety of petroleum operations.[18]
 In the oil and gas sector, anomaly detection is not just crucial for maintenance and safety, but also for environmental protection.[19] Aljameel et al. propose an advanced machine learning-based model for detecting minor leaks in oil and gas pipelines, a task traditional methods may miss.[19]
 Many anomaly detection techniques have been proposed in literature.[1][20] The performance of methods usually depend on the data sets. For example, some may be suited to detecting local outliers, while others global, and methods have little systematic advantages over another when compared across many data sets.[21][22] Almost all algorithms also require the setting of non-intuitive parameters critical for performance, and usually unknown before application. Some of the popular techniques are mentioned below and are broken down into categories:
 Also referred to as frequency-based or counting-based, the simplest non-parametric anomaly detection method is to build a histogram with the training data or a set of known normal instances, and if a test point does not fall in any of the histogram bins mark it as anomalous, or assign an anomaly score to test data based on the height of the bin it falls in.[1]  The size of bins are key to the effectiveness of this technique but must be determined by the implementer.
 
A more sophisticated technique uses kernel functions to approximate the distribution of the normal data.  Instances in low probability areas of the distribution are then considered anomalies[23]. Histogram-based Outlier Score (HBOS) uses value histograms and assumes feature independence for fast predictions.[53]
 Dynamic networks, such as those representing financial systems, social media interactions, and transportation infrastructure, are subject to constant change, making anomaly detection within them a complex task. Unlike static graphs, dynamic networks reflect evolving relationships and states, requiring adaptive techniques for anomaly detection.
 Many of the methods discussed above only yield an anomaly score prediction, which often can be explained to users as the point being in a region of low data density (or relatively low density compared to the neighbor's densities). In explainable artificial intelligence, the users demand methods with higher explainability. Some methods allow for more detailed explanations:
",data analysi anomali detect also refer outlier detect sometim novelti detect gener understood identif rare item event observ deviat significantli major data conform well defin notion normal behavior exampl may arous suspicion gener differ mechan appear inconsist remaind set data anomali detect find applic mani domain includ cybersecur medicin machin vision statist neurosci law enforc financi fraud name anomali initi search clear reject omiss data aid statist analysi exampl comput mean standard deviat also remov better predict model linear regress recent remov aid perform machin learn algorithm howev mani applic anomali interest observ desir entir data set need identifi separ nois irrelev outlier three broad categori anomali detect techniqu exist supervis anomali detect techniqu requir data set label normal abnorm involv train classifi howev approach rare use anomali detect due gener unavail label data inher unbalanc natur class anomali detect techniqu assum portion data label may combin normal anomal data often techniqu construct model repres normal behavior given normal train data set test likelihood test instanc gener model unsupervis anomali detect techniqu assum data unlabel far commonli use due wider relev applic mani attempt made statist comput scienc commun defin anomali preval one includ follow categoris three group ambigu specif method threshold usual chosen empir formal defin concept intrus detect critic compon anomali detect evolv significantli time initi manual process system administr would monitor unusu activ vacat user account access unexpect printer activ approach scalabl soon supersed analysi audit log system log sign malici behavior late earli analysi log primarili use retrospect investig incid volum data made impract monitor afford digit storag eventu led audit log analyz onlin special program develop sift data program howev typic run hour due comput intens brought advent intrus detect system capabl analyz audit data gener allow immedi detect respons attack mark signific shift toward proactiv intrus detect field continu develop focu shift creat solut effici implement across larg complex network environ adapt varieti secur threat dynam natur modern comput infrastructur anomali detect applic larg number varieti domain import subarea unsupervis machin learn applic intrus detect fraud detect fault detect system health monitor event detect sensor network detect ecosystem disturb defect detect imag use machin vision medic diagnosi law enforc anomali detect propos intrus detect system id dorothi den anomali detect id normal accomplish threshold statist also done soft comput induct learn type featur propos includ profil user workstat network remot host group user program base frequenc mean varianc covari standard deviat counterpart anomali detect intrus detect misus detect anomali detect vital fintech fraud prevent preprocess data remov anomali import step data analysi done number reason statist mean standard deviat accur remov anomali visualis data also improv supervis learn remov anomal data dataset often result statist signific increas accuraci anomali detect becom increasingli vital video surveil enhanc secur safeti advent deep learn technolog method use convolut neural network cnn simpl recurr unit sru shown signific promis identifi unusu activ behavior video data model process analyz extens video feed recogn pattern deviat norm may indic potenti secur threat safeti violat infrastructur manag anomali detect crucial ensur smooth oper reliabl servic techniqu like infrastructur librari itil monitor framework employ track manag system perform user experi detect anomali help identifi potenti perform degrad system failur thu maintain product busi process effect anomali detect critic secur effici internet thing iot system help identifi system failur secur breach complex network iot devic method must manag data divers devic type scale effect garb et al introduc anomali detect framework improv upon tradit method incorpor spatial cluster cluster hash tailor approach design better handl vast vari natur iot data therebi enhanc secur oper reliabl smart infrastructur industri iot system anomali detect crucial petroleum industri monitor critic machineri martí et al use novel segment algorithm analyz sensor data anomali detect approach help promptli identifi address irregular sensor read ensur reliabl safeti petroleum oper oil ga sector anomali detect crucial mainten safeti also environment protect aljameel et al propos advanc machin model detect minor leak oil ga pipelin task tradit method may miss mani anomali detect techniqu propos literatur perform method usual depend data set exampl may suit detect local outlier other global method littl systemat advantag anoth compar across mani data set almost algorithm also requir set paramet critic perform usual unknown applic popular techniqu mention broken categori also refer simplest anomali detect method build histogram train data set known normal instanc test point fall histogram bin mark anomal assign anomali score test data base height bin fall size bin key effect techniqu must determin implement sophist techniqu use kernel function approxim distribut normal data instanc low probabl area distribut consid anomali outlier score hbo use valu histogram assum featur independ fast predict dynam network repres financi system social media interact transport infrastructur subject constant chang make anomali detect within complex task unlik static graph dynam network reflect evolv relationship state requir adapt techniqu anomali detect mani method discuss yield anomali score predict often explain user point region low data densiti rel low densiti compar neighbor densiti explain artifici intellig user demand method higher explain method allow detail explan
Data cleansing,https://en.wikipedia.org/wiki/Data_cleaning,"Data cleansing or data cleaning is the process of identifying and correcting (or removing) corrupt, inaccurate, or irrelevant records from a dataset, table, or database. It involves detecting incomplete, incorrect, or inaccurate parts of the data and then replacing, modifying, or deleting the affected data.[1] Data cleansing can be performed interactively using data wrangling tools, or through batch processing often via scripts or a data quality firewall.
 After cleansing, a data set should be consistent with other similar data sets in the system. The inconsistencies detected or removed may have been originally caused by user entry errors, by corruption in transmission or storage, or by different data dictionary definitions of similar entities in different stores. Data cleaning differs from data validation in that validation almost invariably means data is rejected from the system at entry and is performed at the time of entry, rather than on batches of data.
 The actual process of data cleansing may involve removing typographical errors or validating and correcting values against a known list of entities. The validation may be strict (such as rejecting any address that does not have a valid postal code), or with fuzzy or approximate string matching (such as correcting records that partially match existing, known records). Some data cleansing solutions will clean data by cross-checking with a validated data set. A common data cleansing practice is data enhancement, where data is made more complete by adding related information. For example, appending addresses with any phone numbers related to that address. Data cleansing may also involve harmonization (or normalization) of data, which is the process of bringing together data of ""varying file formats, naming conventions, and columns"",[2] and transforming it into one cohesive data set; a simple example is the expansion of abbreviations (""st, rd, etc."" to ""street, road, etcetera"").
 Administratively incorrect, inconsistent data can lead to false conclusions and misdirect investments on both public and private scales. For instance, the government may want to analyze population census figures to decide which regions require further spending and investment on infrastructure and services. In this case, it will be important to have access to reliable data to avoid erroneous fiscal decisions. In the business world, incorrect data can be costly. Many companies use customer information databases that record data like contact information, addresses, and preferences. For instance, if the addresses are inconsistent, the company will suffer the cost of resending mail or even losing customers.
 High-quality data needs to pass a set of quality criteria. Those include:
 The term integrity encompasses accuracy, consistency and some aspects of validation (see also data integrity) but is rarely used by itself in data-cleansing contexts because it is insufficiently specific. (For example, ""referential integrity"" is a term used to refer to the enforcement of foreign-key constraints above.)
 Good quality source data has to do with “Data Quality Culture” and must be initiated at the top of the organization. It is not just a matter of implementing strong validation checks on input screens, because almost no matter how strong these checks are, they can often still be circumvented by the users. There is a nine-step guide for organizations that wish to improve data quality:[3][4]
 Others include:
 The essential job of this system is to find a suitable balance between fixing dirty data and maintaining the data as close as possible to the original data from the source production system. This is a challenge for the Extract, transform, load architect. The system should offer an architecture that can cleanse data, record quality events and measure/control quality of data in the data warehouse. A good start is to perform a thorough data profiling analysis that will help define to the required complexity of the data cleansing system and also give an idea of the current data quality in the source system(s).
 Part of the data cleansing system is a set of diagnostic filters known as quality screens. They each implement a test in the data flow that, if it fails, records an error in the Error Event Schema. Quality screens are divided into three categories:
 When a quality screen records an error, it can either stop the dataflow process, send the faulty data somewhere else than the target system or tag the data.
The latter option is considered the best solution because the first option requires, that someone has to manually deal with the issue each time it occurs and the second implies that data are missing from the target system (integrity) and it is often unclear what should happen to these data.
 Most data cleansing tools have limitations in usability:
 The error event schema holds records of all error events thrown by the quality screens. It consists of an error event fact table with foreign keys to three dimension tables that represent date (when), batch job (where) and screen (who produced error). It also holds information about exactly when the error occurred and the severity of the error. Also, there is an error event detail fact table with a foreign key to the main table that contains detailed information about in which table, record and field the error occurred and the error condition.
",data cleans data clean process identifi correct remov corrupt inaccur irrelev record dataset tabl databas involv detect incomplet incorrect inaccur part data replac modifi delet affect data data cleans perform interact use data wrangl tool batch process often via script data qualiti firewal cleans data set consist similar data set system inconsist detect remov may origin caus user entri error corrupt transmiss storag differ data dictionari definit similar entiti differ store data clean differ data valid valid almost invari mean data reject system entri perform time entri rather batch data actual process data cleans may involv remov typograph error valid correct valu known list entiti valid may strict reject address valid postal code fuzzi approxim string match correct record partial match exist known record data cleans solut clean data valid data set common data cleans practic data enhanc data made complet ad relat inform exampl append address phone number relat address data cleans may also involv harmon normal data process bring togeth data vari file format name convent column transform one cohes data set simpl exampl expans abbrevi st rd etc street road etcetera administr incorrect inconsist data lead fals conclus misdirect invest public privat scale instanc govern may want analyz popul censu figur decid region requir spend invest infrastructur servic case import access reliabl data avoid erron fiscal decis busi world incorrect data costli mani compani use custom inform databas record data like contact inform address prefer instanc address inconsist compani suffer cost resend mail even lose custom data need pass set qualiti criteria includ term integr encompass accuraci consist aspect valid see also data integr rare use context insuffici specif exampl referenti integr term use refer enforc constraint good qualiti sourc data data qualiti cultur must initi top organ matter implement strong valid check input screen almost matter strong check often still circumv user guid organ wish improv data qualiti other includ essenti job system find suitabl balanc fix dirti data maintain data close possibl origin data sourc product system challeng extract transform load architect system offer architectur cleans data record qualiti event qualiti data data warehous good start perform thorough data profil analysi help defin requir complex data cleans system also give idea current data qualiti sourc system part data cleans system set diagnost filter known qualiti screen implement test data flow fail record error error event schema qualiti screen divid three categori qualiti screen record error either stop dataflow process send faulti data somewher els target system tag data latter option consid best solut first option requir someon manual deal issu time occur second impli data miss target system integr often unclear happen data data cleans tool limit usabl error event schema hold record error event thrown qualiti screen consist error event fact tabl foreign key three dimens tabl repres date batch job screen produc error also hold inform exactli error occur sever error also error event detail fact tabl foreign key main tabl contain detail inform tabl record field error occur error condit
Automated machine learning,https://en.wikipedia.org/wiki/Automated_machine_learning,"Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. It is the combination of automation and ML.[1]
 AutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning.[2][3] The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models.[4]
 Common techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.
 In a typical machine learning application, practitioners have a set of input data points to be used for training. The raw data may not be in a form that all algorithms can be applied to. To make the data amenable for machine learning, an expert may have to apply appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods. After these steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their model. If deep learning is used, the architecture of the neural network must also be chosen manually by the machine learning expert. 
 Each of these steps may be challenging, resulting in significant hurdles to using machine learning. AutoML aims to simplify these steps for non-experts, and to make it easier for them to use machine learning techniques correctly and effectively.
 AutoML plays an important role within the broader approach of automating data science, which also includes challenging tasks such as data engineering, data exploration and model interpretation and prediction.[5]
 Automated machine learning can target various stages of the machine learning process.[3]  Steps to automate are:
 There are a number of key challenges being tackled around automated machine learning. A big issue surrounding the field is referred to as ""development as a cottage industry"".[7] This phrase refers to the issue in machine learning where development relies on manual decisions and biases of experts. This is contrasted to the goal of machine learning which is to create systems that can learn and improve from their own usage and analysis of the data. Basically, it's the struggle between how much experts should get involved in the learning of the systems versus how much freedom they should be giving the machines. However, experts and developers must help create and guide these machines to prepare them for their own learning. To create this system, it requires labor intensive work with knowledge of machine learning algorithms and system design.[8]
 Additionally, some other challenges include meta-learning challenges[9] and computational resource allocation.
",autom machin learn automl process autom task appli machin learn problem combin autom ml automl potenti includ everi stage begin raw dataset build machin learn model readi deploy automl propos artifici solut grow challeng appli machin learn high degre autom automl aim allow make use machin learn model techniqu without requir becom expert machin learn autom process appli machin learn addit offer advantag produc simpler solut faster creation solut model often outperform model common techniqu use automl includ hyperparamet optim neural architectur search typic machin learn applic practition set input data point use train raw data may form algorithm appli make data amen machin learn expert may appli appropri data featur engin featur extract featur select method step practition must perform algorithm select hyperparamet optim maxim predict perform model deep learn use architectur neural network must also chosen manual machin learn expert step may challeng result signific hurdl use machin learn automl aim simplifi step make easier use machin learn techniqu correctli effect automl play import role within broader approach autom data scienc also includ challeng task data engin data explor model interpret predict autom machin learn target variou stage machin learn process step autom number key challeng tackl around autom machin learn big issu surround field refer develop cottag industri phrase refer issu machin learn develop reli manual decis bias expert contrast goal machin learn creat system learn improv usag analysi data basic struggl much expert get involv learn system versu much freedom give machin howev expert develop must help creat guid machin prepar learn creat system requir labor intens work knowledg machin learn algorithm system design addit challeng includ challeng comput resourc alloc
Association rule learning,https://en.wikipedia.org/wiki/Association_rule_learning,"Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.[1] In any given transaction with a variety of items, association rules are meant to discover the rules that determine how or why certain items are connected.
 Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami[2] introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule 



{

o
n
i
o
n
s
,
p
o
t
a
t
o
e
s

}
⇒
{

b
u
r
g
e
r

}


{\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}

 found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements.
 In addition to the above example from market basket analysis, association rules are employed today in many application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.
 The association rule algorithm itself consists of various parameters that can make it difficult for those without some expertise in data mining to execute, with many rules that are arduous to understand.[3]
 Following the original definition by Agrawal, Imieliński, Swami[2] the problem of association rule mining is defined as:
 Let 



I
=
{

i

1


,

i

2


,
…
,

i

n


}


{\displaystyle I=\{i_{1},i_{2},\ldots ,i_{n}\}}

 be a set of n binary attributes called items.
 Let 



D
=
{

t

1


,

t

2


,
…
,

t

m


}


{\displaystyle D=\{t_{1},t_{2},\ldots ,t_{m}\}}

 be a set of transactions called the database.
 Each transaction in D has a unique transaction ID and contains a subset of the items in I.
 A rule is defined as an implication of the form:
 In Agrawal, Imieliński, Swami[2] a rule is defined only between a set and a single item, 



X
⇒

i

j




{\displaystyle X\Rightarrow i_{j}}

 for 




i

j


∈
I


{\displaystyle i_{j}\in I}

.
 Every rule is composed by two different sets of items, also known as itemsets, X and Y, where X is called antecedent or left-hand-side (LHS) and Y consequent or right-hand-side (RHS). The antecedent is that item that can be found in the data while the consequent is the item found when combined with the antecedent. The statement 



X
⇒
Y


{\displaystyle X\Rightarrow Y}

 is often read as if X then Y, where the antecedent (X ) is the if and the consequent (Y) is the then. This simply implies that, in theory, whenever X occurs in a dataset, then Y will as well.
 Association rules are made by searching data for frequent if-then patterns and by using a certain criterion under Support and Confidence to define what the most important relationships are. Support is the evidence of how frequent an item appears in the data given, as Confidence is defined by how many times the if-then statements are found true. However, there is a third criteria that can be used, it is called Lift and it can be used to compare the expected Confidence and the actual Confidence. Lift will show how many times the if-then statement is expected to be found to be true.
 Association rules are made to calculate from itemsets, which are created by two or more items. If the rules were built from the analyzing from all the possible itemsets from the data then there would be so many rules that they wouldn’t have any meaning. That is why Association rules are typically made from rules that are well represented by the data.
 There are many different data mining techniques you could use to find certain analytics and results, for example, there is Classification analysis, Clustering analysis, and Regression analysis.[4] What technique you should use depends on what you are looking for with your data. Association rules are primarily used to find analytics and a prediction of customer behavior. For Classification analysis, it would most likely be used to question, make decisions, and predict behavior.[5] Clustering analysis is primarily used when there are no assumptions made about the likely relationships within the data.[5] Regression analysis Is used when you want to predict the value of a continuous dependent from a number of independent variables.[5]
 Benefits
 There are many benefits of using Association rules like finding the pattern that helps understand the correlations and co-occurrences between data sets. A very good real-world example that uses Association rules would be medicine. Medicine uses Association rules to help diagnose patients. When diagnosing patients there are many variables to consider as many diseases will share similar symptoms. With the use of the Association rules, doctors can determine the conditional probability of an illness by comparing symptom relationships from past cases.[6]
 Downsides
 However, Association rules also lead to many different downsides such as finding the appropriate parameter and threshold settings for the mining algorithm. But there is also the downside of having a large number of discovered rules. The reason is that this does not guarantee that the rules will be found relevant, but it could also cause the algorithm to have low performance. Sometimes the implemented algorithms will contain too many variables and parameters. For someone that doesn’t have a good concept of data mining, this might cause them to have trouble understanding it.[7]
 Thresholds When using Association rules, you are most likely to only use Support and Confidence. However, this means you have to satisfy a user-specified minimum support and a user-specified minimum confidence at the same time. Usually, the Association rule generation is split into two different steps that needs to be applied:
 The Support Threshold is 30%, Confidence Threshold is 50%
 The Table on the left is the original unorganized data and the table on the right is organized by the thresholds. In this case Item C is better than the thresholds for both Support and Confidence which is why it is first. Item A is second because its threshold values are spot on. Item D has met the threshold for Support but not Confidence. Item B has not met the threshold for either Support or Confidence and that is why it is last.
 To find all the frequent itemsets in a database is not an easy task since it involves going through all the data to find all possible item combinations from all possible itemsets. The set of possible itemsets is the power set over I and has size 




2

n


−
1


{\displaystyle 2^{n}-1}

 , of course this means to exclude the empty set which is not considered to be a valid itemset. However, the size of the power set will grow exponentially in the number of item n that is within the power set I. An efficient search is possible by using the downward-closure property of support[2][8] (also called anti-monotonicity[9]). This would guarantee that a frequent itemset and all its subsets are also frequent and thus will have no infrequent itemsets as a subset of a frequent itemset. Exploiting this property, efficient algorithms (e.g., Apriori[10] and Eclat[11]) can find all frequent itemsets.
 To illustrate the concepts, we use a small example from the supermarket domain. Table 2 shows a small database containing the items where, in each entry, the value 1 means the presence of the item in the corresponding transaction, and the value 0 represents the absence of an item in that transaction. The set of items is 



I
=
{

m
i
l
k
,
b
r
e
a
d
,
b
u
t
t
e
r
,
b
e
e
r
,
d
i
a
p
e
r
s
,
e
g
g
s
,
f
r
u
i
t

}


{\displaystyle I=\{\mathrm {milk,bread,butter,beer,diapers,eggs,fruit} \}}

.
 An example rule for the supermarket could be 



{

b
u
t
t
e
r
,
b
r
e
a
d

}
⇒
{

m
i
l
k

}


{\displaystyle \{\mathrm {butter,bread} \}\Rightarrow \{\mathrm {milk} \}}

 meaning that if butter and bread are bought, customers also buy milk.
 In order to select interesting rules from the set of all possible rules, constraints on various measures of significance and interest are used. The best-known constraints are minimum thresholds on support and confidence.
 Let 



X
,
Y


{\displaystyle X,Y}

 be itemsets, 



X
⇒
Y


{\displaystyle X\Rightarrow Y}

 an association rule and T a set of transactions of a given database.
 Note: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant,[citation needed] and datasets often contain thousands or millions of transactions.
 Support is an indication of how frequently the itemset appears in the dataset.
 In our example, it can be easier to explain support by writing 




support

=
P
(
A
∩
B
)
=



(

number of transactions containing 

A

 and 

B
)

 (total number of transactions)




{\displaystyle {\text{support}}=P(A\cap B)={\frac {({\text{number of transactions containing }}A{\text{ and }}B)}{\text{ (total number of transactions)}}}}

 [12] where A and B are separate item sets that occur at the same time in a transaction.
 Using Table 2 as an example, the itemset 



X
=
{

b
e
e
r
,
d
i
a
p
e
r
s

}


{\displaystyle X=\{\mathrm {beer,diapers} \}}

 has a support of 1/5=0.2 since it occurs in 20% of all transactions (1 out of 5 transactions). The argument of support of X is a set of preconditions, and thus becomes more restrictive as it grows (instead of more inclusive).[13]
 Furthermore, the itemset 



Y
=
{

m
i
l
k
,
b
r
e
a
d
,
b
u
t
t
e
r

}


{\displaystyle Y=\{\mathrm {milk,bread,butter} \}}

 has a support of 1/5=0.2 as it appears in 20% of all transactions as well.
 When using antecedents and consequents, it allows a data miner to determine the support of multiple items being bought together in comparison to the whole data set. For example, Table 2 shows that if milk is bought, then bread is bought has a support of 0.4 or 40%. This because in 2 out 5 of the transactions, milk as well as bread are bought. In smaller data sets like this example, it is harder to see a strong correlation when there are few samples, but when the data set grows larger, support can be used to find correlation between two or more products in the supermarket example.
 Minimum support thresholds are useful for determining which itemsets are preferred or interesting.
 If we set the support threshold to ≥0.4 in Table 3, then the 



{

m
i
l
k

}
⇒
{

e
g
g
s

}


{\displaystyle \{\mathrm {milk} \}\Rightarrow \{\mathrm {eggs} \}}

 would be removed since it did not meet the minimum threshold of 0.4. Minimum threshold is used to remove samples where there is not a strong enough support or confidence to deem the sample as important or interesting in the dataset.
 Another way of finding interesting samples is to find the value of (support)×(confidence); this allows a data miner to see the samples where support and confidence are high enough to be highlighted in the dataset and prompt a closer look at the sample to find more information on the connection between the items.
 Support can be beneficial for finding the connection between products in comparison to the whole dataset, whereas confidence looks at the connection between one or more items and another item. Below is a table that shows the comparison and contrast between support and support × confidence, using the information from Table 4 to derive the confidence values.
 The support of X with respect to T is defined as the proportion of transactions in the dataset which contains the itemset X. Denoting a transaction by 



(
i
,
t
)


{\displaystyle (i,t)}

 where i is the unique identifier of the transaction and t is its itemset, the support may be written as:
 This notation can be used when defining more complicated datasets where the items and itemsets may not be as easy as our supermarket example above. Other examples of where support can be used is in finding groups of genetic mutations that work collectively to cause a disease, investigating the number of subscribers that respond to upgrade offers, and discovering which products in a drug store are never bought together.[12]
 Confidence is the percentage of all transactions satisfying X that also satisfy Y.[14]
 With respect to T, the confidence value of an association rule, often denoted as 



X
⇒
Y


{\displaystyle X\Rightarrow Y}

, is the ratio of transactions containing both X and Y to the total amount of X values present, where X is the antecedent and Y is the consequent.
 Confidence can also be interpreted as an estimate of the conditional probability 



P
(

E

Y



|


E

X


)


{\displaystyle P(E_{Y}|E_{X})}

, the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS.[13][15]
 It is commonly depicted as:
 The equation illustrates that confidence can be computed by calculating the co-occurrence of transactions X and Y within the dataset in ratio to transactions containing only X. This means that the number of transactions in both  X and Y  is divided by those just in X .
 For example, Table 2 shows the rule 



{

b
u
t
t
e
r
,
b
r
e
a
d

}
⇒
{

m
i
l
k

}


{\displaystyle \{\mathrm {butter,bread} \}\Rightarrow \{\mathrm {milk} \}}

 which has a confidence of 






1

/

5


1

/

5



=


0.2
0.2


=
1.0


{\displaystyle {\frac {1/5}{1/5}}={\frac {0.2}{0.2}}=1.0}

 in the dataset, which denotes that every time a customer buys butter and bread, they also buy milk. This particular example demonstrates the rule being correct 100% of the time for transactions containing both butter and bread. The rule 



{

f
r
u
i
t

}
⇒
{

e
g
g
s

}


{\displaystyle \{\mathrm {fruit} \}\Rightarrow \{\mathrm {eggs} \}}

, however, has a confidence of 






2

/

5


3

/

5



=


0.4
0.6


=
0.67


{\displaystyle {\frac {2/5}{3/5}}={\frac {0.4}{0.6}}=0.67}

. This suggests that eggs are bought 67% of the times that fruit is brought. Within this particular dataset, fruit is purchased a total of 3 times, with two of those times consisting of egg purchases.
 For larger datasets, a minimum threshold, or a percentage cutoff, for the confidence can be useful for determining item relationships. When applying this method to some of the data in Table 2, information that does not meet the requirements are removed. Table 4 shows association rule examples where the minimum threshold for confidence is 0.5 (50%). Any data that does not have a confidence of at least 0.5 is omitted. Generating thresholds allow for the association between items to become stronger as the data is further researched by emphasizing those that co-occur the most. The table uses the confidence information from Table 3 to implement the Support × Confidence column, where the relationship between items via their both confidence and support, instead of just one concept, is highlighted. Ranking the rules by Support × Confidence multiples the confidence of a particular rule to its support and is often implemented for a more in-depth understanding of the relationship between the items.
 Overall, using confidence in association rule mining is great way to bring awareness to data relations. Its greatest benefit is highlighting the relationship between particular items to one another within the set, as it compares co-occurrences of items to the total occurrence of the antecedent in the specific rule. However, confidence is not the optimal method for every concept in association rule mining. The disadvantage of using it is that it does not offer multiple difference outlooks on the associations. Unlike support, for instance, confidence does not provide the perspective of relationships between certain items in comparison to the entire dataset, so while milk and bread, for example, may occur 100% of the time for confidence, it only has a support of 0.4 (40%). This is why it is important to look at other viewpoints, such as Support × Confidence, instead of solely relying on one concept incessantly to define the relationships.
 The lift of a rule is defined as:
 




l
i
f
t

(
X
⇒
Y
)
=




s
u
p
p

(
X
∪
Y
)



s
u
p
p

(
X
)
×

s
u
p
p

(
Y
)





{\displaystyle \mathrm {lift} (X\Rightarrow Y)={\frac {\mathrm {supp} (X\cup Y)}{\mathrm {supp} (X)\times \mathrm {supp} (Y)}}}


 or the ratio of the observed support to that expected if X and Y were independent.
 For example, the rule 



{

m
i
l
k
,
b
r
e
a
d

}
⇒
{

b
u
t
t
e
r

}


{\displaystyle \{\mathrm {milk,bread} \}\Rightarrow \{\mathrm {butter} \}}

 has a lift of 





0.2

0.4
×
0.4



=
1.25


{\displaystyle {\frac {0.2}{0.4\times 0.4}}=1.25}

.
 If the rule had a lift of 1, it would imply that the probability of occurrence of the antecedent and that of the consequent are independent of each other. When two events are independent of each other, no rule can be drawn involving those two events.
 If the lift is > 1, that lets us know the degree to which those two occurrences are dependent on one another, and makes those rules potentially useful for predicting the consequent in future data sets.
 If the lift is < 1, that lets us know the items are substitute to each other. This means that presence of one item has negative effect on presence of other item and vice versa.
 The value of lift is that it considers both the support of the rule and the overall data set.[13]
 [rede]
 The conviction of a rule is defined as 




c
o
n
v

(
X
⇒
Y
)
=



1
−

s
u
p
p

(
Y
)


1
−

c
o
n
f

(
X
⇒
Y
)





{\displaystyle \mathrm {conv} (X\Rightarrow Y)={\frac {1-\mathrm {supp} (Y)}{1-\mathrm {conf} (X\Rightarrow Y)}}}

.[16]
 For example, the rule 



{

m
i
l
k
,
b
r
e
a
d

}
⇒
{

b
u
t
t
e
r

}


{\displaystyle \{\mathrm {milk,bread} \}\Rightarrow \{\mathrm {butter} \}}

 has a conviction of 






1
−
0.4


1
−
0.5



=
1.2


{\displaystyle {\frac {1-0.4}{1-0.5}}=1.2}

, and can be interpreted as the ratio of the expected frequency that X occurs without Y (that is to say, the frequency that the rule makes an incorrect prediction) if X and Y were independent divided by the observed frequency of incorrect predictions. In this example, the conviction value of 1.2 shows that the rule 



{

m
i
l
k
,
b
r
e
a
d

}
⇒
{

b
u
t
t
e
r

}


{\displaystyle \{\mathrm {milk,bread} \}\Rightarrow \{\mathrm {butter} \}}

 would be incorrect 20% more often (1.2 times as often) if the association between X and Y was purely random chance.
 In addition to confidence, other measures of interestingness for rules have been proposed. Some popular measures are:
 Several more measures are presented and compared by Tan et al.[20] and by Hahsler.[21] Looking for techniques that can model what the user has known (and using these models as interestingness measures) is currently an active research trend under the name of ""Subjective Interestingness.""
 The concept of association rules was popularized particularly due to the 1993 article of Agrawal et al.,[2] which has acquired more than 23,790 citations according to Google Scholar, as of April 2021, and is thus one of the most cited papers in the Data Mining field. However, what is now called ""association rules"" is introduced already in the 1966 paper[22] on GUHA, a general data mining method developed by Petr Hájek et al.[23]
 An early (circa 1989) use of minimum support and confidence to find all association rules is the Feature Based Modeling framework, which found all rules with 




s
u
p
p

(
X
)


{\displaystyle \mathrm {supp} (X)}

 and 




c
o
n
f

(
X
⇒
Y
)


{\displaystyle \mathrm {conf} (X\Rightarrow Y)}

 greater than user defined constraints.[24]
 One limitation of the standard approach to discovering associations is that by searching massive numbers of possible associations to look for collections of items that appear to be associated, there is a large risk of finding many spurious associations. These are collections of items that co-occur with unexpected frequency in the data, but only do so by chance. For example, suppose we are considering a collection of 10,000 items and looking for rules containing two items in the left-hand-side and 1 item in the right-hand-side. There are approximately 1,000,000,000,000 such rules. If we apply a statistical test for independence with a significance level of 0.05 it means there is only a 5% chance of accepting a rule if there is no association. If we assume there are no associations, we should nonetheless expect to find 50,000,000,000 rules. Statistically sound association discovery[25][26] controls this risk, in most cases reducing the risk of finding any spurious associations to a user-specified significance level.
 Many algorithms for generating association rules have been proposed.
 Some well-known algorithms are Apriori, Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets. Another step needs to be done after to generate rules from frequent itemsets found in a database.
 Apriori is given by R. Agrawal and R. Srikant in 1994 for frequent item set mining and association rule learning. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often. The name of the algorithm is Apriori because it uses prior knowledge of frequent itemset properties.
 Overview: Apriori uses a ""bottom up"" approach, where frequent subsets are extended one item at a time (a step known as candidate generation), and groups of candidates are tested against the data. The algorithm terminates when no further successful extensions are found. Apriori uses breadth-first search and a Hash tree structure to count candidate item sets efficiently. It generates candidate item sets of length  from item sets of length . Then it prunes the candidates which have an infrequent sub pattern. According to the downward closure lemma, the candidate set contains all frequent -length item sets. After that, it scans the transaction database to determine frequent item sets among the candidates.
 Example: Assume that each row is a cancer sample with a certain combination of mutations labeled by a character in the alphabet. For example a row could have {a, c} which means it is affected by mutation 'a' and mutation 'c'. 
 Now we will generate the frequent item set by counting the number of occurrences of each character. This is also known as finding the support values. Then we will prune the item set by picking a minimum support threshold. For this pass of the algorithm we will pick 3. 
 Since all support values are three or above there is no pruning. The frequent item set is {a}, {b}, {c}, and {d}. After this we will repeat the process by counting pairs of mutations in the input set. 
 Now we will make our minimum support value 4 so only {a, d} will remain after pruning. Now we will use the frequent item set to make combinations of triplets.  We will then repeat the process by counting occurrences of triplets of mutations in the input set. 
 Since we only have one item the next set of combinations of quadruplets is empty so the algorithm will stop.
 Advantages and Limitations:
 Apriori has some limitations. Candidate generation can result in large candidate sets. For example a 10^4 frequent 1-itemset will generate a 10^7 candidate 2-itemset. The algorithm also needs to frequently scan the database, to be specific n+1 scans where n is the length of the longest pattern. Apriori is slower than the Eclat algorithm. However, Apriori performs well compared to Eclat when the dataset is large. This is because in the Eclat algorithm if the dataset is too large the tid-lists become too large for memory. FP-growth outperforms the Apriori and Eclat. This is due to the FP-growth algorithm not having candidate generation or test, using a compact data structure, and only having one database scan.[27]
 Eclat[11] (alt. ECLAT, stands for Equivalence Class Transformation) is a backtracking algorithm, which traverses the frequent itemset lattice graph in a depth-first search (DFS) fashion. Whereas the breadth-first search (BFS) traversal used in the Apriori algorithm will end up checking every subset of an itemset before checking it, DFS traversal checks larger itemsets and can save on checking the support of some of its subsets by virtue of the downward-closer property. Furthermore it will almost certainly use less memory as DFS has a lower space complexity than BFS.
 To illustrate this, let there be a frequent itemset {a, b, c}. a DFS may check the nodes in the frequent itemset lattice in the following order: {a} → {a, b} → {a, b, c}, at which point it is known that {b}, {c}, {a, c}, {b, c} all satisfy the support constraint by the downward-closure property. BFS would explore each subset of {a, b, c} before finally checking it. As the size of an itemset increases, the number of its subsets undergoes combinatorial explosion.
 It is suitable for both sequential as well as parallel execution with locality-enhancing properties.[28][29]
 FP stands for frequent pattern.[30]
 In the first pass, the algorithm counts the occurrences of items (attribute-value pairs) in the dataset of transactions, and stores these counts in a 'header table'. In the second pass, it builds the FP-tree structure by inserting transactions into a trie.
 Items in each transaction have to be sorted by descending order of their frequency in the dataset before being inserted so that the tree can be processed quickly.
Items in each transaction that do not meet the minimum support requirement are discarded.
If many transactions share most frequent items, the FP-tree provides high compression close to tree root.
 Recursive processing of this compressed version of the main dataset grows frequent item sets directly, instead of generating candidate items and testing them against the entire database (as in the apriori algorithm).
 Growth begins from the bottom of the header table i.e. the item with the smallest support by finding all sorted transactions that end in that item. Call this item 



I


{\displaystyle I}

.
 A new conditional tree is created which is the original FP-tree projected onto 



I


{\displaystyle I}

. The supports of all nodes in the projected tree are re-counted with each node getting the sum of its children counts. Nodes (and hence subtrees) that do not meet the minimum support are pruned. Recursive growth ends when no individual items conditional on 



I


{\displaystyle I}

 meet the minimum support threshold. The resulting paths from root to 



I


{\displaystyle I}

 will be frequent itemsets. After this step, processing continues with the next least-supported header item of the original FP-tree.
 Once the recursive process has completed, all frequent item sets will have been found, and association rule creation begins.[31]
 The ASSOC procedure[32] is a GUHA method which mines for generalized association rules using fast bitstrings operations. The association rules mined by this method are more general than those output by apriori, for example ""items"" can be connected both with conjunction and disjunctions and the relation between antecedent and consequent of the rule is not restricted to setting minimum support and confidence as in apriori: an arbitrary combination of supported interest measures can be used.
 OPUS is an efficient algorithm for rule discovery that, in contrast to most alternatives, does not require either monotone or anti-monotone constraints such as minimum support.[33] Initially used to find rules for a fixed consequent[33][34] it has subsequently been extended to find rules with any item as a consequent.[35] OPUS search is the core technology in the popular Magnum Opus association discovery system.
 A famous story about association rule mining is the ""beer and diaper"" story. A purported survey of behavior of supermarket shoppers discovered that customers (presumably young men) who buy diapers tend also to buy beer. This anecdote became popular as an example of how unexpected association rules might be found from everyday data. There are varying opinions as to how much of the story is true.[36] Daniel Powers says:[36]
 In 1992, Thomas Blischok, manager of a retail consulting group at Teradata, and his staff prepared an analysis of 1.2 million market baskets from about 25 Osco Drug stores. Database queries were developed to identify affinities. The analysis ""did discover that between 5:00 and 7:00 p.m. that consumers bought beer and diapers"". Osco managers did NOT exploit the beer and diapers relationship by moving the products closer together on the shelves. Multi-Relation Association Rules (MRAR): These are association rules where each item may have several relations. These relations indicate indirect relationships between the entities. Consider the following MRAR where the first item consists of three relations live in, nearby and humid: “Those who live in a place which is nearby a city with humid climate type and also are younger than 20 




⟹



{\displaystyle \implies }

 their health condition is good”. Such association rules can be extracted from RDBMS data or semantic web data.[37]
 Contrast set learning is a form of associative learning. Contrast set learners use rules that differ meaningfully in their distribution across subsets.[38][39]
 Weighted class learning is another form of associative learning where weights may be assigned to classes to give focus to a particular issue of concern for the consumer of the data mining results.
 High-order pattern discovery facilitates the capture of high-order (polythetic) patterns or event associations that are intrinsic to complex real-world data.
[40]
 K-optimal pattern discovery provides an alternative to the standard approach to association rule learning which requires that each pattern appear frequently in the data.
 Approximate Frequent Itemset mining is a relaxed version of Frequent Itemset mining that allows some of the items in some of the rows to be 0.[41]
 Generalized Association Rules hierarchical taxonomy (concept hierarchy)
 Quantitative Association Rules categorical and quantitative data
 Interval Data Association Rules e.g. partition the age into 5-year-increment ranged
 Sequential pattern mining  discovers subsequences that are common to more than minsup (minimum support threshold) sequences in a sequence database, where minsup is set by the user. A sequence is an ordered list of transactions.[42]
 Subspace Clustering, a specific type of clustering high-dimensional data, is in many variants also based on the downward-closure property for specific clustering models.[43]
 Warmr, shipped as part of the ACE data mining suite, allows association rule learning for first order relational rules.[44]
",associ rule learn machin learn method discov interest relat variabl larg databas intend identifi strong rule discov databas use measur interesting given transact varieti item associ rule meant discov rule determin certain item connect base concept strong rule rakesh agraw tomasz imieliński arun swami introduc associ rule discov regular product transact data record po system supermarket exampl rule n n p e b u r g e r onion potato burger found sale data supermarket would indic custom buy onion potato togeth like also buy hamburg meat inform use basi decis market activ promot price product placement addit exampl market basket analysi associ rule employ today mani applic area includ web usag mine intrus detect continu product bioinformat contrast sequenc mine associ rule learn typic consid order item either within transact across transact associ rule algorithm consist variou paramet make difficult without expertis data mine execut mani rule arduou understand follow origin definit agraw imieliński swami problem associ rule mine defin let n n set n binari attribut call item let set transact call databas transact uniqu transact id contain subset item rule defin implic form agraw imieliński swami rule defin set singl item x j j j j everi rule compos two differ set item also known itemset x x call anteced lh consequ rh anteced item found data consequ item found combin anteced statement x often read x anteced x consequ simpli impli theori whenev x occur dataset well associ rule made search data frequent pattern use certain criterion support confid defin import relationship support evid frequent item appear data given confid defin mani time statement found true howev third criteria use call lift use compar expect confid actual confid lift show mani time statement expect found true associ rule made calcul itemset creat two item rule built analyz possibl itemset data would mani rule mean associ rule typic made rule well repres data mani differ data mine techniqu could use find certain analyt result exampl classif analysi cluster analysi regress analysi techniqu use depend look data associ rule primarili use find analyt predict custom behavior classif analysi would like use question make decis predict behavior cluster analysi primarili use assumpt made like relationship within data regress analysi use want predict valu continu depend number independ variabl benefit mani benefit use associ rule like find pattern help understand correl data set good exampl use associ rule would medicin medicin use associ rule help diagnos patient diagnos patient mani variabl consid mani diseas share similar symptom use associ rule doctor determin condit probabl ill compar symptom relationship past case downsid howev associ rule also lead mani differ downsid find appropri paramet threshold set mine algorithm also downsid larg number discov rule reason guarante rule found relev could also caus algorithm low perform sometim implement algorithm contain mani variabl paramet someon good concept data mine might caus troubl understand threshold use associ rule like use support confid howev mean satisfi minimum support minimum confid time usual associ rule gener split two differ step need appli support threshold confid threshold tabl left origin unorgan data tabl right organ threshold case item c better threshold support confid first item second threshold valu spot item met threshold support confid item b met threshold either support confid last find frequent itemset databas easi task sinc involv go data find possibl item combin possibl itemset set possibl itemset power set size n n cours mean exclud empti set consid valid itemset howev size power set grow exponenti number item n within power set effici search possibl use properti support also call would guarante frequent itemset subset also frequent thu infrequ itemset subset frequent itemset exploit properti effici algorithm apriori eclat find frequent itemset illustr concept use small exampl supermarket domain tabl show small databas contain item entri valu mean presenc item correspond transact valu repres absenc item transact set item l k b r e b u e r b e e r p e r e g g f r u milk bread butter beer diaper egg fruit exampl rule supermarket could b u e r b r e l k butter bread milk mean butter bread bought custom also buy milk order select interest rule set possibl rule constraint variou measur signific interest use constraint minimum threshold support confid let x x itemset x associ rule set transact given databas note exampl extrem small practic applic rule need support sever hundr transact consid statist signific citat need dataset often contain thousand million transact support indic frequent itemset appear dataset exampl easier explain support write support p b number transact contain b total number transact support b number transact contain b total number transact b separ item set occur time transact use tabl exampl itemset x b e e r p e r beer diaper support sinc occur transact transact argument support x set precondit thu becom restrict grow instead inclus furthermor itemset l k b r e b u e r milk bread butter support appear transact well use anteced consequ allow data miner determin support multipl item bought togeth comparison whole data set exampl tabl show milk bought bread bought support transact milk well bread bought smaller data set like exampl harder see strong correl sampl data set grow larger support use find correl two product supermarket exampl minimum support threshold use determin itemset prefer interest set support threshold tabl l k e g g milk egg would remov sinc meet minimum threshold minimum threshold use remov sampl strong enough support confid deem sampl import interest dataset anoth way find interest sampl find valu support confid allow data miner see sampl support confid high enough highlight dataset prompt closer look sampl find inform connect item support benefici find connect product comparison whole dataset wherea confid look connect one item anoth item tabl show comparison contrast support support confid use inform tabl deriv confid valu support x respect defin proport transact dataset contain itemset denot transact uniqu identifi transact itemset support may written notat use defin complic dataset item itemset may easi supermarket exampl exampl support use find group genet mutat work collect caus diseas investig number subscrib respond upgrad offer discov product drug store never bought togeth confid percentag transact satisfi x also satisfi respect confid valu associ rule often denot x ratio transact contain x total amount x valu present x anteced consequ confid also interpret estim condit probabl p e e x p x probabl find rh rule transact condit transact also contain lh commonli depict equat illustr confid comput calcul transact x within dataset ratio transact contain mean number transact x divid x exampl tabl show rule b u e r b r e l k butter bread milk confid dataset denot everi time custom buy butter bread also buy milk particular exampl demonstr rule correct time transact contain butter bread rule f r u e g g fruit egg howev confid suggest egg bought time fruit brought within particular dataset fruit purchas total time two time consist egg purchas larger dataset minimum threshold percentag cutoff confid use determin item relationship appli method data tabl inform meet requir remov tabl show associ rule exampl minimum threshold confid data confid least omit gener threshold allow associ item becom stronger data research emphas tabl use confid inform tabl implement support confid column relationship item via confid support instead one concept highlight rank rule support confid multipl confid particular rule support often implement understand relationship item overal use confid associ rule mine great way bring awar data relat greatest benefit highlight relationship particular item one anoth within set compar item total occurr anteced specif rule howev confid optim method everi concept associ rule mine disadvantag use offer multipl differ outlook associ unlik support instanc confid provid perspect relationship certain item comparison entir dataset milk bread exampl may occur time confid support import look viewpoint support confid instead sole reli one concept incessantli defin relationship lift rule defin l f x u p p x u p p x u p p lift supp supp x supp ratio observ support expect x independ exampl rule l k b r e b u e r milk bread butter lift rule lift would impli probabl occurr anteced consequ independ two event independ rule drawn involv two event lift let us know degre two occurr depend one anoth make rule potenti use predict consequ futur data set lift let us know item substitut mean presenc one item neg effect presenc item vice versa valu lift consid support rule overal data set rede convict rule defin c n v x u p p c n f x conv supp conf exampl rule l k b r e b u e r milk bread butter convict interpret ratio expect frequenc x occur without say frequenc rule make incorrect predict x independ divid observ frequenc incorrect predict exampl convict valu show rule l k b r e b u e r milk bread butter would incorrect often time often associ x pure random chanc addit confid measur interesting rule propos popular measur sever measur present compar tan et al hahsler look techniqu model user known use model interesting measur current activ research trend name subject interesting concept associ rule popular particularli due articl agraw et acquir citat accord googl scholar april thu one cite paper data mine field howev call associ rule introduc alreadi paper guha gener data mine method develop petr hájek et al earli circa use minimum support confid find associ rule featur base model framework found rule u p p x supp x c n f x conf greater user defin constraint one limit standard approach discov associ search massiv number possibl associ look collect item appear associ larg risk find mani spuriou associ collect item unexpect frequenc data chanc exampl suppos consid collect item look rule contain two item item approxim rule appli statist test independ signific level mean chanc accept rule associ assum associ nonetheless expect find rule statist sound associ discoveri control risk case reduc risk find spuriou associ signific level mani algorithm gener associ rule propos algorithm apriori eclat half job sinc algorithm mine frequent itemset anoth step need done gener rule frequent itemset found databas apriori given agraw srikant frequent item set mine associ rule learn proce identifi frequent individu item databas extend larger larger item set long item set appear suffici often name algorithm apriori use prior knowledg frequent itemset properti overview apriori use bottom approach frequent subset extend one item time step known candid gener group candid test data algorithm termin success extens found apriori use search hash tree structur count candid item set effici gener candid item set length item set length prune candid infrequ sub pattern accord downward closur lemma candid set contain frequent item set scan transact databas determin frequent item set among candid exampl assum row cancer sampl certain combin mutat label charact alphabet exampl row could c mean affect mutat mutat c gener frequent item set count number occurr charact also known find support valu prune item set pick minimum support threshold pass algorithm pick sinc support valu three prune frequent item set b c repeat process count pair mutat input set make minimum support valu remain prune use frequent item set make combin triplet repeat process count occurr triplet mutat input set sinc one item next set combin quadruplet empti algorithm stop advantag limit apriori limit candid gener result larg candid set exampl frequent gener candid algorithm also need frequent scan databas specif scan n length longest pattern apriori slower eclat algorithm howev apriori perform well compar eclat dataset larg eclat algorithm dataset larg becom larg memori outperform apriori eclat due algorithm candid gener test use compact data structur one databas scan eclat alt eclat stand equival class transform backtrack algorithm travers frequent itemset lattic graph search df fashion wherea search bf travers use apriori algorithm end check everi subset itemset check df travers check larger itemset save check support subset virtu properti furthermor almost certainli use less memori df lower space complex bf illustr let frequent itemset b c df may check node frequent itemset lattic follow order b b c point known b c c b c satisfi support constraint properti bf would explor subset b c final check size itemset increas number subset undergo combinatori explos suitabl sequenti well parallel execut properti fp stand frequent pattern first pass algorithm count occurr item pair dataset transact store count tabl second pass build structur insert transact trie item transact sort descend order frequenc dataset insert tree process quickli item transact meet minimum support requir discard mani transact share frequent item provid high compress close tree root recurs process compress version main dataset grow frequent item set directli instead gener candid item test entir databas apriori algorithm growth begin bottom header tabl item smallest support find sort transact end item call item new condit tree creat origin project onto support node project tree node get sum children count node henc subtre meet minimum support prune recurs growth end individu item condit meet minimum support threshold result path root frequent itemset step process continu next header item origin recurs process complet frequent item set found associ rule creation begin assoc procedur guha method mine gener associ rule use fast bitstr oper associ rule mine method gener output apriori exampl item connect conjunct disjunct relat anteced consequ rule restrict set minimum support confid apriori arbitrari combin support interest measur use opu effici algorithm rule discoveri contrast altern requir either monoton constraint minimum support initi use find rule fix consequ subsequ extend find rule item consequ opu search core technolog popular magnum opu associ discoveri system famou stori associ rule mine beer diaper stori purport survey behavior supermarket shopper discov custom presum young men buy diaper tend also buy beer anecdot becam popular exampl unexpect associ rule might found everyday data vari opinion much stori true daniel power say thoma blischok manag retail consult group teradata staff prepar analysi million market basket osco drug store databas queri develop identifi affin analysi discov consum bought beer diaper osco manag exploit beer diaper relationship move product closer togeth shelv associ rule mrar associ rule item may sever relat relat indic indirect relationship entiti consid follow mrar first item consist three relat live nearbi humid live place nearbi citi humid climat type also younger health condit good associ rule extract rdbm data semant web data contrast set learn form associ learn contrast set learner use rule differ meaning distribut across subset weight class learn anoth form associ learn weight may assign class give focu particular issu concern consum data mine result pattern discoveri facilit captur polythet pattern event associ intrins complex data pattern discoveri provid altern standard approach associ rule learn requir pattern appear frequent data approxim frequent itemset mine relax version frequent itemset mine allow item row gener associ rule hierarch taxonomi concept hierarchi quantit associ rule categor quantit data interv data associ rule partit age rang sequenti pattern mine discov subsequ common minsup minimum support threshold sequenc sequenc databas minsup set user sequenc order list transact subspac cluster specif type cluster data mani variant also base properti specif cluster model warmr ship part ace data mine suit allow associ rule learn first order relat rule
Semantic analysis (machine learning),https://en.wikipedia.org/wiki/Semantic_analysis_(machine_learning),"In machine learning, semantic analysis of a text corpus is the task of building structures that approximate concepts from a large set of documents. It generally does not involve prior semantic understanding of the documents.
 Semantic analysis strategies include:
 This computer science article is a stub. You can help Wikipedia by expanding it.",machin learn semant analysi text corpu task build structur approxim concept larg set document gener involv prior semant understand document semant analysi strategi includ comput scienc articl stub help wikipedia expand
Structured prediction,https://en.wikipedia.org/wiki/Structured_prediction,"Structured prediction or structured output learning is an umbrella term for supervised machine learning techniques that involves predicting structured objects, rather than discrete or real values.[1]
 Similar to commonly used supervised learning techniques, structured prediction models are typically trained by means of observed data in which the predicted value is compared to the ground truth, and this is used to adjust the model parameters. Due to the complexity of the model and the interrelations of predicted variables, the processes of model training and inference are often computationally infeasible, so approximate inference and learning methods are used.
 An example application is the problem of translating a natural language sentence into a syntactic representation such as a parse tree. This can be seen as a structured prediction problem[2] in which the structured output domain is the set of all possible parse trees. Structured prediction is used in a wide variety of domains including bioinformatics, natural language processing (NLP), speech recognition, and computer vision. 
 Sequence tagging is a class of problems prevalent in NLP in which input data are often sequential, for instance sentences of text. The sequence tagging problem appears in several guises, such as part-of-speech tagging (POS tagging) and named entity recognition. In POS tagging, for example, each word in a sequence must be 'tagged' with a class label representing the type of word:
 The main challenge of this problem is to resolve ambiguity: in the above example, the words ""sentence"" and ""tagged"" in English can also be verbs.
 While this problem can be solved by simply performing classification of individual tokens, this approach does not take into account the empirical fact that tags do not occur independently; instead, each tag displays a strong conditional dependence on the tag of the previous word. This fact can be exploited in a sequence model such as a hidden Markov model or conditional random field[2] that predicts the entire tag sequence for a sentence (rather than just individual tags) via the Viterbi algorithm.
 Probabilistic graphical models form a large class of structured prediction models. In particular, Bayesian networks and random fields are popular. Other algorithms and models for structured prediction include inductive logic programming, case-based reasoning, structured SVMs, Markov logic networks, Probabilistic Soft Logic, and constrained conditional models. The main techniques are:
 One of the easiest ways to understand algorithms for general structured prediction is the structured perceptron by Collins.[3] This algorithm combines the perceptron algorithm for learning linear classifiers with an inference algorithm (classically the Viterbi algorithm when used on sequence data) and can be described abstractly as follows:
 In practice, finding the argmax over 




G
E
N

(

x

)


{\displaystyle {GEN}({x})}

 is done using an algorithm such as Viterbi or a max-sum, rather than an exhaustive search through an exponentially large set of candidates.
 The idea of learning is similar to that for multiclass perceptrons.
",structur predict structur output learn umbrella term supervis machin learn techniqu involv predict structur object rather discret real valu similar commonli use supervis learn techniqu structur predict model typic train mean observ data predict valu compar ground truth use adjust model paramet due complex model interrel predict variabl process model train infer often comput infeas approxim infer learn method use exampl applic problem translat natur languag sentenc syntact represent pars tree seen structur predict problem structur output domain set possibl pars tree structur predict use wide varieti domain includ bioinformat natur languag process nlp speech recognit comput vision sequenc tag class problem preval nlp input data often sequenti instanc sentenc text sequenc tag problem appear sever guis tag po tag name entiti recognit po tag exampl word sequenc must class label repres type word main challeng problem resolv ambigu exampl word sentenc tag english also verb problem solv simpli perform classif individu token approach take account empir fact tag occur independ instead tag display strong condit depend tag previou word fact exploit sequenc model hidden markov model condit random field predict entir tag sequenc sentenc rather individu tag via viterbi algorithm probabilist graphic model form larg class structur predict model particular bayesian network random field popular algorithm model structur predict includ induct logic program reason structur svm markov logic network probabilist soft logic constrain condit model main techniqu one easiest way understand algorithm gener structur predict structur perceptron collin algorithm combin perceptron algorithm learn linear classifi infer algorithm classic viterbi algorithm use sequenc data describ abstractli follow practic find argmax g e n x gen x done use algorithm viterbi rather exhaust search exponenti larg set candid idea learn similar multiclass perceptron
Feature engineering,https://en.wikipedia.org/wiki/Feature_engineering,"Feature engineering is a preprocessing step in supervised machine learning and statistical modeling[1] which transforms raw data into a more effective set of inputs. Each input comprises several attributes, known as features. By providing models with relevant information, feature engineering significantly enhances their predictive accuracy and decision-making capability.[2][3][4]
 Beyond machine learning, the principles of feature engineering are applied in various scientific fields, including physics. For example, physicists construct dimensionless numbers such as the Reynolds number in fluid dynamics, the Nusselt number in heat transfer, and the Archimedes number in sedimentation. They also develop first approximations of solutions, such as analytical solutions for the strength of materials in mechanics.[5]
 One of the applications of feature engineering has been clustering of feature-objects or sample-objects in a dataset. Especially, feature engineering based on matrix decomposition has been extensively used for data clustering under non-negativity constraints on the feature coefficients. These include Non-Negative Matrix Factorization (NMF),[6] Non-Negative Matrix-Tri Factorization (NMTF),[7] Non-Negative Tensor Decomposition/Factorization (NTF/NTD),[8] etc. The non-negativity constraints on coefficients of the feature vectors mined by the above-stated algorithms yields a part-based representation, and different factor matrices exhibit natural clustering properties. Several extensions of the above-stated feature engineering methods have been reported in literature, including orthogonality-constrained factorization for hard clustering, and manifold learning to overcome inherent issues with these algorithms.
 Other classes of feature engineering algorithms include leveraging a common hidden structure across multiple inter-related datasets to obtain a consensus (common) clustering scheme. An example is Multi-view Classification based on Consensus Matrix Decomposition (MCMD),[2] which mines a common clustering scheme across multiple datasets. MCMD is designed to output two types of class labels (scale-variant and scale-invariant clustering), and:
 Coupled matrix and tensor decompositions are popular in multi-view feature engineering.[9]
 Feature engineering in machine learning and statistical modeling involves selecting, creating, transforming, and extracting data features. Key components include feature creation from existing data, transforming and imputing missing or invalid features, reducing data dimensionality through methods like Principal Components Analysis (PCA), Independent Component Analysis (ICA), and Linear Discriminant Analysis (LDA), and selecting the most relevant features for model training based on importance scores and correlation matrices.[10]
 Features vary in significance.[11] Even relatively insignificant features may contribute to a model. Feature selection can reduce the number of features to prevent a model from becoming too specific to the training data set (overfitting).[12]
 Feature explosion occurs when the number of identified features is too large for effective model estimation or optimization. Common causes include:
 Feature explosion can be limited via techniques such as: regularization, kernel methods, and feature selection.[13]
 Automation of feature engineering is a research topic that dates back to the 1990s.[14] Machine learning software that incorporates automated feature engineering has been commercially available since 2016.[15] Related academic literature can be roughly separated into two types:
 Multi-relational Decision Tree Learning (MRDTL) extends traditional decision tree methods to relational databases, handling complex data relationships across tables. It innovatively uses selection graphs as decision nodes, refined systematically until a specific termination criterion is reached.[14]
 Most MRDTL studies base implementations on relational databases, which results in many redundant operations. These redundancies can be reduced by using techniques such as tuple id propagation.[16][17]
 There are a number of open-source libraries and tools that automate feature engineering on relational data and time series:
 [OneBM] helps data scientists reduce data exploration time allowing them to try and error many ideas in short time. On the other hand, it enables non-experts, who are not familiar with data science, to quickly extract value from their data with a little effort, time, and cost.[22] The deep feature synthesis (DFS) algorithm beat 615 of 906 human teams in a competition.[32][33]
 The feature store is where the features are stored and organized for the explicit purpose of being used to either train models (by data scientists) or make predictions (by applications that have a trained model). It is a central location where you can either create or update groups of features created from multiple different data sources, or create and update new datasets from those feature groups for training models or for use in applications that do not want to compute the features but just retrieve them when it needs them to make predictions.[34]
 A feature store includes the ability to store code used to generate features, apply the code to raw data, and serve those features to models upon request. Useful capabilities include feature versioning and policies governing the circumstances under which features can be used.[35]
 Feature stores can be standalone software tools or built into machine learning platforms.
 Feature engineering can be a time-consuming and error-prone process, as it requires domain expertise and often involves trial and error.[36][37] Deep learning algorithms may be used to process a large raw dataset without having to resort to feature engineering.[38] However, deep learning algorithms still require careful preprocessing and cleaning of the input data.[39] In addition, choosing the right architecture, hyperparameters, and optimization algorithm for a deep neural network can be a challenging and iterative process.[40]
",featur engin preprocess step supervis machin learn statist model transform raw data effect set input input compris sever attribut known featur provid model relev inform featur engin significantli enhanc predict accuraci capabl beyond machin learn principl featur engin appli variou scientif field includ physic exampl physicist construct dimensionless number reynold number fluid dynam nusselt number heat transfer archimed number sediment also develop first approxim solut analyt solut strength materi mechan one applic featur engin cluster dataset especi featur engin base matrix decomposit extens use data cluster constraint featur coeffici includ matrix factor nmf factor nmtf tensor etc constraint coeffici featur vector mine algorithm yield represent differ factor matric exhibit natur cluster properti sever extens featur engin method report literatur includ factor hard cluster manifold learn overcom inher issu algorithm class featur engin algorithm includ leverag common hidden structur across multipl dataset obtain consensu common cluster scheme exampl classif base consensu matrix decomposit mcmd mine common cluster scheme across multipl dataset mcmd design output two type class label cluster coupl matrix tensor decomposit popular featur engin featur engin machin learn statist model involv select creat transform extract data featur key compon includ featur creation exist data transform imput miss invalid featur reduc data dimension method like princip compon analysi pca independ compon analysi ica linear discrimin analysi lda select relev featur model train base import score correl matric featur vari signific even rel insignific featur may contribut model featur select reduc number featur prevent model becom specif train data set overfit featur explos occur number identifi featur larg effect model estim optim common caus includ featur explos limit via techniqu regular kernel method featur select autom featur engin research topic date back machin learn softwar incorpor autom featur engin commerci avail sinc relat academ literatur roughli separ two type decis tree learn mrdtl extend tradit decis tree method relat databas handl complex data relationship across tabl innov use select graph decis node refin systemat specif termin criterion reach mrdtl studi base implement relat databas result mani redund oper redund reduc use techniqu tupl id propag number librari tool autom featur engin relat data time seri onebm help data scientist reduc data explor time allow tri error mani idea short time hand enabl familiar data scienc quickli extract valu data littl effort time cost deep featur synthesi df algorithm beat human team competit featur store featur store organ explicit purpos use either train model data scientist make predict applic train model central locat either creat updat group featur creat multipl differ data sourc creat updat new dataset featur group train model use applic want comput featur retriev need make predict featur store includ abil store code use gener featur appli code raw data serv featur model upon request use capabl includ featur version polici govern circumst featur use featur store standalon softwar tool built machin learn platform featur engin process requir domain expertis often involv trial error deep learn algorithm may use process larg raw dataset without resort featur engin howev deep learn algorithm still requir care preprocess clean input data addit choos right architectur hyperparamet optim algorithm deep neural network challeng iter process
Feature learning,https://en.wikipedia.org/wiki/Feature_learning,"In machine learning (ML), feature learning or representation learning[2] is a set of techniques that allow a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform  a specific task.
 Feature learning is motivated by the fact that ML tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data, such as image, video, and sensor data, have not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.
 Feature learning can be either supervised, unsupervised, or self-supervised:
 Supervised feature learning is learning features from labeled data. The data label allows the system to compute an error term, the degree to which the system fails to produce the label, which can then be used as feedback to correct the learning process (reduce/minimize the error). Approaches include:
 Dictionary learning develops a set (dictionary) of representative elements from the input data such that each data point can be represented as a weighted sum of the representative elements. The dictionary elements and the weights may be found by minimizing the average representation error  (over the input data), together with L1 regularization on the weights to enable sparsity (i.e., the representation of each data point has only a few nonzero weights).
 Supervised dictionary learning exploits both the structure underlying the input data and the labels for optimizing the dictionary elements. For example, this[12] supervised dictionary learning technique applies dictionary learning on classification problems by jointly optimizing the dictionary elements, weights for representing data points, and parameters of the classifier based on the input data. In particular, a minimization problem is formulated, where the objective function consists of the classification error, the representation error, an L1 regularization on the representing weights for each data point (to enable sparse representation of data), and an L2 regularization on the parameters of the classifier.
 Neural networks are a family of learning algorithms that use a ""network"" consisting of multiple layers of inter-connected nodes. It is inspired by the animal nervous system, where the nodes are viewed as neurons and edges are viewed as synapses. Each edge has an associated weight, and the network defines computational rules for passing input data from the network's input layer to the output layer. A network function associated with a neural network characterizes the relationship between input and output layers, which is parameterized by the weights. With appropriately defined network functions, various learning tasks can be performed by minimizing a cost function over the network function (weights).
 Multilayer neural networks can be used to perform feature learning, since they learn a representation of their input at the hidden layer(s) which is subsequently used for classification or regression at the output layer. The most popular network architecture of this type is Siamese networks.
 Unsupervised feature learning is learning features from unlabeled data. The goal of unsupervised feature learning is often to discover low-dimensional features that capture some structure underlying the high-dimensional input data. When the feature learning is performed in an unsupervised way, it enables a form of semisupervised learning where features learned from an unlabeled dataset are then employed to improve performance in a supervised setting with labeled data.[13][14] Several approaches are introduced in the following.
 K-means clustering is an approach for vector quantization. In particular, given a set of n vectors, k-means clustering groups them into k clusters (i.e., subsets) in such a way that each vector belongs to the cluster with the closest mean. The problem is computationally NP-hard, although suboptimal greedy algorithms have been developed.
 K-means clustering can be used to group an unlabeled set of inputs into k clusters, and then use the centroids of these clusters to produce features. These features can be produced in several ways. The simplest is to add k binary features to each sample, where each feature j has value one iff the jth centroid learned by k-means is the closest to the sample under consideration.[6] It is also possible to use the distances to the clusters as features, perhaps after transforming them through a radial basis function (a technique that has been used to train RBF networks[15]). Coates and Ng note that certain variants of k-means behave similarly to sparse coding algorithms.[16]
 In a comparative evaluation of unsupervised feature learning methods, Coates, Lee and Ng found that k-means clustering with an appropriate transformation outperforms the more recently invented auto-encoders and RBMs on an image classification task.[6] K-means also improves performance in the domain of NLP, specifically for named-entity recognition;[17] there, it competes with Brown clustering, as well as with distributed word representations (also known as neural word embeddings).[14]
 Principal component analysis (PCA) is often used for dimension reduction. Given an unlabeled set of n input data vectors, PCA generates p (which is much smaller than the dimension of the input data) right singular vectors corresponding to the p largest singular values of the data matrix, where the kth row of the data matrix is the kth input data vector shifted by the sample mean of the input (i.e., subtracting the sample mean from the data vector). Equivalently, these singular vectors are the eigenvectors corresponding to the p largest eigenvalues of the sample covariance matrix of the input vectors. These p singular vectors are the feature vectors learned from the input data, and they represent directions along which the data has the largest variations.
 PCA is a linear feature learning approach since the p singular vectors are linear functions of the data matrix. The singular vectors can be generated via a simple algorithm with p iterations. In the ith iteration, the projection of the data matrix on the (i-1)th eigenvector is subtracted, and the ith singular vector is found as the right singular vector corresponding to the largest singular of the residual data matrix.
 PCA has several limitations. First, it assumes that the directions with large variance are of most interest, which may not be the case. PCA only relies on orthogonal transformations of the original data, and it exploits only the first- and second-order moments of the data, which may not well characterize the data distribution. Furthermore, PCA can effectively reduce dimension only when the input data vectors are correlated (which results in a few dominant eigenvalues).
 Local linear embedding (LLE) is a nonlinear learning approach for generating low-dimensional neighbor-preserving representations from (unlabeled) high-dimension input. The approach was proposed by Roweis and Saul (2000).[18][19] The general idea of LLE is to reconstruct the original high-dimensional data using lower-dimensional points while maintaining some geometric properties of the neighborhoods in the original data set.
 LLE consists of two major steps. The first step is for ""neighbor-preserving"", where each input data point Xi is reconstructed as a weighted sum of K nearest neighbor data points, and the optimal weights are found by minimizing the average squared reconstruction error (i.e., difference between an input point and its reconstruction) under the constraint that the weights associated with each point sum up to one. The second step is for ""dimension reduction,"" by looking for vectors in a lower-dimensional space that minimizes the representation error using the optimized weights in the first step. Note that in the first step, the weights are optimized with fixed data, which can be solved as a least squares problem. In the second step, lower-dimensional points are optimized with fixed weights, which can be solved via sparse eigenvalue decomposition.
 The reconstruction weights obtained in the first step capture the ""intrinsic geometric properties"" of a neighborhood in the input data.[19] It is assumed that original data lie on a smooth lower-dimensional manifold, and the ""intrinsic geometric properties"" captured by the weights of the original data are also expected to be on the manifold. This is why the same weights are used in the second step of LLE. Compared with PCA, LLE is more powerful in exploiting the underlying data structure.
 Independent component analysis (ICA) is a technique for forming a data representation using a weighted sum of independent non-Gaussian components.[20] The assumption of non-Gaussian is imposed since the weights cannot be uniquely determined when all the components follow Gaussian distribution.
 Unsupervised dictionary learning does not utilize data labels and exploits the structure underlying the data for optimizing dictionary elements. An example of unsupervised dictionary learning is sparse coding, which aims to learn basis functions (dictionary elements) for data representation from unlabeled input data. Sparse coding can be applied to learn overcomplete dictionaries, where the number of dictionary elements is larger than the dimension of the input data.[21] Aharon et al. proposed algorithm K-SVD for learning a dictionary of elements that enables sparse representation.[22]
 The hierarchical architecture of the biological neural system inspires deep learning architectures for feature learning by stacking multiple layers of learning nodes.[23] These architectures are often designed based on the assumption of distributed representation: observed data is generated by the interactions of many different factors on multiple levels. In a deep learning architecture, the output of each intermediate layer can be viewed as a representation of the original input data. Each level uses the representation produced by the previous, lower level as input, and produces new representations as output, which are then fed to higher levels. The input at the bottom layer is raw data, and the output of the final, highest layer is the final low-dimensional feature or representation.
 Restricted Boltzmann machines (RBMs) are often used as a building block for multilayer learning architectures.[6][24] An RBM can be represented by an undirected bipartite graph consisting of a group of binary hidden variables, a group of visible variables, and edges connecting the hidden and visible nodes. It is a special case of the more general Boltzmann machines with the constraint of no intra-node connections. Each edge in an RBM is associated with a weight. The weights together with the connections define an energy function, based on which a joint distribution of visible and hidden nodes can be devised. Based on the topology of the RBM, the hidden (visible) variables are independent, conditioned on the visible (hidden) variables.[clarification needed] Such conditional independence facilitates computations.
 An RBM can be viewed as a single layer architecture for unsupervised feature learning. In particular, the visible variables correspond to input data, and the hidden variables correspond to feature detectors. The weights can be trained by maximizing the probability of visible variables using Hinton's contrastive divergence (CD) algorithm.[24]
 In general, training RBMs by solving the maximization problem tends to result in non-sparse representations. Sparse RBM[25] was proposed to enable sparse representations. The idea is to add a regularization term in the objective function of data likelihood, which penalizes the deviation of the expected hidden variables from a small constant 



p


{\displaystyle p}

. RBMs have also been used to obtain disentangled representations of data, where interesting features map to separate hidden units.[26]
 An autoencoder consisting of an encoder and a decoder is a paradigm for deep learning architectures. An example is provided by Hinton and Salakhutdinov[24] where the encoder uses raw data (e.g., image) as input and produces feature or representation as output and the decoder uses the extracted feature from the encoder as input and reconstructs the original input raw data as output. The encoder and decoder are constructed by stacking multiple layers of RBMs. The parameters involved in the architecture were originally trained in a greedy layer-by-layer manner: after one layer of feature detectors is learned, they are fed up as visible variables for training the corresponding RBM. Current approaches typically apply end-to-end training with stochastic gradient descent methods. Training can be repeated until some stopping criteria are satisfied.
 Self-supervised representation learning is learning features by training on the structure of unlabeled data rather than relying on explicit labels for an information signal. This approach has enabled the combined use of deep neural network architectures and larger unlabeled datasets to produce deep feature representations.[9] Training tasks typically fall under the classes of either contrastive, generative or both.[27] Contrastive representation learning trains representations for associated data pairs, called positive samples, to be aligned, while pairs with no relation, called negative samples, are contrasted. A larger portion of negative samples is typically necessary in order to prevent catastrophic collapse, which is when all inputs are mapped to the same representation.[9] Generative representation learning tasks the model with producing the correct data to either match a restricted input or reconstruct the full input from a lower dimensional representation.[27]
 A common setup for self-supervised representation learning of a certain data type (e.g. text, image, audio, video) is to pretrain the model using large datasets of general context, unlabeled data.[11] Depending on the context, the result of this is either a set of representations for common data segments (e.g. words) which new data can be broken into, or a neural network able to convert each new data point (e.g. image) into a set of lower dimensional features.[9] In either case, the output representations can then be used as an initialization in many different problem settings where labeled data may be limited. Specialization of the model to specific tasks is typically done with supervised learning, either by fine-tuning the model / representations with the labels as the signal, or freezing the representations and training an additional model which takes them as an input.[11]
 Many self-supervised training schemes have been developed for use in representation learning of various modalities, often first showing successful application in text or image before being transferred to other data types.[9]
 Word2vec is a word embedding technique which learns to represent words through self-supervision over each word and its neighboring words in a sliding window across a large corpus of text.[28] The model has two possible training schemes to produce word vector representations, one generative and one contrastive.[27] The first is word prediction given each of the neighboring words as an input.[28] The second is training on the representation similarity for neighboring words and representation dissimilarity for random pairs of words.[10] A limitation of word2vec is that only the pairwise co-occurrence structure of the data is used, and not the ordering or entire set of context words. More recent transformer-based representation learning approaches attempt to solve this with word prediction tasks.[9] GPTs pretrain on next word prediction using prior input words as context,[29] whereas BERT masks random tokens in order to provide bidirectional context.[30]
 Other self-supervised techniques extend word embeddings by finding representations for larger text structures such as sentences or paragraphs in the input data.[9] Doc2vec extends the generative training approach in word2vec by adding an additional input to the word prediction task based on the paragraph it is within, and is therefore intended to represent paragraph level context.[31]
 The domain of image representation learning has employed many different self-supervised training techniques, including transformation,[32] inpainting,[33] patch discrimination[34] and clustering.[35]
 Examples of generative approaches are Context Encoders, which trains an AlexNet CNN architecture to generate a removed image region given the masked image as input,[33] and iGPT, which applies the GPT-2 language model architecture to images by training on pixel prediction after reducing the image resolution.[36]
 Many other self-supervised methods use siamese networks, which generate different views of the image through various augmentations that are then aligned to have similar representations. The challenge is avoiding collapsing solutions where the model encodes all images to the same representation.[37] SimCLR is a contrastive approach which uses negative examples in order to generate image representations with a ResNet CNN.[34] Bootstrap Your Own Latent (BYOL) removes the need for negative samples by encoding one of the views with a slow moving average of the model parameters as they are being modified during training.[38]
 The goal of many graph representation learning techniques is to produce an embedded representation of each node based on the overall network topology.[39] node2vec extends the word2vec training technique to nodes in a graph by using co-occurrence in random walks through the graph as the measure of association.[40] Another approach is to maximize mutual information, a measure of similarity, between the representations of associated structures within the graph.[9] An example is Deep Graph Infomax, which uses contrastive self-supervision based on mutual information between the representation of a “patch” around each node, and a summary representation of the entire graph. Negative samples are obtained by pairing the graph representation with either representations from another graph in a multigraph training setting, or corrupted patch representations in single graph training.[41]
 With analogous results in masked prediction[42] and clustering,[43] video representation learning approaches are often similar to image techniques but must utilize the temporal sequence of video frames as an additional learned structure. Examples include VCP, which masks video clips and trains to choose the correct one given a set of clip options, and Xu et al., who train a 3D-CNN to identify the original order given a shuffled set of video clips.[44]
 Self-supervised representation techniques have also been applied to many audio data formats, particularly for speech processing.[9] Wav2vec 2.0 discretizes the audio waveform into timesteps via temporal convolutions, and then trains a transformer on masked prediction of random timesteps using a contrastive loss.[45] This is similar to the BERT language model, except as in many SSL approaches to video, the model chooses among a set of options rather than over the entire word vocabulary.[30][45]
 Self-supervised learning has also been used to develop joint representations of multiple data types.[9] Approaches usually rely on some natural or human-derived association between the modalities as an implicit label, for instance video clips of animals or objects with characteristic sounds,[46] or captions written to describe images.[47] CLIP produces a joint image-text representation space by training to align image and text encodings from a large dataset of image-caption pairs using a contrastive loss.[47] MERLOT Reserve trains a transformer-based encoder to jointly represent audio, subtitles and video frames from a large dataset of videos through 3 joint pretraining tasks: contrastive masked prediction of either audio or text segments given the video frames and surrounding audio and text context, along with contrastive alignment of video frames with their corresponding captions.[46]
 Multimodal representation models are typically unable to assume direct correspondence of representations in the different modalities, since the precise alignment can often be noisy or ambiguous. For example, the text ""dog"" could be paired with many different pictures of dogs, and correspondingly a picture of a dog could be captioned with varying degrees of specificity. This limitation means that downstream tasks may require an additional generative mapping network between modalities to achieve optimal performance, such as in DALLE-2 for text to image generation.[48]
 Dynamic representation learning methods[49] [50] generate latent embeddings for dynamic systems such as dynamic networks. Since particular distance functions are invariant under particular linear transformations, different sets of embedding vectors can actually represent the same/similar information. Therefore, for a dynamic system, a temporal difference in its embeddings may be explained by misalignment of embeddings due to arbitrary transformations and/or actual changes in the system.[51] Therefore, generally speaking, temporal embeddings learned via dynamic representation learning methods should be inspected for any spurious changes and be aligned before consequent dynamic analyses.
",machin learn ml featur learn represent learn set techniqu allow system automat discov represent need featur detect classif raw data replac manual featur engin allow machin learn featur use perform specif task featur learn motiv fact ml task classif often requir input mathemat comput conveni process howev data imag video sensor data yield attempt algorithm defin specif featur altern discov featur represent examin without reli explicit algorithm featur learn either supervis unsupervis supervis featur learn learn featur label data data label allow system comput error term degre system fail produc label use feedback correct learn process error approach includ dictionari learn develop set dictionari repres element input data data point repres weight sum repres element dictionari element weight may found minim averag represent error input data togeth regular weight enabl sparsiti represent data point nonzero weight supervis dictionari learn exploit structur underli input data label optim dictionari element exampl supervis dictionari learn techniqu appli dictionari learn classif problem jointli optim dictionari element weight repres data point paramet classifi base input data particular minim problem formul object function consist classif error represent error regular repres weight data point enabl spars represent data regular paramet classifi neural network famili learn algorithm use network consist multipl layer node inspir anim nervou system node view neuron edg view synaps edg associ weight network defin comput rule pass input data network input layer output layer network function associ neural network character relationship input output layer parameter weight appropri defin network function variou learn task perform minim cost function network function weight multilay neural network use perform featur learn sinc learn represent input hidden layer subsequ use classif regress output layer popular network architectur type siames network unsupervis featur learn learn featur unlabel data goal unsupervis featur learn often discov featur captur structur underli input data featur learn perform unsupervis way enabl form semisupervis learn featur learn unlabel dataset employ improv perform supervis set label data sever approach introduc follow cluster approach vector quantiz particular given set n vector cluster group k cluster subset way vector belong cluster closest mean problem comput although suboptim greedi algorithm develop cluster use group unlabel set input k cluster use centroid cluster produc featur featur produc sever way simplest add k binari featur sampl featur j valu one iff jth centroid learn closest sampl consider also possibl use distanc cluster featur perhap transform radial basi function techniqu use train rbf network coat ng note certain variant behav similarli spars code algorithm compar evalu unsupervis featur learn method coat lee ng found cluster appropri transform outperform recent invent rbm imag classif task also improv perform domain nlp specif recognit compet brown cluster well distribut word represent also known neural word embed princip compon analysi pca often use dimens reduct given unlabel set n input data vector pca gener p much smaller dimens input data right singular vector correspond p largest singular valu data matrix kth row data matrix kth input data vector shift sampl mean input subtract sampl mean data vector equival singular vector eigenvector correspond p largest eigenvalu sampl covari matrix input vector p singular vector featur vector learn input data repres direct along data largest variat pca linear featur learn approach sinc p singular vector linear function data matrix singular vector gener via simpl algorithm p iter ith iter project data matrix th eigenvector subtract ith singular vector found right singular vector correspond largest singular residu data matrix pca sever limit first assum direct larg varianc interest may case pca reli orthogon transform origin data exploit moment data may well character data distribut furthermor pca effect reduc dimens input data vector correl result domin eigenvalu local linear embed lle nonlinear learn approach gener represent unlabel input approach propos rowei saul gener idea lle reconstruct origin data use point maintain geometr properti neighborhood origin data set lle consist two major step first step input data point xi reconstruct weight sum k nearest neighbor data point optim weight found minim averag squar reconstruct error differ input point reconstruct constraint weight associ point sum one second step dimens reduct look vector space minim represent error use optim weight first step note first step weight optim fix data solv least squar problem second step point optim fix weight solv via spars eigenvalu decomposit reconstruct weight obtain first step captur intrins geometr properti neighborhood input data assum origin data lie smooth manifold intrins geometr properti captur weight origin data also expect manifold weight use second step lle compar pca lle power exploit underli data structur independ compon analysi ica techniqu form data represent use weight sum independ compon assumpt impos sinc weight uniqu determin compon follow gaussian distribut unsupervis dictionari learn util data label exploit structur underli data optim dictionari element exampl unsupervis dictionari learn spars code aim learn basi function dictionari element data represent unlabel input data spars code appli learn overcomplet dictionari number dictionari element larger dimens input data aharon et al propos algorithm learn dictionari element enabl spars represent hierarch architectur biolog neural system inspir deep learn architectur featur learn stack multipl layer learn node architectur often design base assumpt distribut represent observ data gener interact mani differ factor multipl level deep learn architectur output intermedi layer view represent origin input data level use represent produc previou lower level input produc new represent output fed higher level input bottom layer raw data output final highest layer final featur represent restrict boltzmann machin rbm often use build block multilay learn architectur rbm repres undirect bipartit graph consist group binari hidden variabl group visibl variabl edg connect hidden visibl node special case gener boltzmann machin constraint connect edg rbm associ weight weight togeth connect defin energi function base joint distribut visibl hidden node devis base topolog rbm hidden visibl variabl independ condit visibl hidden variabl clarif need condit independ facilit comput rbm view singl layer architectur unsupervis featur learn particular visibl variabl correspond input data hidden variabl correspond featur detector weight train maxim probabl visibl variabl use hinton contrast diverg cd algorithm gener train rbm solv maxim problem tend result represent spars rbm propos enabl spars represent idea add regular term object function data likelihood penal deviat expect hidden variabl small constant p p rbm also use obtain disentangl represent data interest featur map separ hidden unit autoencod consist encod decod paradigm deep learn architectur exampl provid hinton salakhutdinov encod use raw data imag input produc featur represent output decod use extract featur encod input reconstruct origin input raw data output encod decod construct stack multipl layer rbm paramet involv architectur origin train greedi manner one layer featur detector learn fed visibl variabl train correspond rbm current approach typic appli train stochast gradient descent method train repeat stop criteria satisfi represent learn learn featur train structur unlabel data rather reli explicit label inform signal approach enabl combin use deep neural network architectur larger unlabel dataset produc deep featur represent train task typic fall class either contrast gener contrast represent learn train represent associ data pair call posit sampl align pair relat call neg sampl contrast larger portion neg sampl typic necessari order prevent catastroph collaps input map represent gener represent learn task model produc correct data either match restrict input reconstruct full input lower dimension represent common setup represent learn certain data type text imag audio video pretrain model use larg dataset gener context unlabel data depend context result either set represent common data segment word new data broken neural network abl convert new data point imag set lower dimension featur either case output represent use initi mani differ problem set label data may limit special model specif task typic done supervis learn either model represent label signal freez represent train addit model take input mani train scheme develop use represent learn variou modal often first show success applic text imag transfer data type word embed techniqu learn repres word word neighbor word slide window across larg corpu text model two possibl train scheme produc word vector represent one gener one contrast first word predict given neighbor word input second train represent similar neighbor word represent dissimilar random pair word limit pairwis structur data use order entir set context word recent represent learn approach attempt solv word predict task gpt pretrain next word predict use prior input word context wherea bert mask random token order provid bidirect context techniqu extend word embed find represent larger text structur sentenc paragraph input data extend gener train approach ad addit input word predict task base paragraph within therefor intend repres paragraph level context domain imag represent learn employ mani differ train techniqu includ transform inpaint patch discrimin cluster exampl gener approach context encod train alexnet cnn architectur gener remov imag region given mask imag input igpt appli languag model architectur imag train pixel predict reduc imag resolut mani method use siames network gener differ view imag variou augment align similar represent challeng avoid collaps solut model encod imag represent simclr contrast approach use neg exampl order gener imag represent resnet cnn bootstrap latent byol remov need neg sampl encod one view slow move averag model paramet modifi train goal mani graph represent learn techniqu produc embed represent node base overal network topolog extend train techniqu node graph use random walk graph measur associ anoth approach maxim mutual inform measur similar represent associ structur within graph exampl deep graph infomax use contrast base mutual inform represent patch around node summari represent entir graph neg sampl obtain pair graph represent either represent anoth graph multigraph train set corrupt patch represent singl graph train analog result mask predict cluster video represent learn approach often similar imag techniqu must util tempor sequenc video frame addit learn structur exampl includ vcp mask video clip train choos correct one given set clip option xu et train identifi origin order given shuffl set video clip represent techniqu also appli mani audio data format particularli speech process discret audio waveform timestep via tempor convolut train transform mask predict random timestep use contrast loss similar bert languag model except mani ssl approach video model choos among set option rather entir word vocabulari learn also use develop joint represent multipl data type approach usual reli natur associ modal implicit label instanc video clip anim object characterist sound caption written describ imag clip produc joint represent space train align imag text encod larg dataset pair use contrast loss merlot reserv train encod jointli repres audio subtitl video frame larg dataset video joint pretrain task contrast mask predict either audio text segment given video frame surround audio text context along contrast align video frame correspond caption multimod represent model typic unabl assum direct correspond represent differ modal sinc precis align often noisi ambigu exampl text dog could pair mani differ pictur dog correspondingli pictur dog could caption vari degre specif limit mean downstream task may requir addit gener map network modal achiev optim perform text imag gener dynam represent learn method gener latent embed dynam system dynam network sinc particular distanc function invari particular linear transform differ set embed vector actual repres inform therefor dynam system tempor differ embed may explain misalign embed due arbitrari transform actual chang system therefor gener speak tempor embed learn via dynam represent learn method inspect spuriou chang align consequ dynam analys
Learning to rank,https://en.wikipedia.org/wiki/Learning_to_rank,"Learning to rank[1] or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems.[2] Training data may, for example, consist of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. ""relevant"" or ""not relevant"") for each item. The goal of constructing the ranking model is to rank new, unseen lists in a similar way to rankings in the training data.
 Ranking is a central part of many information retrieval problems, such as document retrieval, collaborative filtering, sentiment analysis, and online advertising.
 A possible architecture of a machine-learned search engine is shown in the accompanying figure.
 Training data consists of queries and documents matching them together with the relevance degree of each match. It may be prepared manually by human assessors (or raters, as Google calls them), who check results for some queries and determine relevance of each result. It is not feasible to check the relevance of all documents, and so typically a technique called pooling is used — only the top few documents, retrieved by some existing ranking models are checked. This technique may introduce selection bias. Alternatively, training data may be derived automatically by analyzing clickthrough logs (i.e. search results which got clicks from users),[3] query chains,[4] or such search engines' features as Google's (since-replaced) SearchWiki. Clickthrough logs can be biased by the tendency of users to click on the top search results on the assumption that they are already well-ranked.
 Training data is used by a learning algorithm to produce a ranking model which computes the relevance of documents for actual queries.
 Typically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used.[5] First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as the vector space model, Boolean model, weighted AND,[6] or BM25. This phase is called top-



k


{\displaystyle k}

 document retrieval and many heuristics were proposed in the literature to accelerate it, such as using a document's static quality score and tiered indexes.[7] In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents.
 Learning to rank algorithms have been applied in areas other than information retrieval:
 For the convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called feature vectors. Such an approach is sometimes called bag of features and is analogous to the bag of words model and vector space model used in information retrieval for representation of documents.
 Components of such vectors are called features, factors or ranking signals. They may be divided into three groups (features from document retrieval are shown as examples):
 Some examples of features, which were used in the well-known LETOR dataset:
 Selecting and designing good features is an important area in machine learning, which is called feature engineering.
 There are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare the performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics.
 Examples of ranking quality measures:
 DCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used.[11] Other metrics such as MAP, MRR and precision, are defined only for binary judgments.
 Recently, there have been proposed several new evaluation metrics which claim to model user's satisfaction with search results better than the DCG metric:
 Both of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document.
 Tie-Yan Liu of Microsoft Research Asia has analyzed existing algorithms for learning to rank problems in his book Learning to Rank for Information Retrieval.[1] He categorized them into three groups by their input spaces, output spaces, hypothesis spaces (the core function of the model) and loss functions: the pointwise, pairwise, and listwise approach. In practice, listwise approaches often outperform pairwise approaches and pointwise approaches. This statement was further supported by a large scale experiment on the performance of different learning-to-rank methods on a large collection of benchmark data sets.[14]
 In this section, without further notice, 



x


{\displaystyle x}

 denotes an object to be evaluated, for example, a document or an image, 



f
(
x
)


{\displaystyle f(x)}

 denotes a single-value hypothesis, 



h
(
⋅
)


{\displaystyle h(\cdot )}

 denotes a bi-variate or multi-variate function and 



L
(
⋅
)


{\displaystyle L(\cdot )}

 denotes the loss function.
 In this case, it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then the learning-to-rank problem can be approximated by a regression problem — given a single query-document pair, predict its score. Formally speaking, the pointwise approach aims at learning a function 



f
(
x
)


{\displaystyle f(x)}

 predicting the real-value or ordinal score of a document 



x


{\displaystyle x}

 using the loss function 



L
(
f
;

x

j


,

y

j


)


{\displaystyle L(f;x_{j},y_{j})}

.
 A number of existing supervised machine learning algorithms can be readily used for this purpose. Ordinal regression and classification algorithms can also be used in pointwise approach when they are used to predict the score of a single query-document pair, and it takes a small, finite number of values.
 In this case, the learning-to-rank problem is approximated by a classification problem — learning a binary classifier 



h
(

x

u


,

x

v


)


{\displaystyle h(x_{u},x_{v})}

 that can tell which document is better in a given pair of documents. The classifier shall take two documents as its input and the goal is to minimize a loss function 



L
(
h
;

x

u


,

x

v


,

y

u
,
v


)


{\displaystyle L(h;x_{u},x_{v},y_{u,v})}

. The loss function typically reflects the number and magnitude of inversions in the induced ranking.
 In many cases, the binary classifier 



h
(

x

u


,

x

v


)


{\displaystyle h(x_{u},x_{v})}

 is implemented with a scoring function 



f
(
x
)


{\displaystyle f(x)}

. As an example, RankNet [15] adapts a probability model and defines 



h
(

x

u


,

x

v


)


{\displaystyle h(x_{u},x_{v})}

 as the estimated probability of the document 




x

u




{\displaystyle x_{u}}

 has higher quality than 




x

v




{\displaystyle x_{v}}

:
 where 




CDF

(
⋅
)


{\displaystyle {\text{CDF}}(\cdot )}

 is a cumulative distribution function, for example, the standard logistic CDF, i.e.
 These algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is often difficult in practice because most evaluation measures are not continuous functions with respect to ranking model's parameters, and so continuous approximations or bounds on evaluation measures have to be used. For example the SoftRank algorithm.[16] LambdaMART is a pairwise algorithm which has been empirically shown to approximate listwise objective functions.[17]
 A partial list of published learning-to-rank algorithms is shown below with years of first publication of each method:
 Regularized least-squares based ranking. The work is extended in
[26] to learning to rank from general preference graphs.
 Note: as most supervised learning-to-rank algorithms can be applied to pointwise, pairwise and listwise case, only those methods which are specifically designed with ranking in mind are shown above.
 Norbert Fuhr introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation;[49] a specific variant of this approach (using polynomial regression) had been published by him three years earlier.[18] Bill Cooper proposed logistic regression for the same purpose in 1992 [19] and used it with his Berkeley research group to train a successful ranking function for TREC. Manning et al.[50] suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques.
 Several conferences, such as NeurIPS, SIGIR and ICML have had workshops devoted to the learning-to-rank problem since the mid-2000s (decade).
 Commercial web search engines began using machine-learned ranking systems since the 2000s (decade). One of the first search engines to start using it was AltaVista (later its technology was acquired by Overture, and then Yahoo), which launched a gradient boosting-trained ranking function in April 2003.[51][52]
 Bing's search is said to be powered by RankNet algorithm,[53][when?] which was invented at Microsoft Research in 2005.
 In November 2009 a Russian search engine Yandex announced[54] that it had significantly increased its search quality due to deployment of a new proprietary MatrixNet algorithm, a variant of gradient boosting method which uses oblivious decision trees.[55] Recently they have also sponsored a machine-learned ranking competition ""Internet Mathematics 2009""[56] based on their own search engine's production data. Yahoo has announced a similar competition in 2010.[57]
 As of 2008, Google's Peter Norvig denied that their search engine exclusively relies on machine-learned ranking.[58] Cuil's CEO, Tom Costello, suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models ""learn what people say they like, not what people actually like"".[59]
 In January 2017, the technology was included in the open source search engine Apache Solr.[60] It is also available in the open source OpenSearch and the source-available Elasticsearch.[61][62] These implementations make learning to rank widely accessible for enterprise search.
 Similar to recognition applications in computer vision, recent neural network based ranking algorithms are also found to be susceptible to covert adversarial attacks, both on the candidates and the queries.[63] With small perturbations imperceptible to human beings, ranking order could be arbitrarily altered. In addition, model-agnostic transferable adversarial examples are found to be possible, which enables black-box adversarial attacks on deep ranking systems without requiring access to their underlying implementations.[63][64]
 Conversely, the robustness of such ranking systems can be improved via adversarial defenses such as the Madry defense.[65]
",learn rank rank mlr applic machin learn typic supervis reinforc learn construct rank model inform retriev system train data may exampl consist list item partial order specifi item list order typic induc give numer ordin score binari judgment relev relev item goal construct rank model rank new unseen list similar way rank train data rank central part mani inform retriev problem document retriev collabor filter sentiment analysi onlin advertis possibl architectur search engin shown accompani figur train data consist queri document match togeth relev degre match may prepar manual human assessor rater googl call check result queri determin relev result feasibl check relev document typic techniqu call pool use top document retriev exist rank model check techniqu may introduc select bia altern train data may deriv automat analyz clickthrough log search result got click user queri chain search engin featur googl searchwiki clickthrough log bias tendenc user click top search result assumpt alreadi train data use learn algorithm produc rank model comput relev document actual queri typic user expect search queri complet short time hundr millisecond web search make imposs evalu complex rank model document corpu scheme use first small number potenti relev document identifi use simpler retriev model permit fast queri evalu vector space model boolean model weight phase call k k document retriev mani heurist propos literatur acceler use document static qualiti score tier index second phase accur comput expens model use document learn rank algorithm appli area inform retriev conveni mlr algorithm pair usual repres numer vector call featur vector approach sometim call bag featur analog bag word model vector space model use inform retriev represent document compon vector call featur factor rank signal may divid three group featur document retriev shown exampl exampl featur use letor dataset select design good featur import area machin learn call featur engin sever measur metric commonli use judg well algorithm train data compar perform differ mlr algorithm often problem reformul optim problem respect one metric exampl rank qualiti measur dcg normal variant ndcg usual prefer academ research multipl level relev use metric map mrr precis defin binari judgment recent propos sever new evalu metric claim model user satisfact search result better dcg metric metric base assumpt user like stop look search result examin relev document less relev document liu microsoft research asia analyz exist algorithm learn rank problem book learn rank inform retriev categor three group input space output space hypothesi space core function model loss function pointwis pairwis listwis approach practic listwis approach often outperform pairwis approach pointwis approach statement support larg scale experi perform differ method larg collect benchmark data set section without notic x x denot object evalu exampl document imag f x f x denot hypothesi h h denot function l l denot loss function case assum pair train data numer ordin score problem approxim regress problem given singl pair predict score formal speak pointwis approach aim learn function f x f x predict ordin score document x x use loss function l f x j j l f j j number exist supervis machin learn algorithm readili use purpos ordin regress classif algorithm also use pointwis approach use predict score singl pair take small finit number valu case problem approxim classif problem learn binari classifi h x u x v h u v tell document better given pair document classifi shall take two document input goal minim loss function l h x u x v u v l h u v u v loss function typic reflect number magnitud invers induc rank mani case binari classifi h x u x v h u v implement score function f x f x exampl ranknet adapt probabl model defin h x u x v h u v estim probabl document x u u higher qualiti x v v cdf cdf cumul distribut function exampl standard logist cdf algorithm tri directli optim valu one evalu measur averag queri train data often difficult practic evalu measur continu function respect rank model paramet continu approxim bound evalu measur use exampl softrank algorithm lambdamart pairwis algorithm empir shown approxim listwis object function partial list publish algorithm shown year first public method regular base rank work extend learn rank gener prefer graph note supervis algorithm appli pointwis pairwis listwis case method specif design rank mind shown norbert fuhr introduc gener idea mlr describ learn approach inform retriev gener paramet estim specif variant approach use polynomi regress publish three year earlier bill cooper propos logist regress purpos use berkeley research group train success rank function trec man et al suggest earli work achiev limit result time due littl avail train data poor machin learn techniqu sever confer neurip sigir icml workshop devot problem sinc decad commerci web search engin began use rank system sinc decad one first search engin start use altavista later technolog acquir overtur yahoo launch gradient rank function april bing search said power ranknet algorithm invent microsoft research novemb russian search engin yandex announc significantli increas search qualiti due deploy new proprietari matrixnet algorithm variant gradient boost method use oblivi decis tree recent also sponsor rank competit internet mathemat base search engin product data yahoo announc similar competit googl peter norvig deni search engin exclus reli rank cuil ceo tom costello suggest prefer model outperform model measur metric like rate time land page model learn peopl say like peopl actual like januari technolog includ open sourc search engin apach solr also avail open sourc opensearch elasticsearch implement make learn rank wide access enterpris search similar recognit applic comput vision recent neural network base rank algorithm also found suscept covert adversari attack candid queri small perturb impercept human be rank order could arbitrarili alter addit transfer adversari exampl found possibl enabl adversari attack deep rank system without requir access underli implement convers robust rank system improv via adversari defens madri defens
Grammar induction,https://en.wikipedia.org/wiki/Grammar_induction,"Grammar induction (or grammatical inference)[1] is the process in machine learning of learning a formal grammar (usually as a collection of re-write rules or productions or alternatively as a finite-state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.
 Grammatical inference has often been very focused on the problem of learning finite-state machines of various types (see the article Induction of regular languages for details on these approaches), since there have been efficient algorithms for this problem since the 1980s.
 Since the beginning of the century, these approaches have been extended to the problem of inference of context-free grammars and richer formalisms, such as multiple context-free grammars and parallel multiple context-free grammars.
Other classes of grammars for which grammatical inference has been studied are combinatory categorial grammars,[2] stochastic context-free grammars,[3] contextual grammars and pattern languages.
 The simplest form of learning is where the learning algorithm merely receives a set of examples drawn from the language in question: the aim is to learn the language from examples of it (and, rarely, from counter-examples, that is, example that do not belong to the language).
However, other learning models have been studied. One frequently studied alternative is the case where the learner can ask membership queries as in the exact query learning model or minimally adequate teacher model introduced by Angluin.[4]
 There is a wide variety of methods for grammatical inference.  Two of the classic sources are Fu (1977) and Fu (1982). Duda, Hart & Stork (2001) also devote a brief section to the problem, and cite a number of references.  The basic trial-and-error method they present is discussed below. For approaches to infer subclasses of regular languages in particular, see Induction of regular languages. A more recent textbook is de la Higuera (2010),[1] which covers the theory of grammatical inference of regular languages and finite state automata. D'Ulizia, Ferri and Grifoni[5] provide a survey that explores grammatical inference methods for natural languages.
 There are several methods for induction of probabilistic context-free grammars.[6][7][further explanation needed]
 The method proposed in Section 8.7 of Duda, Hart & Stork (2001) suggests successively guessing grammar rules (productions) and testing them against positive and negative observations.  The rule set is expanded so as to be able to generate each positive example, but if a given rule set also generates a negative example, it must be discarded.  This particular approach can be characterized as ""hypothesis testing"" and bears some similarity to Mitchel's version space algorithm. The Duda, Hart & Stork (2001) text provide a simple example which nicely illustrates the process, but the feasibility of such an unguided trial-and-error approach for more substantial problems is dubious.
 Grammatical induction using evolutionary algorithms is the process of evolving a representation of the grammar of a target language through some evolutionary process. Formal grammars can easily be represented as tree structures of production rules that can be subjected to evolutionary operators. Algorithms of this sort stem from the genetic programming paradigm pioneered by John Koza.[citation needed] Other early work on simple formal languages used the binary string representation of genetic algorithms, but the inherently hierarchical structure of grammars couched in the EBNF language made trees a more flexible approach.
 Koza represented Lisp programs as trees. He was able to find analogues to the genetic operators within the standard set of tree operators. For example, swapping sub-trees is equivalent to the corresponding process of genetic crossover, where sub-strings of a genetic code are transplanted into an individual of the next generation. Fitness is measured by scoring the output from the functions of the Lisp code. Similar analogues between the tree structured lisp representation and the representation of grammars as trees, made the application of genetic programming techniques possible for grammar induction.
 In the case of grammar induction, the transplantation of sub-trees corresponds to the swapping of production rules that enable the parsing of phrases from some language. The fitness operator for the grammar is based upon some measure of how well it performed in parsing some group of sentences from the target language. In a tree representation of a grammar, a terminal symbol of a production rule corresponds to a leaf node of the tree. Its parent nodes corresponds to a non-terminal symbol (e.g. a noun phrase or a verb phrase) in the rule set. Ultimately, the root node might correspond to a sentence non-terminal.
 Like all greedy algorithms, greedy grammar inference algorithms make, in iterative manner, decisions that seem to be the best at that stage.
The decisions made usually deal with things like the creation of new rules, the removal of existing rules, the choice of a rule to be applied or the merging of some existing rules.
Because there are several ways to define 'the stage' and 'the best', there are also several greedy grammar inference algorithms.
 These context-free grammar generating algorithms make the decision after every read symbol:
 These context-free grammar generating algorithms first read the whole given symbol-sequence and then start to make decisions:
 A more recent approach is based on distributional learning. Algorithms using these approaches have been applied to learning context-free grammars and mildly context-sensitive languages and have been proven to be correct and efficient for large subclasses of these grammars.[8]
 Angluin defines a pattern to be ""a string of constant symbols from Σ and variable symbols from a disjoint set"".
The language of such a pattern is the set of all its nonempty ground instances  i.e. all strings resulting from consistent replacement of its variable symbols by nonempty strings of constant symbols.[note 1]
A pattern is called descriptive for a finite input set of strings if its language is minimal (with respect to set inclusion) among all pattern languages subsuming the input set.
 Angluin gives a polynomial algorithm to compute, for a given input string set, all descriptive patterns in one variable x.[note 2]
To this end, she builds an automaton representing all possibly relevant patterns; using sophisticated arguments about word lengths, which rely on x being the only variable, the state count can be drastically reduced.[9]
 Erlebach et al. give a more efficient version of Angluin's pattern learning algorithm, as well as a parallelized version.[10]
 Arimura et al. show that a language class  obtained from limited unions of patterns can be learned in polynomial time.[11]
 Pattern theory, formulated by Ulf Grenander,[12] is a mathematical formalism to describe knowledge of the world as patterns. It differs from other approaches to artificial intelligence in that it does not begin by prescribing algorithms and machinery to recognize and classify patterns; rather, it prescribes a vocabulary to articulate and recast the pattern concepts in precise language.
 In addition to the new algebraic vocabulary, its statistical approach was novel in its aim to:
 Broad in its mathematical coverage, pattern theory spans algebra and statistics, as well as local topological and global entropic properties.
 The principle of grammar induction has been applied to other aspects of natural language processing, and has been applied (among many other problems) to semantic parsing,[2] natural language understanding,[13] example-based translation,[14] language acquisition,[15] grammar-based compression,[16] and anomaly detection.[17]
 Grammar-based codes or Grammar-based compression are compression algorithms based on the idea of constructing a context-free grammar (CFG) for the string to be compressed. Examples include universal lossless data compression algorithms.[18] To compress a data sequence 



x
=

x

1


⋯

x

n




{\displaystyle x=x_{1}\cdots x_{n}}

, a grammar-based code transforms 



x


{\displaystyle x}

 into a context-free grammar 



G


{\displaystyle G}

.
The problem of finding a smallest grammar for an input sequence (smallest grammar problem) is known to be NP-hard,[19] so many grammar-transform algorithms are proposed from theoretical and practical viewpoints.
",grammar induct grammat infer process machin learn learn formal grammar usual collect rule product altern machin automaton kind set observ thu construct model account characterist observ object gener grammat infer branch machin learn instanc space consist discret combinatori object string tree graph grammat infer often focus problem learn machin variou type see articl induct regular languag detail approach sinc effici algorithm problem sinc sinc begin centuri approach extend problem infer grammar richer formal multipl grammar parallel multipl grammar class grammar grammat infer studi combinatori categori grammar stochast grammar contextu grammar pattern languag simplest form learn learn algorithm mere receiv set exampl drawn languag question aim learn languag exampl rare exampl belong languag howev learn model studi one frequent studi altern case learner ask membership queri exact queri learn model minim adequ teacher model introduc angluin wide varieti method grammat infer two classic sourc fu fu duda hart stork also devot brief section problem cite number refer basic method present discuss approach infer subclass regular languag particular see induct regular languag recent textbook de la higuera cover theori grammat infer regular languag finit state automata ferri grifoni provid survey explor grammat infer method natur languag sever method induct probabilist grammar explan need method propos section duda hart stork suggest success guess grammar rule product test posit neg observ rule set expand abl gener posit exampl given rule set also gener neg exampl must discard particular approach character hypothesi test bear similar mitchel version space algorithm duda hart stork text provid simpl exampl nice illustr process feasibl unguid approach substanti problem dubiou grammat induct use evolutionari algorithm process evolv represent grammar target languag evolutionari process formal grammar easili repres tree structur product rule subject evolutionari oper algorithm sort stem genet program paradigm pioneer john koza citat need earli work simpl formal languag use binari string represent genet algorithm inher hierarch structur grammar couch ebnf languag made tree flexibl approach koza repres lisp program tree abl find analogu genet oper within standard set tree oper exampl swap equival correspond process genet crossov genet code transplant individu next gener fit measur score output function lisp code similar analogu tree structur lisp represent represent grammar tree made applic genet program techniqu possibl grammar induct case grammar induct transplant correspond swap product rule enabl pars phrase languag fit oper grammar base upon measur well perform pars group sentenc target languag tree represent grammar termin symbol product rule correspond leaf node tree parent node correspond symbol noun phrase verb phrase rule set ultim root node might correspond sentenc like greedi algorithm greedi grammar infer algorithm make iter manner decis seem best stage decis made usual deal thing like creation new rule remov exist rule choic rule appli merg exist rule sever way defin stage best also sever greedi grammar infer algorithm grammar gener algorithm make decis everi read symbol grammar gener algorithm first read whole given start make decis recent approach base distribut learn algorithm use approach appli learn grammar mildli languag proven correct effici larg subclass grammar angluin defin pattern string constant symbol σ variabl symbol disjoint set languag pattern set nonempti ground instanc string result consist replac variabl symbol nonempti string constant symbol note pattern call descript finit input set string languag minim respect set inclus among pattern languag subsum input set angluin give polynomi algorithm comput given input string set descript pattern one variabl x note end build automaton repres possibl relev pattern use sophist argument word length reli x variabl state count drastic reduc erlebach et al give effici version angluin pattern learn algorithm well parallel version arimura et al show languag class obtain limit union pattern learn polynomi time pattern theori formul ulf grenand mathemat formal describ knowledg world pattern differ approach artifici intellig begin prescrib algorithm machineri recogn classifi pattern rather prescrib vocabulari articul recast pattern concept precis languag addit new algebra vocabulari statist approach novel aim broad mathemat coverag pattern theori span algebra statist well local topolog global entrop properti principl grammar induct appli aspect natur languag process appli among mani problem semant pars natur languag understand translat languag acquisit compress anomali detect code compress compress algorithm base idea construct grammar cfg string compress exampl includ univers lossless data compress algorithm compress data sequenc x x x n n code transform x x grammar g g problem find smallest grammar input sequenc smallest grammar problem known mani algorithm propos theoret practic viewpoint
Ontology learning,https://en.wikipedia.org/wiki/Ontology_learning,"Ontology learning (ontology extraction,ontology augmentation generation, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval. As building ontologies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process.
 Typically, the process starts by extracting terms and concepts or noun phrases from plain text using linguistic processors such as part-of-speech tagging and phrase chunking. Then statistical[1] 
or symbolic[2][3]
techniques are used to extract relation signatures, often based on pattern-based[4] or definition-based[5] hypernym extraction techniques.
 Ontology learning (OL) is used to (semi-)automatically extract whole ontologies from natural language text.[6][7] The process is usually split into the following eight tasks, which are not all necessarily applied in every ontology learning system.
 During the domain terminology extraction step, domain-specific terms are extracted, which are used in the following step (concept discovery) to derive concepts. Relevant terms can be determined, e.g., by calculation of the TF/IDF values or by application of the C-value / NC-value method. The resulting list of terms has to be filtered by a domain expert. In the subsequent step, similarly to coreference resolution in information extraction, the OL system determines synonyms, because they share the same meaning and therefore correspond to the same concept. The most common methods therefore are clustering and the application of statistical similarity measures.
 In the concept discovery step, terms are grouped to meaning bearing units, which correspond to an abstraction of the world and therefore to concepts. The grouped terms are these domain-specific terms and their synonyms, which were identified in the domain terminology extraction step.
 In the concept hierarchy derivation step, the OL system tries to arrange the extracted concepts in a taxonomic structure. This is mostly achieved with unsupervised hierarchical clustering methods. Because the result of such methods is often noisy, a supervision step, e.g., user evaluation, is added. A further method for the derivation of a concept hierarchy exists in the usage of several patterns that should indicate a sub- or supersumption relationship. Patterns like “X, that is a Y” or “X is a Y” indicate that X is a subclass of Y. Such pattern can be analyzed efficiently, but they often occur too infrequently to extract enough sub- or supersumption relationships. Instead, bootstrapping methods are developed, which learn these patterns automatically and therefore ensure broader coverage.
 In the learning of non-taxonomic relations step, relationships are extracted that do not express any sub- or supersumption. Such relationships are, e.g., works-for or located-in. There are two common approaches to solve this subtask. The first is based upon the extraction of anonymous associations, which are named appropriately in a second step. The second approach extracts verbs, which indicate a relationship between entities, represented by the surrounding words. The result of both approaches need to be evaluated by an ontologist to ensure accuracy.
 During rule discovery,[8] axioms (formal description of concepts) are generated for the extracted concepts. This can be achieved, e.g., by analyzing the syntactic structure of a natural language definition and the application of transformation rules on the resulting dependency tree. The result of this process is a list of axioms, which, afterwards, is comprehended to a concept description. This output is then evaluated by an ontologist.
 At this step, the ontology is augmented with instances of concepts and properties. For the augmentation with instances of concepts, methods based on the matching of lexico-syntactic patterns are used. Instances of properties are added through the application of bootstrapping methods, which collect relation tuples.
 In this step, the OL system tries to extend the taxonomic structure of an existing ontology with further concepts. This can be performed in a supervised manner with a trained classifier or in an unsupervised manner via the application of similarity measures.
 During frame/event detection, the OL system tries to extract complex relationships from text, e.g., who departed from where to what place and when. Approaches range from applying SVM with kernel methods to semantic role labeling (SRL)[9] to deep semantic parsing techniques.[10]
 Dog4Dag (Dresden Ontology Generator for Directed Acyclic Graphs) is an ontology generation plugin for Protégé 4.1 and OBOEdit 2.1. It allows for term generation, sibling generation, definition generation, and relationship induction. Integrated into Protégé 4.1 and OBO-Edit 2.1, DOG4DAG allows ontology extension for all common ontology formats (e.g., OWL and OBO). Limited largely to EBI and Bio Portal lookup service extensions.[11]
 
",ontolog learn ontolog extract ontolog augment gener ontolog gener ontolog acquisit automat creation ontolog includ extract correspond domain term relationship concept term repres corpu natur languag text encod ontolog languag easi retriev build ontolog manual extrem great motiv autom process typic process start extract term concept noun phrase plain text use linguist processor tag phrase chunk statist symbol techniqu use extract relat signatur often base hypernym extract techniqu ontolog learn ol use automat extract whole ontolog natur languag text process usual split follow eight task necessarili appli everi ontolog learn system domain terminolog extract step term extract use follow step concept discoveri deriv concept relev term determin calcul valu applic method result list term filter domain expert subsequ step similarli corefer resolut inform extract ol system determin synonym share mean therefor correspond concept common method therefor cluster applic statist similar measur concept discoveri step term group mean bear unit correspond abstract world therefor concept group term term synonym identifi domain terminolog extract step concept hierarchi deriv step ol system tri arrang extract concept taxonom structur mostli achiev unsupervis hierarch cluster method result method often noisi supervis step user evalu ad method deriv concept hierarchi exist usag sever pattern indic supersumpt relationship pattern like x x indic x subclass pattern analyz effici often occur infrequ extract enough supersumpt relationship instead bootstrap method develop learn pattern automat therefor ensur broader coverag learn relat step relationship extract express supersumpt relationship two common approach solv subtask first base upon extract anonym associ name appropri second step second approach extract verb indic relationship entiti repres surround word result approach need evalu ontologist ensur accuraci rule discoveri axiom formal descript concept gener extract concept achiev analyz syntact structur natur languag definit applic transform rule result depend tree result process list axiom afterward comprehend concept descript output evalu ontologist step ontolog augment instanc concept properti augment instanc concept method base match pattern use instanc properti ad applic bootstrap method collect relat tupl step ol system tri extend taxonom structur exist ontolog concept perform supervis manner train classifi unsupervis manner via applic similar measur detect ol system tri extract complex relationship text depart place approach rang appli svm kernel method semant role label srl deep semant pars techniqu dresden ontolog gener direct acycl graph ontolog gener plugin protégé oboedit allow term gener sibl gener definit gener relationship induct integr protégé allow ontolog extens common ontolog format owl obo limit larg ebi bio portal lookup servic extens
Multimodal learning,https://en.wikipedia.org/wiki/Multimodal_learning,"Multimodal learning is a type of deep learning that integrates and processes multiple types of data, referred to as  modalities, such as text, audio, images, or video. This integration allows for a more holistic understanding of complex data, improving model performance in tasks like visual question answering, cross-modal retrieval,[1] text-to-image generation,[2] aesthetic ranking,[3] and image captioning.[4]
 Large multimodal models, such as Google Gemini and GPT-4o, have become increasingly popular since 2023, enabling increased versatility and a broader understanding of real-world phenomena.[5]
 Data usually comes with different modalities which carry different information. For example, it is very common to caption an image to convey the information not presented in the image itself. Similarly, sometimes it is more straightforward to use an image to describe information which may not be obvious from text. As a result, if different words appear in similar images, then these words likely describe the same thing. Conversely, if a word is used to describe seemingly dissimilar images, then these images may represent the same object. Thus, in cases dealing with multi-modal data, it is important to use a model which is able to jointly represent the information such that the model can capture the combined information from different modalities.
 Transformers can also be used/adapted for modalities (input or output) beyond just text, usually by finding a way to ""tokenize"" the modality.
 Multimodal models can either be trained from scratch, or by finetuning. A 2022 study found that Transformers pretrained only on natural language can be finetuned on only 0.03% of parameters and become competitive with LSTMs on a variety of logical and visual tasks, demonstrating transfer learning.[6] The LLaVA was a vision-language model composed of a language model (Vicuna-13B)[7] and a vision model (ViT-L/14), connected by a linear layer. Only the linear layer is finetuned.[8]
 Vision transformers[9] adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like tokens in a standard transformer.
 Conformer[10] and later Whisper[11] follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e. broken down into a series of patches, turned into vectors and treated like tokens in a standard transformer.
 Perceivers[12][13] are a variant of Transformers designed for multimodality.
 Multimodality means ""having several modalities"", and a ""modality"" refers to a type of input or output, such as video, image, audio, text, proprioception, etc.[19] There have been many AI models trained specifically to ingest one modality and output another modality, such as AlexNet for image to label,[20] visual question answering for image-text to text,[21] and speech recognition for speech to text.
 A common method to create multimodal models out of an LLM is to ""tokenize"" the output of a trained encoder. Concretely, one can construct an LLM that can understand images as follows: take a trained LLM, and take a trained image encoder 



E


{\displaystyle E}

. Make a small multilayered perceptron 



f


{\displaystyle f}

, so that for any image 



y


{\displaystyle y}

, the post-processed vector 



f
(
E
(
y
)
)


{\displaystyle f(E(y))}

 has the same dimensions as an encoded token. That is an ""image token"". Then, one can interleave text tokens and image tokens. The compound model is then fine-tuned on an image-text dataset. This basic construction can be applied with more sophistication to improve the model. The image encoder may be frozen to improve stability.[22]
 Flamingo demonstrated the effectiveness of the tokenization method, finetuning a pair of pretrained language model and image encoder to perform better on visual question answering than models trained from scratch.[23] Google PaLM model was fine-tuned into a multimodal model PaLM-E using the tokenization method, and applied to robotic control.[24] LLaMA models have also been turned multimodal using the tokenization method, to allow image inputs,[25] and video inputs.[26]
 A Boltzmann machine is a type of stochastic neural network invented by Geoffrey Hinton and Terry Sejnowski in 1985. Boltzmann machines can be seen as the stochastic, generative counterpart of Hopfield nets. They are named after the Boltzmann distribution in statistical mechanics. The units in Boltzmann machines are divided into two groups: visible units and hidden units. Each unit is like a neuron with a binary output that represents whether it is activated or not.[31] General Boltzmann machines allow connection between any units. However, learning is impractical using general Boltzmann Machines because the computational time is exponential to the size of the machine[citation needed]. A more efficient architecture is called restricted Boltzmann machine where connection is only allowed between hidden unit and visible unit, which is described in the next section.
 Multimodal deep Boltzmann machines can process and learn from different types of information, such as images and text, simultaneously. This can notably be done by having a separate deep Boltzmann machine for each modality, for example one for images and one for text, joined at an additional top hidden layer.[32]
 Multimodal machine learning has numerous applications across various domains:
 Cross-modal retrieval allows users to search for data across different modalities (e.g., retrieving images based on text descriptions), improving multimedia search engines and content recommendation systems. Models like CLIP facilitate efficient, accurate retrieval by embedding data in a shared space, demonstrating strong performance even in zero-shot settings.[33]
 Multimodal Deep Boltzmann Machines outperform traditional models like support vector machines and latent Dirichlet allocation in classification tasks and can predict missing data in multimodal datasets, such as images and text.
 Multimodal models integrate medical imaging, genomic data, and patient records to improve diagnostic accuracy and early disease detection, especially in cancer screening.[34][35][36]
 Models like DALL·E generate images from textual descriptions, benefiting creative industries, while cross-modal retrieval enables dynamic multimedia searches.[37]
 Multimodal learning improves interaction in robotics and AI by integrating sensory inputs like speech, vision, and touch, aiding autonomous systems and human-computer interaction.
 Combining visual, audio, and text data, multimodal systems enhance sentiment analysis and emotion recognition, applied in customer service, social media, and marketing.
",multimod learn type deep learn integr process multipl type data refer modal text audio imag video integr allow holist understand complex data improv model perform task like visual question answer retriev gener aesthet rank imag caption larg multimod model googl gemini becom increasingli popular sinc enabl increas versatil broader understand phenomena data usual come differ modal carri differ inform exampl common caption imag convey inform present imag similarli sometim straightforward use imag describ inform may obviou text result differ word appear similar imag word like describ thing convers word use describ seemingli dissimilar imag imag may repres object thu case deal data import use model abl jointli repres inform model captur combin inform differ modal transform also modal input output beyond text usual find way token modal multimod model either train scratch finetun studi found transform pretrain natur languag finetun paramet becom competit lstm varieti logic visual task demonstr transfer learn llava model compos languag model vision model connect linear layer linear layer finetun vision transform adapt transform comput vision break input imag seri patch turn vector treat like token standard transform conform later whisper follow pattern speech recognit first turn speech signal spectrogram treat like imag broken seri patch turn vector treat like token standard transform perceiv variant transform design multimod multimod mean sever modal modal refer type input output video imag audio text propriocept etc mani ai model train specif ingest one modal output anoth modal alexnet imag label visual question answer text speech recognit speech text common method creat multimod model llm token output train encod concret one construct llm understand imag follow take train llm take train imag encod e e make small multilay perceptron f f imag vector f e f e dimens encod token imag token one interleav text token imag token compound model dataset basic construct appli sophist improv model imag encod may frozen improv stabil flamingo demonstr effect token method finetun pair pretrain languag model imag encod perform better visual question answer model train scratch googl palm model multimod model use token method appli robot control llama model also turn multimod use token method allow imag input video input boltzmann machin type stochast neural network invent geoffrey hinton terri sejnowski boltzmann machin seen stochast gener counterpart hopfield net name boltzmann distribut statist mechan unit boltzmann machin divid two group visibl unit hidden unit unit like neuron binari output repres whether activ gener boltzmann machin allow connect unit howev learn impract use gener boltzmann machin comput time exponenti size machin citat need effici architectur call restrict boltzmann machin connect allow hidden unit visibl unit describ next section multimod deep boltzmann machin process learn differ type inform imag text simultan notabl done separ deep boltzmann machin modal exampl one imag one text join addit top hidden layer multimod machin learn numer applic across variou domain retriev allow user search data across differ modal retriev imag base text descript improv multimedia search engin content recommend system model like clip facilit effici accur retriev embed data share space demonstr strong perform even set multimod deep boltzmann machin outperform tradit model like support vector machin latent dirichlet alloc classif task predict miss data multimod dataset imag text multimod model integr medic imag genom data patient record improv diagnost accuraci earli diseas detect especi cancer screen model like gener imag textual descript benefit creativ industri retriev enabl dynam multimedia search multimod learn improv interact robot ai integr sensori input like speech vision touch aid autonom system interact combin visual audio text data multimod system enhanc sentiment analysi emot recognit appli custom servic social media market
Apprenticeship learning,https://en.wikipedia.org/wiki/Apprenticeship_learning,"In artificial intelligence, apprenticeship learning (or learning from demonstration or imitation learning) is the process of learning by observing an expert.[1][2] It can be viewed as a form of supervised learning, where the training dataset consists of task executions by a demonstration teacher.[2]
 Mapping methods try to mimic the expert by forming a direct mapping either from states to actions,[2] or from states to reward values.[1] For example, in 2002 researchers used such an approach to teach an AIBO robot basic soccer skills.[2]
 Inverse reinforcement learning (IRL) is the process of deriving a reward function from observed behavior. While ordinary ""reinforcement learning"" involves using rewards and punishments to learn behavior, in IRL the direction is reversed, and a robot observes a person's behavior to figure out what goal that behavior seems to be trying to achieve.[3] The IRL problem can be defined as:[4]
 Given 1) measurements of an agent's behaviour over time, in a variety of circumstances; 2) measurements of the sensory inputs to that agent; 3) a model of the physical environment (including the agent's body): Determine the reward function that the agent is optimizing. IRL researcher Stuart J. Russell proposes that IRL might be used to observe humans and attempt to codify their complex ""ethical values"", in an effort to create ""ethical robots"" that might someday know ""not to cook your cat"" without needing to be explicitly told.[5] The scenario can be modeled as a ""cooperative inverse reinforcement learning game"", where a ""person"" player and a ""robot"" player cooperate to secure the person's implicit goals, despite these goals not being explicitly known by either the person nor the robot.[6][7]
 In 2017, OpenAI and DeepMind applied deep learning to the cooperative inverse reinforcement learning in simple domains such as Atari games and straightforward robot tasks such as backflips. The human role was limited to answering queries from the robot as to which of two different actions were preferred. The researchers found evidence that the techniques may be economically scalable to modern systems.[8][9]
 Apprenticeship via inverse reinforcement learning (AIRP) was developed by in 2004 Pieter Abbeel, Professor in Berkeley's EECS department, and Andrew Ng, Associate Professor in Stanford University's Computer Science Department. AIRP deals with ""Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform"".[1] AIRP has been used to model reward functions of highly dynamic scenarios where there is no obvious reward function intuitively. Take the task of driving for example, there are many different objectives working simultaneously - such as maintaining safe following distance, a good speed, not changing lanes too often, etc. This task, may seem easy at first glance, but a trivial reward function may not converge to the policy wanted.
 One domain where AIRP has been used extensively is helicopter control. While simple trajectories can be intuitively derived, complicated tasks like aerobatics for shows has been successful. These include aerobatic maneuvers like - in-place flips, in-place rolls, loops, hurricanes and even auto-rotation landings. This work was developed by Pieter Abbeel, Adam Coates, and Andrew Ng - ""Autonomous Helicopter Aerobatics through Apprenticeship Learning""[10]
 System models try to mimic the expert by modeling world dynamics.[2]
 The system learns rules to associate preconditions and postconditions with each action. In one 1994 demonstration, a humanoid learns a generalized plan from only two demonstrations of a repetitive ball
collection task.[2]
 Learning from demonstration is often explained from a perspective that the working Robot-control-system is available and the human-demonstrator is using it. And indeed, if the software works, the Human operator takes the robot-arm, makes a move with it, and the robot will reproduce the action later. For example, he teaches the robot-arm how to put a cup under a coffeemaker and press the start-button. In the replay phase, the robot is imitating this behavior 1:1. But that is not how the system works internally; it is only what the audience can observe. In reality, Learning from demonstration is much more complex. One of the first works on learning by robot apprentices (anthropomorphic robots learning by imitation) was Adrian Stoica's PhD thesis in 1995.[11]
 In 1997, robotics expert Stefan Schaal was working on the Sarcos robot-arm. The goal was simple: solve the pendulum swingup task. The robot itself can execute a movement, and as a result, the pendulum is moving. The problem is, that it is unclear what actions will result into which movement. It is an Optimal control-problem which can be described with mathematical formulas but is hard to solve. The idea from Schaal was, not to use a Brute-force solver but record the movements of a human-demonstration. The angle of the pendulum is logged over three seconds at the y-axis. This results into a diagram which produces a pattern.[12]
 In computer animation, the principle is called spline animation.[13] That means, on the x-axis the time is given, for example 0.5 seconds, 1.0 seconds, 1.5 seconds, while on the y-axis is the variable given. In most cases it's the position of an object. In the inverted pendulum it is the angle.
 The overall task consists of two parts: recording the angle over time and reproducing the recorded motion. The reproducing step is surprisingly simple. As an input we know, in which time step which angle the pendulum must have. Bringing the system to a state is called “Tracking control” or PID control. That means, we have a trajectory over time, and must find control actions to map the system to this trajectory. Other authors call the principle “steering behavior”,[14] because the aim is to bring a robot to a given line.
 
",artifici intellig apprenticeship learn learn demonstr imit learn process learn observ expert view form supervis learn train dataset consist task execut demonstr teacher map method tri mimic expert form direct map either state action state reward valu exampl research use approach teach aibo robot basic soccer skill invers reinforc learn irl process deriv reward function observ behavior ordinari reinforc learn involv use reward punish learn behavior irl direct revers robot observ person behavior figur goal behavior seem tri achiev irl problem defin given measur agent behaviour time varieti circumst measur sensori input agent model physic environ includ agent bodi determin reward function agent optim irl research stuart russel propos irl might use observ human attempt codifi complex ethic valu effort creat ethic robot might someday know cook cat without need explicitli told scenario model cooper invers reinforc learn game person player robot player cooper secur person implicit goal despit goal explicitli known either person robot openai deepmind appli deep learn cooper invers reinforc learn simpl domain atari game straightforward robot task backflip human role limit answer queri robot two differ action prefer research found evid techniqu may econom scalabl modern system apprenticeship via invers reinforc learn airp develop pieter abbeel professor berkeley eec depart andrew ng associ professor stanford univers comput scienc depart airp deal markov decis process explicitli given reward function instead observ expert demonstr task want learn perform airp use model reward function highli dynam scenario obviou reward function intuit take task drive exampl mani differ object work simultan maintain safe follow distanc good speed chang lane often etc task may seem easi first glanc trivial reward function may converg polici want one domain airp use extens helicopt control simpl trajectori intuit deriv complic task like aerobat show success includ aerobat maneuv like flip roll loop hurrican even land work develop pieter abbeel adam coat andrew ng autonom helicopt aerobat apprenticeship learn system model tri mimic expert model world dynam system learn rule associ precondit postcondit action one demonstr humanoid learn gener plan two demonstr repetit ball collect task learn demonstr often explain perspect work avail use inde softwar work human oper take make move robot reproduc action later exampl teach put cup coffeemak press replay phase robot imit behavior system work intern audienc observ realiti learn demonstr much complex one first work learn robot apprentic anthropomorph robot learn imit adrian stoica phd thesi robot expert stefan schaal work sarco goal simpl solv pendulum swingup task robot execut movement result pendulum move problem unclear action result movement optim describ mathemat formula hard solv idea schaal use solver record movement angl pendulum log three second result diagram produc pattern comput anim principl call spline anim mean time given exampl second second second variabl given case posit object invert pendulum angl overal task consist two part record angl time reproduc record motion reproduc step surprisingli simpl input know time step angl pendulum must bring system state call track control pid control mean trajectori time must find control action map system trajectori author call principl steer behavior aim bring robot given line
Decision tree learning,https://en.wikipedia.org/wiki/Decision_tree_learning,"Decision tree learning is a supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations.
 Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. More generally, the concept of regression tree can be extended to any kind of object equipped with pairwise dissimilarities such as categorical sequences.[1]
 Decision trees are among the most popular machine learning algorithms given their intelligibility and simplicity.[2]
 In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data (but the resulting classification tree can be an input for decision making).
 Decision tree learning is a method commonly used in data mining.[3] The goal is to create a model that predicts the value of a target variable based on several input variables.
 A decision tree is a simple representation for classifying examples. For this section, assume that all of the input features have finite discrete domains, and there is a single target feature called the ""classification"". Each element of the domain of the classification is called a class.
A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature. The arcs coming from a node labeled with an input feature are labeled with each of the possible values of the target feature or the arc leads to a subordinate decision node on a different input feature. Each leaf of the tree is labeled with a class or a probability distribution over the classes, signifying that the data set has been classified by the tree into either a specific class, or into a particular probability distribution (which, if the decision tree is well-constructed, is skewed towards certain subsets of classes).
 A tree is built by splitting the source set, constituting the root node of the tree, into subsets—which constitute the successor children. The splitting is based on a set of splitting rules based on classification features.[4]  This process is repeated on each derived subset in a recursive manner called recursive partitioning.
The recursion is completed when the subset at a node has all the same values of the target variable, or when splitting no longer adds value to the predictions. This process of top-down induction of decision trees (TDIDT)[5] is an example of a greedy algorithm, and it is by far the most common strategy for learning decision trees from data.[6]
 In data mining, decision trees can be described also as the combination of mathematical and computational techniques to aid the description, categorization and generalization of a given set of data.
 Data comes in records of the form:
 The dependent variable, 



Y


{\displaystyle Y}

, is the target variable that we are trying to understand, classify or generalize. The vector 





x




{\displaystyle {\textbf {x}}}

 is composed of the features, 




x

1


,

x

2


,

x

3




{\displaystyle x_{1},x_{2},x_{3}}

 etc., that are used for that task.
 Decision trees used in data mining are of two main types:
 The term classification and regression tree (CART) analysis is an umbrella term used to refer to either of the above procedures, first introduced by Breiman et al. in 1984.[7] Trees used for regression and trees used for classification have some similarities – but also some differences, such as the procedure used to determine where to split.[7]
 Some techniques, often called ensemble methods, construct more than one decision tree:
 A special case of a decision tree is a decision list,[14] which is a one-sided decision tree, so that every internal node has exactly 1 leaf node and exactly 1 internal node as a child (except for the bottommost node, whose only child is a single leaf node).  While less expressive, decision lists are arguably easier to understand than general decision trees due to their added sparsity[citation needed], permit non-greedy learning methods[15] and monotonic constraints to be imposed.[16]
 Notable decision tree algorithms include:
 ID3 and CART were invented independently at around the same time (between 1970 and 1980)[citation needed], yet follow a similar approach for learning a decision tree from training tuples.
 It has also been proposed to leverage concepts of fuzzy set theory for the definition of a special version of decision tree, known as Fuzzy Decision Tree (FDT).[23]
In this type of fuzzy classification, generally, an input vector 





x




{\displaystyle {\textbf {x}}}

 is associated with multiple classes, each with a different confidence value.
Boosted ensembles of FDTs have been recently investigated as well, and they have shown performances comparable to those of other very efficient fuzzy classifiers.[24]
 Algorithms for constructing decision trees usually work top-down, by choosing a variable at each step that best splits the set of items.[6] Different algorithms use different metrics for measuring ""best"".  These generally measure the homogeneity of the target variable within the subsets. Some examples are given below. These metrics are applied to each candidate subset, and the resulting values are combined (e.g., averaged) to provide a measure of the quality of the split. Depending on the underlying metric, the performance of various heuristic algorithms for decision tree learning may vary significantly.[25]
 A simple and effective metric can be used to identify the degree to which true positives outweigh false positives (see Confusion matrix). This metric, ""Estimate of Positive Correctness"" is defined below:
 




E

P


=
T
P
−
F
P


{\displaystyle E_{P}=TP-FP}


 In this equation, the total false positives (FP) are subtracted from the total true positives (TP). The resulting number gives an estimate on how many positive examples the feature could correctly identify within the data, with higher numbers meaning that the feature could correctly classify more positive samples. Below is an example of how to use the metric when the full confusion matrix of a certain feature is given:
 Feature A Confusion Matrix
 Here we can see that the TP value would be 8 and the FP value would be 2 (the underlined numbers in the table). When we plug these numbers in the equation we are able to calculate the estimate: 




E

p


=
T
P
−
F
P
=
8
−
2
=
6


{\displaystyle E_{p}=TP-FP=8-2=6}

. This means that using the estimate on this feature would have it receive a score of 6.
 However, it should be worth noting that this number is only an estimate. For example, if two features both had a FP value of 2 while one of the features had a higher TP value, that feature would be ranked higher than the other because the resulting estimate when using the equation would give a higher value. This could lead to some inaccuracies when using the metric if some features have more positive samples than others. To combat this, one could use a more powerful metric known as Sensitivity that takes into account the proportions of the values from the confusion matrix to give the actual true positive rate (TPR). The difference between these metrics is shown in the example below:
 



T
P
R
=
T
P

/

(
T
P
+
F
N
)
=
8

/

(
8
+
3
)
≈
0.73


{\displaystyle TPR=TP/(TP+FN)=8/(8+3)\approx 0.73}


 



T
P
R
=
T
P

/

(
T
P
+
F
N
)
=
6

/

(
6
+
2
)
=
0.75


{\displaystyle TPR=TP/(TP+FN)=6/(6+2)=0.75}


 In this example, Feature A had an estimate of 6 and a TPR of approximately 0.73 while Feature B had an estimate of 4 and a TPR of 0.75. This shows that although the positive estimate for some feature may be higher, the more accurate TPR value for that feature may be lower when compared to other features that have a lower positive estimate. Depending on the situation and knowledge of the data and decision trees, one may opt to use the positive estimate for a quick and easy solution to their problem. On the other hand, a more experienced user would most likely prefer to use the TPR value to rank the features because it takes into account the proportions of the data and all the samples that should have been classified as positive.
 Gini impurity, Gini's diversity index,[26] or Gini-Simpson Index in biodiversity research, is named after Italian mathematician Corrado Gini and used by the CART (classification and regression tree) algorithm for classification trees. Gini impurity measures how often a randomly chosen element of a set would be incorrectly labeled if it were labeled randomly and independently according to the distribution of labels in the set. It reaches its minimum (zero) when all cases in the node fall into a single target category.
 For a set of items with 



J


{\displaystyle J}

 classes and relative frequencies 




p

i




{\displaystyle p_{i}}

, 



i
∈
{
1
,
2
,
.
.
.
,
J
}


{\displaystyle i\in \{1,2,...,J\}}

, the probability of choosing an item with label 



i


{\displaystyle i}

 is 




p

i




{\displaystyle p_{i}}

, and the probability of miscategorizing that item is 




∑

k
≠
i



p

k


=
1
−

p

i




{\displaystyle \sum _{k\neq i}p_{k}=1-p_{i}}

. The Gini impurity is computed by summing pairwise products of these probabilities for each class label:
 The Gini impurity is also an information theoretic measure and corresponds to Tsallis Entropy with deformation coefficient 



q
=
2


{\displaystyle q=2}

, which in physics is associated with the lack of information in out-of-equilibrium, non-extensive, dissipative and quantum systems. For the limit 



q
→
1


{\displaystyle q\to 1}

 one recovers the usual Boltzmann-Gibbs or Shannon entropy. In this sense, the Gini impurity is nothing but a variation of the usual entropy measure for decision trees.
 Used by the ID3, C4.5 and C5.0 tree-generation algorithms. Information gain is based on the concept of entropy and information content from information theory.
 Entropy is defined as below
 where 




p

1


,

p

2


,
…


{\displaystyle p_{1},p_{2},\ldots }

 are fractions that add up to 1 and represent the percentage of each class present in the child node that results from a split in the tree.[27]
 Averaging over the possible values of 



A


{\displaystyle A}

,
 That is, the expected information gain is the mutual information, meaning that on average, the reduction in the entropy of T is the mutual information.
 Information gain is used to decide which feature to split on at each step in building the tree. Simplicity is best, so we want to keep our tree small. To do so, at each step we should choose the split that results in the most consistent child nodes. A commonly used measure of consistency is called information which is measured in bits. For each node of the tree, the information value ""represents the expected amount of information that would be needed to specify whether a new instance should be classified yes or no, given that the example reached that node"".[27]
 Consider an example data set with four attributes: outlook (sunny, overcast, rainy), temperature (hot, mild, cool), humidity (high, normal), and windy (true, false), with a binary (yes or no) target variable, play, and 14 data points. To construct a decision tree on this data, we need to compare the information gain of each of four trees, each split on one of the four features. The split with the highest information gain will be taken as the first split and the process will continue until all children nodes each have consistent data, or until the information gain is 0.
 To find the information gain of the split using windy, we must first calculate the information in the data before the split. The original data contained nine yes's and five no's.
 The split using the feature windy results in two children nodes, one for a windy value of true and one for a windy value of false. In this data set, there are six data points with a true windy value, three of which have a play (where play is the target variable) value of yes and three with a play value of no. The eight remaining data points with a windy value of false contain two no's and six yes's. The information of the windy=true node is calculated using the entropy equation above. Since there is an equal number of yes's and no's in this node, we have
 For the node where windy=false there were eight data points, six yes's and two no's. Thus we have
 To find the information of the split, we take the weighted average of these two numbers based on how many observations fell into which node.
 Now we can calculate the information gain achieved by splitting on the windy feature.
 To build the tree, the information gain of each possible first split would need to be calculated. The best first split is the one that provides the most information gain. This process is repeated for each impure node until the tree is complete. This example is adapted from the example appearing in Witten et al.[27]
 Information gain is also known as Shannon index in bio diversity research.
 Introduced in CART,[7] variance reduction is often employed in cases where the target variable is continuous (regression tree), meaning that use of many other metrics would first require discretization before being applied. The variance reduction of a node N is defined as the total reduction of the variance of the target variable Y due to the split at this node:
 where 



S


{\displaystyle S}

, 




S

t




{\displaystyle S_{t}}

, and 




S

f




{\displaystyle S_{f}}

 are the set of presplit sample indices, set of sample indices for which the split test is true, and set of sample indices for which the split test is false, respectively. Each of the above summands are indeed variance estimates, though, written in a form without directly referring to the mean.
 By replacing 



(

y

i


−

y

j



)

2




{\displaystyle (y_{i}-y_{j})^{2}}

 in the formula above with the dissimilarity 




d

i
j




{\displaystyle d_{ij}}

 between two objects 



i


{\displaystyle i}

 and 



j


{\displaystyle j}

, the variance reduction criterion applies to any kind of object for which pairwise dissimilarities can be computed.[1]
 Used by CART in 1984,[28] the measure of ""goodness"" is a function that seeks to optimize the balance of a candidate split's capacity to create pure children with its capacity to create equally-sized children. This process is repeated for each impure node until the tree is complete. The function 



φ
(
s
∣
t
)


{\displaystyle \varphi (s\mid t)}

, where 



s


{\displaystyle s}

 is a candidate split at node 



t


{\displaystyle t}

, is defined as below
 where 




t

L




{\displaystyle t_{L}}

 and 




t

R




{\displaystyle t_{R}}

 are the left and right children of node 



t


{\displaystyle t}

 using split 



s


{\displaystyle s}

, respectively; 




P

L




{\displaystyle P_{L}}

 and 




P

R




{\displaystyle P_{R}}

 are the proportions of records in 



t


{\displaystyle t}

 in 




t

L




{\displaystyle t_{L}}

 and 




t

R




{\displaystyle t_{R}}

, respectively; and 



P
(
j
∣

t

L


)


{\displaystyle P(j\mid t_{L})}

 and 



P
(
j
∣

t

R


)


{\displaystyle P(j\mid t_{R})}

 are the proportions of class 



j


{\displaystyle j}

 records in 




t

L




{\displaystyle t_{L}}

 and 




t

R




{\displaystyle t_{R}}

, respectively.
 Consider an example data set with three attributes: savings(low, medium, high), assets(low, medium, high), income(numerical value), and a binary target variable credit risk(good, bad) and 8 data points.[28] The full data is presented in the table below. To start a decision tree, we will calculate the maximum value of 



φ
(
s
∣
t
)


{\displaystyle \varphi (s\mid t)}

 using each feature to find which one will split the root node. This process will continue until all children are pure or all 



φ
(
s
∣
t
)


{\displaystyle \varphi (s\mid t)}

 values are below a set threshold.
 To find 



φ
(
s
∣
t
)


{\displaystyle \varphi (s\mid t)}

 of the feature savings, we need to note the quantity of each value. The original data contained three low's, three medium's, and two high's. Out of the low's, one had a good credit risk while out of the medium's and high's, 4 had a good credit risk. Assume a candidate split 



s


{\displaystyle s}

 such that records with a low savings will be put in the left child and all other records will be put into the right child.
 To build the tree, the ""goodness"" of all candidate splits for the root node need to be calculated. The candidate with the maximum value will split the root node, and the process will continue for each impure node until the tree is complete.
 Compared to other metrics such as information gain, the measure of ""goodness"" will attempt to create a more balanced tree, leading to more-consistent decision time. However, it sacrifices some priority for creating pure children which can lead to additional splits that are not present with other metrics.
 Amongst other data mining methods, decision trees have various advantages:
 Many data mining software packages provide implementations of one or more decision tree algorithms (e.g. random forest).
 Open source examples include:
 Notable commercial software:
 In a decision tree, all paths from the root node to the leaf node proceed by way of conjunction, or AND. In a decision graph, it is possible to use disjunctions (ORs) to join two more paths together using minimum message length (MML).[43]  Decision graphs have been further extended to allow for previously unstated new attributes to be learnt dynamically and used at different places within the graph.[44]  The more general coding scheme results in better predictive accuracy and log-loss probabilistic scoring.[citation needed]  In general, decision graphs infer models with fewer leaves than decision trees.
 Evolutionary algorithms have been used to avoid local optimal decisions and search the decision tree space with little a priori bias.[45][46]
 It is also possible for a tree to be sampled using MCMC.[47]
 The tree can be searched for in a bottom-up fashion.[48] Or several trees can be constructed parallelly to reduce the expected number of tests till classification.[38]
",decis tree learn supervis learn approach use statist data mine machin learn formal classif regress decis tree use predict model draw conclus set observ tree model target variabl take discret set valu call classif tree tree structur leav repres class label branch repres conjunct featur lead class label decis tree target variabl take continu valu typic real number call regress tree gener concept regress tree extend kind object equip pairwis dissimilar categor sequenc decis tree among popular machin learn algorithm given intellig simplic decis analysi decis tree use visual explicitli repres decis decis make data mine decis tree describ data result classif tree input decis make decis tree learn method commonli use data mine goal creat model predict valu target variabl base sever input variabl decis tree simpl represent classifi exampl section assum input featur finit discret domain singl target featur call classif element domain classif call class decis tree classif tree tree intern node label input featur arc come node label input featur label possibl valu target featur arc lead subordin decis node differ input featur leaf tree label class probabl distribut class signifi data set classifi tree either specif class particular probabl distribut decis tree skew toward certain subset class tree built split sourc set constitut root node tree constitut successor children split base set split rule base classif featur process repeat deriv subset recurs manner call recurs partit recurs complet subset node valu target variabl split longer add valu predict process induct decis tree tdidt exampl greedi algorithm far common strategi learn decis tree data data mine decis tree describ also combin mathemat comput techniqu aid descript categor gener given set data data come record form depend variabl target variabl tri understand classifi gener vector x x compos featur x x x use task decis tree use data mine two main type term classif regress tree cart analysi umbrella term use refer either procedur first introduc breiman et al tree use regress tree use classif similar also differ procedur use determin split techniqu often call ensembl method construct one decis tree special case decis tree decis list decis tree everi intern node exactli leaf node exactli intern node child except bottommost node whose child singl leaf node less express decis list arguabl easier understand gener decis tree due ad sparsiti citat need permit learn method monoton constraint impos notabl decis tree algorithm includ cart invent independ around time citat need yet follow similar approach learn decis tree train tupl also propos leverag concept fuzzi set theori definit special version decis tree known fuzzi decis tree fdt type fuzzi classif gener input vector x x associ multipl class differ confid valu boost ensembl fdt recent investig well shown perform compar effici fuzzi classifi algorithm construct decis tree usual work choos variabl step best split set item differ algorithm use differ metric measur best gener measur homogen target variabl within subset exampl given metric appli candid subset result valu combin averag provid measur qualiti split depend underli metric perform variou heurist algorithm decis tree learn may vari significantli simpl effect metric use identifi degre true posit outweigh fals posit see confus matrix metric estim posit correct defin e p p f p p equat total fals posit fp subtract total true posit tp result number give estim mani posit exampl featur could correctli identifi within data higher number mean featur could correctli classifi posit sampl exampl use metric full confus matrix certain featur given featur confus matrix see tp valu would fp valu would underlin number tabl plug number equat abl calcul estim e p p f p p mean use estim featur would receiv score howev worth note number estim exampl two featur fp valu one featur higher tp valu featur would rank higher result estim use equat would give higher valu could lead inaccuraci use metric featur posit sampl other combat one could use power metric known sensit take account proport valu confus matrix give actual true posit rate tpr differ metric shown exampl p r p p f n p r p p f n exampl featur estim tpr approxim featur b estim tpr show although posit estim featur may higher accur tpr valu featur may lower compar featur lower posit estim depend situat knowledg data decis tree one may opt use posit estim quick easi solut problem hand experienc user would like prefer use tpr valu rank featur take account proport data sampl classifi posit gini impur gini divers index index biodivers research name italian mathematician corrado gini use cart classif regress tree algorithm classif tree gini impur measur often randomli chosen element set would incorrectli label label randomli independ accord distribut label set reach minimum zero case node fall singl target categori set item j j class rel frequenc p j probabl choos item label p probabl miscategor item k p k p k gini impur comput sum pairwis product probabl class label gini impur also inform theoret measur correspond tsalli entropi deform coeffici q physic associ lack inform dissip quantum system limit q one recov usual shannon entropi sens gini impur noth variat usual entropi measur decis tree use algorithm inform gain base concept entropi inform content inform theori entropi defin p p fraction add repres percentag class present child node result split tree averag possibl valu expect inform gain mutual inform mean averag reduct entropi mutual inform inform gain use decid featur split step build tree simplic best want keep tree small step choos split result consist child node commonli use measur consist call inform measur bit node tree inform valu repres expect amount inform would need specifi whether new instanc classifi ye given exampl reach node consid exampl data set four attribut outlook sunni overcast raini temperatur hot mild cool humid high normal windi true fals binari ye target variabl play data point construct decis tree data need compar inform gain four tree split one four featur split highest inform gain taken first split process continu children node consist data inform gain find inform gain split use windi must first calcul inform data split origin data contain nine ye five split use featur windi result two children node one windi valu true one windi valu fals data set six data point true windi valu three play play target variabl valu ye three play valu eight remain data point windi valu fals contain two six ye inform node calcul use entropi equat sinc equal number ye node node eight data point six ye two thu find inform split take weight averag two number base mani observ fell node calcul inform gain achiev split windi featur build tree inform gain possibl first split would need calcul best first split one provid inform gain process repeat impur node tree complet exampl adapt exampl appear witten et al inform gain also known shannon index bio divers research introduc cart varianc reduct often employ case target variabl continu regress tree mean use mani metric would first requir discret appli varianc reduct node n defin total reduct varianc target variabl due split node f f set presplit sampl indic set sampl indic split test true set sampl indic split test fals respect summand inde varianc estim though written form without directli refer mean replac j j formula dissimilar j ij two object j j varianc reduct criterion appli kind object pairwis dissimilar comput use cart measur good function seek optim balanc candid split capac creat pure children capac creat children process repeat impur node tree complet function φ candid split node defin l l r r left right children node use split respect p l l p r r proport record l l r r respect p j l p l p j r p r proport class j j record l l r r respect consid exampl data set three attribut save low medium high asset low medium high incom numer valu binari target variabl credit risk good bad data point full data present tabl start decis tree calcul maximum valu φ use featur find one split root node process continu children pure φ valu set threshold find φ featur save need note quantiti valu origin data contain three low three medium two high low one good credit risk medium high good credit risk assum candid split record low save put left child record put right child build tree good candid split root node need calcul candid maximum valu split root node process continu impur node tree complet compar metric inform gain measur good attempt creat balanc tree lead decis time howev sacrific prioriti creat pure children lead addit split present metric amongst data mine method decis tree variou advantag mani data mine softwar packag provid implement one decis tree algorithm random forest open sourc exampl includ notabl commerci softwar decis tree path root node leaf node proceed way conjunct decis graph possibl use disjunct or join two path togeth use minimum messag length mml decis graph extend allow previous unstat new attribut learnt dynam use differ place within graph gener code scheme result better predict accuraci probabilist score citat need gener decis graph infer model fewer leav decis tree evolutionari algorithm use avoid local optim decis search decis tree space littl priori bia also possibl tree sampl use mcmc tree search fashion sever tree construct parallelli reduc expect number test till classif
Ensemble learning,https://en.wikipedia.org/wiki/Ensemble_learning,"In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.[1][2][3]
Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.
 Supervised learning algorithms search through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem.[4] Even if this space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. Ensembles combine multiple hypotheses to form one which should be theoretically better.
 Ensemble learning trains two or more machine learning algorithms on a specific classification or regression task. The algorithms within the ensemble model are generally referred as ""base models"", ""base learners"", or ""weak learners"" in literature. These base models can be constructed using a single modelling algorithm, or several different algorithms. The idea is to train a diverse set of weak models on the same modelling task, such that the outputs of each weak learner have poor predictive ability (i.e., high bias), and among all weak learners, the outcome and error values exhibit high variance. Fundamentally, an ensemble learning model trains at least two high-bias (weak) and high-variance (diverse) models to be combined into a better-performing model. The set of weak models — which would not produce satisfactory predictive results individually — are combined or averaged to produce a single, high performing, accurate, and low-variance model to fit the task as required.
 Ensemble learning typically refers to bagging (bootstrap aggregating), boosting or stacking/blending techniques to induce high variance among the base models. Bagging creates diversity by generating random samples from the training observations and fitting the same model to each different sample — also known as homogeneous parallel ensembles. Boosting follows an iterative process by sequentially training each base model on the up-weighted errors of the previous base model, producing an additive model to reduce the final model errors — also known as sequential ensemble learning. Stacking or blending consists of different base models, each trained independently (i.e. diverse/high variance) to be combined into the ensemble model — producing a heterogeneous parallel ensemble. Common applications of ensemble learning include random forests (an extension of bagging), Boosted Tree models, and Gradient Boosted Tree Models. Models in applications of stacking are generally more task-specific — such as combining clustering techniques with other parametric and/or non-parametric techniques.[5]
 The broader term of multiple classifier systems also covers hybridization of hypotheses that are not induced by the same base learner.[citation needed]
 Evaluating the prediction of an ensemble typically requires more computation than evaluating the prediction of a single model. In one sense, ensemble learning may be thought of as a way to compensate for poor learning algorithms by performing a lot of extra computation. On the other hand, the alternative is to do a lot more learning with one non-ensemble model. An ensemble may be more efficient at improving overall accuracy for the same increase in compute, storage, or communication resources by using that increase on two or more methods, than would have been improved by increasing resource use for a single method. Fast algorithms such as decision trees are commonly used in ensemble methods (e.g., random forests), although slower algorithms can benefit from ensemble techniques as well.
 By analogy, ensemble techniques have been used also in unsupervised learning scenarios, for example in consensus clustering or in anomaly detection.
 Empirically, ensembles tend to yield better results when there is a significant diversity among the models.[6][7] Many ensemble methods, therefore, seek to promote diversity among the models they combine.[8][9] Although perhaps non-intuitive, more random algorithms (like random decision trees) can be used to produce a stronger ensemble than very deliberate algorithms (like entropy-reducing decision trees).[10] Using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to dumb-down the models in order to promote diversity.[11] It is possible to increase diversity in the training stage of the model using correlation for regression tasks [12] or using information measures such as cross entropy for classification tasks.[13]
 Theoretically, one can justify the diversity concept because the lower bound of the error rate of an ensemble system can be decomposed into accuracy, diversity, and the other term.[14]
 Ensemble learning, including both regression and classification tasks, can be explained using a geometric framework.[15] Within this framework, the output of each individual classifier or regressor for the entire dataset can be viewed as a point in a multi-dimensional space. Additionally, the target result is also represented as a point in this space, referred to as the ""ideal point.""
 The Euclidean distance is used as the metric to measure both the performance of a single classifier or regressor (the distance between its point and the ideal point) and the dissimilarity between two classifiers or regressors (the distance between their respective points). This perspective transforms ensemble learning into a deterministic problem.
 For example, within this geometric framework, it can be proved that the averaging of the outputs (scores) of all base classifiers or regressors can lead to equal or better results than the average of all the individual models. It can also be proved that if the optimal weighting scheme is used, then a weighted averaging approach can outperform any of the individual classifiers or regressors that make up the ensemble or as good as the best performer at least.
 While the number of component classifiers of an ensemble has a great impact on the accuracy of prediction, there is a limited number of studies addressing this problem. A priori determining of ensemble size and the volume and velocity of big data streams make this even more crucial for online ensemble classifiers. Mostly statistical tests were used for determining the proper number of components. More recently, a theoretical framework suggested that there is an ideal number of component classifiers for an ensemble such that having more or less than this number of classifiers would deteriorate the accuracy. It is called ""the law of diminishing returns in ensemble construction."" Their theoretical framework shows that using the same number of independent component classifiers as class labels gives the highest accuracy.[16][17]
 The Bayes optimal classifier is a classification technique. It is an ensemble of all the hypotheses in the hypothesis space. On average, no other ensemble can outperform it.[18] The Naive Bayes classifier is a version of this that assumes that the data is conditionally independent on the class and makes the computation more feasible. Each hypothesis is given a vote proportional to the likelihood that the training dataset would be sampled from a system if that hypothesis were true. To facilitate training data of finite size, the vote of each hypothesis is also multiplied by the prior probability of that hypothesis. The Bayes optimal classifier can be expressed with the following equation:
 where 



y


{\displaystyle y}

 is the predicted class, 



C


{\displaystyle C}

 is the set of all possible classes, 



H


{\displaystyle H}

 is the hypothesis space, 



P


{\displaystyle P}

 refers to a probability, and 



T


{\displaystyle T}

 is the training data. As an ensemble, the Bayes optimal classifier represents a hypothesis that is not necessarily in 



H


{\displaystyle H}

. The hypothesis represented by the Bayes optimal classifier, however, is the optimal hypothesis in ensemble space (the space of all possible ensembles consisting only of hypotheses in 



H


{\displaystyle H}

).
 This formula can be restated using Bayes' theorem, which says that the posterior is proportional to the likelihood times the prior:
 hence,
 Bootstrap aggregation (bagging) involves training an ensemble on bootstrapped data sets. A bootstrapped set is created by selecting from original training data set with replacement. Thus, a bootstrap set may contain a given example zero, one, or multiple times. Ensemble members can also have limits on the features (e.g., nodes of a decision tree), to encourage exploring of diverse features.[19] The variance of local information in the bootstrap sets and feature considerations promote diversity in the ensemble, and can strengthen the ensemble.[20] To reduce overfitting, a member can be validated using the out-of-bag set (the examples that are not in its bootstrap set).[21]
 Inference is done by voting of predictions of ensemble members, called aggregation. It is illustrated below with an ensemble of four decision trees. The query example is classified by each tree. Because three of the four predict the positive class, the ensemble's overall classification is positive. Random forests like the one shown are a common application of bagging.
 Boosting involves training successive models by emphasizing training data mis-classified by previously learned models. Initially, all data (D1) has equal weight and is used to learn a base model M1. The examples mis-classified by M1 are assigned a weight greater than correctly classified examples. This boosted data (D2) is used to train a second base model M2, and so on. Inference is done by voting.
 In some cases, boosting has yielded better accuracy than bagging, but tends to over-fit more. The most common implementation of boosting is Adaboost, but some newer algorithms are reported to achieve better results.[citation needed]
 Bayesian model averaging (BMA) makes predictions by averaging the predictions of models weighted by their posterior probabilities given the data.[22] BMA is known to generally give better answers than a single model, obtained, e.g., via stepwise regression, especially where very different models have nearly identical performance in the training set but may otherwise perform quite differently.
 The question with any use of Bayes' theorem is the prior, i.e., the probability (perhaps subjective) that each model is the best to use for a given purpose.  Conceptually, BMA can be used with any prior.  R packages ensembleBMA[23] and BMA[24] use the prior implied by the Bayesian information criterion, (BIC), following Raftery (1995).[25] R package BAS supports the use of the priors implied by Akaike information criterion (AIC) and other criteria over the alternative models as well as priors over the coefficients.[26]
 The difference between BIC and AIC is the strength of preference for parsimony.  BIC's penalty for model complexity is 



ln
⁡
(
n
)
k


{\displaystyle \ln(n)k}

 , while AIC's is 



2
k


{\displaystyle 2k}

. Large-sample asymptotic theory establishes that if there is a best model, then with increasing sample sizes, BIC is strongly consistent, i.e., will almost certainly find it, while AIC may not, because AIC may continue to place excessive posterior probability on models that are more complicated than they need to be. On the other hand, AIC and AICc are asymptotically ""efficient"" (i.e., minimum mean square prediction error), while BIC is not .[27]
 Haussler et al. (1994) showed that when BMA is used for classification, its expected error is at most twice the expected error of the Bayes optimal classifier.[28] Burnham and Anderson (1998, 2002) contributed greatly to introducing a wider audience to the basic ideas of Bayesian model averaging and popularizing the methodology.[29] The availability of software, including other free open-source packages for R beyond those mentioned above, helped make the methods accessible to a wider audience.[30]
 Bayesian model combination (BMC) is an algorithmic correction to Bayesian model averaging (BMA). Instead of sampling each model in the ensemble individually, it samples from the space of possible ensembles (with model weights drawn randomly from a Dirichlet distribution having uniform parameters). This modification overcomes the tendency of BMA to converge toward giving all the weight to a single model. Although BMC is somewhat more computationally expensive than BMA, it tends to yield dramatically better results. BMC has been shown to be better on average (with statistical significance) than BMA and bagging.[31]
 Use of Bayes' law to compute model weights requires computing the probability of the data given each model. Typically, none of the models in the ensemble are exactly the distribution from which the training data were generated, so all of them correctly receive a value close to zero for this term. This would work well if the ensemble were big enough to sample the entire model-space, but this is rarely possible. Consequently, each pattern in the training data will cause the ensemble weight to shift toward the model in the ensemble that is closest to the distribution of the training data. It essentially reduces to an unnecessarily complex method for doing model selection.
 The possible weightings for an ensemble can be visualized as lying on a simplex. At each vertex of the simplex, all of the weight is given to a single model in the ensemble. BMA converges toward the vertex that is closest to the distribution of the training data. By contrast, BMC converges toward the point where this distribution projects onto the simplex. In other words, instead of selecting the one model that is closest to the generating distribution, it seeks the combination of models that is closest to the generating distribution.
 The results from BMA can often be approximated by using cross-validation to select the best model from a bucket of models. Likewise, the results from BMC may be approximated by using cross-validation to select the best ensemble combination from a random sampling of possible weightings.
 A ""bucket of models"" is an ensemble technique in which a model selection algorithm is used to choose the best model for each problem. When tested with only one problem, a bucket of models can produce no better results than the best model in the set, but when evaluated across many problems, it will typically produce much better results, on average, than any model in the set.
 The most common approach used for model-selection is cross-validation selection (sometimes called a ""bake-off contest""). It is described with the following pseudo-code:
 Cross-Validation Selection can be summed up as: ""try them all with the training set, and pick the one that works best"".[32]
 Gating is a generalization of Cross-Validation Selection. It involves training another learning model to decide which of the models in the bucket is best-suited to solve the problem. Often, a perceptron is used for the gating model. It can be used to pick the ""best"" model, or it can be used to give a linear weight to the predictions from each model in the bucket.
 When a bucket of models is used with a large set of problems, it may be desirable to avoid training some of the models that take a long time to train. Landmark learning is a meta-learning approach that seeks to solve this problem. It involves training only the fast (but imprecise) algorithms in the bucket, and then using the performance of these algorithms to help determine which slow (but accurate) algorithm is most likely to do best.[33]
 The most common approach for training classifier is using Cross-entropy cost function. However, one would like to train an ensemble of models that have diversity so when we combine them it would provide best results.[34][35]
Assuming we use a simple ensemble of averaging 



K


{\displaystyle K}

 classifiers. Then the Amended Cross-Entropy Cost is
 where 




e

k




{\displaystyle e^{k}}

 is the cost function of the 




k

t
h




{\displaystyle k^{th}}

 classifier, 




q

k




{\displaystyle q^{k}}

 is the probability of the 




k

t
h




{\displaystyle k^{th}}

 classifier, 



p


{\displaystyle p}

 is the true probability that we need to estimate and 



λ


{\displaystyle \lambda }

 is a parameter between 0 and 1 that define the diversity that we would like to establish. When 



λ
=
0


{\displaystyle \lambda =0}

 we want each classifier to do its best regardless of the ensemble and when 



λ
=
1


{\displaystyle \lambda =1}

 we would like the classifier to be as diverse as possible.
 Stacking (sometimes called stacked generalization) involves training a model to combine the predictions of several other learning algorithms. First, all of the other algorithms are trained using the available data, then a combiner algorithm (final estimator) is trained to make a final prediction using all the predictions of the other algorithms (base estimators) as additional inputs or using cross-validated predictions from the base estimators which can prevent overfitting.[36] If an arbitrary combiner algorithm is used, then stacking can theoretically represent any of the ensemble techniques described in this article, although, in practice, a logistic regression model is often used as the combiner.
 Stacking typically yields performance better than any single one of the trained models.[37] It has been successfully used on both supervised learning tasks (regression,[38] classification and distance learning [39]) and unsupervised learning (density estimation).[40] It has also been used to estimate bagging's error rate.[3][41] It has been reported to out-perform Bayesian model-averaging.[42] The two top-performers in the Netflix competition utilized blending, which may be considered a form of stacking.[43]
 Voting is another form of ensembling. See e.g. Weighted majority algorithm (machine learning).
 In recent years, due to growing computational power, which allows for training in large ensemble learning in a reasonable time frame, the number of ensemble learning applications has grown increasingly.[49] Some of the applications of ensemble classifiers include:
 Land cover mapping is one of the major applications of Earth observation satellite sensors, using remote sensing and geospatial data, to identify the materials and objects which are located on the surface of target areas. Generally, the classes of target materials include roads, buildings, rivers, lakes, and vegetation.[50] Some different ensemble learning approaches based on artificial neural networks,[51] kernel principal component analysis (KPCA),[52] decision trees with boosting,[53] random forest[50][54] and automatic design of multiple classifier systems,[55] are proposed to efficiently identify land cover objects.
 Change detection is an image analysis problem, consisting of the identification of places where the land cover has changed over time. Change detection is widely used in fields such as urban growth, forest and vegetation dynamics, land use and disaster monitoring.[56]
The earliest applications of ensemble classifiers in change detection are designed with the majority voting,[57] Bayesian model averaging,[58] and the maximum posterior probability.[59] Given the growth of satellite data over time, the past decade sees more use of time series methods for continuous change detection from image stacks.[60] One example is a Bayesian ensemble changepoint detection method called BEAST, with the software available as a package Rbeast in R, Python, and Matlab.[61]
 Distributed denial of service is one of the most threatening cyber-attacks that may happen to an internet service provider.[49] By combining the output of single classifiers, ensemble classifiers reduce the total error of detecting and discriminating such attacks from legitimate flash crowds.[62]
 Classification of malware codes such as computer viruses, computer worms, trojans, ransomware and spywares with the usage of machine learning techniques, is inspired by the document categorization problem.[63] Ensemble learning systems have shown a proper efficacy in this area.[64][65]
 An intrusion detection system monitors computer network or computer systems to identify intruder codes like an anomaly detection process. Ensemble learning successfully aids such monitoring systems to reduce their total error.[66][67]
 Face recognition, which recently has become one of the most popular research areas of pattern recognition, copes with identification or verification of a person by their digital images.[68]
 Hierarchical ensembles based on Gabor Fisher classifier and independent component analysis preprocessing techniques are some of the earliest ensembles employed in this field.[69][70][71]
 While speech recognition is mainly based on deep learning because most of the industry players in this field like Google, Microsoft and IBM reveal that the core technology of their speech recognition is based on this approach, speech-based emotion recognition can also have a satisfactory performance with ensemble learning.[72][73]
 It is also being successfully used in facial emotion recognition.[74][75][76]
 Fraud detection deals with the identification of bank fraud, such as money laundering, credit card fraud and telecommunication fraud, which have vast domains of research and applications of machine learning. Because ensemble learning improves the robustness of the normal behavior modelling, it has been proposed as an efficient technique to detect such fraudulent cases and activities in banking and credit card systems.[77][78]
 The accuracy of prediction of business failure is a very crucial issue in financial decision-making. Therefore, different ensemble classifiers are proposed to predict financial crises and financial distress.[79] Also, in the trade-based manipulation problem, where traders attempt to manipulate stock prices by buying and selling activities, ensemble classifiers are required to analyze the changes in the stock market data and detect suspicious symptom of stock price manipulation.[79]
 Ensemble classifiers have been successfully applied in neuroscience, proteomics and medical diagnosis like in neuro-cognitive disorder (i.e. Alzheimer or myotonic dystrophy) detection based on MRI datasets,[80][81][82] and cervical cytology classification.[83][84]
",statist machin learn ensembl method use multipl learn algorithm obtain better predict perform could obtain constitu learn algorithm alon unlik statist ensembl statist mechan usual infinit machin learn ensembl consist concret finit set altern model typic allow much flexibl structur exist among altern supervis learn algorithm search hypothesi space find suitabl hypothesi make good predict particular problem even space contain hypothes particular problem may difficult find good one ensembl combin multipl hypothes form one theoret better ensembl learn train two machin learn algorithm specif classif regress task algorithm within ensembl model gener refer base model base learner weak learner literatur base model construct use singl model algorithm sever differ algorithm idea train divers set weak model model task output weak learner poor predict abil high bia among weak learner outcom error valu exhibit high varianc fundament ensembl learn model train least two weak divers model combin model set weak model would produc satisfactori predict result individu combin averag produc singl high perform accur model fit task requir ensembl learn typic refer bag bootstrap aggreg boost techniqu induc high varianc among base model bag creat divers gener random sampl train observ fit model differ sampl also known homogen parallel ensembl boost follow iter process sequenti train base model error previou base model produc addit model reduc final model error also known sequenti ensembl learn stack blend consist differ base model train independ varianc combin ensembl model produc heterogen parallel ensembl common applic ensembl learn includ random forest extens bag boost tree model gradient boost tree model model applic stack gener combin cluster techniqu parametr techniqu broader term multipl classifi system also cover hybrid hypothes induc base learner citat need evalu predict ensembl typic requir comput evalu predict singl model one sens ensembl learn may thought way compens poor learn algorithm perform lot extra comput hand altern lot learn one model ensembl may effici improv overal accuraci increas comput storag commun resourc use increas two method would improv increas resourc use singl method fast algorithm decis tree commonli use ensembl method random forest although slower algorithm benefit ensembl techniqu well analog ensembl techniqu use also unsupervis learn scenario exampl consensu cluster anomali detect empir ensembl tend yield better result signific divers among model mani ensembl method therefor seek promot divers among model combin although perhap random algorithm like random decis tree use produc stronger ensembl deliber algorithm like decis tree use varieti strong learn algorithm howev shown effect use techniqu attempt model order promot divers possibl increas divers train stage model use correl regress task use inform measur cross entropi classif task theoret one justifi divers concept lower bound error rate ensembl system decompos accuraci divers term ensembl learn includ regress classif task explain use geometr framework within framework output individu classifi regressor entir dataset view point space addit target result also repres point space refer ideal point euclidean distanc use metric measur perform singl classifi regressor distanc point ideal point dissimilar two classifi regressor distanc respect point perspect transform ensembl learn determinist problem exampl within geometr framework prove averag output score base classifi regressor lead equal better result averag individu model also prove optim weight scheme use weight averag approach outperform individu classifi regressor make ensembl good best perform least number compon classifi ensembl great impact accuraci predict limit number studi address problem priori determin ensembl size volum veloc big data stream make even crucial onlin ensembl classifi mostli statist test use determin proper number compon recent theoret framework suggest ideal number compon classifi ensembl less number classifi would deterior accuraci call law diminish return ensembl construct theoret framework show use number independ compon classifi class label give highest accuraci bay optim classifi classif techniqu ensembl hypothes hypothesi space averag ensembl outperform naiv bay classifi version assum data condit independ class make comput feasibl hypothesi given vote proport likelihood train dataset would sampl system hypothesi true facilit train data finit size vote hypothesi also multipli prior probabl hypothesi bay optim classifi express follow equat predict class c c set possibl class h h hypothesi space p p refer probabl train data ensembl bay optim classifi repres hypothesi necessarili h h hypothesi repres bay optim classifi howev optim hypothesi ensembl space space possibl ensembl consist hypothes h h formula restat use bay theorem say posterior proport likelihood time prior henc bootstrap aggreg bag involv train ensembl bootstrap data set bootstrap set creat select origin train data set replac thu bootstrap set may contain given exampl zero one multipl time ensembl member also limit featur node decis tree encourag explor divers featur varianc local inform bootstrap set featur consider promot divers ensembl strengthen ensembl reduc overfit member valid use set exampl bootstrap set infer done vote predict ensembl member call aggreg illustr ensembl four decis tree queri exampl classifi tree three four predict posit class ensembl overal classif posit random forest like one shown common applic bag boost involv train success model emphas train data previous learn model initi data equal weight use learn base model exampl assign weight greater correctli classifi exampl boost data use train second base model infer done vote case boost yield better accuraci bag tend common implement boost adaboost newer algorithm report achiev better result citat need bayesian model averag bma make predict averag predict model weight posterior probabl given data bma known gener give better answer singl model obtain via stepwis regress especi differ model nearli ident perform train set may otherwis perform quit differ question use bay theorem prior probabl perhap subject model best use given purpos conceptu bma use prior r packag ensemblebma bma use prior impli bayesian inform criterion bic follow rafteri r packag ba support use prior impli akaik inform criterion aic criteria altern model well prior coeffici differ bic aic strength prefer parsimoni bic penalti model complex ln n k n k aic k asymptot theori establish best model increas sampl size bic strongli consist almost certainli find aic may aic may continu place excess posterior probabl model complic need hand aic aicc asymptot effici minimum mean squar predict error bic haussler et al show bma use classif expect error twice expect error bay optim classifi burnham anderson contribut greatli introduc wider audienc basic idea bayesian model averag popular methodolog avail softwar includ free packag r beyond mention help make method access wider audienc bayesian model combin bmc algorithm correct bayesian model averag bma instead sampl model ensembl individu sampl space possibl ensembl model weight drawn randomli dirichlet distribut uniform paramet modif overcom tendenc bma converg toward give weight singl model although bmc somewhat comput expens bma tend yield dramat better result bmc shown better averag statist signific bma bag use bay law comput model weight requir comput probabl data given model typic none model ensembl exactli distribut train data gener correctli receiv valu close zero term would work well ensembl big enough sampl entir rare possibl consequ pattern train data caus ensembl weight shift toward model ensembl closest distribut train data essenti reduc unnecessarili complex method model select possibl weight ensembl visual lie simplex vertex simplex weight given singl model ensembl bma converg toward vertex closest distribut train data contrast bmc converg toward point distribut project onto simplex word instead select one model closest gener distribut seek combin model closest gener distribut result bma often approxim use select best model bucket model likewis result bmc may approxim use select best ensembl combin random sampl possibl weight bucket model ensembl techniqu model select algorithm use choos best model problem test one problem bucket model produc better result best model set evalu across mani problem typic produc much better result averag model set common approach use select sometim call contest describ follow select sum tri train set pick one work best gate gener select involv train anoth learn model decid model bucket solv problem often perceptron use gate model use pick best model use give linear weight predict model bucket bucket model use larg set problem may desir avoid train model take long time train landmark learn approach seek solv problem involv train fast imprecis algorithm bucket use perform algorithm help determin slow accur algorithm like best common approach train classifi use cost function howev one would like train ensembl model divers combin would provid best result assum use simpl ensembl averag k k classifi amend cost e k k cost function k h th classifi q k k probabl k h th classifi p p true probabl need estim λ paramet defin divers would like establish λ want classifi best regardless ensembl λ would like classifi divers possibl stack sometim call stack gener involv train model combin predict sever learn algorithm first algorithm train use avail data combin algorithm final estim train make final predict use predict algorithm base estim addit input use predict base estim prevent overfit arbitrari combin algorithm use stack theoret repres ensembl techniqu describ articl although practic logist regress model often use combin stack typic yield perform better singl one train model success use supervis learn task regress classif distanc learn unsupervis learn densiti estim also use estim bag error rate report bayesian two netflix competit util blend may consid form stack vote anoth form ensembl see weight major algorithm machin learn recent year due grow comput power allow train larg ensembl learn reason time frame number ensembl learn applic grown increasingli applic ensembl classifi includ land cover map one major applic earth observ satellit sensor use remot sens geospati data identifi materi object locat surfac target area gener class target materi includ road build river lake veget differ ensembl learn approach base artifici neural network kernel princip compon analysi kpca decis tree boost random forest automat design multipl classifi system propos effici identifi land cover object chang detect imag analysi problem consist identif place land cover chang time chang detect wide use field urban growth forest veget dynam land use disast monitor earliest applic ensembl classifi chang detect design major vote bayesian model averag maximum posterior probabl given growth satellit data time past decad see use time seri method continu chang detect imag stack one exampl bayesian ensembl changepoint detect method call beast softwar avail packag rbeast r python matlab distribut denial servic one threaten may happen internet servic provid combin output singl classifi ensembl classifi reduc total error detect discrimin attack legitim flash crowd classif malwar code comput virus comput worm trojan ransomwar spywar usag machin learn techniqu inspir document categor problem ensembl learn system shown proper efficaci area intrus detect system monitor comput network comput system identifi intrud code like anomali detect process ensembl learn success aid monitor system reduc total error face recognit recent becom one popular research area pattern recognit cope identif verif person digit imag hierarch ensembl base gabor fisher classifi independ compon analysi preprocess techniqu earliest ensembl employ field speech recognit mainli base deep learn industri player field like googl microsoft ibm reveal core technolog speech recognit base approach emot recognit also satisfactori perform ensembl learn also success use facial emot recognit fraud detect deal identif bank fraud money launder credit card fraud telecommun fraud vast domain research applic machin learn ensembl learn improv robust normal behavior model propos effici techniqu detect fraudul case activ bank credit card system accuraci predict busi failur crucial issu financi therefor differ ensembl classifi propos predict financi crise financi distress also manipul problem trader attempt manipul stock price buy sell activ ensembl classifi requir analyz chang stock market data detect suspici symptom stock price manipul ensembl classifi success appli neurosci proteom medic diagnosi like disord alzheim myoton dystrophi detect base mri dataset cervic cytolog classif
Bootstrap aggregating,https://en.wikipedia.org/wiki/Bootstrap_aggregating,"Bootstrap aggregating, also called bagging (from bootstrap aggregating) or bootstrapping, is a machine learning (ML) ensemble meta-algorithm designed to improve the stability and accuracy of ML classification and regression algorithms. It also reduces variance and overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the ensemble averaging approach.
 Given a standard training set 



D


{\displaystyle D}

 of size 



n


{\displaystyle n}

, bagging generates 



m


{\displaystyle m}

 new training sets 




D

i




{\displaystyle D_{i}}

, each of size 




n
′



{\displaystyle n'}

, by sampling from 



D


{\displaystyle D}

 uniformly and with replacement. By sampling with replacement, some observations may be repeated in each 




D

i




{\displaystyle D_{i}}

. If 




n
′

=
n


{\displaystyle n'=n}

, then for large 



n


{\displaystyle n}

 the set 




D

i




{\displaystyle D_{i}}

 is expected to have the fraction (1 - 1/e) (~63.2%) of the unique samples of 



D


{\displaystyle D}

, the rest being duplicates.[1] This kind of sample is known as a bootstrap sample. Sampling with replacement ensures each bootstrap is independent from its peers, as it does not depend on previous chosen samples when sampling. Then, 



m


{\displaystyle m}

 models are fitted using the above bootstrap samples and combined by averaging the output (for regression) or voting (for classification).
 Bagging leads to ""improvements for unstable procedures"",[2] which include, for example, artificial neural networks, classification and regression trees, and subset selection in linear regression.[3] Bagging was shown to improve preimage learning.[4][5] On the other hand, it can mildly degrade the performance of stable methods such as k-nearest neighbors.[2]
 There are three types of datasets in bootstrap aggregating. These are the original, bootstrap, and out-of-bag datasets. Each section below will explain how each dataset is made except for the original dataset. The original dataset is whatever information is given.
 The bootstrap dataset is made by randomly picking objects from the original dataset. Also, it must be the same size as the original dataset. However, the difference is that the bootstrap dataset can have duplicate objects. Here is a simple example to demonstrate how it works along with the illustration below:
 
 Suppose the original dataset is a group of 12 people. Their names are Emily, Jessie, George, Constantine, Lexi, Theodore, John, James, Rachel, Anthony, Ellie, and Jamal.
 By randomly picking a group of names, let us say our bootstrap dataset had James, Ellie, Constantine, Lexi, John, Constantine, Theodore, Constantine, Anthony, Lexi, Constantine, and Theodore. In this case, the bootstrap sample contained four duplicates for Constantine, and two duplicates for Lexi, and Theodore.
 The out-of-bag dataset represents the remaining people who were not in the bootstrap dataset. It can be calculated by taking the difference between the original and the bootstrap datasets. In this case, the remaining samples who were not selected are Emily, Jessie, George, Rachel, and Jamal. Keep in mind that since both datasets are sets, when taking the difference the duplicate names are ignored in the bootstrap dataset. The illustration below shows how the math is done:
 
 Creating the bootstrap and out-of-bag datasets is crucial since it is used to test the accuracy of a random forest algorithm. For example, a model that produces 50 trees using the bootstrap/out-of-bag datasets will have a better accuracy than if it produced 10 trees. Since the algorithm generates multiple trees and therefore multiple datasets the chance that an object is left out of the bootstrap dataset is low. The next few sections talk about how the random forest algorithm works in more detail.
 The next step of the algorithm involves the generation of decision trees from the bootstrapped dataset. To achieve this, the process examines each gene/feature and determines for how many samples the feature's presence or absence yields a positive or negative result. This information is then used to compute a confusion matrix, which lists the true positives, false positives, true negatives, and false negatives of the feature when used as a classifier. These features are then ranked according to various classification metrics based on their confusion matrices. Some common metrics include estimate of positive correctness (calculated by subtracting false positives from true positives), measure of ""goodness"", and information gain. These features are then used to partition the samples into two sets: those who possess the top feature, and those who do not.
 The diagram below shows a decision tree of depth two being used to classify data. For example, a data point that exhibits Feature 1, but not Feature 2, will be given a ""No"". Another point that does not exhibit Feature 1, but does exhibit Feature 3, will be given a ""Yes"".
 
 This process is repeated recursively for successive levels of the tree until the desired depth is reached. At the very bottom of the tree, samples that test positive for the final feature are generally classified as positive, while those that lack the feature are classified as negative. These trees are then used as predictors to classify new data.
 The next part of the algorithm involves introducing yet another element of variability amongst the bootstrapped trees. In addition to each tree only examining a bootstrapped set of samples, only a small but consistent number of unique features are considered when ranking them as classifiers. This means that each tree only knows about the data pertaining to a small constant number of features, and a variable number of samples that is less than or equal to that of the original dataset. Consequently, the trees are more likely to return a wider array of answers, derived from more diverse knowledge. This results in a random forest, which possesses numerous benefits over a single decision tree generated without randomness. In a random forest, each tree ""votes"" on whether or not to classify a sample as positive based on its features. The sample is then classified based on majority vote. An example of this is given in the diagram below, where the four trees in a random forest vote on whether or not a patient with mutations A, B, F, and G has cancer. Since three out of four trees vote yes, the patient is then classified as cancer positive.
 Because of their properties, random forests are considered one of the most accurate data mining algorithms, are less likely to overfit their data, and run quickly and efficiently even for large datasets.[6] They are primarily useful for classification as opposed to regression, which attempts to draw observed connections between statistical variables in a dataset. This makes random forests particularly useful in such fields as banking, healthcare, the stock market, and e-commerce where it is important to be able to predict future results based on past data.[7] One of their applications would be as a useful tool for predicting cancer based on genetic factors, as seen in the above example.
 There are several important factors to consider when designing a random forest. If the trees in the random forests are too deep, overfitting can still occur due to over-specificity. If the forest is too large, the algorithm may become less efficient due to an increased runtime. Random forests also do not generally perform well when given sparse data with little variability.[7] However, they still have numerous advantages over similar data classification algorithms such as neural networks, as they are much easier to interpret and generally require less data for training.[citation needed] As an integral component of random forests, bootstrap aggregating is very important to classification algorithms, and provides a critical element of variability that allows for increased accuracy when analyzing new data, as discussed below.
 While the techniques described above utilize random forests and bagging (otherwise known as bootstrapping), there are certain techniques that can be used in order to improve their execution and voting time, their prediction accuracy, and their overall performance. The following are key steps in creating an efficient random forest:
 For classification, use a training set 



D


{\displaystyle D}

, Inducer 



I


{\displaystyle I}

 and the number of bootstrap samples 



m


{\displaystyle m}

 as input. Generate a classifier 




C

∗




{\displaystyle C^{*}}

 as output[12]
 To illustrate the basic principles of bagging, below is an analysis on the relationship between ozone and temperature (data from Rousseeuw and Leroy[clarification needed] (1986), analysis done in R).
 The relationship between temperature and ozone appears to be nonlinear in this dataset, based on the scatter plot. To mathematically describe this relationship, LOESS smoothers (with bandwidth 0.5) are used. Rather than building a single smoother for the complete dataset, 100 bootstrap samples were drawn. Each sample is composed of a random subset of the original data and maintains a semblance of the master set's distribution and variability. For each bootstrap sample, a LOESS smoother was fit. Predictions from these 100 smoothers were then made across the range of the data. The black lines represent these initial predictions. The lines lack agreement in their predictions and tend to overfit their data points: evident by the wobbly flow of the lines.
 By taking the average of 100 smoothers, each corresponding to a subset of the original dataset, we arrive at one bagged predictor (red line). The red line's flow is stable and does not overly conform to any data point(s).
 Advantages:
 Disadvantages:
 The concept of bootstrap aggregating is derived from the concept of bootstrapping which was developed by Bradley Efron.[15]
Bootstrap aggregating was proposed by Leo Breiman who also coined the abbreviated term ""bagging"" (bootstrap aggregating). Breiman developed the concept of bagging in 1994 to improve classification by combining classifications of randomly generated training sets. He argued, ""If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy"".[3]
",bootstrap aggreg also call bag bootstrap aggreg bootstrap machin learn ml ensembl design improv stabil accuraci ml classif regress algorithm also reduc varianc overfit although usual appli decis tree method use type method bag special case ensembl averag approach given standard train set size n n bag gener new train set size n n sampl uniformli replac sampl replac observ may repeat n n larg n n set expect fraction uniqu sampl rest duplic kind sampl known bootstrap sampl sampl replac ensur bootstrap independ peer depend previou chosen sampl sampl model fit use bootstrap sampl combin averag output regress vote classif bag lead improv unstabl procedur includ exampl artifici neural network classif regress tree subset select linear regress bag shown improv preimag learn hand mildli degrad perform stabl method neighbor three type dataset bootstrap aggreg origin bootstrap dataset section explain dataset made except origin dataset origin dataset whatev inform given bootstrap dataset made randomli pick object origin dataset also must size origin dataset howev differ bootstrap dataset duplic object simpl exampl demonstr work along illustr suppos origin dataset group peopl name emili jessi georg constantin lexi theodor john jame rachel anthoni elli jamal randomli pick group name let us say bootstrap dataset jame elli constantin lexi john constantin theodor constantin anthoni lexi constantin theodor case bootstrap sampl contain four duplic constantin two duplic lexi theodor dataset repres remain peopl bootstrap dataset calcul take differ origin bootstrap dataset case remain sampl select emili jessi georg rachel jamal keep mind sinc dataset set take differ duplic name ignor bootstrap dataset illustr show math done creat bootstrap dataset crucial sinc use test accuraci random forest algorithm exampl model produc tree use dataset better accuraci produc tree sinc algorithm gener multipl tree therefor multipl dataset chanc object left bootstrap dataset low next section talk random forest algorithm work detail next step algorithm involv gener decis tree bootstrap dataset achiev process examin determin mani sampl featur presenc absenc yield posit neg result inform use comput confus matrix list true posit fals posit true neg fals neg featur use classifi featur rank accord variou classif metric base confus matric common metric includ estim posit correct calcul subtract fals posit true posit measur good inform gain featur use partit sampl two set possess top featur diagram show decis tree depth two use classifi data exampl data point exhibit featur featur given anoth point exhibit featur exhibit featur given ye process repeat recurs success level tree desir depth reach bottom tree sampl test posit final featur gener classifi posit lack featur classifi neg tree use predictor classifi new data next part algorithm involv introduc yet anoth element variabl amongst bootstrap tree addit tree examin bootstrap set sampl small consist number uniqu featur consid rank classifi mean tree know data pertain small constant number featur variabl number sampl less equal origin dataset consequ tree like return wider array answer deriv divers knowledg result random forest possess numer benefit singl decis tree gener without random random forest tree vote whether classifi sampl posit base featur sampl classifi base major vote exampl given diagram four tree random forest vote whether patient mutat b f g cancer sinc three four tree vote ye patient classifi cancer posit properti random forest consid one accur data mine algorithm less like overfit data run quickli effici even larg dataset primarili use classif oppos regress attempt draw observ connect statist variabl dataset make random forest particularli use field bank healthcar stock market import abl predict futur result base past data one applic would use tool predict cancer base genet factor seen exampl sever import factor consid design random forest tree random forest deep overfit still occur due forest larg algorithm may becom less effici due increas runtim random forest also gener perform well given spars data littl variabl howev still numer advantag similar data classif algorithm neural network much easier interpret gener requir less data train citat need integr compon random forest bootstrap aggreg import classif algorithm provid critic element variabl allow increas accuraci analyz new data discuss techniqu describ util random forest bag otherwis known bootstrap certain techniqu use order improv execut vote time predict accuraci overal perform follow key step creat effici random forest classif use train set induc number bootstrap sampl input gener classifi c output illustr basic principl bag analysi relationship ozon temperatur data rousseeuw leroy clarif need analysi done r relationship temperatur ozon appear nonlinear dataset base scatter plot mathemat describ relationship loess smoother bandwidth use rather build singl smoother complet dataset bootstrap sampl drawn sampl compos random subset origin data maintain semblanc master set distribut variabl bootstrap sampl loess smoother fit predict smoother made across rang data black line repres initi predict line lack agreement predict tend overfit data point evid wobbl flow line take averag smoother correspond subset origin dataset arriv one bag predictor red line red line flow stabl overli conform data point advantag disadvantag concept bootstrap aggreg deriv concept bootstrap develop bradley efron bootstrap aggreg propos leo breiman also coin abbrevi term bag bootstrap aggreg breiman develop concept bag improv classif combin classif randomli gener train set argu perturb learn set caus signific chang predictor construct bag improv accuraci
Boosting (machine learning),https://en.wikipedia.org/wiki/Boosting_(machine_learning),"In machine learning (ML), boosting is an ensemble metaheuristic for primarily reducing bias (as opposed to variance).[1] It can also improve the stability and accuracy of ML classification and regression algorithms. Hence, it is prevalent in supervised learning for converting weak learners to strong learners.[2]
 The concept of boosting is based on the question posed by Kearns and Valiant (1988, 1989):[3][4] ""Can a set of weak learners create a single strong learner?"" A weak learner is defined as a classifier that is only slightly correlated with the true classification. A strong learner is a classifier that is arbitrarily well-correlated with the true classification. Robert Schapire answered the question in the affirmative in a paper published in 1990.[5] This has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.[6]
 Initially, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner.[3] Algorithms that achieve this quickly became known as ""boosting"". Freund and Schapire's arcing (Adapt[at]ive Resampling and Combining),[7] as a general technique, is more or less synonymous with boosting.[8]
 While boosting is not algorithmically constrained, most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are weighted in a way that is related to the weak learners' accuracy.  After a weak learner is added, the data weights are readjusted, known as ""re-weighting"". Misclassified input data gain a higher weight and examples that are classified correctly lose weight.[note 1] Thus, future weak learners focus more on the examples that previous weak learners misclassified.
 There are many boosting algorithms. The original ones, proposed by Robert Schapire (a recursive majority gate formulation),[5] and Yoav Freund (boost by majority),[9] were not adaptive and could not take full advantage of the weak learners. Schapire and Freund then developed AdaBoost, an adaptive boosting algorithm that won the prestigious Gödel Prize.
 Only algorithms that are provable boosting algorithms in the probably approximately correct learning formulation can accurately be called boosting algorithms.  Other algorithms that are similar in spirit[clarification needed] to boosting algorithms are sometimes called ""leveraging algorithms"", although they are also sometimes incorrectly called boosting algorithms.[9]
 The main variation between many boosting algorithms is their method of weighting training data points and hypotheses. AdaBoost is very popular and the most significant historically as it was the first algorithm that could adapt to the weak learners. It is often the basis of introductory coverage of boosting in university machine learning courses.[10] There are many more recent algorithms such as LPBoost, TotalBoost, BrownBoost, xgboost, MadaBoost, LogitBoost, and others. Many boosting algorithms fit into the AnyBoost framework,[9] which shows that boosting performs gradient descent in a function space using a convex cost function.
 Given images containing various known objects in the world, a classifier can be learned from them to automatically classify the objects in future images.  Simple classifiers built based on some image feature of the object tend to be weak in categorization performance. Using boosting methods for object categorization is a way to unify the weak classifiers in a special way to boost the overall ability of categorization.[citation needed]
 Object categorization is a typical task of computer vision that involves determining whether or not an image contains some specific category of object.  The idea is closely related with recognition, identification, and detection.  Appearance based object categorization typically contains feature extraction, learning a classifier, and applying the classifier to new examples.  There are many ways to represent a category of objects, e.g. from shape analysis, bag of words models, or local descriptors such as SIFT, etc.  Examples of supervised classifiers are Naive Bayes classifiers, support vector machines, mixtures of Gaussians, and neural networks.  However, research[which?] has shown that object categories and their locations in images can be discovered in an unsupervised manner as well.[11]
 The recognition of object categories in images is a challenging problem in computer vision, especially when the number of categories is large.  This is due to high intra class variability and the need for generalization across variations of objects within the same category. Objects within one category may look quite different. Even the same object may appear unalike under different viewpoint, scale, and illumination. Background clutter and partial occlusion add difficulties to recognition as well.[12]  Humans are able to recognize thousands of object types, whereas most of the existing object recognition systems are trained to recognize only a few,[quantify] e.g. human faces, cars, simple objects, etc.[13][needs update?]  Research has been very active on dealing with more categories and enabling incremental additions of new categories, and although the general problem remains unsolved, several multi-category objects detectors (for up to hundreds or thousands of categories[14]) have been developed.  One means is by feature sharing and boosting.
 AdaBoost can be used for face detection as an example of binary categorization. The two categories are faces versus background. The general algorithm is as follows:
 After boosting, a classifier constructed from 200 features could yield a 95% detection rate under a 




10

−
5




{\displaystyle 10^{-5}}

 false positive rate.[15]
 Another application of boosting for binary categorization is a system that detects pedestrians using patterns of motion and appearance.[16] This work is the first to combine both motion information and appearance information as features to detect a walking person. It takes a similar approach to the Viola-Jones object detection framework.
 Compared with binary categorization, multi-class categorization looks for common features that can be shared across the categories at the same time.  They turn to be more generic edge like features. During learning, the detectors for each category can be trained jointly. Compared with training separately, it generalizes better, needs less training data, and requires fewer features to achieve the same performance.
 The main flow of the algorithm is similar to the binary case. What is different is that a measure of the joint training error shall be defined in advance. During each iteration the algorithm chooses a classifier of a single feature (features that can be shared by more categories shall be encouraged). This can be done via converting multi-class classification into a binary one (a set of categories versus the rest),[17] or by introducing a penalty error from the categories that do not have the feature of the classifier.[18]
 In the paper ""Sharing visual features for multiclass and multiview object detection"", A. Torralba et al. used GentleBoost for boosting and showed that when training data is limited, learning via sharing features does a much better job than no sharing, given same boosting rounds. Also, for a given performance level, the total number of features required (and therefore the run time cost of the classifier) for the feature sharing detectors, is observed to scale approximately logarithmically with the number of class, i.e., slower than linear growth in the non-sharing case. Similar results are shown in the paper ""Incremental learning of object detectors using a visual shape alphabet"", yet the authors used AdaBoost for boosting.
 Boosting algorithms can be based on convex or non-convex optimization algorithms.  Convex algorithms, such as AdaBoost and LogitBoost, can be ""defeated"" by random  noise such that they can't learn basic and learnable combinations of weak hypotheses.[19][20] This limitation was pointed out by Long & Servedio in 2008.  However, by 2009, multiple authors demonstrated that  boosting algorithms based on non-convex optimization, such as BrownBoost, can learn from noisy datasets and can specifically learn the underlying classifier of the Long–Servedio dataset.
",machin learn ml boost ensembl metaheurist primarili reduc bia oppos varianc also improv stabil accuraci ml classif regress algorithm henc preval supervis learn convert weak learner strong learner concept boost base question pose kearn valiant set weak learner creat singl strong learner weak learner defin classifi slightli correl true classif strong learner classifi arbitrarili true classif robert schapir answer question affirm paper publish signific ramif machin learn statist notabl lead develop boost initi hypothesi boost problem simpli refer process turn weak learner strong learner algorithm achiev quickli becam known boost freund schapir arc adapt ive resampl combin gener techniqu less synonym boost boost algorithm constrain boost algorithm consist iter learn weak classifi respect distribut ad final strong classifi ad weight way relat weak learner accuraci weak learner ad data weight readjust known misclassifi input data gain higher weight exampl classifi correctli lose weight note thu futur weak learner focu exampl previou weak learner misclassifi mani boost algorithm origin one propos robert schapir recurs major gate formul yoav freund boost major adapt could take full advantag weak learner schapir freund develop adaboost adapt boost algorithm prestigi gödel prize algorithm provabl boost algorithm probabl approxim correct learn formul accur call boost algorithm algorithm similar spirit clarif need boost algorithm sometim call leverag algorithm although also sometim incorrectli call boost algorithm main variat mani boost algorithm method weight train data point hypothes adaboost popular signific histor first algorithm could adapt weak learner often basi introductori coverag boost univers machin learn cours mani recent algorithm lpboost totalboost brownboost xgboost madaboost logitboost other mani boost algorithm fit anyboost framework show boost perform gradient descent function space use convex cost function given imag contain variou known object world classifi learn automat classifi object futur imag simpl classifi built base imag featur object tend weak categor perform use boost method object categor way unifi weak classifi special way boost overal abil categor citat need object categor typic task comput vision involv determin whether imag contain specif categori object idea close relat recognit identif detect appear base object categor typic contain featur extract learn classifi appli classifi new exampl mani way repres categori object shape analysi bag word model local descriptor sift etc exampl supervis classifi naiv bay classifi support vector machin mixtur gaussian neural network howev research shown object categori locat imag discov unsupervis manner well recognit object categori imag challeng problem comput vision especi number categori larg due high intra class variabl need gener across variat object within categori object within one categori may look quit differ even object may appear unalik differ viewpoint scale illumin background clutter partial occlus add difficulti recognit well human abl recogn thousand object type wherea exist object recognit system train recogn quantifi human face car simpl object etc need updat research activ deal categori enabl increment addit new categori although gener problem remain unsolv sever object detector hundr thousand categori develop one mean featur share boost adaboost use face detect exampl binari categor two categori face versu background gener algorithm follow boost classifi construct featur could yield detect rate fals posit rate anoth applic boost binari categor system detect pedestrian use pattern motion appear work first combin motion inform appear inform featur detect walk person take similar approach object detect framework compar binari categor categor look common featur share across categori time turn gener edg like featur learn detector categori train jointli compar train separ gener better need less train data requir fewer featur achiev perform main flow algorithm similar binari case differ measur joint train error shall defin advanc iter algorithm choos classifi singl featur featur share categori shall encourag done via convert classif binari one set categori versu rest introduc penalti error categori featur classifi paper share visual featur multiclass multiview object detect torralba et al use gentleboost boost show train data limit learn via share featur much better job share given boost round also given perform level total number featur requir therefor run time cost classifi featur share detector observ scale approxim logarithm number class slower linear growth case similar result shown paper increment learn object detector use visual shape alphabet yet author use adaboost boost boost algorithm base convex optim algorithm convex algorithm adaboost logitboost defeat random nois ca learn basic learnabl combin weak hypothes limit point long servedio howev multipl author demonstr boost algorithm base optim brownboost learn noisi dataset specif learn underli classifi dataset
Random forest,https://en.wikipedia.org/wiki/Random_forest,"Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that works by creating a multitude of decision trees during training. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the output is the average of the predictions of the trees.[1][2] Random forests correct for decision trees' habit of overfitting to their training set.[3]: 587–588 
 The first algorithm for random decision forests was created in 1995 by Tin Kam Ho[1] using the random subspace method,[2] which, in Ho's formulation, is a way to implement the ""stochastic discrimination"" approach to classification proposed by Eugene Kleinberg.[4][5][6]
 An extension of the algorithm was developed by Leo Breiman[7] and Adele Cutler,[8] who registered[9] ""Random Forests"" as a trademark in 2006 (as of 2019[update], owned by Minitab, Inc.).[10] The extension combines Breiman's ""bagging"" idea and random selection of features, introduced first by Ho[1] and later independently by Amit and Geman[11] in order to construct a collection of decision trees with controlled variance.
 The general method of random decision forests was first proposed by Salzberg and Heath in 1993,[12] with a method that used a randomized decision tree algorithm to create multiple trees and then combine them using majority voting.  This idea was developed further by Ho in 1995.[1]  Ho established that forests of trees splitting with oblique hyperplanes can gain accuracy as they grow without suffering from overtraining, as long as the forests are randomly restricted to be sensitive to only selected feature dimensions.  A subsequent work along the same lines[2] concluded that other splitting methods behave similarly, as long as they are randomly forced to be insensitive to some feature dimensions.  This observation that a more complex classifier (a larger forest) gets more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level of accuracy before being hurt by overfitting.  The explanation of the forest method's resistance to overtraining can be found in Kleinberg's theory of stochastic discrimination.[4][5][6]
 The early development of Breiman's notion of random forests was influenced by the work of Amit and Geman[11] who introduced the idea of searching over a random subset of the available decisions when splitting a node, in the context of growing a single tree.  The idea of random subspace selection from Ho[2] was also influential in the design of random forests.  This method grows a forest of trees, and introduces variation among the trees by projecting the training data into a randomly chosen subspace before fitting each tree or each node.  Finally, the idea of randomized node optimization, where the decision at each node is selected by a randomized procedure, rather than a deterministic optimization was first introduced by Thomas G. Dietterich.[13]
 The proper introduction of random forests was made in a paper by Leo Breiman.[7]  This paper describes a method of building a forest of uncorrelated trees using a CART like procedure, combined with randomized node optimization and bagging.  In addition, this paper combines several ingredients, some previously known and some novel, which form the basis of the modern practice of random forests, in particular:
 The report also offers the first theoretical result for random forests in the form of a bound on the generalization error which depends on the strength of the trees in the forest and their correlation.
 Decision trees are a popular method for various machine learning tasks. Tree learning is almost ""an off-the-shelf procedure for data mining"", say Hastie et al., ""because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate"".[3]: 352 
 In particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, i.e. have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance.[3]: 587–588  This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance in the final model.
 The training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set X = x1, ..., xn with responses Y = y1, ..., yn, bagging repeatedly (B times) selects a random sample with replacement of the training set and fits trees to these samples:
 After training, predictions for unseen samples x' can be made by averaging the predictions from all the individual regression trees on x':
 






f
^



=


1
B



∑

b
=
1


B



f

b


(

x
′

)


{\displaystyle {\hat {f}}={\frac {1}{B}}\sum _{b=1}^{B}f_{b}(x')}


 or by taking the plurality vote in the case of classification trees.
 This bootstrapping procedure leads to better model performance because it decreases the variance of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. Simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets.
 Additionally, an estimate of the uncertainty of the prediction can be made as the standard deviation of the predictions from all the individual regression trees on x′:




σ
=





∑

b
=
1


B


(

f

b


(

x
′

)
−



f
^




)

2




B
−
1




.


{\displaystyle \sigma ={\sqrt {\frac {\sum _{b=1}^{B}(f_{b}(x')-{\hat {f}})^{2}}{B-1}}}.}


 The number B of samples (equivalently, of trees) is a free parameter. Typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. B can be optimized using cross-validation, or by observing the out-of-bag error: the mean prediction error on each training sample xi, using only the trees that did not have xi in their bootstrap sample.[14]
 The training and test error tend to level off after some number of trees have been fit.
 The above procedure describes the original bagging algorithm for trees. Random forests also include another type of bagging scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called ""feature bagging"". The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the B trees, causing them to become correlated. An analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by Ho.[15]
 Typically, for a classification problem with p features, √p (rounded down) features are used in each split.[3]: 592   For regression problems the inventors recommend p/3 (rounded down) with a minimum node size of 5 as the default.[3]: 592  In practice, the best values for these parameters should be tuned on a case-to-case basis for every problem.[3]: 592 
 Adding one further step of randomization yields extremely randomized trees, or ExtraTrees. As with ordinary random forests, they are an ensemble of individual trees, but there are two main differences: (1) each tree is trained using the whole learning sample (rather than a bootstrap sample), and (2) the top-down splitting is randomized: for each feature under consideration, a number of random cut-points are selected, instead of computing the locally optimal cut-point (based on, e.g., information gain or the Gini impurity). The values are chosen from a uniform distribution within the feature's empirical range (in the tree's training set). Then, of all the randomly chosen splits, the split that yields the highest score is chosen to split the node.
 Similar to ordinary random forests, the number of randomly selected features to be considered at each node can be specified. Default values for this parameter are 





p




{\displaystyle {\sqrt {p}}}

 for classification and 



p


{\displaystyle p}

 for regression, where 



p


{\displaystyle p}

 is the number of features in the model.[16]
 The basic random forest procedure may not work well in situations where there are a large number of features but only a small proportion of these features are informative with respect to sample classification. This can be addressed by encouraging the procedure to focus mainly on features and trees that are informative. Some methods for accomplishing this are:
 Random forests can be used to rank the importance of variables in a regression or classification problem in a natural way.  The following technique was described in Breiman's original paper[7] and is implemented in the R package randomForest.[8]
 To measure a feature's importance in a data set 






D



n


=
{
(

X

i


,

Y

i


)

}

i
=
1


n




{\displaystyle {\mathcal {D}}_{n}=\{(X_{i},Y_{i})\}_{i=1}^{n}}

, first a random forest is trained on the data.  During training, the out-of-bag error for each data point is recorded and averaged over the forest. (If bagging is not used during training, we can instead compute errors on an independent test set.)
 After training, the values of the feature are permuted in the out-of-bag samples and the out-of-bag error is again computed on this perturbed data set. The importance for the feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees.  The score is normalized by the standard deviation of these differences.
 Features which produce large values for this score are ranked as more important than features which produce small values. The statistical definition of the variable importance measure was given and analyzed by Zhu et al.[23]
 This method of determining variable importance has some drawbacks:
 This approach to feature importance for random forests considers as important the variables which decrease a lot the impurity during splitting.[31] It is described in the book Classification and Regression Trees by Leo Breiman[32] and is the default implementation in sci-kit learn and R. The definition is:




unormalized average importance

(
x
)
=


1

n

T





∑

i
=
1



n

T





∑


node 

j
∈

T

i



|


split variable

(
j
)
=
x



p


T

i




(
j
)
Δ

i


T

i




(
j
)
,


{\displaystyle {\text{unormalized average importance}}(x)={\frac {1}{n_{T}}}\sum _{i=1}^{n_{T}}\sum _{{\text{node }}j\in T_{i}|{\text{split variable}}(j)=x}p_{T_{i}}(j)\Delta i_{T_{i}}(j),}

where 
 As impurity measure for samples falling in a node e.g. the following statistics can be used:
 The normalized importance is then obtained by normalizing over all features, so that the sum of normalized feature importances is 1.
 The sci-kit learn default implementation can report misleading feature importance:[30]
 A relationship between random forests and the k-nearest neighbor algorithm (k-NN) was pointed out by Lin and Jeon in 2002.[34] Both can be viewed as so-called weighted neighborhoods schemes. These are models built from a training set 



{
(

x

i


,

y

i


)

}

i
=
1


n




{\displaystyle \{(x_{i},y_{i})\}_{i=1}^{n}}

 that make predictions 






y
^





{\displaystyle {\hat {y}}}

 for new points x' by looking at the ""neighborhood"" of the point, formalized by a weight function W:






y
^



=

∑

i
=
1


n


W
(

x

i


,

x
′

)


y

i


.


{\displaystyle {\hat {y}}=\sum _{i=1}^{n}W(x_{i},x')\,y_{i}.}

Here, 



W
(

x

i


,

x
′

)


{\displaystyle W(x_{i},x')}

 is the non-negative weight of the i'th training point relative to the new point x' in the same tree. For any x', the weights for points 




x

i




{\displaystyle x_{i}}

 must sum to 1. Weight functions are as follows:
 Since a forest averages the predictions of a set of m trees with individual weight functions 




W

j




{\displaystyle W_{j}}

, its predictions are






y
^



=


1
m



∑

j
=
1


m



∑

i
=
1


n



W

j


(

x

i


,

x
′

)


y

i


=

∑

i
=
1


n



(



1
m



∑

j
=
1


m



W

j


(

x

i


,

x
′

)

)



y

i


.


{\displaystyle {\hat {y}}={\frac {1}{m}}\sum _{j=1}^{m}\sum _{i=1}^{n}W_{j}(x_{i},x')\,y_{i}=\sum _{i=1}^{n}\left({\frac {1}{m}}\sum _{j=1}^{m}W_{j}(x_{i},x')\right)\,y_{i}.}


 This shows that the whole forest is again a weighted neighborhood scheme, with weights that average those of the individual trees. The neighbors of x' in this interpretation are the points 




x

i




{\displaystyle x_{i}}

 sharing the same leaf in any tree 



j


{\displaystyle j}

. In this way, the neighborhood of x' depends in a complex way on the structure of the trees, and thus on the structure of the training set. Lin and Jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature.[34]
 As part of their construction, random forest predictors naturally lead to a dissimilarity measure among observations. One can analogously define dissimilarity between unlabeled data, by training a forest to distinguish original ""observed"" data from suitably generated synthetic data drawn from a reference distribution.[7][35] A random forest dissimilarity is attractive because it handles mixed variable types very well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. Random forest dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the ""Addcl 1"" random forest dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. Random forest dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.[36]
 Instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes classifiers.[37][38][39] In cases that the relationship between the predictors and the target variable is linear, the base learners may have an equally high accuracy as the ensemble learner.[40][37]
 In machine learning, kernel random forests (KeRF) establish the connection between random forests and kernel methods. By slightly modifying their definition, random forests can be rewritten as kernel methods, which are more interpretable and easier to analyze.[41]
 Leo Breiman[42] was the first person to notice the link between random forest and kernel methods. He pointed out that random forests trained using i.i.d. random vectors in the tree construction are equivalent to a kernel acting on the true margin. Lin and Jeon[43] established the connection between random forests and adaptive nearest neighbor, implying that random forests can be seen as adaptive kernel estimates. Davies and Ghahramani[44] proposed Kernel Random Forest (KeRF) and showed that it can empirically outperform state-of-art kernel methods. Scornet[41] first defined KeRF estimates and gave the explicit link between KeRF estimates and random forest. He also gave explicit expressions for kernels based on centered random forest[45] and uniform random forest,[46] two simplified models of random forest. He named these two KeRFs Centered KeRF and Uniform KeRF, and proved upper bounds on their rates of consistency.
 Centered forest[45] is a simplified model for Breiman's original random forest, which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the pre-chosen attribute. The algorithm stops when a fully binary tree of level 



k


{\displaystyle k}

 is built, where 



k
∈

N



{\displaystyle k\in \mathbb {N} }

 is a parameter of the algorithm.
 Uniform forest[46] is another simplified model for Breiman's original random forest, which uniformly selects a feature among all features and performs splits at a point uniformly drawn on the side of the cell, along the preselected feature.
 Given a training sample  






D



n


=
{
(


X


i


,

Y

i


)

}

i
=
1


n




{\displaystyle {\mathcal {D}}_{n}=\{(\mathbf {X} _{i},Y_{i})\}_{i=1}^{n}}

 of 



[
0
,
1

]

p


×

R



{\displaystyle [0,1]^{p}\times \mathbb {R} }

-valued independent random variables distributed as the independent prototype pair 



(

X

,
Y
)


{\displaystyle (\mathbf {X} ,Y)}

, where 



E
⁡
[

Y

2


]
<
∞


{\displaystyle \operatorname {E} [Y^{2}]<\infty }

. We aim at predicting the response 



Y


{\displaystyle Y}

, associated with the random variable 




X



{\displaystyle \mathbf {X} }

, by estimating the regression function 



m
(

x

)
=
E
⁡
[
Y
∣

X

=

x

]


{\displaystyle m(\mathbf {x} )=\operatorname {E} [Y\mid \mathbf {X} =\mathbf {x} ]}

. A random regression forest is an ensemble of 



M


{\displaystyle M}

 randomized regression trees. Denote 




m

n


(

x

,


Θ


j


)


{\displaystyle m_{n}(\mathbf {x} ,\mathbf {\Theta } _{j})}

 the predicted value at point 




x



{\displaystyle \mathbf {x} }

 by the 



j


{\displaystyle j}

-th tree, where 





Θ


1


,
…
,


Θ


M




{\displaystyle \mathbf {\Theta } _{1},\ldots ,\mathbf {\Theta } _{M}}

 are independent random variables, distributed as a generic random variable 




Θ



{\displaystyle \mathbf {\Theta } }

, independent of the sample 






D



n




{\displaystyle {\mathcal {D}}_{n}}

. This random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. The trees are combined to form the finite forest estimate 




m

M
,
n


(

x

,

Θ

1


,
…
,

Θ

M


)
=


1
M



∑

j
=
1


M



m

n


(

x

,

Θ

j


)


{\displaystyle m_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {1}{M}}\sum _{j=1}^{M}m_{n}(\mathbf {x} ,\Theta _{j})}

.
For regression trees, we have 




m

n


=

∑

i
=
1


n






Y

i




1




X


i


∈

A

n


(

x

,

Θ

j


)





N

n


(

x

,

Θ

j


)





{\displaystyle m_{n}=\sum _{i=1}^{n}{\frac {Y_{i}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})}}{N_{n}(\mathbf {x} ,\Theta _{j})}}}

, where 




A

n


(

x

,

Θ

j


)


{\displaystyle A_{n}(\mathbf {x} ,\Theta _{j})}

 is the cell containing 




x



{\displaystyle \mathbf {x} }

, designed with randomness 




Θ

j




{\displaystyle \Theta _{j}}

 and dataset 






D



n




{\displaystyle {\mathcal {D}}_{n}}

, and 




N

n


(

x

,

Θ

j


)
=

∑

i
=
1


n




1




X


i


∈

A

n


(

x

,

Θ

j


)




{\displaystyle N_{n}(\mathbf {x} ,\Theta _{j})=\sum _{i=1}^{n}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})}}

.
 Thus random forest estimates satisfy, for all 




x

∈
[
0
,
1

]

d




{\displaystyle \mathbf {x} \in [0,1]^{d}}

, 




m

M
,
n


(

x

,

Θ

1


,
…
,

Θ

M


)
=


1
M



∑

j
=
1


M



(


∑

i
=
1


n






Y

i




1




X


i


∈

A

n


(

x

,

Θ

j


)





N

n


(

x

,

Θ

j


)




)



{\displaystyle m_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {1}{M}}\sum _{j=1}^{M}\left(\sum _{i=1}^{n}{\frac {Y_{i}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})}}{N_{n}(\mathbf {x} ,\Theta _{j})}}\right)}

. Random regression forest has two levels of averaging, first over the samples in the target cell of a tree, then over all trees. Thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. In order to improve the random forest methods and compensate the misestimation, Scornet[41] defined KeRF by








m
~




M
,
n


(

x

,

Θ

1


,
…
,

Θ

M


)
=


1


∑

j
=
1


M



N

n


(

x

,

Θ

j


)




∑

j
=
1


M



∑

i
=
1


n



Y

i




1




X


i


∈

A

n


(

x

,

Θ

j


)


,


{\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {1}{\sum _{j=1}^{M}N_{n}(\mathbf {x} ,\Theta _{j})}}\sum _{j=1}^{M}\sum _{i=1}^{n}Y_{i}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})},}


which is equal to the mean of the 




Y

i




{\displaystyle Y_{i}}

's falling in the cells containing 




x



{\displaystyle \mathbf {x} }

 in the forest. If we define the connection function of the 



M


{\displaystyle M}

 finite forest as 




K

M
,
n


(

x

,

z

)
=


1
M



∑

j
=
1


M




1



z

∈

A

n


(

x

,

Θ

j


)




{\displaystyle K_{M,n}(\mathbf {x} ,\mathbf {z} )={\frac {1}{M}}\sum _{j=1}^{M}\mathbf {1} _{\mathbf {z} \in A_{n}(\mathbf {x} ,\Theta _{j})}}

, i.e. the proportion of cells shared between 




x



{\displaystyle \mathbf {x} }

 and 




z



{\displaystyle \mathbf {z} }

, then almost surely we have 







m
~




M
,
n


(

x

,

Θ

1


,
…
,

Θ

M


)
=




∑

i
=
1


n



Y

i



K

M
,
n


(

x

,


x


i


)



∑

ℓ
=
1


n



K

M
,
n


(

x

,


x


ℓ


)





{\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {\sum _{i=1}^{n}Y_{i}K_{M,n}(\mathbf {x} ,\mathbf {x} _{i})}{\sum _{\ell =1}^{n}K_{M,n}(\mathbf {x} ,\mathbf {x} _{\ell })}}}

, which defines the KeRF.
 The construction of Centered KeRF of level 



k


{\displaystyle k}

 is the same as for centered forest, except that predictions are made by 







m
~




M
,
n


(

x

,

Θ

1


,
…
,

Θ

M


)


{\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})}

, the corresponding kernel function, or connection function is





K

k


c
c


(

x

,

z

)
=

∑


k

1


,
…
,

k

d


,

∑

j
=
1


d



k

j


=
k





k
!



k

1


!
⋯

k

d


!





(


1
d


)


k



∏

j
=
1


d




1


⌈

2


k

j





x

j


⌉
=
⌈

2


k

j





z

j


⌉


,


 for all 


x

,

z

∈
[
0
,
1

]

d


.


{\displaystyle K_{k}^{cc}(\mathbf {x} ,\mathbf {z} )=\sum _{k_{1},\ldots ,k_{d},\sum _{j=1}^{d}k_{j}=k}{\frac {k!}{k_{1}!\cdots k_{d}!}}\left({\frac {1}{d}}\right)^{k}\prod _{j=1}^{d}\mathbf {1} _{\lceil 2^{k_{j}}x_{j}\rceil =\lceil 2^{k_{j}}z_{j}\rceil },\qquad {\text{ for all }}\mathbf {x} ,\mathbf {z} \in [0,1]^{d}.}


 Uniform KeRF is built in the same way as uniform forest, except that predictions are made by 







m
~




M
,
n


(

x

,

Θ

1


,
…
,

Θ

M


)


{\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})}

, the corresponding kernel function, or connection function is





K

k


u
f


(

0

,

x

)
=

∑


k

1


,
…
,

k

d


,

∑

j
=
1


d



k

j


=
k





k
!



k

1


!
…

k

d


!





(


1
d


)


k



∏

m
=
1


d



(

1
−

|


x

m



|


∑

j
=
0



k

m


−
1






(

−
ln
⁡

|


x

m



|


)


j



j
!




)


 for all 


x

∈
[
0
,
1

]

d


.


{\displaystyle K_{k}^{uf}(\mathbf {0} ,\mathbf {x} )=\sum _{k_{1},\ldots ,k_{d},\sum _{j=1}^{d}k_{j}=k}{\frac {k!}{k_{1}!\ldots k_{d}!}}\left({\frac {1}{d}}\right)^{k}\prod _{m=1}^{d}\left(1-|x_{m}|\sum _{j=0}^{k_{m}-1}{\frac {\left(-\ln |x_{m}|\right)^{j}}{j!}}\right){\text{ for all }}\mathbf {x} \in [0,1]^{d}.}


 Predictions given by KeRF and random forests are close if the number of points in each cell is controlled:
 Assume that there exist sequences 



(

a

n


)
,
(

b

n


)


{\displaystyle (a_{n}),(b_{n})}

 such that, almost surely,





a

n


≤

N

n


(

x

,
Θ
)
≤

b

n



 and 


a

n


≤


1
M



∑

m
=
1


M



N

n




x

,

Θ

m



≤

b

n


.


{\displaystyle a_{n}\leq N_{n}(\mathbf {x} ,\Theta )\leq b_{n}{\text{ and }}a_{n}\leq {\frac {1}{M}}\sum _{m=1}^{M}N_{n}{\mathbf {x} ,\Theta _{m}}\leq b_{n}.}


Then almost surely,





|


m

M
,
n


(

x

)
−




m
~




M
,
n


(

x

)

|

≤




b

n


−

a

n




a

n








m
~




M
,
n


(

x

)
.


{\displaystyle |m_{M,n}(\mathbf {x} )-{\tilde {m}}_{M,n}(\mathbf {x} )|\leq {\frac {b_{n}-a_{n}}{a_{n}}}{\tilde {m}}_{M,n}(\mathbf {x} ).}


 When the number of trees 



M


{\displaystyle M}

 goes to infinity, then we have infinite random forest and infinite KeRF. Their estimates are close if the number of observations in each cell is bounded:
 Assume that there exist sequences 



(

ε

n


)
,
(

a

n


)
,
(

b

n


)


{\displaystyle (\varepsilon _{n}),(a_{n}),(b_{n})}

 such that, almost surely
 Then almost surely,





|


m

∞
,
n


(

x

)
−




m
~




∞
,
n


(

x

)

|

≤




b

n


−

a

n




a

n








m
~




∞
,
n


(

x

)
+
n

ε

n



(


max

1
≤
i
≤
n



Y

i



)

.


{\displaystyle |m_{\infty ,n}(\mathbf {x} )-{\tilde {m}}_{\infty ,n}(\mathbf {x} )|\leq {\frac {b_{n}-a_{n}}{a_{n}}}{\tilde {m}}_{\infty ,n}(\mathbf {x} )+n\varepsilon _{n}\left(\max _{1\leq i\leq n}Y_{i}\right).}


 Assume that 



Y
=
m
(

X

)
+
ε


{\displaystyle Y=m(\mathbf {X} )+\varepsilon }

, where 



ε


{\displaystyle \varepsilon }

 is a centered Gaussian noise, independent of 




X



{\displaystyle \mathbf {X} }

, with finite variance 




σ

2


<
∞


{\displaystyle \sigma ^{2}<\infty }

. Moreover, 




X



{\displaystyle \mathbf {X} }

 is uniformly distributed on 



[
0
,
1

]

d




{\displaystyle [0,1]^{d}}

 and 



m


{\displaystyle m}

 is Lipschitz. Scornet[41] proved upper bounds on the rates of consistency for centered KeRF and uniform KeRF.
 Providing 



k
→
∞


{\displaystyle k\rightarrow \infty }

 and 



n

/


2

k


→
∞


{\displaystyle n/2^{k}\rightarrow \infty }

, there exists a constant 




C

1


>
0


{\displaystyle C_{1}>0}

 such that, for all 



n


{\displaystyle n}

,





E

[




m
~




n


c
c


(

X

)
−
m
(

X

)

]

2


≤

C

1



n

−
1

/

(
3
+
d
log
⁡
2
)


(
log
⁡
n

)

2




{\displaystyle \mathbb {E} [{\tilde {m}}_{n}^{cc}(\mathbf {X} )-m(\mathbf {X} )]^{2}\leq C_{1}n^{-1/(3+d\log 2)}(\log n)^{2}}

.
 Providing 



k
→
∞


{\displaystyle k\rightarrow \infty }

 and 



n

/


2

k


→
∞


{\displaystyle n/2^{k}\rightarrow \infty }

, there exists a constant 



C
>
0


{\displaystyle C>0}

 such that,





E

[




m
~




n


u
f


(

X

)
−
m
(

X

)

]

2


≤
C

n

−
2

/

(
6
+
3
d
log
⁡
2
)


(
log
⁡
n

)

2




{\displaystyle \mathbb {E} [{\tilde {m}}_{n}^{uf}(\mathbf {X} )-m(\mathbf {X} )]^{2}\leq Cn^{-2/(6+3d\log 2)}(\log n)^{2}}

.
 While random forests often achieve higher accuracy than a single decision tree, they sacrifice the intrinsic interpretability of decision trees. Decision trees are among a fairly small family of machine learning models that are easily interpretable along with linear models, rule-based models, and attention-based models. This interpretability is one of the main advantages of decision trees. It allows developers to confirm that the model has learned realistic information from the data and allows end-users to have trust and confidence in the decisions made by the model.[37][3] For example, following the path that a decision tree takes to make its decision is quite trivial, but following the paths of tens or hundreds of trees is much harder. To achieve both performance and interpretability, some model compression techniques allow transforming a random forest into a minimal ""born-again"" decision tree that faithfully reproduces the same decision function.[37][47][48]
 Another limitation of random forests is that if features are linearly correlated with the target, random forest may not enhance the accuracy of the base learner.[37][40] Likewise in problems with multiple categorical variables.[49]
",random forest random decis forest ensembl learn method classif regress task work creat multitud decis tree train classif task output random forest class select tree regress task output averag predict tree random forest correct decis tree habit overfit train set first algorithm random decis forest creat tin kam ho use random subspac method ho formul way implement stochast discrimin approach classif propos eugen kleinberg extens algorithm develop leo breiman adel cutler regist random forest trademark updat own minitab extens combin breiman bag idea random select featur introduc first ho later independ amit geman order construct collect decis tree control varianc gener method random decis forest first propos salzberg heath method use random decis tree algorithm creat multipl tree combin use major vote idea develop ho ho establish forest tree split obliqu hyperplan gain accuraci grow without suffer overtrain long forest randomli restrict sensit select featur dimens subsequ work along line conclud split method behav similarli long randomli forc insensit featur dimens observ complex classifi larger forest get accur nearli monoton sharp contrast common belief complex classifi grow certain level accuraci hurt overfit explan forest method resist overtrain found kleinberg theori stochast discrimin earli develop breiman notion random forest influenc work amit geman introduc idea search random subset avail decis split node context grow singl tree idea random subspac select ho also influenti design random forest method grow forest tree introduc variat among tree project train data randomli chosen subspac fit tree node final idea random node optim decis node select random procedur rather determinist optim first introduc thoma g dietterich proper introduct random forest made paper leo breiman paper describ method build forest uncorrel tree use cart like procedur combin random node optim bag addit paper combin sever ingredi previous known novel form basi modern practic random forest particular report also offer first theoret result random forest form bound gener error depend strength tree forest correl decis tree popular method variou machin learn task tree learn almost procedur data mine say hasti et invari scale variou transform featur valu robust inclus irrelev featur produc inspect model howev seldom accur particular tree grown deep tend learn highli irregular pattern overfit train set low bia high varianc random forest way averag multipl deep decis tree train differ part train set goal reduc varianc come expens small increas bia loss interpret gener greatli boost perform final model train algorithm random forest appli gener techniqu bootstrap aggreg bag tree learner given train set x xn respons yn bag repeatedli b time select random sampl replac train set fit tree sampl train predict unseen sampl x made averag predict individu regress tree x f b b b f b x f b b b x take plural vote case classif tree bootstrap procedur lead better model perform decreas varianc model without increas bia mean predict singl tree highli sensit nois train set averag mani tree long tree correl simpli train mani tree singl train set would give strongli correl tree even tree mani time train algorithm determinist bootstrap sampl way tree show differ train set addit estim uncertainti predict made standard deviat predict individu regress tree σ b b f b x f b b b x f number b sampl equival tree free paramet typic hundr sever thousand tree use depend size natur train set b optim use observ error mean predict error train sampl xi use tree xi bootstrap sampl train test error tend level number tree fit procedur describ origin bag algorithm tree random forest also includ anoth type bag scheme use modifi tree learn algorithm select candid split learn process random subset featur process sometim call featur bag reason correl tree ordinari bootstrap sampl one featur strong predictor respons variabl target output featur select mani b tree caus becom correl analysi bag random subspac project contribut accuraci gain differ condit given ho typic classif problem p featur round featur use split regress problem inventor recommend round minimum node size default practic best valu paramet tune basi everi problem ad one step random yield extrem random tree extratre ordinari random forest ensembl individu tree two main differ tree train use whole learn sampl rather bootstrap sampl split random featur consider number random select instead comput local optim base inform gain gini impur valu chosen uniform distribut within featur empir rang tree train set randomli chosen split split yield highest score chosen split node similar ordinari random forest number randomli select featur consid node specifi default valu paramet p p classif p p regress p p number featur model basic random forest procedur may work well situat larg number featur small proport featur inform respect sampl classif address encourag procedur focu mainli featur tree inform method accomplish random forest use rank import variabl regress classif problem natur way follow techniqu describ breiman origin paper implement r packag randomforest measur featur import data set n x n n n first random forest train data train error data point record averag forest bag use train instead comput error independ test set train valu featur permut sampl error comput perturb data set import featur comput averag differ error permut tree score normal standard deviat differ featur produc larg valu score rank import featur produc small valu statist definit variabl import measur given analyz zhu et al method determin variabl import drawback approach featur import random forest consid import variabl decreas lot impur split describ book classif regress tree leo breiman default implement learn definit unorm averag import x n n node j split variabl j x p j δ j unorm averag import x node split variabl j j j impur measur sampl fall node follow statist use normal import obtain normal featur sum normal featur import learn default implement report mislead featur import relationship random forest neighbor algorithm point lin jeon view weight neighborhood scheme model built train set x n n make predict new point x look neighborhood point formal weight function w n w x x n w x w x x w x weight train point rel new point x tree x weight point x must sum weight function follow sinc forest averag predict set tree individu weight function w j j predict j n w j x x n j w j x x n j x n j x show whole forest weight neighborhood scheme weight averag individu tree neighbor x interpret point x share leaf tree j j way neighborhood x depend complex way structur tree thu structur train set lin jeon show shape neighborhood use random forest adapt local import featur part construct random forest predictor natur lead dissimilar measur among observ one analog defin dissimilar unlabel data train forest distinguish origin observ data suitabl gener synthet data drawn refer distribut random forest dissimilar attract handl mix variabl type well invari monoton transform input variabl robust outli observ random forest dissimilar easili deal larg number variabl due intrins variabl select exampl addcl random forest dissimilar weigh contribut variabl accord depend variabl random forest dissimilar use varieti applic find cluster patient base tissu marker data instead decis tree linear model propos evalu base estim random forest particular multinomi logist regress naiv bay classifi case relationship predictor target variabl linear base learner may equal high accuraci ensembl learner machin learn kernel random forest kerf establish connect random forest kernel method slightli modifi definit random forest rewritten kernel method interpret easier analyz leo breiman first person notic link random forest kernel method point random forest train use random vector tree construct equival kernel act true margin lin jeon establish connect random forest adapt nearest neighbor impli random forest seen adapt kernel estim davi ghahramani propos kernel random forest kerf show empir outperform kernel method scornet first defin kerf estim gave explicit link kerf estim random forest also gave explicit express kernel base center random forest uniform random forest two simplifi model random forest name two kerf center kerf uniform kerf prove upper bound rate consist center forest simplifi model breiman origin random forest uniformli select attribut among attribut perform split center cell along attribut algorithm stop fulli binari tree level k k built k n n paramet algorithm uniform forest anoth simplifi model breiman origin random forest uniformli select featur among featur perform split point uniformli drawn side cell along preselect featur given train sampl n x n n x n p r p r independ random variabl distribut independ prototyp pair x x e e aim predict respons associ random variabl x x estim regress function x e x x x e x x random regress forest ensembl random regress tree denot n x θ j n x j predict valu point x x j j tree θ θ independ random variabl distribut gener random variabl θ independ sampl n n random variabl use describ random induc node split sampl procedur tree construct tree combin form finit forest estim n x θ θ j n x θ j n x n x j regress tree n n x n x θ j n n x θ j n n x n x j n x j n x θ j n x j cell contain x x design random θ j j dataset n n n n x θ j n x n x θ j n x j n x n x j thu random forest estim satisfi x x n x θ θ j n x n x θ j n n x θ j n x n x n x j n x j random regress forest two level averag first sampl target cell tree tree thu contribut observ cell high densiti data point smaller observ belong less popul cell order improv random forest method compens misestim scornet defin kerf n x θ θ j n n x θ j j n x n x θ j n x n x j n x n x j equal mean fall cell contain x x forest defin connect function finit forest k n x z j z n x θ j n x z z n x j proport cell share x x z z almost sure n x θ θ n k n x x ℓ n k n x x ℓ n x n n x x n n x x defin kerf construct center kerf level k k center forest except predict made n x θ θ n x correspond kernel function connect function k k c c x z k k j k j k k k k k j k j x j k j z j x z k cc x z j k k j j j j x z uniform kerf built way uniform forest except predict made n x θ θ n x correspond kernel function connect function k k u f x k k j k j k k k k k x j k ln x j j x k uf x j k k j j x predict given kerf random forest close number point cell control assum exist sequenc n b n n n almost sure n n n x θ b n n n n x θ b n n n x n n n x n almost sure n x n x b n n n n x n x n x n n n n x number tree goe infin infinit random forest infinit kerf estim close number observ cell bound assum exist sequenc ε n n b n n n n almost sure almost sure n x n x b n n n n x n ε n max n n x n x n n n n x n n assum x ε x ε center gaussian nois independ x x finit varianc σ moreov x x uniformli distribut lipschitz scornet prove upper bound rate consist center kerf uniform kerf provid k n k k exist constant c n n e n c c x x c n log log n e n cc x x n provid k n k k exist constant c c e n u f x x c n log log n e n uf x x n random forest often achiev higher accuraci singl decis tree sacrific intrins interpret decis tree decis tree among fairli small famili machin learn model easili interpret along linear model model model interpret one main advantag decis tree allow develop confirm model learn realist inform data allow trust confid decis made model exampl follow path decis tree take make decis quit trivial follow path ten hundr tree much harder achiev perform interpret model compress techniqu allow transform random forest minim decis tree faith reproduc decis function anoth limit random forest featur linearli correl target random forest may enhanc accuraci base learner likewis problem multipl categor variabl
k-nearest neighbors algorithm,https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm,"In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method. It was first developed by Evelyn Fix and Joseph Hodges in 1951,[1] and later expanded by Thomas Cover.[2] 
Most often, it is used for classification, as a k-NN classifier, the output of which is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.
 The k-NN algorithm can also be generalized for regression. In k-NN regression, also known as nearest neighbor smoothing, the output is the property value for the object. This value is the average of the values of k nearest neighbors. If k = 1, then the output is simply assigned to the value of that single nearest neighbor, also known as nearest neighbor interpolation.
 For both classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that nearer neighbors contribute more to the average than distant ones. For example, a common weighting scheme consists of giving each neighbor a weight of 1/d, where d is the distance to the neighbor.[3]
 The input consists of the k closest training examples in a data set. 
The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.
 A peculiarity (sometimes even a disadvantage) of the k-NN algorithm is its sensitivity to the local structure of the data.
In k-NN classification the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance, if the features represent different physical units or come in vastly different scales, then feature-wise normalizing of the training data can greatly improve its accuracy.[4]
 Suppose we have pairs 



(

X

1


,

Y

1


)
,
(

X

2


,

Y

2


)
,
…
,
(

X

n


,

Y

n


)


{\displaystyle (X_{1},Y_{1}),(X_{2},Y_{2}),\dots ,(X_{n},Y_{n})}

 taking values in 





R


d


×
{
1
,
2
}


{\displaystyle \mathbb {R} ^{d}\times \{1,2\}}

, where Y is the class label of X, so that 



X

|

Y
=
r
∼

P

r




{\displaystyle X|Y=r\sim P_{r}}

 for 



r
=
1
,
2


{\displaystyle r=1,2}

 (and probability distributions 




P

r




{\displaystyle P_{r}}

). Given some norm 



‖
⋅
‖


{\displaystyle \|\cdot \|}

 on 





R


d




{\displaystyle \mathbb {R} ^{d}}

 and a point 



x
∈


R


d




{\displaystyle x\in \mathbb {R} ^{d}}

, let 



(

X

(
1
)


,

Y

(
1
)


)
,
…
,
(

X

(
n
)


,

Y

(
n
)


)


{\displaystyle (X_{(1)},Y_{(1)}),\dots ,(X_{(n)},Y_{(n)})}

 be a reordering of the training data such that 



‖

X

(
1
)


−
x
‖
≤
⋯
≤
‖

X

(
n
)


−
x
‖


{\displaystyle \|X_{(1)}-x\|\leq \dots \leq \|X_{(n)}-x\|}

.
 The training examples are vectors in a multidimensional feature space, each with a class label. The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.
 In the classification phase, k is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point.
 A commonly used distance metric for continuous variables is Euclidean distance. For discrete variables, such as for text classification, another metric can be used, such as the overlap metric (or Hamming distance). In the context of gene expression microarray data, for example, k-NN has been employed with correlation coefficients, such as Pearson and Spearman, as a metric.[5] Often, the classification accuracy of k-NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis.
 A drawback of the basic ""majority voting"" classification occurs when the class distribution is skewed. That is, examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common among the k nearest neighbors due to their large number.[6] One way to overcome this problem is to weight the classification, taking into account the distance from the test point to each of its k nearest neighbors. The class (or value, in regression problems) of each of the k nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point. Another way to overcome skew is by abstraction in data representation. For example, in a self-organizing map (SOM), each node is a representative (a center) of a cluster of similar points, regardless of their density in the original training data. K-NN can then be applied to the SOM.
 The best choice of k depends upon the data; generally, larger values of k reduces effect of the noise on the classification,[7] but make boundaries between classes less distinct. A good k can be selected by various heuristic techniques (see hyperparameter optimization). The special case where the class is predicted to be the class of the closest training sample (i.e. when k = 1) is called the nearest neighbor algorithm.
 The accuracy of the k-NN algorithm can be severely degraded by the presence of noisy or irrelevant features, or if the feature scales are not consistent with their importance. Much research effort has been put into selecting or scaling features to improve classification. A particularly popular[citation needed] approach is the use of evolutionary algorithms to optimize feature scaling.[8] Another popular approach is to scale features by the mutual information of the training data with the training classes.[citation needed]
 In binary (two class) classification problems, it is helpful to choose k to be an odd number as this avoids tied votes. One popular way of choosing the empirically optimal k in this setting is via bootstrap method.[9]
 The most intuitive nearest neighbour type classifier is the one nearest neighbour classifier that assigns a point x to the class of its closest neighbour in the feature space, that is 




C

n


1
n
n


(
x
)
=

Y

(
1
)




{\displaystyle C_{n}^{1nn}(x)=Y_{(1)}}

.
 As the size of training data set approaches infinity, the one nearest neighbour classifier guarantees an error rate of no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data).
 The k-nearest neighbour classifier can be viewed as assigning the k nearest neighbours a weight 



1

/

k


{\displaystyle 1/k}

 and all others 0 weight. This can be generalised to weighted nearest neighbour classifiers. That is, where the ith nearest neighbour is assigned a weight 




w

n
i




{\displaystyle w_{ni}}

, with 




∑

i
=
1


n



w

n
i


=
1


{\textstyle \sum _{i=1}^{n}w_{ni}=1}

. An analogous result on the strong consistency of weighted nearest neighbour classifiers also holds.[10]
 Let 




C

n


w
n
n




{\displaystyle C_{n}^{wnn}}

 denote the weighted nearest classifier with weights 



{

w

n
i



}

i
=
1


n




{\displaystyle \{w_{ni}\}_{i=1}^{n}}

. Subject to regularity conditions, which in asymptotic theory are conditional variables which require assumptions to differentiate among parameters with some criteria. On the class distributions the excess risk has the following asymptotic expansion[11]







R




R



(

C

n


w
n
n


)
−



R




R



(

C

Bayes


)
=

(


B

1



s

n


2


+

B

2



t

n


2



)

{
1
+
o
(
1
)
}
,


{\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{wnn})-{\mathcal {R}}_{\mathcal {R}}(C^{\text{Bayes}})=\left(B_{1}s_{n}^{2}+B_{2}t_{n}^{2}\right)\{1+o(1)\},}


for constants 




B

1




{\displaystyle B_{1}}

 and 




B

2




{\displaystyle B_{2}}

 where 




s

n


2


=

∑

i
=
1


n



w

n
i


2




{\displaystyle s_{n}^{2}=\sum _{i=1}^{n}w_{ni}^{2}}

 and 




t

n


=

n

−
2

/

d



∑

i
=
1


n



w

n
i



{


i

1
+
2

/

d


−
(
i
−
1

)

1
+
2

/

d



}



{\displaystyle t_{n}=n^{-2/d}\sum _{i=1}^{n}w_{ni}\left\{i^{1+2/d}-(i-1)^{1+2/d}\right\}}

.
 The optimal weighting scheme 



{

w

n
i


∗



}

i
=
1


n




{\displaystyle \{w_{ni}^{*}\}_{i=1}^{n}}

, that balances the two terms in the display above, is given as follows: set 




k

∗


=
⌊
B

n


4

d
+
4




⌋


{\displaystyle k^{*}=\lfloor Bn^{\frac {4}{d+4}}\rfloor }

, 





w

n
i


∗


=


1

k

∗





[

1
+


d
2


−


d

2



k

∗




2

/

d





{

i

1
+
2

/

d


−
(
i
−
1

)

1
+
2

/

d


}

]



{\displaystyle w_{ni}^{*}={\frac {1}{k^{*}}}\left[1+{\frac {d}{2}}-{\frac {d}{2{k^{*}}^{2/d}}}\{i^{1+2/d}-(i-1)^{1+2/d}\}\right]}

 for 



i
=
1
,
2
,
…
,

k

∗




{\displaystyle i=1,2,\dots ,k^{*}}

 and 





w

n
i


∗


=
0


{\displaystyle w_{ni}^{*}=0}

 for 



i
=

k

∗


+
1
,
…
,
n


{\displaystyle i=k^{*}+1,\dots ,n}

.
 With optimal weights the dominant term in the asymptotic expansion of the excess risk is 





O


(

n

−


4

d
+
4





)


{\displaystyle {\mathcal {O}}(n^{-{\frac {4}{d+4}}})}

. Similar results are true when using a bagged nearest neighbour classifier.
 k-NN is a special case of a variable-bandwidth, kernel density ""balloon"" estimator with a uniform kernel.[12][13]
 The naive version of the algorithm is easy to implement by computing the distances from the test example to all stored examples, but it is computationally intensive for large training sets. Using an approximate nearest neighbor search algorithm makes k-NN computationally tractable even for large data sets. Many nearest neighbor search algorithms have been proposed over the years; these generally seek to reduce the number of distance evaluations actually performed.
 k-NN has some strong consistency results. As the amount of data approaches infinity, the two-class k-NN algorithm is guaranteed to yield an error rate no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data).[2] Various improvements to the k-NN speed are possible by using proximity graphs.[14]
 For multi-class k-NN classification, Cover and Hart (1967) prove an upper bound error rate of





R

∗


 
≤
 

R

k

N
N



 
≤
 

R

∗



(

2
−



M

R

∗




M
−
1




)



{\displaystyle R^{*}\ \leq \ R_{k\mathrm {NN} }\ \leq \ R^{*}\left(2-{\frac {MR^{*}}{M-1}}\right)}


where 




R

∗




{\displaystyle R^{*}}

 is the Bayes error rate (which is the minimal error rate possible), 




R

k
N
N




{\displaystyle R_{kNN}}

 is the asymptotic k-NN error rate, and M is the number of classes in the problem. This bound is tight in the sense that both the lower and upper bounds are achievable by some distribution.[15] For 



M
=
2


{\displaystyle M=2}

 and as the Bayesian error rate 




R

∗




{\displaystyle R^{*}}

 approaches zero, this limit reduces to ""not more than twice the Bayesian error rate"".
 There are many results on the error rate of the k nearest neighbour classifiers.[16] The k-nearest neighbour classifier is strongly (that is for any joint distribution on 



(
X
,
Y
)


{\displaystyle (X,Y)}

) consistent provided 



k
:=

k

n




{\displaystyle k:=k_{n}}

 diverges and 




k

n



/

n


{\displaystyle k_{n}/n}

 converges to zero as 



n
→
∞


{\displaystyle n\to \infty }

.
 Let 




C

n


k
n
n




{\displaystyle C_{n}^{knn}}

 denote the k nearest neighbour classifier based on a training set of size n. Under certain regularity conditions, the excess risk yields the following asymptotic expansion[11]







R




R



(

C

n


k
n
n


)
−



R




R



(

C

Bayes


)
=

{


B

1




1
k


+

B

2




(


k
n


)


4

/

d



}

{
1
+
o
(
1
)
}
,


{\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{knn})-{\mathcal {R}}_{\mathcal {R}}(C^{\text{Bayes}})=\left\{B_{1}{\frac {1}{k}}+B_{2}\left({\frac {k}{n}}\right)^{4/d}\right\}\{1+o(1)\},}


for some constants 




B

1




{\displaystyle B_{1}}

 and 




B

2




{\displaystyle B_{2}}

.
 The choice 




k

∗


=

⌊

B

n


4

d
+
4





⌋



{\displaystyle k^{*}=\left\lfloor Bn^{\frac {4}{d+4}}\right\rfloor }

 offers a trade off between the two terms in the above display, for which the 




k

∗




{\displaystyle k^{*}}

-nearest neighbour error converges to the Bayes error at the optimal (minimax) rate 





O



(

n

−


4

d
+
4





)



{\displaystyle {\mathcal {O}}\left(n^{-{\frac {4}{d+4}}}\right)}

.
 The K-nearest neighbor classification performance can often be significantly improved through (supervised) metric learning. Popular algorithms are neighbourhood components analysis and large margin nearest neighbor. Supervised metric learning algorithms use the label information to learn a new metric or pseudo-metric.
 When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters) then the input data will be transformed into a reduced representation set of features (also named features vector). Transforming the input data into the set of features is called feature extraction. If the features extracted are carefully chosen it is expected that the features set will extract the relevant information from the input data in order to perform the desired task using this reduced representation instead of the full size input. Feature extraction is performed on raw data prior to applying k-NN algorithm on the transformed data in feature space.
 An example of a typical computer vision computation pipeline for face recognition using k-NN including feature extraction and dimension reduction pre-processing steps (usually implemented with OpenCV):
 For high-dimensional data (e.g., with number of dimensions more than 10) dimension reduction is usually performed prior to applying the k-NN algorithm in order to avoid the effects of the curse of dimensionality.[17]
 The curse of dimensionality in the k-NN context basically means that Euclidean distance is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector (imagine multiple points lying more or less on a circle with the query point at the center; the distance from the query to all data points in the search space is almost the same).
 Feature extraction and dimension reduction can be combined in one step using principal component analysis (PCA),  linear discriminant analysis (LDA), or canonical correlation analysis (CCA) techniques as a pre-processing step, followed by clustering by k-NN on feature vectors in reduced-dimension space. This process is also called low-dimensional embedding.[18]
 For very-high-dimensional datasets (e.g. when performing a similarity search on live video streams, DNA data or high-dimensional time series) running a fast approximate k-NN search using locality sensitive hashing, ""random projections"",[19] ""sketches""[20] or other high-dimensional similarity search techniques from the VLDB toolbox might be the only feasible option.
 Nearest neighbor rules in effect implicitly compute the decision boundary. It is also possible to compute the decision boundary explicitly, and to do so efficiently, so that the computational complexity is a function of the boundary complexity.[21]
 Data reduction is one of the most important problems for work with huge data sets. Usually, only some of the data points are needed for accurate classification. Those data are called the prototypes and can be found as follows:
 A training example surrounded by examples of other classes is called a class outlier. Causes of class outliers include:
 Class outliers with k-NN produce noise. They can be detected and separated for future analysis. Given two natural numbers, k>r>0, a training example is called a (k,r)NN class-outlier if its k nearest neighbors include more than r examples of other classes.
 Condensed nearest neighbor (CNN, the Hart algorithm) is an algorithm designed to reduce the data set for k-NN classification.[22] It selects the set of prototypes U from the training data, such that 1NN with U can classify the examples almost as accurately as 1NN does with the whole data set.
 Given a training set X, CNN works iteratively:
 Use U instead of X for classification. The examples that are not prototypes are called ""absorbed"" points.
 It is efficient to scan the training examples in order of decreasing border ratio.[23] The border ratio of a training example x is defined as 
 where ‖x-y‖ is the distance to the closest example y having a different color than x, and ‖x'-y‖ is the distance from y to its closest example x'  with the same label as x.
 The border ratio is in the interval [0,1] because ‖x'-y‖ never exceeds ‖x-y‖. This ordering gives preference to the borders of the classes for inclusion in the set of prototypes U. A point of a different label than x is called external to x. The calculation of the border ratio is illustrated by the figure on the right. The data points are labeled by colors: the initial point is x and its label is red. External points are blue and green. The closest to x external point is y. The closest to y red point is x' . The border ratio a(x) = ‖x'-y‖ / ‖x-y‖is the attribute of the initial point x.
 Below is an illustration of CNN in a series of figures. There are three classes (red, green and blue). Fig. 1: initially there are 60 points in each class. Fig. 2 shows the 1NN classification map: each pixel is classified by 1NN using all the data. Fig. 3 shows the 5NN classification map. White areas correspond to the unclassified regions, where 5NN voting is tied (for example, if there are two green, two red and one blue points among 5 nearest neighbors). Fig. 4 shows the reduced data set. The crosses are the class-outliers selected by the (3,2)NN rule (all the three nearest neighbors of these instances belong to other classes); the squares are the prototypes, and the empty circles are the absorbed points. The left bottom corner shows the numbers of the class-outliers, prototypes and absorbed points for all three classes. The number of prototypes varies from 15% to 20% for different classes in this example. Fig. 5 shows that the 1NN classification map with the prototypes is very similar to that with the initial data set. The figures were produced using the Mirkes applet.[23]
 In k-NN regression, also known as k-NN smoothing, the k-NN algorithm is used for estimating continuous variables.[citation needed] One such algorithm uses a weighted average of the k nearest neighbors, weighted by the inverse of their distance. This algorithm works as follows:
 The distance to the kth nearest neighbor can also be seen as a local density estimate and thus is also a popular outlier score in anomaly detection. The larger the distance to the k-NN, the lower the local density, the more likely the query point is an outlier.[24] Although quite simple, this outlier model, along with another classic data mining method, local outlier factor, works quite well also in comparison to more recent and more complex approaches, according to a large scale experimental analysis.[25]
 A confusion matrix or ""matching matrix"" is often used as a tool to validate the accuracy of k-NN classification. More robust statistical methods such as likelihood-ratio test can also be applied.[how?]
",statist neighbor algorithm supervis learn method first develop evelyn fix joseph hodg later expand thoma cover often use classif classifi output class membership object classifi plural vote neighbor object assign class common among k nearest neighbor k posit integ typic small k object simpli assign class singl nearest neighbor algorithm also gener regress regress also known nearest neighbor smooth output properti valu object valu averag valu k nearest neighbor k output simpli assign valu singl nearest neighbor also known nearest neighbor interpol classif regress use techniqu assign weight contribut neighbor nearer neighbor contribut averag distant one exampl common weight scheme consist give neighbor weight distanc neighbor input consist k closest train exampl data set neighbor taken set object class classif object properti valu regress known thought train set algorithm though explicit train step requir peculiar sometim even disadvantag algorithm sensit local structur data classif function approxim local comput defer function evalu sinc algorithm reli distanc featur repres differ physic unit come vastli differ scale normal train data greatli improv accuraci suppos pair x x x n n n n take valu r r class label x x r p r r r probabl distribut p r r given norm r r point x r r let x x n n n n reorder train data x x x n x n train exampl vector multidimension featur space class label train phase algorithm consist store featur vector class label train sampl classif phase k constant unlabel vector queri test point classifi assign label frequent among k train sampl nearest queri point commonli use distanc metric continu variabl euclidean distanc discret variabl text classif anoth metric use overlap metric ham distanc context gene express microarray data exampl employ correl coeffici pearson spearman metric often classif accuraci improv significantli distanc metric learn special algorithm larg margin nearest neighbor neighbourhood compon analysi drawback basic major vote classif occur class distribut skew exampl frequent class tend domin predict new exampl tend common among k nearest neighbor due larg number one way overcom problem weight classif take account distanc test point k nearest neighbor class valu regress problem k nearest point multipli weight proport invers distanc point test point anoth way overcom skew abstract data represent exampl map som node repres center cluster similar point regardless densiti origin train data appli som best choic k depend upon data gener larger valu k reduc effect nois classif make boundari class less distinct good k select variou heurist techniqu see hyperparamet optim special case class predict class closest train sampl k call nearest neighbor algorithm accuraci algorithm sever degrad presenc noisi irrelev featur featur scale consist import much research effort put select scale featur improv classif particularli popular citat need approach use evolutionari algorithm optim featur scale anoth popular approach scale featur mutual inform train data train class citat need binari two class classif problem help choos k odd number avoid tie vote one popular way choos empir optim k set via bootstrap method intuit nearest neighbour type classifi one nearest neighbour classifi assign point x class closest neighbour featur space c n n n x n x size train data set approach infin one nearest neighbour classifi guarante error rate wors twice bay error rate minimum achiev error rate given distribut data neighbour classifi view assign k nearest neighbour weight k other weight generalis weight nearest neighbour classifi ith nearest neighbour assign weight w n ni n w n n ni analog result strong consist weight nearest neighbour classifi also hold let c n w n n n wnn denot weight nearest classifi weight w n n ni n subject regular condit asymptot theori condit variabl requir assumpt differenti among paramet criteria class distribut excess risk follow asymptot expans r r c n w n n r r c bay b n b n r r n wnn r r bay n n constant b b n n w n n n ni n n n w n n n ni optim weight scheme w n n ni n balanc two term display given follow set k b n w n k k ni k w n ni k n n optim weight domin term asymptot expans excess risk n similar result true use bag nearest neighbour classifi special case kernel densiti balloon estim uniform kernel naiv version algorithm easi implement comput distanc test exampl store exampl comput intens larg train set use approxim nearest neighbor search algorithm make comput tractabl even larg data set mani nearest neighbor search algorithm propos year gener seek reduc number distanc evalu actual perform strong consist result amount data approach infin algorithm guarante yield error rate wors twice bay error rate minimum achiev error rate given distribut data variou improv speed possibl use proxim graph classif cover hart prove upper bound error rate r r k n n r r nn r bay error rate minim error rate possibl r k n n knn asymptot error rate number class problem bound tight sens lower upper bound achiev distribut bayesian error rate r approach zero limit reduc twice bayesian error rate mani result error rate k nearest neighbour classifi neighbour classifi strongli joint distribut x x consist provid k k n k n diverg k n n n converg zero n let c n k n n n knn denot k nearest neighbour classifi base train set size certain regular condit excess risk yield follow asymptot expans r r c n k n n r r c bay b k b k n r r n knn r r bay k k n constant b b choic k b n offer trade two term display k neighbour error converg bay error optim minimax rate n neighbor classif perform often significantli improv supervis metric learn popular algorithm neighbourhood compon analysi larg margin nearest neighbor supervis metric learn algorithm use label inform learn new metric input data algorithm larg process suspect redund measur feet meter input data transform reduc represent set featur also name featur vector transform input data set featur call featur extract featur extract care chosen expect featur set extract relev inform input data order perform desir task use reduc represent instead full size input featur extract perform raw data prior appli algorithm transform data featur space exampl typic comput vision comput pipelin face recognit use includ featur extract dimens reduct step usual implement opencv data number dimens dimens reduct usual perform prior appli algorithm order avoid effect curs dimension curs dimension context basic mean euclidean distanc unhelp high dimens vector almost equidist search queri vector imagin multipl point lie less circl queri point center distanc queri data point search space almost featur extract dimens reduct combin one step use princip compon analysi pca linear discrimin analysi lda canon correl analysi cca techniqu step follow cluster featur vector space process also call embed dataset perform similar search live video stream dna data time seri run fast approxim search use local sensit hash random project sketch similar search techniqu vldb toolbox might feasibl option nearest neighbor rule effect implicitli comput decis boundari also possibl comput decis boundari explicitli effici comput complex function boundari complex data reduct one import problem work huge data set usual data point need accur classif data call prototyp found follow train exampl surround exampl class call class outlier caus class outlier includ class outlier produc nois detect separ futur analysi given two natur number k r train exampl call k r nn k nearest neighbor includ r exampl class condens nearest neighbor cnn hart algorithm algorithm design reduc data set classif select set prototyp u train data u classifi exampl almost accur whole data set given train set x cnn work iter use u instead x classif exampl prototyp call absorb point effici scan train exampl order decreas border ratio border ratio train exampl x defin distanc closest exampl differ color x distanc closest exampl x label border ratio interv never exce order give prefer border class inclus set prototyp point differ label x call extern calcul border ratio illustr figur right data point label color initi point x label red extern point blue green closest x extern point closest red point x border ratio x attribut initi point illustr cnn seri figur three class red green blue fig initi point class fig show classif map pixel classifi use data fig show classif map white area correspond unclassifi region vote tie exampl two green two red one blue point among nearest neighbor fig show reduc data set cross select nn rule three nearest neighbor instanc belong class squar prototyp empti circl absorb point left bottom corner show number prototyp absorb point three class number prototyp vari differ class exampl fig show classif map prototyp similar initi data set figur produc use mirk applet regress also known smooth algorithm use estim continu variabl citat need one algorithm use weight averag k nearest neighbor weight invers distanc algorithm work follow distanc kth nearest neighbor also seen local densiti estim thu also popular outlier score anomali detect larger distanc lower local densiti like queri point outlier although quit simpl outlier model along anoth classic data mine method local outlier factor work quit well also comparison recent complex approach accord larg scale experiment analysi confus matrix match matrix often use tool valid accuraci classif robust statist method test also
Linear regression,https://en.wikipedia.org/wiki/Linear_regression,"In statistics, linear regression is a model that estimates the linear relationship between a scalar response (dependent variable) and one or more explanatory variables (regressor or independent variable). A model with exactly one explanatory variable is a simple linear regression; a model with two or more explanatory variables is a multiple linear regression.[1] This term is distinct from multivariate linear regression, which predicts multiple correlated dependent variables rather than a single dependent variable.[2]
 In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.
 Linear regression is also a type of machine learning algorithm, more specifically a supervised algorithm, that learns from the labelled datasets and maps the data points to the most optimized linear functions that can be used for prediction on new datasets. [3]
 Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications.[4] This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.
 Linear regression has many practical uses. Most applications fall into one of the following two broad categories:
 Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the ""lack of fit"" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Use of the Mean Squared Error (MSE) as the cost on a dataset that has many large outliers, can result in a model that fits the outliers more than the true data due to the higher importance assigned by MSE to large errors. So, cost functions that are robust to outliers should be used if the dataset has many large outliers. Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms ""least squares"" and ""linear model"" are closely linked, they are not synonymous.
 Given a data set 



{

y

i


,


x

i
1


,
…
,

x

i
p



}

i
=
1


n




{\displaystyle \{y_{i},\,x_{i1},\ldots ,x_{ip}\}_{i=1}^{n}}

 of n statistical units, a linear regression model assumes that the relationship between the dependent variable y and the vector of regressors x is linear. This relationship is modeled through a disturbance term or error variable ε—an unobserved random variable that adds ""noise"" to the linear relationship between the dependent variable and regressors. Thus the model takes the form




y

i


=

β

0


+

β

1



x

i
1


+
⋯
+

β

p



x

i
p


+

ε

i


=


x


i



T




β

+

ε

i


,

i
=
1
,
…
,
n
,


{\displaystyle y_{i}=\beta _{0}+\beta _{1}x_{i1}+\cdots +\beta _{p}x_{ip}+\varepsilon _{i}=\mathbf {x} _{i}^{\mathsf {T}}{\boldsymbol {\beta }}+\varepsilon _{i},\qquad i=1,\ldots ,n,}

where T denotes the transpose, so that xiTβ is the inner product between vectors xi and β.
 Often these n equations are stacked together and written in matrix notation as
 where
 Fitting a linear model to a given data set usually requires estimating the regression coefficients 




β



{\displaystyle {\boldsymbol {\beta }}}

 such that the error term 




ε

=

y

−

X


β



{\displaystyle {\boldsymbol {\varepsilon }}=\mathbf {y} -\mathbf {X} {\boldsymbol {\beta }}}

 is minimized. For example, it is common to use the sum of squared errors 



‖

ε


‖

2


2




{\displaystyle \|{\boldsymbol {\varepsilon }}\|_{2}^{2}}

 as a measure of 




ε



{\displaystyle {\boldsymbol {\varepsilon }}}

 for minimization.
 Consider a situation where a small ball is being tossed up in the air and then we measure its heights of ascent hi at various moments in time ti. Physics tells us that, ignoring the drag, the relationship can be modeled as
 where β1 determines the initial velocity of the ball, β2 is proportional to the standard gravity, and εi is due to measurement errors. Linear regression can be used to estimate the values of β1 and β2 from the measured data. This model is non-linear in the time variable, but it is linear in the parameters β1 and β2; if we take regressors xi = (xi1, xi2)  = (ti, ti2), the model takes on the standard form
 Standard linear regression models with standard estimation techniques make a number of assumptions about the predictor variables, the response variable and their relationship. Numerous extensions have been developed that allow each of these assumptions to be relaxed (i.e. reduced to a weaker form), and in some cases eliminated entirely. Generally these extensions make the estimation procedure more complex and time-consuming, and may also require more data in order to produce an equally precise model.[citation needed]
 The following are the major assumptions made by standard linear regression models with standard estimation techniques (e.g. ordinary least squares):
 Violations of these assumptions can result in biased estimations of β, biased standard errors, untrustworthy confidence intervals and significance tests. Beyond these assumptions, several other statistical properties of the data strongly influence the performance of different estimation methods:
 A fitted linear regression model can be used to identify the relationship between a single predictor variable xj and the response variable y when all the other predictor variables in the model are ""held fixed"". Specifically, the interpretation of βj is the expected change in y for a one-unit change in xj when the other covariates are held fixed—that is, the expected value of the partial derivative of y with respect to xj. This is sometimes called the unique effect of xj on y. In contrast, the marginal effect of xj on y can be assessed using a correlation coefficient or simple linear regression model relating only xj to y; this effect is the total derivative of y with respect to xj.
 Care must be taken when interpreting regression results, as some of the regressors may not allow for marginal changes (such as dummy variables, or the intercept term), while others cannot be held fixed (recall the example from the introduction: it would be impossible to ""hold ti fixed"" and at the same time change the value of ti2).
 It is possible that the unique effect be nearly zero even when the marginal effect is large. This may imply that some other covariate captures all the information in xj, so that once that variable is in the model, there is no contribution of xj to the variation in y. Conversely, the unique effect of xj can be large while its marginal effect is nearly zero. This would happen if the other covariates explained a great deal of the variation of y, but they mainly explain variation in a way that is complementary to what is captured by xj. In this case, including the other variables in the model reduces the part of the variability of y that is unrelated to xj, thereby strengthening the apparent relationship with xj.
 The meaning of the expression ""held fixed"" may depend on how the values of the predictor variables arise. If the experimenter directly sets the values of the predictor variables according to a study design, the comparisons of interest may literally correspond to comparisons among units whose predictor variables have been ""held fixed"" by the experimenter. Alternatively, the expression ""held fixed"" can refer to a selection that takes place in the context of data analysis. In this case, we ""hold a variable fixed"" by restricting our attention to the subsets of the data that happen to have a common value for the given predictor variable. This is the only interpretation of ""held fixed"" that can be used in an observational study.
 The notion of a ""unique effect"" is appealing when studying a complex system where multiple interrelated components influence the response variable. In some cases, it can literally be interpreted as the causal effect of an intervention that is linked to the value of a predictor variable. However, it has been argued that in many cases multiple regression analysis fails to clarify the relationships between the predictor variables and the response variable when the predictors are correlated with each other and are not assigned following a study design.[9]
 Numerous extensions of linear regression have been developed, which allow some or all of the assumptions underlying the basic model to be relaxed.
 The simplest case of a single scalar predictor variable x and a single scalar response variable y is known as simple linear regression. The extension to multiple and/or vector-valued predictor variables (denoted with a capital X) is known as multiple linear regression, also known as multivariable linear regression (not to be confused with multivariate linear regression).[10]
 Multiple linear regression is a generalization of simple linear regression to the case of more than one independent variable, and a special case of general linear models, restricted to one dependent variable. The basic model for multiple linear regression is
 for each observation 



i
=
1
,
…
,
n


{\textstyle i=1,\ldots ,n}

.
 In the formula above we consider n observations of one dependent variable and p independent variables. Thus, Yi is the ith observation of the dependent variable, Xij is ith observation of the jth independent variable, j = 1, 2, ..., p. The values βj represent parameters to be estimated, and εi is the ith independent identically distributed normal error.
 In the more general multivariate linear regression, there is one equation of the above form for each of m > 1 dependent variables that share the same set of explanatory variables and hence are estimated simultaneously with each other:
 for all observations indexed as i = 1, ... , n and for all dependent variables indexed as j = 1, ... , m.
 Nearly all real-world regression models involve multiple predictors, and basic descriptions of linear regression are often phrased in terms of the multiple regression model. Note, however, that in these cases the response variable y is still a scalar. Another term, multivariate linear regression, refers to cases where y is a vector, i.e., the same as general linear regression.
 The general linear model considers the situation when the response variable is not a scalar (for each observation) but a vector, yi. Conditional linearity of 



E
(

y

∣


x


i


)
=


x


i



T



B


{\displaystyle E(\mathbf {y} \mid \mathbf {x} _{i})=\mathbf {x} _{i}^{\mathsf {T}}B}

 is still assumed, with a matrix B replacing the vector β of the classical linear regression model. Multivariate analogues of ordinary least squares (OLS) and generalized least squares (GLS) have been developed. ""General linear models"" are also called ""multivariate linear models"". These are not the same as multivariable linear models (also called ""multiple linear models"").
 Various models have been created that allow for heteroscedasticity, i.e. the errors for different response variables may have different variances. For example, weighted least squares is a method for estimating linear regression models when the response variables may have different error variances, possibly with correlated errors. (See also Weighted linear least squares, and Generalized least squares.) Heteroscedasticity-consistent standard errors is an improved method for use with uncorrelated but potentially heteroscedastic errors.
 The Generalized linear model (GLM) is a framework for modeling response variables that are bounded or discrete. This is used, for example:
 Generalized linear models allow for an arbitrary link function, g, that relates the mean of the response variable(s) to the predictors: 



E
(
Y
)
=

g

−
1


(
X
B
)


{\displaystyle E(Y)=g^{-1}(XB)}

. The link function is often related to the distribution of the response, and in particular it typically has the effect of transforming between the 



(
−
∞
,
∞
)


{\displaystyle (-\infty ,\infty )}

 range of the linear predictor and the range of the response variable.
 Some common examples of GLMs are:
 Single index models[clarification needed] allow some degree of nonlinearity in the relationship between x and y, while preserving the central role of the linear predictor β′x as in the classical linear regression model. Under certain conditions, simply applying OLS to data from a single-index model will consistently estimate β up to a proportionality constant.[11]
 Hierarchical linear models (or multilevel regression) organizes the data into a hierarchy of regressions, for example where A is regressed on B, and B is regressed on C. It is often used where the variables of interest have a natural hierarchical structure such as in educational statistics, where students are nested in classrooms, classrooms are nested in schools, and schools are nested in some administrative grouping, such as a school district. The response variable might be a measure of student achievement such as a test score, and different covariates would be collected at the classroom, school, and school district levels.
 Errors-in-variables models (or ""measurement error models"") extend the traditional linear regression model to allow the predictor variables X to be observed with error. This error causes standard estimators of β to become biased. Generally, the form of bias is an attenuation, meaning that the effects are biased toward zero.
 In a multiple linear regression model
 parameter 




β

j




{\displaystyle \beta _{j}}

 of predictor variable 




x

j




{\displaystyle x_{j}}

 represents the individual effect of 




x

j




{\displaystyle x_{j}}

. It has an interpretation as the expected change in the response variable 



y


{\displaystyle y}

 when 




x

j




{\displaystyle x_{j}}

 increases by one unit with other predictor variables held constant. When 




x

j




{\displaystyle x_{j}}

 is strongly correlated with other predictor variables, it is improbable that 




x

j




{\displaystyle x_{j}}

 can increase by one unit with other variables held constant. In this case, the interpretation of 




β

j




{\displaystyle \beta _{j}}

 becomes problematic as it is based on an improbable condition, and the effect of 




x

j




{\displaystyle x_{j}}

 cannot be evaluated in isolation.
 For a group of predictor variables, say, 



{

x

1


,

x

2


,
…
,

x

q


}


{\displaystyle \{x_{1},x_{2},\dots ,x_{q}\}}

, a group effect 



ξ
(

w

)


{\displaystyle \xi (\mathbf {w} )}

 is defined as a linear combination of their parameters
 where 




w

=
(

w

1


,

w

2


,
…
,

w

q



)

⊺




{\displaystyle \mathbf {w} =(w_{1},w_{2},\dots ,w_{q})^{\intercal }}

 is a weight vector satisfying 




∑

j
=
1


q



|


w

j



|

=
1


{\textstyle \sum _{j=1}^{q}|w_{j}|=1}

. Because of the constraint on 





w

j





{\displaystyle {w_{j}}}

, 



ξ
(

w

)


{\displaystyle \xi (\mathbf {w} )}

 is also referred to as a normalized group effect. A group effect 



ξ
(

w

)


{\displaystyle \xi (\mathbf {w} )}

 has an interpretation as the expected change in 



y


{\displaystyle y}

 when variables in the group 




x

1


,

x

2


,
…
,

x

q




{\displaystyle x_{1},x_{2},\dots ,x_{q}}

 change by the amount 




w

1


,

w

2


,
…
,

w

q




{\displaystyle w_{1},w_{2},\dots ,w_{q}}

, respectively, at the same time with other variables (not in the group) held constant. It generalizes the individual effect of a variable to a group of variables in that (



i


{\displaystyle i}

) if 



q
=
1


{\displaystyle q=1}

, then the group effect reduces to an individual effect, and (



i
i


{\displaystyle ii}

) if 




w

i


=
1


{\displaystyle w_{i}=1}

 and 




w

j


=
0


{\displaystyle w_{j}=0}

 for 



j
≠
i


{\displaystyle j\neq i}

, then the group effect also reduces to an individual effect.
A group effect 



ξ
(

w

)


{\displaystyle \xi (\mathbf {w} )}

 is said to be meaningful if the underlying simultaneous changes of the 



q


{\displaystyle q}

 variables 



(

x

1


,

x

2


,
…
,

x

q



)

⊺




{\displaystyle (x_{1},x_{2},\dots ,x_{q})^{\intercal }}

 is probable.
 Group effects provide a means to study the collective impact of strongly correlated predictor variables in linear regression models. Individual effects of such variables are not well-defined as their parameters do not have good interpretations. Furthermore, when the sample size is not large, none of their parameters can be accurately estimated by the least squares regression due to the multicollinearity problem. Nevertheless, there are meaningful group effects that have good interpretations and can be accurately estimated by the least squares regression. A simple way to identify these meaningful group effects is to use an all positive correlations (APC) arrangement of the strongly correlated variables under which pairwise correlations among these variables are all positive, and standardize all 



p


{\displaystyle p}

 predictor variables in the model so that they all have mean zero and length one. To illustrate this, suppose that 



{

x

1


,

x

2


,
…
,

x

q


}


{\displaystyle \{x_{1},x_{2},\dots ,x_{q}\}}

 is a group of strongly correlated variables in an APC arrangement and that they are not strongly correlated with predictor variables outside the group. Let 




y
′



{\displaystyle y'}

 be the centred 



y


{\displaystyle y}

 and 




x

j

′



{\displaystyle x_{j}'}

 be the standardized 




x

j




{\displaystyle x_{j}}

. Then, the standardized linear regression model is
 Parameters 




β

j




{\displaystyle \beta _{j}}

 in the original model, including 




β

0




{\displaystyle \beta _{0}}

, are simple functions of 




β

j

′



{\displaystyle \beta _{j}'}

 in the standardized model. The standardization of variables does not change their correlations, so 



{

x

1

′

,

x

2

′

,
…
,

x

q

′

}


{\displaystyle \{x_{1}',x_{2}',\dots ,x_{q}'\}}

 is a group of strongly correlated variables in an APC arrangement and they are not strongly correlated with other predictor variables in the standardized model. A group effect of 



{

x

1

′

,

x

2

′

,
…
,

x

q

′

}


{\displaystyle \{x_{1}',x_{2}',\dots ,x_{q}'\}}

 is
 and its minimum-variance unbiased linear estimator is
 where 







β
^




j

′



{\displaystyle {\hat {\beta }}_{j}'}

 is the least squares estimator of 




β

j

′



{\displaystyle \beta _{j}'}

. In particular, the average group effect of the 



q


{\displaystyle q}

 standardized variables is
 which has an interpretation as the expected change in 




y
′



{\displaystyle y'}

 when all 




x

j

′



{\displaystyle x_{j}'}

 in the strongly correlated group increase by 



(
1

/

q
)


{\displaystyle (1/q)}

th of a unit at the same time with variables outside the group held constant. With strong positive correlations and in standardized units, variables in the group are approximately equal, so they are likely to increase at the same time and in similar amount. Thus, the average group effect 




ξ

A




{\displaystyle \xi _{A}}

 is a meaningful effect. It can be accurately estimated by its minimum-variance unbiased linear estimator 







ξ
^




A


=


1
q


(




β
^




1

′

+




β
^




2

′

+
⋯
+




β
^




q

′

)


{\textstyle {\hat {\xi }}_{A}={\frac {1}{q}}({\hat {\beta }}_{1}'+{\hat {\beta }}_{2}'+\dots +{\hat {\beta }}_{q}')}

, even when individually none of the 




β

j

′



{\displaystyle \beta _{j}'}

 can be accurately estimated by 







β
^




j

′



{\displaystyle {\hat {\beta }}_{j}'}

.
 Not all group effects are meaningful or can be accurately estimated. For example, 




β

1

′



{\displaystyle \beta _{1}'}

 is a special group effect with weights 




w

1


=
1


{\displaystyle w_{1}=1}

 and 




w

j


=
0


{\displaystyle w_{j}=0}

 for 



j
≠
1


{\displaystyle j\neq 1}

, but it cannot be accurately estimated by 







β
^




1

′



{\displaystyle {\hat {\beta }}'_{1}}

. It is also not a meaningful effect. In general, for a group of 



q


{\displaystyle q}

 strongly correlated predictor variables in an APC arrangement in the standardized model, group effects whose weight vectors 




w



{\displaystyle \mathbf {w} }

 are at or near the centre of the simplex 




∑

j
=
1


q



w

j


=
1


{\textstyle \sum _{j=1}^{q}w_{j}=1}

 (




w

j


≥
0


{\displaystyle w_{j}\geq 0}

) are meaningful and can be accurately estimated by their minimum-variance unbiased linear estimators. Effects with weight vectors far away from the centre are not meaningful as such weight vectors represent simultaneous changes of the variables that violate the strong positive correlations of the standardized variables in an APC arrangement. As such, they are not probable. These effects also cannot be accurately estimated.
 Applications of the group effects include (1) estimation and inference for meaningful group effects on the response variable, (2) testing for ""group significance"" of the 



q


{\displaystyle q}

 variables via testing 




H

0


:

ξ

A


=
0


{\displaystyle H_{0}:\xi _{A}=0}

 versus 




H

1


:

ξ

A


≠
0


{\displaystyle H_{1}:\xi _{A}\neq 0}

, and (3) characterizing the region of the predictor variable space over which predictions by the least squares estimated model are accurate.
 A group effect of the original variables 



{

x

1


,

x

2


,
…
,

x

q


}


{\displaystyle \{x_{1},x_{2},\dots ,x_{q}\}}

 can be expressed as a constant times a group effect of the standardized variables 



{

x

1

′

,

x

2

′

,
…
,

x

q

′

}


{\displaystyle \{x_{1}',x_{2}',\dots ,x_{q}'\}}

. The former is meaningful when the latter is. Thus meaningful group effects of the original variables can be found through meaningful group effects of the standardized variables.[12]
 In Dempster–Shafer theory, or a linear belief function in particular, a linear regression model may be represented as a partially swept matrix, which can be combined with similar matrices representing observations and other assumed normal distributions and state equations. The combination of swept or unswept matrices provides an alternative method for estimating linear regression models.
 A large number of procedures have been developed for parameter estimation and inference in linear regression. These methods differ in computational simplicity of algorithms, presence of a closed-form solution, robustness with respect to heavy-tailed distributions, and theoretical assumptions needed to validate desirable statistical properties such as consistency and asymptotic efficiency.
 Some of the more common estimation techniques for linear regression are summarized below.
 Assuming that the independent variables are 







x

i


→



=

[


x

1


i


,

x

2


i


,
…
,

x

m


i



]



{\displaystyle {\vec {x_{i}}}=\left[x_{1}^{i},x_{2}^{i},\ldots ,x_{m}^{i}\right]}

 and the model's parameters are 






β
→



=

[


β

0


,

β

1


,
…
,

β

m



]



{\displaystyle {\vec {\beta }}=\left[\beta _{0},\beta _{1},\ldots ,\beta _{m}\right]}

, then the model's prediction would be
 If 







x

i


→





{\displaystyle {\vec {x_{i}}}}

 is extended to 







x

i


→



=

[

1
,

x

1


i


,

x

2


i


,
…
,

x

m


i



]



{\displaystyle {\vec {x_{i}}}=\left[1,x_{1}^{i},x_{2}^{i},\ldots ,x_{m}^{i}\right]}

 then 




y

i




{\displaystyle y_{i}}

 would become a dot product of the parameter and the independent vectors, i.e.
 In the least-squares setting, the optimum parameter vector is defined as such that minimizes the sum of mean squared loss:
 Now putting the independent and dependent variables in matrices 



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

 respectively, the loss function can be rewritten as:
 As the loss function is convex, the optimum solution lies at gradient zero. The gradient of the loss function is (using Denominator layout convention):
 Setting the gradient to zero produces the optimum parameter:
 Note: The 






β
^





{\displaystyle {\hat {\beta }}}

 obtained may indeed be the local minimum, one needs to differentiate once more to obtain the Hessian matrix and show that it is positive definite. This is provided by the Gauss–Markov theorem.
 Linear least squares methods include mainly:
 Maximum likelihood estimation can be performed when the distribution of the error terms is known to belong to a certain parametric family ƒθ of probability distributions.[15] When fθ is a normal distribution with zero mean and variance θ, the resulting estimate is identical to the OLS estimate. GLS estimates are maximum likelihood estimates when ε follows a multivariate normal distribution with a known covariance matrix.
Let's denote each data point by 



(




x

i


→



,

y

i


)


{\displaystyle ({\vec {x_{i}}},y_{i})}

 and the regression parameters as 






β
→





{\displaystyle {\vec {\beta }}}

, and the set of all data by 



D


{\displaystyle D}

 and the cost function by 



L
(
D
,



β
→



)
=

∑

i


(

y

i


−



β
→




⋅





x

i


→




)

2




{\displaystyle L(D,{\vec {\beta }})=\sum _{i}(y_{i}-{\vec {\beta }}\,\cdot \,{\vec {x_{i}}})^{2}}

.
 As shown below the same optimal parameter that minimizes 



L
(
D
,



β
→



)


{\displaystyle L(D,{\vec {\beta }})}

 achieves maximum likelihood too.[16] Here the assumption is that the dependent variable 



y


{\displaystyle y}

 is a random variable that follows a Gaussian distribution, where the standard deviation is fixed and the mean is a linear combination of 






x
→





{\displaystyle {\vec {x}}}

:







H
(
D
,



β
→



)



=

∏

i
=
1


n


P
r
(

y

i



|





x

i


→





;



β
→



,
σ
)






=

∏

i
=
1


n




1



2
π


σ



exp
⁡

(

−




(


y

i


−



β
→




⋅





x

i


→




)


2



2

σ

2






)







{\displaystyle {\begin{aligned}H(D,{\vec {\beta }})&=\prod _{i=1}^{n}Pr(y_{i}|{\vec {x_{i}}}\,\,;{\vec {\beta }},\sigma )\\&=\prod _{i=1}^{n}{\frac {1}{{\sqrt {2\pi }}\sigma }}\exp \left(-{\frac {\left(y_{i}-{\vec {\beta }}\,\cdot \,{\vec {x_{i}}}\right)^{2}}{2\sigma ^{2}}}\right)\end{aligned}}}


 Now, we need to look for a parameter that maximizes this likelihood function. Since the logarithmic function is strictly increasing, instead of maximizing this function, we can also maximize its logarithm and find the optimal parameter that way.[16]
 







I
(
D
,



β
→



)



=
log
⁡

∏

i
=
1


n


P
r
(

y

i



|





x

i


→





;



β
→



,
σ
)






=
log
⁡

∏

i
=
1


n




1



2
π


σ



exp
⁡

(

−




(


y

i


−



β
→




⋅





x

i


→




)


2



2

σ

2






)







=
n
log
⁡


1



2
π


σ



−


1

2

σ

2






∑

i
=
1


n




(


y

i


−



β
→




⋅





x

i


→




)


2








{\displaystyle {\begin{aligned}I(D,{\vec {\beta }})&=\log \prod _{i=1}^{n}Pr(y_{i}|{\vec {x_{i}}}\,\,;{\vec {\beta }},\sigma )\\&=\log \prod _{i=1}^{n}{\frac {1}{{\sqrt {2\pi }}\sigma }}\exp \left(-{\frac {\left(y_{i}-{\vec {\beta }}\,\cdot \,{\vec {x_{i}}}\right)^{2}}{2\sigma ^{2}}}\right)\\&=n\log {\frac {1}{{\sqrt {2\pi }}\sigma }}-{\frac {1}{2\sigma ^{2}}}\sum _{i=1}^{n}\left(y_{i}-{\vec {\beta }}\,\cdot \,{\vec {x_{i}}}\right)^{2}\end{aligned}}}


 The optimal parameter is thus equal to:[16]
 










arg max



β
→





I
(
D
,



β
→



)



=



arg max



β
→





(

n
log
⁡


1



2
π


σ



−


1

2

σ

2






∑

i
=
1


n




(


y

i


−



β
→




⋅





x

i


→




)


2



)







=



arg min



β
→





∑

i
=
1


n




(


y

i


−



β
→




⋅





x

i


→




)


2








=



arg min



β
→





L
(
D
,



β
→



)






=





β
^


→









{\displaystyle {\begin{aligned}{\underset {\vec {\beta }}{\mbox{arg max}}}\,I(D,{\vec {\beta }})&={\underset {\vec {\beta }}{\mbox{arg max}}}\left(n\log {\frac {1}{{\sqrt {2\pi }}\sigma }}-{\frac {1}{2\sigma ^{2}}}\sum _{i=1}^{n}\left(y_{i}-{\vec {\beta }}\,\cdot \,{\vec {x_{i}}}\right)^{2}\right)\\&={\underset {\vec {\beta }}{\mbox{arg min}}}\sum _{i=1}^{n}\left(y_{i}-{\vec {\beta }}\,\cdot \,{\vec {x_{i}}}\right)^{2}\\&={\underset {\vec {\beta }}{\mbox{arg min}}}\,L(D,{\vec {\beta }})\\&={\vec {\hat {\beta }}}\end{aligned}}}


 In this way, the parameter that maximizes 



H
(
D
,



β
→



)


{\displaystyle H(D,{\vec {\beta }})}

 is the same as the one that minimizes 



L
(
D
,



β
→



)


{\displaystyle L(D,{\vec {\beta }})}

. This means that in linear regression, the result of the least squares method is the same as the result of the maximum likelihood estimation method.[16]
 Ridge regression[17][18][19] and other forms of penalized estimation, such as Lasso regression,[5] deliberately introduce bias into the estimation of β in order to reduce the variability of the estimate. The resulting estimates generally have lower mean squared error than the OLS estimates, particularly when multicollinearity is present or when overfitting is a problem. They are generally used when the goal is to predict the value of the response variable y for values of the predictors x that have not yet been observed. These methods are not as commonly used when the goal is inference, since it is difficult to account for the bias.
 Least absolute deviation (LAD) regression is a robust estimation technique in that it is less sensitive to the presence of outliers than OLS (but is less efficient than OLS when no outliers are present). It is equivalent to maximum likelihood estimation under a Laplace distribution model for ε.[20]
 If we assume that error terms are independent of the regressors, 




ε

i


⊥


x


i




{\displaystyle \varepsilon _{i}\perp \mathbf {x} _{i}}

, then the optimal estimator is the 2-step MLE, where the first step is used to non-parametrically estimate the distribution of the error term.[21]
 Linear regression is widely used in biological, behavioral and social sciences to describe possible relationships between variables. It ranks as one of the most important tools used in these disciplines.
 A trend line represents a trend, the long-term movement in time series data after other components have been accounted for. It tells whether a particular data set (say GDP, oil prices or stock prices) have increased or decreased over the period of time. A trend line could simply be drawn by eye through a set of data points, but more properly their position and slope is calculated using statistical techniques like linear regression. Trend lines typically are straight lines, although some variations use higher degree polynomials depending on the degree of curvature desired in the line.
 Trend lines are sometimes used in business analytics to show changes in data over time. This has the advantage of being simple. Trend lines are often used to argue that a particular action or event (such as training, or an advertising campaign) caused observed changes at a point in time. This is a simple technique, and does not require a control group, experimental design, or a sophisticated analysis technique. However, it suffers from a lack of scientific validity in cases where other potential changes can affect the data.
 Early evidence relating tobacco smoking to mortality and morbidity came from observational studies employing regression analysis. In order to reduce spurious correlations when analyzing observational data, researchers usually include several variables in their regression models in addition to the variable of primary interest. For example, in a regression model in which cigarette smoking is the independent variable of primary interest and the dependent variable is lifespan measured in years, researchers might include education and income as additional independent variables, to ensure that any observed effect of smoking on lifespan is not due to those other socio-economic factors. However, it is never possible to include all possible confounding variables in an empirical analysis. For example, a hypothetical gene might increase mortality and also cause people to smoke more. For this reason, randomized controlled trials are often able to generate more compelling evidence of causal relationships than can be obtained using regression analyses of observational data. When controlled experiments are not feasible, variants of regression analysis such as instrumental variables regression may be used to attempt to estimate causal relationships from observational data.
 The capital asset pricing model uses linear regression as well as the concept of beta for analyzing and quantifying the systematic risk of an investment. This comes directly from the beta coefficient of the linear regression model that relates the return on the investment to the return on all risky assets.
 Linear regression is the predominant empirical tool in economics. For example, it is used to predict consumption spending,[24] fixed investment spending, inventory investment, purchases of a country's exports,[25] spending on imports,[25] the demand to hold liquid assets,[26] labor demand,[27] and labor supply.[27]
 Linear regression finds application in a wide range of environmental science applications such as land use,[28] infectious diseases,[29] and air pollution.[30] For example, linear regression can be used to predict the changing effects of car pollution.[31] One notable example of this application in infectious diseases is the flattening the curve strategy emphasized early in the COVID-19 pandemic, where public health officials dealt with sparse data on infected individuals and sophisticated models of disease transmission to characterize the spread of COVID-19.[32]
 Linear regression is commonly used in building science field studies to derive characteristics of building occupants. In a thermal comfort field study, building scientists usually ask occupants' thermal sensation votes, which range from -3 (feeling cold) to 0 (neutral) to +3 (feeling hot), and measure occupants' surrounding temperature data. A neutral or comfort temperature can be calculated based on a linear regression between the thermal sensation vote and indoor temperature, and setting the thermal sensation vote as zero. However, there has been a debate on the regression direction: regressing thermal sensation votes (y-axis) against indoor temperature (x-axis) or the opposite: regressing indoor temperature (y-axis) against thermal sensation votes (x-axis).[33]
 Linear regression plays an important role in the subfield of artificial intelligence known as machine learning. The linear regression algorithm is one of the fundamental supervised machine-learning algorithms due to its relative simplicity and well-known properties.[34]
 Isaac Newton is credited with inventing ""a certain technique known today as linear regression analysis"" in his work on equinoxes in 1700, and wrote down the first of the two normal equations of the ordinary least squares method.[35][36] The Least squares linear regression, as a means of finding a good rough linear fit to a set of points was performed by Legendre (1805) and Gauss (1809) for the prediction of planetary movement. Quetelet was responsible for making the procedure well-known and for using it extensively in the social sciences.[37]
",statist linear regress model estim linear relationship scalar respons depend variabl one explanatori variabl regressor independ variabl model exactli one explanatori variabl simpl linear regress model two explanatori variabl multipl linear regress term distinct multivari linear regress predict multipl correl depend variabl rather singl depend variabl linear regress relationship model use linear predictor function whose unknown model paramet estim data commonli condit mean respons given valu explanatori variabl predictor assum affin function valu less commonli condit median quantil use like form regress analysi linear regress focus condit probabl distribut respons given valu predictor rather joint probabl distribut variabl domain multivari analysi linear regress also type machin learn algorithm specif supervis algorithm learn label dataset map data point optim linear function use predict new dataset linear regress first type regress analysi studi rigor use extens practic applic model depend linearli unknown paramet easier fit model relat paramet statist properti result estim easier determin linear regress mani practic use applic fall one follow two broad categori linear regress model often fit use least squar approach may also fit way minim lack fit norm least absolut deviat regress minim penal version least squar cost function ridg regress penalti lasso penalti use mean squar error mse cost dataset mani larg outlier result model fit outlier true data due higher import assign mse larg error cost function robust outlier use dataset mani larg outlier convers least squar approach use fit model linear model thu although term least squar linear model close link synonym given data set x x p n ip n n statist unit linear regress model assum relationship depend variabl vector regressor x linear relationship model disturb term error variabl unobserv random variabl add nois linear relationship depend variabl regressor thu model take form β β x β p x p ε x β ε n p ip x n denot transpos xitβ inner product vector xi often n equat stack togeth written matrix notat fit linear model given data set usual requir estim regress coeffici β error term ε x β x minim exampl common use sum squar error ε measur ε minim consid situat small ball toss air measur height ascent hi variou moment time ti physic tell us ignor drag relationship model determin initi veloc ball proport standard graviti εi due measur error linear regress use estim valu measur data model time variabl linear paramet take regressor xi ti model take standard form standard linear regress model standard estim techniqu make number assumpt predictor variabl respons variabl relationship numer extens develop allow assumpt relax reduc weaker form case elimin entir gener extens make estim procedur complex may also requir data order produc equal precis model citat need follow major assumpt made standard linear regress model standard estim techniqu ordinari least squar violat assumpt result bias estim β bias standard error untrustworthi confid interv signific test beyond assumpt sever statist properti data strongli influenc perform differ estim method fit linear regress model use identifi relationship singl predictor variabl xj respons variabl predictor variabl model held fix specif interpret βj expect chang chang xj covari held expect valu partial deriv respect xj sometim call uniqu effect xj contrast margin effect xj assess use correl coeffici simpl linear regress model relat xj effect total deriv respect xj care must taken interpret regress result regressor may allow margin chang dummi variabl intercept term other held fix recal exampl introduct would imposs hold ti fix time chang valu possibl uniqu effect nearli zero even margin effect larg may impli covari captur inform xj variabl model contribut xj variat convers uniqu effect xj larg margin effect nearli zero would happen covari explain great deal variat mainli explain variat way complementari captur xj case includ variabl model reduc part variabl unrel xj therebi strengthen appar relationship xj mean express held fix may depend valu predictor variabl aris experiment directli set valu predictor variabl accord studi design comparison interest may liter correspond comparison among unit whose predictor variabl held fix experiment altern express held fix refer select take place context data analysi case hold variabl fix restrict attent subset data happen common valu given predictor variabl interpret held fix use observ studi notion uniqu effect appeal studi complex system multipl interrel compon influenc respons variabl case liter interpret causal effect intervent link valu predictor variabl howev argu mani case multipl regress analysi fail clarifi relationship predictor variabl respons variabl predictor correl assign follow studi design numer extens linear regress develop allow assumpt underli basic model relax simplest case singl scalar predictor variabl x singl scalar respons variabl known simpl linear regress extens multipl predictor variabl denot capit x known multipl linear regress also known multivari linear regress confus multivari linear regress multipl linear regress gener simpl linear regress case one independ variabl special case gener linear model restrict one depend variabl basic model multipl linear regress observ n n formula consid n observ one depend variabl p independ variabl thu yi ith observ depend variabl xij ith observ jth independ variabl j valu βj repres paramet estim εi ith independ ident distribut normal error gener multivari linear regress one equat form depend variabl share set explanatori variabl henc estim simultan observ index n depend variabl index j nearli regress model involv multipl predictor basic descript linear regress often phrase term multipl regress model note howev case respons variabl still scalar anoth term multivari linear regress refer case vector gener linear regress gener linear model consid situat respons variabl scalar observ vector yi condit linear e x x b e x x b still assum matrix b replac vector β classic linear regress model multivari analogu ordinari least squar ol gener least squar gl develop gener linear model also call multivari linear model multivari linear model also call multipl linear model variou model creat allow heteroscedast error differ respons variabl may differ varianc exampl weight least squar method estim linear regress model respons variabl may differ error varianc possibl correl error see also weight linear least squar gener least squar standard error improv method use uncorrel potenti heteroscedast error gener linear model glm framework model respons variabl bound discret use exampl gener linear model allow arbitrari link function g relat mean respons variabl predictor e g x b e xb link function often relat distribut respons particular typic effect transform rang linear predictor rang respons variabl common exampl glm singl index model clarif need allow degre nonlinear relationship x preserv central role linear predictor classic linear regress model certain condit simpli appli ol data model consist estim β proportion constant hierarch linear model multilevel regress organ data hierarchi regress exampl regress b b regress often use variabl interest natur hierarch structur educ statist student nest classroom classroom nest school school nest administr group school district respons variabl might measur student achiev test score differ covari would collect classroom school school district level model measur error model extend tradit linear regress model allow predictor variabl x observ error error caus standard estim β becom bias gener form bia attenu mean effect bias toward zero multipl linear regress model paramet β j j predictor variabl x j j repres individu effect x j j interpret expect chang respons variabl x j j increas one unit predictor variabl held constant x j j strongli correl predictor variabl improb x j j increas one unit variabl held constant case interpret β j j becom problemat base improb condit effect x j j evalu isol group predictor variabl say x x x q q group effect ξ w w defin linear combin paramet w w w w q w q weight vector satisfi j q w j q j constraint w j j ξ w w also refer normal group effect group effect ξ w w interpret expect chang variabl group x x x q q chang amount w w w q q respect time variabl group held constant gener individu effect variabl group variabl q group effect reduc individu effect ii w w j j j group effect also reduc individu effect group effect ξ w w said meaning underli simultan chang q q variabl x x x q q probabl group effect provid mean studi collect impact strongli correl predictor variabl linear regress model individu effect variabl paramet good interpret furthermor sampl size larg none paramet accur estim least squar regress due multicollinear problem nevertheless meaning group effect good interpret accur estim least squar regress simpl way identifi meaning group effect use posit correl apc arrang strongli correl variabl pairwis correl among variabl posit standard p p predictor variabl model mean zero length one illustr suppos x x x q q group strongli correl variabl apc arrang strongli correl predictor variabl outsid group let centr x j j standard x j j standard linear regress model paramet β j j origin model includ β simpl function β j j standard model standard variabl chang correl x x x q q group strongli correl variabl apc arrang strongli correl predictor variabl standard model group effect x x x q q unbias linear estim β j j least squar estim β j j particular averag group effect q q standard variabl interpret expect chang x j j strongli correl group increas q th unit time variabl outsid group held constant strong posit correl standard unit variabl group approxim equal like increas time similar amount thu averag group effect ξ meaning effect accur estim unbias linear estim ξ q β β β q q q even individu none β j j accur estim β j j group effect meaning accur estim exampl β special group effect weight w w j j j accur estim β also meaning effect gener group q q strongli correl predictor variabl apc arrang standard model group effect whose weight vector w w near centr simplex j q w j q j w j j meaning accur estim unbias linear estim effect weight vector far away centr meaning weight vector repres simultan chang variabl violat strong posit correl standard variabl apc arrang probabl effect also accur estim applic group effect includ estim infer meaning group effect respons variabl test group signific q q variabl via test h ξ versu h ξ character region predictor variabl space predict least squar estim model accur group effect origin variabl x x x q q express constant time group effect standard variabl x x x q q former meaning latter thu meaning group effect origin variabl found meaning group effect standard variabl theori linear belief function particular linear regress model may repres partial swept matrix combin similar matric repres observ assum normal distribut state equat combin swept unswept matric provid altern method estim linear regress model larg number procedur develop paramet estim infer linear regress method differ comput simplic algorithm presenc solut robust respect distribut theoret assumpt need valid desir statist properti consist asymptot effici common estim techniqu linear regress summar assum independ variabl x x x x model paramet β β β β model predict would x extend x x x x would becom dot product paramet independ vector set optimum paramet vector defin minim sum mean squar loss put independ depend variabl matric x x respect loss function rewritten loss function convex optimum solut lie gradient zero gradient loss function use denomin layout convent set gradient zero produc optimum paramet note β obtain may inde local minimum one need differenti obtain hessian matrix show posit definit provid theorem linear least squar method includ mainli maximum likelihood estim perform distribut error term known belong certain parametr famili ƒθ probabl distribut fθ normal distribut zero mean varianc θ result estim ident ol estim gl estim maximum likelihood estim ε follow multivari normal distribut known covari matrix let denot data point x regress paramet β set data cost function l β β x l shown optim paramet minim l β l achiev maximum likelihood assumpt depend variabl random variabl follow gaussian distribut standard deviat fix mean linear combin x x h β n p r x β σ n π σ exp β x σ align h n pr n align need look paramet maxim likelihood function sinc logarithm function strictli increas instead maxim function also maxim logarithm find optim paramet way β log n p r x β σ log n π σ exp β x σ n log π σ σ n β x align n pr n n align optim paramet thu equal arg max β β arg max β n log π σ σ n β x arg min β n β x arg min β l β β align arg max arg max n arg min n arg min l align way paramet maxim h β h one minim l β l mean linear regress result least squar method result maximum likelihood estim method ridg regress form penal estim lasso regress deliber introduc bia estim β order reduc variabl estim result estim gener lower mean squar error ol estim particularli multicollinear present overfit problem gener use goal predict valu respons variabl valu predictor x yet observ method commonli use goal infer sinc difficult account bia least absolut deviat lad regress robust estim techniqu less sensit presenc outlier ol less effici ol outlier present equival maximum likelihood estim laplac distribut model ε assum error term independ regressor ε x x optim estim mle first step use estim distribut error term linear regress wide use biolog behavior social scienc describ possibl relationship variabl rank one import tool use disciplin trend line repres trend movement time seri data compon account tell whether particular data set say gdp oil price stock price increas decreas period time trend line could simpli drawn eye set data point properli posit slope calcul use statist techniqu like linear regress trend line typic straight line although variat use higher degre polynomi depend degre curvatur desir line trend line sometim use busi analyt show chang data time advantag simpl trend line often use argu particular action event train advertis campaign caus observ chang point time simpl techniqu requir control group experiment design sophist analysi techniqu howev suffer lack scientif valid case potenti chang affect data earli evid relat tobacco smoke mortal morbid came observ studi employ regress analysi order reduc spuriou correl analyz observ data research usual includ sever variabl regress model addit variabl primari interest exampl regress model cigarett smoke independ variabl primari interest depend variabl lifespan measur year research might includ educ incom addit independ variabl ensur observ effect smoke lifespan due factor howev never possibl includ possibl confound variabl empir analysi exampl hypothet gene might increas mortal also caus peopl smoke reason random control trial often abl gener compel evid causal relationship obtain use regress analys observ data control experi feasibl variant regress analysi instrument variabl regress may use attempt estim causal relationship observ data capit asset price model use linear regress well concept beta analyz quantifi systemat risk invest come directli beta coeffici linear regress model relat return invest return riski asset linear regress predomin empir tool econom exampl use predict consumpt spend fix invest spend inventori invest purchas countri export spend import demand hold liquid asset labor demand labor suppli linear regress find applic wide rang environment scienc applic land use infecti diseas air pollut exampl linear regress use predict chang effect car pollut one notabl exampl applic infecti diseas flatten curv strategi emphas earli pandem public health offici dealt spars data infect individu sophist model diseas transmiss character spread linear regress commonli use build scienc field studi deriv characterist build occup thermal comfort field studi build scientist usual ask occup thermal sensat vote rang feel cold neutral feel hot measur occup surround temperatur data neutral comfort temperatur calcul base linear regress thermal sensat vote indoor temperatur set thermal sensat vote zero howev debat regress direct regress thermal sensat vote indoor temperatur opposit regress indoor temperatur thermal sensat vote linear regress play import role subfield artifici intellig known machin learn linear regress algorithm one fundament supervis algorithm due rel simplic properti isaac newton credit invent certain techniqu known today linear regress analysi work equinox wrote first two normal equat ordinari least squar method least squar linear regress mean find good rough linear fit set point perform legendr gauss predict planetari movement quetelet respons make procedur use extens social scienc
Naive Bayes classifier,https://en.wikipedia.org/wiki/Naive_Bayes_classifier,"In statistics, naive Bayes classifiers are a family of linear ""probabilistic classifiers"" which assumes that the features are conditionally independent, given the target class. The strength (naivety) of this assumption is what gives the classifier its name. These classifiers are among the simplest Bayesian network models.[1]
 Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression,[2]: 718  which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.
 In the statistics literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes.[3] All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method.[2][3]
 Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter.  A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features.
 In many practical applications, parameter estimation for naive Bayes models uses the method of maximum likelihood; in other words, one can work with the naive Bayes model without accepting Bayesian probability or using any Bayesian methods.
 Despite their naive design and apparently oversimplified assumptions, naive Bayes classifiers have worked quite well in many complex real-world situations. In 2004, an analysis of the Bayesian classification problem showed that there are sound theoretical reasons for the apparently implausible efficacy of naive Bayes classifiers.[4] Still, a comprehensive comparison with other classification algorithms in 2006 showed that Bayes classification is outperformed by other approaches, such as boosted trees or random forests.[5]
 An advantage of naive Bayes is that it only requires a small amount of training data to estimate the parameters necessary for classification.[6]
 Abstractly, naive Bayes is a conditional probability model: it assigns probabilities 



p
(

C

k


∣

x

1


,
…
,

x

n


)


{\displaystyle p(C_{k}\mid x_{1},\ldots ,x_{n})}

 for each of the K possible outcomes or classes 




C

k




{\displaystyle C_{k}}

 given a problem instance to be classified, represented by a vector 




x

=
(

x

1


,
…
,

x

n


)


{\displaystyle \mathbf {x} =(x_{1},\ldots ,x_{n})}

 encoding some n features (independent variables).[7]
 The problem with the above formulation is that if the number of features n is large or if a feature can take on a large number of values, then basing such a model on probability tables is infeasible. The model must therefore be reformulated to make it more tractable. Using Bayes' theorem, the conditional probability can be decomposed as:




p
(

C

k


∣

x

)
=



p
(

C

k


)
 
p
(

x

∣

C

k


)


p
(

x

)






{\displaystyle p(C_{k}\mid \mathbf {x} )={\frac {p(C_{k})\ p(\mathbf {x} \mid C_{k})}{p(\mathbf {x} )}}\,}


 In plain English, using Bayesian probability terminology, the above equation can be written as





posterior

=




prior

×

likelihood


evidence





{\displaystyle {\text{posterior}}={\frac {{\text{prior}}\times {\text{likelihood}}}{\text{evidence}}}\,}


 In practice, there is interest only in the numerator of that fraction, because the denominator does not depend on 



C


{\displaystyle C}

 and the values of the features 




x

i




{\displaystyle x_{i}}

 are given, so that the denominator is effectively constant.
The numerator is equivalent to the joint probability model




p
(

C

k


,

x

1


,
…
,

x

n


)



{\displaystyle p(C_{k},x_{1},\ldots ,x_{n})\,}


which can be rewritten as follows, using the chain rule for repeated applications of the definition of conditional probability:








p
(

C

k


,

x

1


,
…
,

x

n


)



=
p
(

x

1


,
…
,

x

n


,

C

k


)






=
p
(

x

1


∣

x

2


,
…
,

x

n


,

C

k


)
 
p
(

x

2


,
…
,

x

n


,

C

k


)






=
p
(

x

1


∣

x

2


,
…
,

x

n


,

C

k


)
 
p
(

x

2


∣

x

3


,
…
,

x

n


,

C

k


)
 
p
(

x

3


,
…
,

x

n


,

C

k


)






=
⋯






=
p
(

x

1


∣

x

2


,
…
,

x

n


,

C

k


)
 
p
(

x

2


∣

x

3


,
…
,

x

n


,

C

k


)
⋯
p
(

x

n
−
1


∣

x

n


,

C

k


)
 
p
(

x

n


∣

C

k


)
 
p
(

C

k


)






{\displaystyle {\begin{aligned}p(C_{k},x_{1},\ldots ,x_{n})&=p(x_{1},\ldots ,x_{n},C_{k})\\&=p(x_{1}\mid x_{2},\ldots ,x_{n},C_{k})\ p(x_{2},\ldots ,x_{n},C_{k})\\&=p(x_{1}\mid x_{2},\ldots ,x_{n},C_{k})\ p(x_{2}\mid x_{3},\ldots ,x_{n},C_{k})\ p(x_{3},\ldots ,x_{n},C_{k})\\&=\cdots \\&=p(x_{1}\mid x_{2},\ldots ,x_{n},C_{k})\ p(x_{2}\mid x_{3},\ldots ,x_{n},C_{k})\cdots p(x_{n-1}\mid x_{n},C_{k})\ p(x_{n}\mid C_{k})\ p(C_{k})\\\end{aligned}}}


 Now the ""naive"" conditional independence assumptions come into play: assume that all features in 




x



{\displaystyle \mathbf {x} }

 are mutually independent, conditional on the category 




C

k




{\displaystyle C_{k}}

. Under this assumption,




p
(

x

i


∣

x

i
+
1


,
…
,

x

n


,

C

k


)
=
p
(

x

i


∣

C

k


)

.


{\displaystyle p(x_{i}\mid x_{i+1},\ldots ,x_{n},C_{k})=p(x_{i}\mid C_{k})\,.}


 Thus, the joint model can be expressed as








p
(

C

k


∣

x

1


,
…
,

x

n


)
∝
 


p
(

C

k


,

x

1


,
…
,

x

n


)






=
p
(

C

k


)
 
p
(

x

1


∣

C

k


)
 
p
(

x

2


∣

C

k


)
 
p
(

x

3


∣

C

k


)
 
⋯






=
p
(

C

k


)

∏

i
=
1


n


p
(

x

i


∣

C

k


)

,






{\displaystyle {\begin{aligned}p(C_{k}\mid x_{1},\ldots ,x_{n})\varpropto \ &p(C_{k},x_{1},\ldots ,x_{n})\\&=p(C_{k})\ p(x_{1}\mid C_{k})\ p(x_{2}\mid C_{k})\ p(x_{3}\mid C_{k})\ \cdots \\&=p(C_{k})\prod _{i=1}^{n}p(x_{i}\mid C_{k})\,,\end{aligned}}}


where 



∝


{\displaystyle \varpropto }

 denotes proportionality since the denominator 



p
(

x

)


{\displaystyle p(\mathbf {x} )}

 is omitted.
 This means that under the above independence assumptions, the conditional distribution over the class variable 



C


{\displaystyle C}

 is:




p
(

C

k


∣

x

1


,
…
,

x

n


)
=


1
Z


 
p
(

C

k


)

∏

i
=
1


n


p
(

x

i


∣

C

k


)


{\displaystyle p(C_{k}\mid x_{1},\ldots ,x_{n})={\frac {1}{Z}}\ p(C_{k})\prod _{i=1}^{n}p(x_{i}\mid C_{k})}


where the evidence 



Z
=
p
(

x

)
=

∑

k


p
(

C

k


)
 
p
(

x

∣

C

k


)


{\displaystyle Z=p(\mathbf {x} )=\sum _{k}p(C_{k})\ p(\mathbf {x} \mid C_{k})}

 is a scaling factor dependent only on 




x

1


,
…
,

x

n




{\displaystyle x_{1},\ldots ,x_{n}}

, that is, a constant if the values of the feature variables are known.
 The discussion so far has derived the independent feature model, that is, the naive Bayes probability model. The naive Bayes classifier combines this model with a decision rule. One common rule is to pick the hypothesis that is most probable so as to minimize the probability of misclassification; this is known as the maximum a posteriori or MAP decision rule. The corresponding classifier, a Bayes classifier, is the function that assigns a class label 






y
^



=

C

k




{\displaystyle {\hat {y}}=C_{k}}

 for some k as follows:







y
^



=


argmax

k
∈
{
1
,
…
,
K
}



 
p
(

C

k


)


∏

i
=
1


n


p
(

x

i


∣

C

k


)
.



{\displaystyle {\hat {y}}={\underset {k\in \{1,\ldots ,K\}}{\operatorname {argmax} }}\ p(C_{k})\displaystyle \prod _{i=1}^{n}p(x_{i}\mid C_{k}).}


 A class's prior may be calculated by assuming equiprobable classes, i.e., 



p
(

C

k


)
=


1
K




{\displaystyle p(C_{k})={\frac {1}{K}}}

, or by calculating an estimate for the class probability from the training set:





prior for a given class

=


no. of samples in that class
total no. of samples





{\displaystyle {\text{prior for a given class}}={\frac {\text{no. of samples in that class}}{\text{total no. of samples}}}\,}


To estimate the parameters for a feature's distribution, one must assume a distribution or generate nonparametric models for the features from the training set.[8]
 The assumptions on distributions of features are called the ""event model"" of the naive Bayes classifier. For discrete features like the ones encountered in document classification (include spam filtering), multinomial and Bernoulli distributions are popular. These assumptions lead to two distinct models, which are often confused.[9][10]
 When dealing with continuous data, a typical assumption is that the continuous values associated with each class are distributed according to a normal (or Gaussian) distribution. For example, suppose the training data contains a continuous attribute, 



x


{\displaystyle x}

. The data is first segmented by the class, and then the mean and variance of 



x


{\displaystyle x}

 is computed in each class. Let 




μ

k




{\displaystyle \mu _{k}}

 be the mean of the values in 



x


{\displaystyle x}

 associated with class 




C

k




{\displaystyle C_{k}}

, and let 




σ

k


2




{\displaystyle \sigma _{k}^{2}}

 be the Bessel corrected variance of the values in 



x


{\displaystyle x}

 associated with class 




C

k




{\displaystyle C_{k}}

. Suppose one has collected some observation value 



v


{\displaystyle v}

. Then, the probability density of 



v


{\displaystyle v}

 given a class 




C

k




{\displaystyle C_{k}}

, i.e., 



p
(
x
=
v
∣

C

k


)


{\displaystyle p(x=v\mid C_{k})}

, can be computed by plugging 



v


{\displaystyle v}

 into the equation for a normal distribution parameterized by 




μ

k




{\displaystyle \mu _{k}}

 and 




σ

k


2




{\displaystyle \sigma _{k}^{2}}

. Formally,




p
(
x
=
v
∣

C

k


)
=


1

2
π

σ

k


2







e

−



(
v
−

μ

k



)

2




2

σ

k


2









{\displaystyle p(x=v\mid C_{k})={\frac {1}{\sqrt {2\pi \sigma _{k}^{2}}}}\,e^{-{\frac {(v-\mu _{k})^{2}}{2\sigma _{k}^{2}}}}}


 Another common technique for handling continuous values is to use binning to discretize the feature values and obtain a new set of Bernoulli-distributed features. Some literature suggests that this is required in order to use naive Bayes, but it is not true, as the discretization may throw away discriminative information.[3]
 Sometimes the distribution of class-conditional marginal densities is far from normal. In these cases, kernel density estimation can be used for a more realistic estimate of the marginal densities of each class. This method, which was introduced by John and Langley,[8] can boost the accuracy of the classifier considerably.[11][12]
 With a multinomial event model, samples (feature vectors) represent the frequencies with which certain events have been generated by a multinomial 



(

p

1


,
…
,

p

n


)


{\displaystyle (p_{1},\dots ,p_{n})}

 where 




p

i




{\displaystyle p_{i}}

 is the probability that event i occurs (or K such multinomials in the multiclass case). A feature vector 




x

=
(

x

1


,
…
,

x

n


)


{\displaystyle \mathbf {x} =(x_{1},\dots ,x_{n})}

 is then a histogram, with 




x

i




{\displaystyle x_{i}}

 counting the number of times event i was observed in a particular instance. This is the event model typically used for document classification, with events representing the occurrence of a word in a single document (see bag of words assumption).[13] The likelihood of observing a histogram x is given by:




p
(

x

∣

C

k


)
=



(

∑

i
=
1


n



x

i


)
!



∏

i
=
1


n



x

i


!




∏

i
=
1


n





p

k
i





x

i






{\displaystyle p(\mathbf {x} \mid C_{k})={\frac {(\sum _{i=1}^{n}x_{i})!}{\prod _{i=1}^{n}x_{i}!}}\prod _{i=1}^{n}{p_{ki}}^{x_{i}}}


where 




p

k
i


:=
p
(
i
∣

C

k


)


{\displaystyle p_{ki}:=p(i\mid C_{k})}

. 
 The multinomial naive Bayes classifier becomes a linear classifier when expressed in log-space:[14]








log
⁡
p
(

C

k


∣

x

)



∝
log
⁡

(

p
(

C

k


)

∏

i
=
1


n





p

k
i





x

i





)







=
log
⁡
p
(

C

k


)
+

∑

i
=
1


n



x

i


⋅
log
⁡

p

k
i








=
b
+


w


k


⊤



x







{\displaystyle {\begin{aligned}\log p(C_{k}\mid \mathbf {x} )&\varpropto \log \left(p(C_{k})\prod _{i=1}^{n}{p_{ki}}^{x_{i}}\right)\\&=\log p(C_{k})+\sum _{i=1}^{n}x_{i}\cdot \log p_{ki}\\&=b+\mathbf {w} _{k}^{\top }\mathbf {x} \end{aligned}}}


where 



b
=
log
⁡
p
(

C

k


)


{\displaystyle b=\log p(C_{k})}

 and 




w

k
i


=
log
⁡

p

k
i




{\displaystyle w_{ki}=\log p_{ki}}

. Estimating the parameters in log space is advantageous since multiplying a large number of small values can lead to significant rounding error. Applying a log transform reduces the effect of this rounding error.
 If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero, because the probability estimate is directly proportional to the number of occurrences of a feature's value. This is problematic because it will wipe out all information in the other probabilities when they are multiplied. Therefore, it is often desirable to incorporate a small-sample correction, called pseudocount, in all probability estimates such that no probability is ever set to be exactly zero. This way of regularizing naive Bayes is called Laplace smoothing when the pseudocount is one, and Lidstone smoothing in the general case.
 Rennie et al. discuss problems with the multinomial assumption in the context of document classification and possible ways to alleviate those problems, including the use of tf–idf weights instead of raw term frequencies and document length normalization, to produce a naive Bayes classifier that is competitive with support vector machines.[14]
 In the multivariate Bernoulli event model, features are independent Boolean variables (binary variables) describing inputs. Like the multinomial model, this model is popular for document classification tasks,[9] where binary term occurrence features are used rather than term frequencies. If 




x

i




{\displaystyle x_{i}}

 is a Boolean expressing the occurrence or absence of the i'th term from the vocabulary, then the likelihood of a document given a class 




C

k




{\displaystyle C_{k}}

 is given by:[9]




p
(

x

∣

C

k


)
=

∏

i
=
1


n



p

k
i



x

i




(
1
−

p

k
i



)

(
1
−

x

i


)




{\displaystyle p(\mathbf {x} \mid C_{k})=\prod _{i=1}^{n}p_{ki}^{x_{i}}(1-p_{ki})^{(1-x_{i})}}


where 




p

k
i




{\displaystyle p_{ki}}

 is the probability of class 




C

k




{\displaystyle C_{k}}

 generating the term 




x

i




{\displaystyle x_{i}}

. This event model is especially popular for classifying short texts. It has the benefit of explicitly modelling the absence of terms. Note that a naive Bayes classifier with a Bernoulli event model is not the same as a multinomial NB classifier with frequency counts truncated to one.
 Given a way to train a naive Bayes classifier from labeled data, it's possible to construct a semi-supervised training algorithm that can learn from a combination of labeled and unlabeled data by running the supervised learning algorithm in a loop:[15]
 Convergence is determined based on improvement to the model likelihood 



P
(
D
∣
θ
)


{\displaystyle P(D\mid \theta )}

, where 



θ


{\displaystyle \theta }

 denotes the parameters of the naive Bayes model.
 This training algorithm is an instance of the more general expectation–maximization algorithm (EM): the prediction step inside the loop is the E-step of EM, while the re-training of naive Bayes is the M-step. The algorithm is formally justified by the assumption that the data are generated by a mixture model, and the components of this mixture model are exactly the classes of the classification problem.[15]
 Despite the fact that the far-reaching independence assumptions are often inaccurate, the naive Bayes classifier has several properties that make it surprisingly useful in practice. In particular, the decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one-dimensional distribution. This helps alleviate problems stemming from the curse of dimensionality, such as the need for data sets that scale exponentially with the number of features. While naive Bayes often fails to produce a good estimate for the correct class probabilities,[16] this may not be a requirement for many applications. For example, the naive Bayes classifier will make the correct MAP decision rule classification so long as the correct class is predicted as more probable than any other class. This is true regardless of whether the probability estimate is slightly, or even grossly inaccurate. In this manner, the overall classifier can be robust enough to ignore serious deficiencies in its underlying naive probability model.[17] Other reasons for the observed success of the naive Bayes classifier are discussed in the literature cited below.
 In the case of discrete inputs (indicator or frequency features for discrete events), naive Bayes classifiers form a generative-discriminative pair with multinomial logistic regression classifiers: each naive Bayes classifier can be considered a way of fitting a probability model that optimizes the joint likelihood 



p
(
C
,

x

)


{\displaystyle p(C,\mathbf {x} )}

, while logistic regression fits the same probability model to optimize the conditional 



p
(
C
∣

x

)


{\displaystyle p(C\mid \mathbf {x} )}

.[18]
 More formally, we have the following:
 Theorem — Naive Bayes classifiers on binary features are subsumed by logistic regression classifiers.
 Consider a generic multiclass classification problem, with possible classes 



Y
∈
{
1
,
.
.
.
,
n
}


{\displaystyle Y\in \{1,...,n\}}

, then the (non-naive) Bayes classifier gives, by Bayes theorem:




p
(
Y
∣
X
=
x
)
=

softmax

(
{
ln
⁡
p
(
Y
=
k
)
+
ln
⁡
p
(
X
=
x
∣
Y
=
k
)

}

k


)


{\displaystyle p(Y\mid X=x)={\text{softmax}}(\{\ln p(Y=k)+\ln p(X=x\mid Y=k)\}_{k})}


 The naive Bayes classifier gives  





softmax


(


{

ln
⁡
p
(
Y
=
k
)
+


1
2



∑

i


(

a

i
,
k


+


−

a

i
,
k


−


)

x

i


+
(

a

i
,
k


+


+

a

i
,
k


−


)

}


k


)



{\displaystyle {\text{softmax}}\left(\left\{\ln p(Y=k)+{\frac {1}{2}}\sum _{i}(a_{i,k}^{+}-a_{i,k}^{-})x_{i}+(a_{i,k}^{+}+a_{i,k}^{-})\right\}_{k}\right)}


where   





a

i
,
s


+


=
ln
⁡
p
(

X

i


=
+
1
∣
Y
=
s
)
;


a

i
,
s


−


=
ln
⁡
p
(

X

i


=
−
1
∣
Y
=
s
)


{\displaystyle a_{i,s}^{+}=\ln p(X_{i}=+1\mid Y=s);\quad a_{i,s}^{-}=\ln p(X_{i}=-1\mid Y=s)}


 This is exactly a logistic regression classifier.
 The link between the two can be seen by observing that the decision function for naive Bayes (in the binary case) can be rewritten as ""predict class 




C

1




{\displaystyle C_{1}}

 if the odds of 



p
(

C

1


∣

x

)


{\displaystyle p(C_{1}\mid \mathbf {x} )}

 exceed those of 



p
(

C

2


∣

x

)


{\displaystyle p(C_{2}\mid \mathbf {x} )}

"". Expressing this in log-space gives:




log
⁡



p
(

C

1


∣

x

)


p
(

C

2


∣

x

)



=
log
⁡
p
(

C

1


∣

x

)
−
log
⁡
p
(

C

2


∣

x

)
>
0


{\displaystyle \log {\frac {p(C_{1}\mid \mathbf {x} )}{p(C_{2}\mid \mathbf {x} )}}=\log p(C_{1}\mid \mathbf {x} )-\log p(C_{2}\mid \mathbf {x} )>0}


 The left-hand side of this equation is the log-odds, or logit, the quantity predicted by the linear model that underlies logistic regression. Since naive Bayes is also a linear model for the two ""discrete"" event models, it can be reparametrised as a linear function 



b
+


w


⊤


x
>
0


{\displaystyle b+\mathbf {w} ^{\top }x>0}

. Obtaining the probabilities is then a matter of applying the logistic function to 



b
+


w


⊤


x


{\displaystyle b+\mathbf {w} ^{\top }x}

, or in the multiclass case, the softmax function.
 Discriminative classifiers have lower asymptotic error than generative ones; however, research by Ng and Jordan has shown that in some practical cases naive Bayes can outperform logistic regression because it reaches its asymptotic error faster.[18]
 Problem: classify whether a given person is a male or a female based on the measured features.
The features include height, weight, and foot size. Although with NB classifier we treat them as independent, they are not in reality.
 Example training set below.
 The classifier created from the training set using a Gaussian distribution assumption would be (given variances are unbiased sample variances):
 The following example assumes equiprobable classes so that P(male)= P(female) = 0.5. This prior probability distribution might be based on prior knowledge of frequencies in the larger population or in the training set.
 Below is a sample to be classified as male or female.
 In order to classify the sample, one has to determine which posterior is greater, male or female. For the classification as male the posterior is given by





posterior (male)

=



P
(

male

)

p
(

height

∣

male

)

p
(

weight

∣

male

)

p
(

foot size

∣

male

)

evidence




{\displaystyle {\text{posterior (male)}}={\frac {P({\text{male}})\,p({\text{height}}\mid {\text{male}})\,p({\text{weight}}\mid {\text{male}})\,p({\text{foot size}}\mid {\text{male}})}{\text{evidence}}}}


 For the classification as female the posterior is given by





posterior (female)

=



P
(

female

)

p
(

height

∣

female

)

p
(

weight

∣

female

)

p
(

foot size

∣

female

)

evidence




{\displaystyle {\text{posterior (female)}}={\frac {P({\text{female}})\,p({\text{height}}\mid {\text{female}})\,p({\text{weight}}\mid {\text{female}})\,p({\text{foot size}}\mid {\text{female}})}{\text{evidence}}}}


 The evidence (also termed normalizing constant) may be calculated:









evidence

=
P
(

male

)

p
(

height

∣

male

)

p
(

weight

∣

male

)

p
(

foot size

∣

male

)




+
P
(

female

)

p
(

height

∣

female

)

p
(

weight

∣

female

)

p
(

foot size

∣

female

)






{\displaystyle {\begin{aligned}{\text{evidence}}=P({\text{male}})\,p({\text{height}}\mid {\text{male}})\,p({\text{weight}}\mid {\text{male}})\,p({\text{foot size}}\mid {\text{male}})\\+P({\text{female}})\,p({\text{height}}\mid {\text{female}})\,p({\text{weight}}\mid {\text{female}})\,p({\text{foot size}}\mid {\text{female}})\end{aligned}}}


 However, given the sample, the evidence is a constant and thus scales both posteriors equally. It therefore does not affect classification and can be ignored.  The probability distribution for the sex of the sample can now be determined:




P
(

male

)
=
0.5


{\displaystyle P({\text{male}})=0.5}






p
(

height

∣

male

)
=


1

2
π

σ

2





exp
⁡

(



−
(
6
−
μ

)

2




2

σ

2





)

≈
1.5789
,


{\displaystyle p({\text{height}}\mid {\text{male}})={\frac {1}{\sqrt {2\pi \sigma ^{2}}}}\exp \left({\frac {-(6-\mu )^{2}}{2\sigma ^{2}}}\right)\approx 1.5789,}


where 



μ
=
5.855


{\displaystyle \mu =5.855}

 and 




σ

2


=
3.5033
⋅

10

−
2




{\displaystyle \sigma ^{2}=3.5033\cdot 10^{-2}}

 are the parameters of normal distribution which have been previously determined from the training set. Note that a value greater than 1 is OK here – it is a probability density rather than a probability, because height is a continuous variable.
 



p
(

weight

∣

male

)
=


1

2
π

σ

2





exp
⁡

(



−
(
130
−
μ

)

2




2

σ

2





)

=
5.9881
⋅

10

−
6




{\displaystyle p({\text{weight}}\mid {\text{male}})={\frac {1}{\sqrt {2\pi \sigma ^{2}}}}\exp \left({\frac {-(130-\mu )^{2}}{2\sigma ^{2}}}\right)=5.9881\cdot 10^{-6}}






p
(

foot size

∣

male

)
=


1

2
π

σ

2





exp
⁡

(



−
(
8
−
μ

)

2




2

σ

2





)

=
1.3112
⋅

10

−
3




{\displaystyle p({\text{foot size}}\mid {\text{male}})={\frac {1}{\sqrt {2\pi \sigma ^{2}}}}\exp \left({\frac {-(8-\mu )^{2}}{2\sigma ^{2}}}\right)=1.3112\cdot 10^{-3}}







posterior numerator (male)

=

their product

=
6.1984
⋅

10

−
9




{\displaystyle {\text{posterior numerator (male)}}={\text{their product}}=6.1984\cdot 10^{-9}}


 



P
(

female

)
=
0.5


{\displaystyle P({\text{female}})=0.5}






p
(

height

∣

female

)
=
2.23
⋅

10

−
1




{\displaystyle p({\text{height}}\mid {\text{female}})=2.23\cdot 10^{-1}}






p
(

weight

∣

female

)
=
1.6789
⋅

10

−
2




{\displaystyle p({\text{weight}}\mid {\text{female}})=1.6789\cdot 10^{-2}}






p
(

foot size

∣

female

)
=
2.8669
⋅

10

−
1




{\displaystyle p({\text{foot size}}\mid {\text{female}})=2.8669\cdot 10^{-1}}







posterior numerator (female)

=

their product

=
5.3778
⋅

10

−
4




{\displaystyle {\text{posterior numerator (female)}}={\text{their product}}=5.3778\cdot 10^{-4}}


 Since posterior numerator is greater in the female case, the prediction is that the sample is female.
 Here is a worked example of naive Bayesian classification to the document classification problem.
Consider the problem of classifying documents by their content, for example into spam and non-spam e-mails. Imagine that documents are drawn from a number of classes of documents which can be modeled as sets of words where the (independent) probability that the i-th word of a given document occurs in a document from class C can be written as




p
(

w

i


∣
C
)



{\displaystyle p(w_{i}\mid C)\,}


 (For this treatment, things are further simplified by assuming that words are randomly distributed in the document - that is, words are not dependent on the length of the document, position within the document with relation to other words, or other document-context.)
 Then the probability that a given document D contains all of the words 




w

i




{\displaystyle w_{i}}

, given a class C, is




p
(
D
∣
C
)
=

∏

i


p
(

w

i


∣
C
)



{\displaystyle p(D\mid C)=\prod _{i}p(w_{i}\mid C)\,}


 The question that has to be answered is: ""what is the probability that a given document D belongs to a given class C?"" In other words, what is 



p
(
C
∣
D
)



{\displaystyle p(C\mid D)\,}

?
 Now by definition




p
(
D
∣
C
)
=



p
(
D
∩
C
)


p
(
C
)





{\displaystyle p(D\mid C)={p(D\cap C) \over p(C)}}


and




p
(
C
∣
D
)
=



p
(
D
∩
C
)


p
(
D
)





{\displaystyle p(C\mid D)={p(D\cap C) \over p(D)}}


 Bayes' theorem manipulates these into a statement of probability in terms of likelihood.




p
(
C
∣
D
)
=



p
(
C
)

p
(
D
∣
C
)


p
(
D
)





{\displaystyle p(C\mid D)={\frac {p(C)\,p(D\mid C)}{p(D)}}}


 Assume for the moment that there are only two mutually exclusive classes, S and ¬S (e.g. spam and not spam), such that every element (email) is in either one or the other;




p
(
D
∣
S
)
=

∏

i


p
(

w

i


∣
S
)



{\displaystyle p(D\mid S)=\prod _{i}p(w_{i}\mid S)\,}


and




p
(
D
∣
¬
S
)
=

∏

i


p
(

w

i


∣
¬
S
)



{\displaystyle p(D\mid \neg S)=\prod _{i}p(w_{i}\mid \neg S)\,}


 Using the Bayesian result above, one can write:




p
(
S
∣
D
)
=



p
(
S
)


p
(
D
)





∏

i


p
(

w

i


∣
S
)


{\displaystyle p(S\mid D)={p(S) \over p(D)}\,\prod _{i}p(w_{i}\mid S)}






p
(
¬
S
∣
D
)
=



p
(
¬
S
)


p
(
D
)





∏

i


p
(

w

i


∣
¬
S
)


{\displaystyle p(\neg S\mid D)={p(\neg S) \over p(D)}\,\prod _{i}p(w_{i}\mid \neg S)}


 Dividing one by the other gives:







p
(
S
∣
D
)


p
(
¬
S
∣
D
)



=



p
(
S
)


∏

i


p
(

w

i


∣
S
)


p
(
¬
S
)


∏

i


p
(

w

i


∣
¬
S
)





{\displaystyle {p(S\mid D) \over p(\neg S\mid D)}={p(S)\,\prod _{i}p(w_{i}\mid S) \over p(\neg S)\,\prod _{i}p(w_{i}\mid \neg S)}}


 Which can be re-factored as:







p
(
S
∣
D
)


p
(
¬
S
∣
D
)



=



p
(
S
)


p
(
¬
S
)





∏

i





p
(

w

i


∣
S
)


p
(

w

i


∣
¬
S
)





{\displaystyle {p(S\mid D) \over p(\neg S\mid D)}={p(S) \over p(\neg S)}\,\prod _{i}{p(w_{i}\mid S) \over p(w_{i}\mid \neg S)}}


 Thus, the probability ratio p(S | D) / p(¬S | D) can be expressed in terms of a series of likelihood ratios.
The actual probability p(S | D) can be easily computed from log (p(S | D) / p(¬S | D)) based on the observation that p(S | D) + p(¬S | D) = 1.
 Taking the logarithm of all these ratios, one obtains:




ln
⁡



p
(
S
∣
D
)


p
(
¬
S
∣
D
)



=
ln
⁡



p
(
S
)


p
(
¬
S
)



+

∑

i


ln
⁡



p
(

w

i


∣
S
)


p
(

w

i


∣
¬
S
)





{\displaystyle \ln {p(S\mid D) \over p(\neg S\mid D)}=\ln {p(S) \over p(\neg S)}+\sum _{i}\ln {p(w_{i}\mid S) \over p(w_{i}\mid \neg S)}}


 (This technique of ""log-likelihood ratios"" is a common technique in statistics.
In the case of two mutually exclusive alternatives (such as this example), the conversion of a log-likelihood ratio to a probability takes the form of a sigmoid curve: see logit for details.)
 Finally, the document can be classified as follows.  It is spam if 



p
(
S
∣
D
)
>
p
(
¬
S
∣
D
)


{\displaystyle p(S\mid D)>p(\neg S\mid D)}

 (i. e., 



ln
⁡



p
(
S
∣
D
)


p
(
¬
S
∣
D
)



>
0


{\displaystyle \ln {p(S\mid D) \over p(\neg S\mid D)}>0}

), otherwise it is not spam.
",statist naiv bay classifi famili linear probabilist classifi assum featur condit independ given target class strength naiveti assumpt give classifi name classifi among simplest bayesian network model naiv bay classifi highli scalabl requir number paramet linear number variabl learn problem train done evalu express take linear time rather expens iter approxim use mani type classifi statist literatur naiv bay model known varieti name includ simpl bay independ bay name refer use bay theorem classifi decis rule naiv bay necessarili bayesian method naiv bay simpl techniqu construct classifi model assign class label problem instanc repres vector featur valu class label drawn finit set singl algorithm train classifi famili algorithm base common principl naiv bay classifi assum valu particular featur independ valu featur given class variabl exampl fruit may consid appl red round cm diamet naiv bay classifi consid featur contribut independ probabl fruit appl regardless possibl correl color round diamet featur mani practic applic paramet estim naiv bay model use method maximum likelihood word one work naiv bay model without accept bayesian probabl use bayesian method despit naiv design appar oversimplifi assumpt naiv bay classifi work quit well mani complex situat analysi bayesian classif problem show sound theoret reason appar implaus efficaci naiv bay classifi still comprehens comparison classif algorithm show bay classif outperform approach boost tree random forest advantag naiv bay requir small amount train data estim paramet necessari classif abstractli naiv bay condit probabl model assign probabl p c k x x n p k n k possibl outcom class c k k given problem instanc classifi repres vector x x x n x n encod n featur independ variabl problem formul number featur n larg featur take larg number valu base model probabl tabl infeas model must therefor reformul make tractabl use bay theorem condit probabl decompos p c k x p c k p x c k p x p k x p k p x k p x plain english use bayesian probabl terminolog equat written posterior prior likelihood evid posterior prior likelihood evid practic interest numer fraction denomin depend c c valu featur x given denomin effect constant numer equival joint probabl model p c k x x n p k n rewritten follow use chain rule repeat applic definit condit probabl p c k x x n p x x n c k p x x x n c k p x x n c k p x x x n c k p x x x n c k p x x n c k p x x x n c k p x x x n c k p x n x n c k p x n c k p c k align p k n n k n k p n k n k p n k p n k n k p n k p n k p n k p k align naiv condit independ assumpt come play assum featur x x mutual independ condit categori c k k assumpt p x x x n c k p x c k p n k k thu joint model express p c k x x n p c k x x n p c k p x c k p x c k p x c k p c k n p x c k align p k n p k n k p k p k p k k n p k align denot proportion sinc denomin p x p x omit mean independ assumpt condit distribut class variabl c c p c k x x n z p c k n p x c k p k n z p k n p k evid z p x k p c k p x c k x k p k p x k scale factor depend x x n n constant valu featur variabl known discuss far deriv independ featur model naiv bay probabl model naiv bay classifi combin model decis rule one common rule pick hypothesi probabl minim probabl misclassif known maximum posteriori map decis rule correspond classifi bay classifi function assign class label c k k k follow argmax k k p c k n p x c k argmax p k n p k class prior may calcul assum equiprob class p c k k p k k calcul estim class probabl train set prior given class sampl class total sampl prior given class sampl class total sampl estim paramet featur distribut one must assum distribut gener nonparametr model featur train set assumpt distribut featur call event model naiv bay classifi discret featur like one encount document classif includ spam filter multinomi bernoulli distribut popular assumpt lead two distinct model often confus deal continu data typic assumpt continu valu associ class distribut accord normal gaussian distribut exampl suppos train data contain continu attribut x x data first segment class mean varianc x x comput class let μ k k mean valu x x associ class c k k let σ k k bessel correct varianc valu x x associ class c k k suppos one collect observ valu v v probabl densiti v v given class c k k p x v c k p k comput plug v v equat normal distribut parameter μ k k σ k k formal p x v c k π σ k e v μ k σ k p k k k k anoth common techniqu handl continu valu use bin discret featur valu obtain new set featur literatur suggest requir order use naiv bay true discret may throw away discrimin inform sometim distribut margin densiti far normal case kernel densiti estim use realist estim margin densiti class method introduc john langley boost accuraci classifi consider multinomi event model sampl featur vector repres frequenc certain event gener multinomi p p n n p probabl event occur k multinomi multiclass case featur vector x x x n x n histogram x count number time event observ particular instanc event model typic use document classif event repres occurr word singl document see bag word assumpt likelihood observ histogram x given p x c k n x n x n p k x p x k n n n ki p k p c k ki k multinomi naiv bay classifi becom linear classifi express log p c k x log p c k n p k x log p c k n x log p k b w k x align p k x p k n ki p k n ki w k x align b log p c k p k w k log p k ki ki estim paramet log space advantag sinc multipli larg number small valu lead signific round error appli log transform reduc effect round error given class featur valu never occur togeth train data probabl estim zero probabl estim directli proport number occurr featur valu problemat wipe inform probabl multipli therefor often desir incorpor correct call pseudocount probabl estim probabl ever set exactli zero way regular naiv bay call laplac smooth pseudocount one lidston smooth gener case renni et al discuss problem multinomi assumpt context document classif possibl way allevi problem includ use weight instead raw term frequenc document length normal produc naiv bay classifi competit support vector machin multivari bernoulli event model featur independ boolean variabl binari variabl describ input like multinomi model model popular document classif task binari term occurr featur use rather term frequenc x boolean express occurr absenc term vocabulari likelihood document given class c k k given p x c k n p k x p k x p x k n ki ki p k ki probabl class c k k gener term x event model especi popular classifi short text benefit explicitli model absenc term note naiv bay classifi bernoulli event model multinomi nb classifi frequenc count truncat one given way train naiv bay classifi label data possibl construct train algorithm learn combin label unlabel data run supervis learn algorithm loop converg determin base improv model likelihood p θ p θ denot paramet naiv bay model train algorithm instanc gener algorithm em predict step insid loop em naiv bay algorithm formal justifi assumpt data gener mixtur model compon mixtur model exactli class classif problem despit fact independ assumpt often inaccur naiv bay classifi sever properti make surprisingli use practic particular decoupl class condit featur distribut mean distribut independ estim distribut help allevi problem stem curs dimension need data set scale exponenti number featur naiv bay often fail produc good estim correct class probabl may requir mani applic exampl naiv bay classifi make correct map decis rule classif long correct class predict probabl class true regardless whether probabl estim slightli even grossli inaccur manner overal classifi robust enough ignor seriou defici underli naiv probabl model reason observ success naiv bay classifi discuss literatur cite case discret input indic frequenc featur discret event naiv bay classifi form pair multinomi logist regress classifi naiv bay classifi consid way fit probabl model optim joint likelihood p c x p c x logist regress fit probabl model optim condit p c x p x formal follow theorem naiv bay classifi binari featur subsum logist regress classifi consid gener multiclass classif problem possibl class n bay classifi give bay theorem p x x softmax ln p k ln p x x k k p softmax p p k naiv bay classifi give softmax ln p k k k x k k k softmax p k k k k k ln p x ln p x p p exactli logist regress classifi link two seen observ decis function naiv bay binari case rewritten predict class c odd p c x p x exceed p c x p x express give log p c x p c x log p c x log p c x p x p x p x p x side equat logit quantiti predict linear model underli logist regress sinc naiv bay also linear model two discret event model reparametris linear function b w x w x obtain probabl matter appli logist function b w x w x multiclass case softmax function discrimin classifi lower asymptot error gener one howev research ng jordan shown practic case naiv bay outperform logist regress reach asymptot error faster problem classifi whether given person male femal base measur featur featur includ height weight foot size although nb classifi treat independ realiti exampl train set classifi creat train set use gaussian distribut assumpt would given varianc unbias sampl varianc follow exampl assum equiprob class p male p femal prior probabl distribut might base prior knowledg frequenc larger popul train set sampl classifi male femal order classifi sampl one determin posterior greater male femal classif male posterior given posterior male p male p height male p weight male p foot size male evid posterior male p male p height male p weight male p foot size male evid classif femal posterior given posterior femal p femal p height femal p weight femal p foot size femal evid posterior femal p femal p height femal p weight femal p foot size femal evid evid also term normal constant may calcul evid p male p height male p weight male p foot size male p femal p height femal p weight femal p foot size femal align evid male p height male p weight male p foot size male femal p height femal p weight femal p foot size femal align howev given sampl evid constant thu scale posterior equal therefor affect classif ignor probabl distribut sex sampl determin p male p male p height male π σ exp μ σ p height male μ σ paramet normal distribut previous determin train set note valu greater ok probabl densiti rather probabl height continu variabl p weight male π σ exp μ σ p weight male p foot size male π σ exp μ σ p foot size male posterior numer male product posterior numer male product p femal p femal p height femal p height femal p weight femal p weight femal p foot size femal p foot size femal posterior numer femal product posterior numer femal product sinc posterior numer greater femal case predict sampl femal work exampl naiv bayesian classif document classif problem consid problem classifi document content exampl spam imagin document drawn number class document model set word independ probabl word given document occur document class c written p w c p c treatment thing simplifi assum word randomli distribut document word depend length document posit within document relat word probabl given document contain word w given class c p c p w c p c p c question answer probabl given document belong given class c word p c p definit p c p c p c p c p c p c p c p c p p p c p bay theorem manipul statement probabl term likelihood p c p c p c p p p c p c p assum moment two mutual exclus class spam spam everi element email either one p p w p p p p w p p use bayesian result one write p p p p w p p p p p p p p w p p p p divid one give p p p p w p p w p p p p p p p p p p p w p w p p p p p p thu probabl ratio p p express term seri likelihood ratio actual probabl p easili comput log p p base observ p p take logarithm ratio one obtain ln p p ln p p ln p w p w p p p p p p techniqu ratio common techniqu statist case two mutual exclus altern exampl convers ratio probabl take form sigmoid curv see logit detail final document classifi follow spam p p p p ln p p p p otherwis spam
Neural network (machine learning),https://en.wikipedia.org/wiki/Artificial_neural_network,"
 In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains.[1][2]
 An ANN consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The ""signal"" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.
 Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers.[3]
 Artificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.
 
Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset.[4] Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network.[4] During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function.[5] This method allows the network to generalize to unseen data. Today's deep neural networks are based on early work in statistics over 200 years ago. The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.[7][8][9][10][11]
 Historically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing.
 Warren McCulloch and Walter Pitts[12] (1943) considered a non-learning computational model for neural networks.[13] This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence.
 In the late 1940s, D. O. Hebb[14] proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. It was used in many early neural networks, such as Rosenblatt's perceptron and the Hopfield network. Farley and Clark[15] (1954) used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956).[16]
 In 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks,[17][18][19][20] funded by the United States Office of Naval Research.[21]
R. D. Joseph (1960)[22] mentions an even earlier perceptron-like device by Farley and Clark:[10] ""Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device."" However, ""they dropped the subject.""
The perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding. This contributed to ""the Golden Age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence.[23]
 The first perceptrons did not have adaptive hidden units. However, Joseph (1960)[22] also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962)[24]: section 16  cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning.
 Fundamental research was conducted on ANNs in the 1960s and 1970s. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in the Soviet Union (1965). They regarded it as a form of polynomial regression,[25] or a generalization of Rosenblatt's perceptron.[26] A 1971 paper described a deep network with eight layers trained by this method,[27] which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or ""gates.""[10]
 The first deep learning multilayer perceptron trained by stochastic gradient descent[28] was published in 1967 by Shun'ichi Amari.[29] In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes.[10] Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.
 In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function.[10][30][31] The rectifier has become the most popular activation function for deep learning.[32]
 Nevertheless, research stagnated in the United States following the work of Minsky and Papert (1969),[33] who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. This insight was irrelevant for the deep networks of Ivakhnenko (1965) and Amari (1967).
 In 1976 transfer learning was introduced in neural networks learning. [34] [35]
 Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.[36][37][38]
 Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673[39] to networks of differentiable nodes. The terminology ""back-propagating errors"" was actually introduced in 1962 by Rosenblatt,[24] but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory.[40] In 1970, Seppo Linnainmaa published the modern form of backpropagation in his master thesis (1970).[41][42][10] G.M. Ostrovski et al. republished it in 1971.[43][44] Paul Werbos applied backpropagation to neural networks in 1982[45][46] (his 1974 PhD thesis, reprinted in a 1994 book,[47] did not yet describe the algorithm[44]). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.[48]
 Kunihiko Fukushima's convolutional neural network (CNN) architecture of 1979[36] also introduced max pooling,[49] a popular downsampling procedure for CNNs. CNNs have become an essential tool for computer vision.
 The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.[50][51] In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.[52]
In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days.[53] In 1990, Wei Zhang implemented a CNN on optical computing hardware.[54] In 1991, a CNN was applied to medical image object segmentation[55] and breast cancer detection in mammograms.[56] LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32×32 pixel images.[57]
 From 1988 onward,[58][59] the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.[60]
 One origin of RNN was statistical mechanics. In 1972, Shun'ichi Amari proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory, adding in the component of learning.[61] This was popularized as the Hopfield network by John Hopfield(1982).[62] Another origin of RNN was neuroscience. The word ""recurrent"" is used to describe loop-like structures in anatomy. In 1901, Cajal observed ""recurrent semicircles"" in the cerebellar cortex.[63] Hebb considered ""reverberating circuit"" as an explanation for short-term memory.[64] The McCulloch and Pitts paper (1943) considered neural networks that contains cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past.[12]
 In 1982 a recurrent neural network, with an array architecture (rather than a multilayer perceptron architecture), named Crossbar Adaptive Array [65][66]  used direct recurrent connections from the output to the supervisor (teaching ) inputs. In addition of computing actions (decisions), it computed internal state evaluations (emotions) of the consequence situations. Eliminating the external supervisor, it introduced the self-learning method in neural networks.  
 In cognitive psychology, the journal American Psychologist in early 1980's carried out a debate on relation between cognition and emotion. Zajonc in 1980 stated that emotion is computed first and is independent from cognition, while Lazarus in 1982 stated that cognition is computed first and is inseparable from emotion. [67][68] In 1982 the Crossbar Adaptive Array gave a neural network model of cognition-emotion relation. [65][69] It was an example of a debate where an AI system, a recurrent neural network, contributed to an issue in the same time addressed by cognitive psychology.
 Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. 
 In the 1980s, backpropagation did not work well for deep RNNs. To overcome this problem, in 1991, Jürgen Schmidhuber proposed the ""neural sequence chunker"" or ""neural history compressor""[70][71] which introduced the important concepts of self-supervised pre-training (the ""P"" in ChatGPT) and neural knowledge distillation.[10] In 1993, a neural history compressor system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time.[72]
 In 1991, Sepp Hochreiter's diploma thesis [73] identified and analyzed the vanishing gradient problem[73][74] and proposed recurrent residual connections to solve it. He and Schmidhuber introduced long short-term memory (LSTM), which set accuracy records in multiple applications domains.[75][76] This was not yet the modern version of LSTM, which required the forget gate, which was introduced in 1999.[77] It became the default choice for RNN architecture.
 During 1985–1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine,[78] restricted Boltzmann machine,[79] Helmholtz machine,[80] and the wake-sleep algorithm.[81] These were designed for unsupervised learning of deep generative models.
 Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition.[82][83] In 2011, a CNN named DanNet[84][85] by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3.[38] It then won more contests.[86][87] They also showed how max-pooling CNNs on GPU improved performance significantly.[88]
 In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton[89] won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman[90] and Google's Inceptionv3.[91]
 In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images.[92] Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as ""deep learning"".[5]
 Radial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.[93]
 Generative adversarial network (GAN) (Ian Goodfellow et al., 2014)[94] became state of the art in generative modeling during 2014–2018 period. The GAN principle was originally published in 1991 by Jürgen Schmidhuber who called it ""artificial curiosity"": two neural networks contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss.[95][96] The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. Excellent image quality is achieved by Nvidia's StyleGAN (2018)[97] based on the Progressive GAN by Tero Karras et al.[98] Here, the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.[99] Diffusion models (2015)[100] eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022).
 In 2014, the state of the art was training ""very deep neural network"" with 20 to 30 layers.[101] Stacking too many layers led to a steep reduction in training accuracy,[102] known as the ""degradation"" problem.[103] In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015,[104] and the residual neural network (ResNet) in December 2015.[105][106] ResNet behaves like an open-gated Highway Net. 
 During the 2010s, the seq2seq model was developed, and attention mechanisms were added. It led to the modern Transformer architecture in 2017 in Attention Is All You Need.[107]
It requires computation time that is quadratic in the size of the context window. Jürgen Schmidhuber's fast weight controller (1992)[108] scales linearly and was later shown to be equivalent to the unnormalized linear Transformer.[109][110][10]
Transformers have increasingly become the model of choice for natural language processing.[111] Many modern large language models such as ChatGPT, GPT-4, and BERT use this architecture.
 ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others. The network forms a directed, weighted graph.[112]
 An artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight, determining the strength of one node's influence on another,[113] allowing weights to choose the signal between neurons.
 ANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons.[114] The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image.[citation needed]
 To find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum.[115] This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image.[116]
 The neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer.[117] Neurons with only such connections form a directed acyclic graph and are known as feedforward networks.[118] Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.[119]
 A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size.[citation needed] The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers.[citation needed]
 Learning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.[112][120]
 The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation.[121] A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate.[122] The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.[citation needed]
 While it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) or because it arises from the model (e.g. in a probabilistic model the model's posterior probability can be used as an inverse cost).[citation needed]
 Backpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backprop calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines,[123] ""no-prop"" networks,[124] training without backtracking,[125] ""weightless"" networks,[126][127] and non-connectionist neural networks.[citation needed]
 Machine learning is commonly separated into three main learning paradigms, supervised learning,[128] unsupervised learning[129] and reinforcement learning.[130] Each corresponds to a particular learning task.
 Supervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case, the cost function is related to eliminating incorrect deductions.[131] A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition). This can be thought of as learning with a ""teacher"", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.
 In unsupervised learning, input data is given along with the cost function, some function of the data 




x



{\displaystyle \textstyle x}

 and the network's output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model 




f
(
x
)
=
a



{\displaystyle \textstyle f(x)=a}

 where 




a



{\displaystyle \textstyle a}

 is a constant and the cost 




C
=
E
[
(
x
−
f
(
x
)

)

2


]



{\displaystyle \textstyle C=E[(x-f(x))^{2}]}

. Minimizing this cost produces a value of 




a



{\displaystyle \textstyle a}

 that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between 




x



{\displaystyle \textstyle x}

 and 




f
(
x
)



{\displaystyle \textstyle f(x)}

, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples, those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.
 In applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly.
 Formally the environment is modeled as a Markov decision process (MDP) with states 






s

1


,
.
.
.
,

s

n



∈
S



{\displaystyle \textstyle {s_{1},...,s_{n}}\in S}

 and actions 






a

1


,
.
.
.
,

a

m



∈
A



{\displaystyle \textstyle {a_{1},...,a_{m}}\in A}

. Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution 




P
(

c

t



|


s

t


)



{\displaystyle \textstyle P(c_{t}|s_{t})}

, the observation distribution 




P
(

x

t



|


s

t


)



{\displaystyle \textstyle P(x_{t}|s_{t})}

 and the transition distribution 




P
(

s

t
+
1



|


s

t


,

a

t


)



{\displaystyle \textstyle P(s_{t+1}|s_{t},a_{t})}

, while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC.
 ANNs serve as the learning component in such applications.[132][133] Dynamic programming coupled with ANNs (giving neurodynamic programming)[134] has been applied to problems such as those involved in vehicle routing,[135] video games, natural resource management[136][137] and medicine[138] because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.
 Self-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA).[139] It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion.[140] Given the memory matrix, W =||w(a,s)||, the crossbar self-learning algorithm in each iteration performs the following computation:
 The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it initially and only once receives initial emotions about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.[141]
 Neuroevolution can create neural network topologies and weights using evolutionary computation. It is competitive with sophisticated gradient descent approaches.[142][143] One advantage of neuroevolution is that it may be less prone to get caught in ""dead ends"".[144]
 Stochastic neural networks originating from Sherrington–Kirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions [citation needed], or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima.[145] Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks.[146]
 In a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods,[147] gene expression programming,[148] simulated annealing,[149] expectation–maximization, non-parametric methods and particle swarm optimization[150] are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks.[151][152]
 Two modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces ""noise"" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use ""mini-batches"", small batches with samples in each batch selected stochastically from the entire data set.
 ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains. The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter is much more complicated but can shorten learning periods and produce better results. Some types allow/require learning to be ""supervised"" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.
 Some of the main breakthroughs include: 
 Using artificial neural networks requires an understanding of their characteristics.
 Neural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the results as feedback to teach the NAS network.[164] Available systems include AutoML and AutoKeras.[165] scikit-learn library provides functions to help with building a deep network from scratch. We can then implement a deep network with TensorFlow or Keras.
 
Hyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc.[166]  [citation needed]
 Because of their ability to reproduce and model nonlinear processes, artificial neural networks have found applications in many disciplines. These include:
 ANNs have been used to diagnose several types of cancers[184][185] and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.[186][187]
 ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters[188][189] and to predict foundation settlements.[190] It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff.[191] ANNs have also been used for building black-box models in geoscience: hydrology,[192][193] ocean modelling and coastal engineering,[194][195] and geomorphology.[196] ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware,[197] for identifying domains belonging to threat actors and for detecting URLs posing a security risk.[198] Research is underway on ANN systems designed for penetration testing, for detecting botnets,[199] credit cards frauds[200] and network intrusions.
 ANNs have been proposed as a tool to solve partial differential equations in physics[201][202][203] and simulate the properties of many-body open quantum systems.[204][205][206][207] In brain research ANNs have studied short-term behavior of individual neurons,[208] the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.
 It is possible to create a profile of a user's interests from pictures, using artificial neural networks trained for object recognition.[209]
 Beyond their traditional applications, artificial neural networks are increasingly being utilized in interdisciplinary research, such as materials science. For instance, graph neural networks (GNNs) have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals. This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence, opening new pathways for scientific discovery and innovation.[210]
 The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.
 A specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine,[211] using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.[212][213][failed verification]
 A model's ""capacity"" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.
Two notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKay's book[214] which summarizes work by Thomas Cover.[215] The capacity of a network of standard neurons (not convolutional) can be derived by four rules[216] that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form. As noted in,[214] the VC Dimension for arbitrary inputs is half the information capacity of a Perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity.[217]
 Models may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical.
 Another issue worthy to mention is that training may cross some Saddle point which may lead the convergence to the wrong direction.
 The convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models.[218][219] Another example is when parameters are small, it is observed that ANNs often fits target functions from low to high frequencies. This behavior is referred to as the spectral bias, or frequency principle, of neural networks.[220][221][222][223] This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method. Deeper neural networks have been observed to be more biased towards low frequency functions.[224]
 Applications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error.
 The second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.
 Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.
 By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications.
 The softmax activation function is:
 
 A common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation.[225]
Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC.[151]
Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.), and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience, and preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns—it should not learn to always turn right).[226]
 A central claim[citation needed] of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed[by whom?] that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997, Alexander Dewdney, a former Scientific American columnist, commented that as a result, artificial neural networks have a ""something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything"".[227] One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft[228] to detecting credit card fraud to mastering the game of Go.
 Technology writer Roger Bridgman commented:
 Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be ""an opaque, unreadable table...valueless as a scientific resource"".
 In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.[229]
 Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.[230]
 Biological brains use both shallow and deep circuits as reported by brain anatomy,[231] displaying a wide variety of invariance. Weng[232] argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.
 Large and effective neural networks require considerable computing resources.[233] While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons –  which require enormous CPU power and time.[citation needed]
 Some argue that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before.[38] The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.[233][234]
 Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.[235]
 Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture.[236]
 Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind.[237][238]
 Neural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases.[239][240] These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute.[239] This imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exacerbate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement.[240][241] For example, in 2018, Amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field.[241] The program would penalize any resume with the word ""woman"" or the name of any women's college. However, the use of synthetic data can help reduce dataset bias and increase representation in datasets.[242]
 Artificial neural networks (ANNs) have undergone significant advancements, particularly in their ability to model complex systems, handle large data sets, and adapt to various types of applications. Their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing, speech recognition, natural language processing, finance, and medicine.[citation needed]
 In the realm of image processing, ANNs are employed in tasks such as image classification, object recognition, and image segmentation. For instance, deep convolutional neural networks (CNNs) have been important in handwritten digit recognition, achieving state-of-the-art performance.[243] This demonstrates the ability of ANNs to effectively process and interpret complex visual information, leading to advancements in fields ranging from automated surveillance to medical imaging.[243]
 By modeling speech signals, ANNs are used for tasks like speaker identification and speech-to-text conversion. Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition, outperforming traditional techniques.[243][244] These advancements have enabled the development of more accurate and efficient voice-activated systems, enhancing user interfaces in technology products.[citation needed]
 In natural language processing, ANNs are used for tasks such as text classification, sentiment analysis, and machine translation. They have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content.[243][244] This has implications for automated customer service, content moderation, and language understanding technologies.[citation needed]
 In the domain of control systems, ANNs are used to model dynamic systems for tasks such as system identification, control design, and optimization. For instance, deep feedforward neural networks are important in system identification and control applications.[citation needed]
 ANNs are used for stock market prediction and credit scoring: 
 ANNs require high-quality data and careful tuning, and their ""black-box"" nature can pose challenges in interpretation. Nevertheless, ongoing advancements suggest that ANNs continue to play a role in finance, offering valuable insights and enhancing risk management strategies.[citation needed]
 ANNs are able to process and analyze vast medical datasets. They enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning.[244] In drug discovery, ANNs speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs.[243] Additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management.[244] Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ANN applications in medicine.[citation needed]
 ANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries.[245] This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions. For instance, DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user.[246] In the field of music, transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck.[247] In the marketing industry generative models are used to create personalized advertisements for consumers.[245] Additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as the partnership between Warner Bros and technology company Cinelytic established in 2020.[248] Furthermore, neural networks have found uses in video game creation, where Non Player Characters (NPCs) can make decisions based on all the characters currently in the game.[249]
",machin learn neural network also artifici neural network neural net abbrevi ann nn model inspir structur function biolog neural network anim brain ann consist connect unit node call artifici neuron loos model neuron brain artifici neuron model mimic biolog neuron close also recent investig shown significantli improv perform connect edg model synaps brain artifici neuron receiv signal connect neuron process send signal connect neuron signal real number output neuron comput function sum input call activ function strength signal connect determin weight adjust learn process typic neuron aggreg layer differ layer may perform differ transform input signal travel first layer input layer last layer output layer possibl pass multipl intermedi layer hidden layer network typic call deep neural network least two hidden layer artifici neural network use variou task includ predict model adapt control solv problem artifici intellig learn experi deriv conclus complex seemingli unrel set inform neural network typic train empir risk minim method base idea optim network paramet minim differ empir risk predict output actual target valu given dataset method backpropag usual use estim paramet network train phase ann learn label train data iter updat paramet minim defin loss function method allow network gener unseen data today deep neural network base earli work statist year ago simplest kind feedforward neural network fnn linear network consist singl layer output node linear activ function input fed directli output via seri weight sum product weight input calcul node mean squar error calcul output given target valu minim creat adjust weight techniqu known two centuri method least squar linear regress use mean find good rough linear fit set point legendr gauss predict planetari movement histor digit comput von neumann model oper via execut explicit instruct access memori number processor neural network hand origin effort model inform process biolog system framework connection unlik von neumann model connectionist comput separ memori process warren mcculloch walter pitt consid comput model neural network model pave way research split two approach one approach focus biolog process focus applic neural network artifici intellig late hebb propos learn hypothesi base mechan neural plastic becam known hebbian learn use mani earli neural network rosenblatt perceptron hopfield network farley clark use comput machin simul hebbian network neural network comput machin creat rochest holland habit duda psychologist frank rosenblatt describ perceptron one first implement artifici neural network fund unit state offic naval research joseph mention even earlier devic farley clark farley clark mit lincoln laboratori actual preced rosenblatt develop devic howev drop subject perceptron rais public excit research artifici neural network caus us govern drastic increas fund contribut golden age ai fuel optimist claim made comput scientist regard abil perceptron emul human intellig first perceptron adapt hidden unit howev joseph also discuss multilay perceptron adapt hidden layer rosenblatt section cite adopt idea also credit work block knight unfortun earli effort lead work learn algorithm hidden unit deep learn fundament research conduct ann first work deep learn algorithm group method data handl method train arbitrarili deep neural network publish alexey ivakhnenko lapa soviet union regard form polynomi regress gener rosenblatt perceptron paper describ deep network eight layer train method base layer layer train regress analysi superflu hidden unit prune use separ valid set sinc activ function node polynomi also first deep network multipl unit gate first deep learn multilay perceptron train stochast gradient descent publish amari comput experi conduct amari student saito five layer mlp two modifi layer learn intern represent classifi separ pattern class subsequ develop hardwar hyperparamet tune made stochast gradient descent current domin train techniqu kunihiko fukushima introduc relu rectifi linear unit activ function rectifi becom popular activ function deep learn nevertheless research stagnat unit state follow work minski papert emphas basic perceptron incap process circuit insight irrelev deep network ivakhnenko amari transfer learn introduc neural network learn deep learn architectur convolut neural network cnn convolut layer downsampl layer weight replic began neocognitron introduc kunihiko fukushima though train backpropag backpropag effici applic chain rule deriv gottfri wilhelm leibniz network differenti node terminolog error actual introduc rosenblatt know implement although henri kelley continu precursor backpropag context control theori seppo linnainmaa publish modern form backpropag master thesi ostrovski et al republish paul werbo appli backpropag neural network phd thesi reprint book yet describ algorithm david rumelhart et al popularis backpropag cite origin work kunihiko fukushima convolut neural network cnn architectur also introduc max pool popular downsampl procedur cnn cnn becom essenti tool comput vision time delay neural network tdnn introduc alex waibel appli cnn phonem recognit use convolut weight share backpropag wei zhang appli cnn alphabet recognit yann lecun et al creat cnn call lenet recogn handwritten zip code mail train requir day wei zhang implement cnn optic comput hardwar cnn appli medic imag object segment breast cancer detect mammogram cnn yann lecun et classifi digit appli sever bank recogn number check digit pixel imag onward use neural network transform field protein structur predict particular first cascad network train profil matric produc multipl sequenc align one origin rnn statist mechan amari propos modifi weight ise model hebbian learn rule model associ memori ad compon learn popular hopfield network john hopfield anoth origin rnn neurosci word recurr use describ structur anatomi cajal observ recurr semicircl cerebellar cortex hebb consid reverber circuit explan memori mcculloch pitt paper consid neural network contain cycl note current activ network affect activ indefinit far past recurr neural network array architectur rather multilay perceptron architectur name crossbar adapt array use direct recurr connect output supervisor teach input addit comput action decis comput intern state evalu emot consequ situat elimin extern supervisor introduc method neural network cognit psycholog journal american psychologist earli carri debat relat cognit emot zajonc state emot comput first independ cognit lazaru state cognit comput first insepar emot crossbar adapt array gave neural network model relat exampl debat ai system recurr neural network contribut issu time address cognit psycholog two earli influenti work jordan network elman network appli rnn studi cognit psycholog backpropag work well deep rnn overcom problem jürgen schmidhub propos neural sequenc chunker neural histori compressor introduc import concept p chatgpt neural knowledg distil neural histori compressor system solv deep learn task requir subsequ layer rnn unfold time sepp hochreit diploma thesi identifi analyz vanish gradient problem propos recurr residu connect solv schmidhub introduc long memori lstm set accuraci record multipl applic domain yet modern version lstm requir forget gate introduc becam default choic rnn architectur inspir statist mechan sever architectur method develop terri sejnowski peter dayan geoffrey hinton includ boltzmann machin restrict boltzmann machin helmholtz machin algorithm design unsupervis learn deep gener model ann began win prize imag recognit contest approach human level perform variou task initi pattern recognit handwrit recognit cnn name dannet dan ciresan ueli meier jonathan masci luca maria gambardella jürgen schmidhub achiev first time superhuman perform visual pattern recognit contest outperform tradit method factor contest also show cnn gpu improv perform significantli octob alexnet alex krizhevski ilya sutskev geoffrey hinton imagenet competit signific margin shallow machin learn method increment improv includ network karen simonyan andrew zisserman googl ng dean creat network learn recogn concept cat watch unlabel imag unsupervis increas comput power gpu distribut comput allow use larger network particularli imag visual recognit problem becam known deep learn radial basi function wavelet network introduc shown offer best approxim properti appli nonlinear system identif classif applic gener adversari network gan ian goodfellow et becam state art gener model period gan principl origin publish jürgen schmidhub call artifici curios two neural network contest form game one network gain network loss first network gener model model probabl distribut output pattern second network learn gradient descent predict reaction environ pattern excel imag qualiti achiev nvidia stylegan base progress gan tero karra et al gan gener grown small larg scale pyramid fashion imag gener gan reach popular success provok discuss concern deepfak diffus model eclips gan gener model sinc system stabl diffus state art train deep neural network layer stack mani layer led steep reduct train accuraci known degrad problem two techniqu develop train deep network highway network publish may residu neural network resnet decemb resnet behav like highway net model develop attent mechan ad led modern transform architectur attent need requir comput time quadrat size context window jürgen schmidhub fast weight control scale linearli later shown equival unnorm linear transform transform increasingli becom model choic natur languag process mani modern larg languag model chatgpt bert use architectur ann began attempt exploit architectur human brain perform task convent algorithm littl success soon reorient toward improv empir result abandon attempt remain true biolog precursor ann abil learn model complex relationship achiev neuron connect variou pattern allow output neuron becom input other network form direct weight graph artifici neural network consist simul neuron neuron connect node via link like biolog connect node connect link take data use perform specif oper task data link weight determin strength one node influenc anoth allow weight choos signal neuron ann compos artifici neuron conceptu deriv biolog neuron artifici neuron input produc singl output sent multipl neuron input featur valu sampl extern data imag document output neuron output final output neuron neural net accomplish task recogn object imag citat need find output neuron take weight sum input weight weight connect input neuron add bia term sum weight sum sometim call activ weight sum pass usual nonlinear activ function produc output initi input extern data imag document ultim output accomplish task recogn object imag neuron typic organ multipl layer especi deep learn neuron one layer connect neuron immedi preced immedi follow layer layer receiv extern data input layer layer produc ultim result output layer zero hidden layer singl layer unlay network also use two layer multipl connect pattern possibl connect everi neuron one layer connect everi neuron next layer pool group neuron one layer connect singl neuron next layer therebi reduc number neuron layer neuron connect form direct acycl graph known feedforward network altern network allow connect neuron previou layer known recurr network hyperparamet constant paramet whose valu set learn process begin valu paramet deriv via learn exampl hyperparamet includ learn rate number hidden layer batch size citat need valu hyperparamet depend hyperparamet exampl size layer depend overal number layer citat need learn adapt network better handl task consid sampl observ learn involv adjust weight option threshold network improv accuraci result done minim observ error learn complet examin addit observ use reduc error rate even learn error rate typic reach learn error rate high network typic must redesign practic done defin cost function evalu period learn long output continu declin learn continu cost frequent defin statist whose valu approxim output actual number error low differ output almost certainli cat correct answer cat small learn attempt reduc total differ across observ learn model view straightforward applic optim theori statist estim learn rate defin size correct step model take adjust error observ high learn rate shorten train time lower ultim accuraci lower learn rate take longer potenti greater accuraci optim quickprop primarili aim speed error minim improv mainli tri increas reliabl order avoid oscil insid network altern connect weight improv rate converg refin use adapt learn rate increas decreas appropri concept momentum allow balanc gradient previou chang weight weight adjust depend degre previou chang momentum close emphas gradient valu close emphas last chang citat need possibl defin cost function ad hoc frequent choic determin function desir properti convex aris model probabilist model model posterior probabl use invers cost citat need backpropag method use adjust connect weight compens error found learn error amount effect divid among connect technic backprop calcul gradient deriv cost function associ given state respect weight weight updat done via stochast gradient descent method extrem learn machin network train without backtrack weightless network neural network citat need machin learn commonli separ three main learn paradigm supervis learn unsupervis learn reinforc learn correspond particular learn task supervis learn use set pair input desir output learn task produc desir output input case cost function relat elimin incorrect deduct commonli use cost error tri minim averag squar error network output desir output task suit supervis learn pattern recognit also known classif regress also known function approxim supervis learn also applic sequenti data handwrit speech gestur recognit thought learn teacher form function provid continu feedback qualiti solut obtain thu far unsupervis learn input data given along cost function function data x x network output cost function depend task model domain priori assumpt implicit properti model paramet observ variabl trivial exampl consid model f x f x constant cost c e x f x x minim cost produc valu equal mean data cost function much complic form depend applic exampl compress could relat mutual inform x x f x f x wherea statist model could relat posterior probabl model given data note exampl quantiti would maxim rather minim task fall within paradigm unsupervis learn gener estim problem applic includ cluster estim statist distribut compress filter applic play video game actor take string action receiv gener unpredict respons environ one goal win game gener posit lowest cost respons reinforc learn aim weight network devis polici perform action minim expect cumul cost point time agent perform action environ gener observ instantan cost accord usual unknown rule rule cost usual estim junctur agent decid whether explor new action uncov cost exploit prior learn proceed quickli formal environ model markov decis process mdp state n n action state transit known probabl distribut use instead instantan cost distribut p c p observ distribut p x p transit distribut p p polici defin condit distribut action given observ taken togeth two defin markov chain mc aim discov mc ann serv learn compon applic dynam program coupl ann give neurodynam program appli problem involv vehicl rout video game natur resourc manag medicin ann abil mitig loss accuraci even reduc discret grid densiti numer approxim solut control problem task fall within paradigm reinforc learn control problem game sequenti decis make task neural network introduc along neural network capabl name crossbar adapt array caa system one input situat one output action behavior neither extern advic input extern reinforc input environ caa comput crossbar fashion decis action emot feel encount situat system driven interact cognit emot given memori matrix w crossbar algorithm iter perform follow comput backpropag valu secondari reinforc emot toward consequ situat caa exist two environ one behavior environ behav genet environ initi receiv initi emot encount situat behavior environ receiv genom vector speci vector genet environ caa learn behavior behavior environ contain desir undesir situat neuroevolut creat neural network topolog weight use evolutionari comput competit sophist gradient descent approach one advantag neuroevolut may less prone get caught dead end stochast neural network origin model type artifici neural network built introduc random variat network either give network artifici neuron stochast transfer function citat need give stochast weight make use tool optim problem sinc random fluctuat help network escap local minima stochast neural network train use bayesian approach known bayesian neural network bayesian framework distribut set allow model chosen minim cost evolutionari method gene express program simul anneal method particl swarm optim learn algorithm converg recurs learn algorithm cerebellar model articul control cmac neural network two mode learn avail stochast batch stochast learn input creat weight adjust batch learn weight adjust base batch input accumul error batch stochast learn introduc nois process use local gradient calcul one data point reduc chanc network get stuck local minima howev batch learn typic yield faster stabl descent local minimum sinc updat perform direct batch averag error common compromis use small batch sampl batch select stochast entir data set ann evolv broad famili techniqu advanc state art across multipl domain simplest type one static compon includ number unit number layer unit weight topolog dynam type allow one evolv via learn latter much complic shorten learn period produc better result type learn supervis oper other oper independ type oper pure hardwar other pure softwar run gener purpos comput main breakthrough includ use artifici neural network requir understand characterist neural architectur search na use machin learn autom ann design variou approach na design network compar well system basic search algorithm propos candid model evalu dataset use result feedback teach na network avail system includ automl autokera librari provid function help build deep network scratch implement deep network tensorflow kera hyperparamet must also defin part design learn govern matter mani neuron layer learn rate step stride depth recept field pad cnn etc citat need abil reproduc model nonlinear process artifici neural network found applic mani disciplin includ ann use diagnos sever type cancer distinguish highli invas cancer cell line less invas line use cell shape inform ann use acceler reliabl analysi infrastructur subject natur disast predict foundat settlement also use mitig flood use ann model ann also use build model geoscienc hydrolog ocean model coastal engin geomorpholog ann employ cybersecur object discrimin legitim activ malici one exampl machin learn use classifi android malwar identifi domain belong threat actor detect url pose secur risk research underway ann system design penetr test detect botnet credit card fraud network intrus ann propos tool solv partial differenti equat physic simul properti open quantum system brain research ann studi behavior individu neuron dynam neural circuitri aris interact individu neuron behavior aris abstract neural modul repres complet subsystem studi consid plastic neural system relat learn memori individu neuron system level possibl creat profil user interest pictur use artifici neural network train object recognit beyond tradit applic artifici neural network increasingli util interdisciplinari research materi scienc instanc graph neural network gnn demonstr capabl scale deep learn discoveri new stabl materi effici predict total energi crystal applic underscor adapt potenti ann tackl complex problem beyond realm predict model artifici intellig open new pathway scientif discoveri innov multilay perceptron univers function approxim proven univers approxim theorem howev proof construct regard number neuron requir network topolog weight learn paramet specif recurr architectur weight oppos full precis real weight power univers ture machin use finit number neuron standard linear connect use irrat valu weight result machin power fail verif model capac properti correspond abil model given function relat amount inform store network notion complex two notion capac known commun inform capac vc dimens inform capac perceptron intens discuss sir david mackay book summar work thoma cover capac network standard neuron convolut deriv four rule deriv understand neuron electr element inform capac captur function model network given data input second notion vc dimens vc dimens use principl measur theori find maximum capac best possibl circumst given input data specif form note vc dimens arbitrari input half inform capac perceptron vc dimens arbitrari point sometim refer memori capac model may consist converg singl solut firstli local minima may exist depend cost function model secondli optim method use might guarante converg begin far local minimum thirdli suffici larg data paramet method becom impract anoth issu worthi mention train may cross saddl point may lead converg wrong direct converg behavior certain type ann architectur understood other width network approach infin ann well describ first order taylor expans throughout train inherit converg behavior affin model anoth exampl paramet small observ ann often fit target function low high frequenc behavior refer spectral bia frequenc principl neural network phenomenon opposit behavior well studi iter numer scheme jacobi method deeper neural network observ bias toward low frequenc function applic whose goal creat system gener well unseen exampl face possibl aris convolut system network capac significantli exce need free paramet two approach address first use similar techniqu check presenc select hyperparamet minim gener error second use form regular concept emerg probabilist bayesian framework regular perform select larger prior probabl simpler model also statist learn theori goal minim two quantiti risk risk roughli correspond error train set predict error unseen data due overfit supervis neural network use mean squar error mse cost function use formal statist method determin confid train model mse valid set use estim varianc valu use calcul confid interv network output assum normal distribut confid analysi made way statist valid long output probabl distribut stay network modifi assign softmax activ function gener logist function output layer neural network softmax compon network categor target variabl output interpret posterior probabl use classif give certainti measur classif softmax activ function common critic neural network particularli robot requir mani train sampl oper learn machin need suffici repres exampl order captur underli structur allow gener new case potenti solut includ randomli shuffl train exampl use numer optim algorithm take larg step chang network connect follow exampl group exampl introduc recurs least squar algorithm cmac dean pomerleau use neural network train robot vehicl drive multipl type road singl lane dirt etc larg amount research devot extrapol multipl train scenario singl train experi preserv past train divers system becom overtrain exampl present seri right learn alway turn right central claim citat need ann embodi new power gener principl process inform principl often claim emerg network allow simpl statist associ basic function artifici neural network describ learn recognit alexand dewdney former scientif american columnist comment result artifici neural network qualiti one impart peculiar aura lazi distinct lack curios good comput system human hand mind interven solut found magic one seem learn anyth one respons dewdney neural network success use handl mani complex divers task rang autonom fli aircraft detect credit card fraud master game go technolog writer roger bridgman comment neural network instanc dock hype high heaven also could creat success net without understand work bunch number captur behaviour would probabl opaqu unread tabl valueless scientif resourc spite emphat declar scienc technolog dewdney seem pillori neural net bad scienc devis tri good engin unread tabl use machin could read would still well worth although true analyz learn artifici neural network difficult much easier analyz learn biolog neural network moreov recent emphasi explain ai contribut toward develop method notabl base attent mechan visual explain learn neural network furthermor research involv explor learn algorithm neural network gradual uncov gener principl allow learn machin success exampl bengio lecun wrote articl regard local vs learn well shallow vs deep architectur biolog brain use shallow deep circuit report brain anatomi display wide varieti invari weng argu brain larg accord signal statist therefor serial cascad catch major statist depend larg effect neural network requir consider comput resourc brain hardwar tailor task process signal graph neuron simul even simplifi neuron von neumann architectur may consum vast amount memori storag furthermor design often need transmit signal mani connect associ neuron requir enorm cpu power time citat need argu resurg neural network centuri larg attribut advanc hardwar comput power especi deliv gpgpu gpu increas around make standard backpropag algorithm feasibl train network sever layer deeper use acceler fpga gpu reduc train time month day neuromorph engin physic neural network address hardwar difficulti directli construct chip directli implement neural network circuitri anoth type chip optim neural network process call tensor process unit tpu analyz learn ann much easier analyz learn biolog neural network furthermor research involv explor learn algorithm neural network gradual uncov gener principl allow learn machin success exampl local learn shallow deep architectur advoc hybrid model combin neural network symbol approach say mixtur better captur mechan human mind neural network depend qualiti data train thu low qualiti data imbalanc repres lead model learn perpetu societ bias inherit bias becom especi critic ann integr scenario train data may imbalanc due scarciti data specif race gender attribut imbal result model inadequ represent understand underrepres group lead discriminatori outcom exacerb societ inequ especi applic like facial recognit hire process law enforc exampl amazon scrap recruit tool model favor men women job softwar engin due higher number male worker field program would penal resum word woman name women colleg howev use synthet data help reduc dataset bia increas represent dataset artifici neural network ann undergon signific advanc particularli abil model complex system handl larg data set adapt variou type applic evolut past decad mark broad rang applic field imag process speech recognit natur languag process financ medicin citat need realm imag process ann employ task imag classif object recognit imag segment instanc deep convolut neural network cnn import handwritten digit recognit achiev perform demonstr abil ann effect process interpret complex visual inform lead advanc field rang autom surveil medic imag model speech signal ann use task like speaker identif convers deep neural network architectur introduc signific improv larg vocabulari continu speech recognit outperform tradit techniqu advanc enabl develop accur effici system enhanc user interfac technolog product citat need natur languag process ann use task text classif sentiment analysi machin translat enabl develop model accur translat languag understand context sentiment textual data categor text base content implic autom custom servic content moder languag understand technolog citat need domain control system ann use model dynam system task system identif control design optim instanc deep feedforward neural network import system identif control applic citat need ann use stock market predict credit score ann requir data care tune natur pose challeng interpret nevertheless ongo advanc suggest ann continu play role financ offer valuabl insight enhanc risk manag strategi citat need ann abl process analyz vast medic dataset enhanc diagnost accuraci especi interpret complex medic imag earli diseas detect predict patient outcom person treatment plan drug discoveri ann speed identif potenti drug candid predict efficaci safeti significantli reduc develop time cost addit applic person medicin healthcar data analysi allow tailor therapi effici patient care manag ongo research aim address remain challeng data privaci model interpret well expand scope ann applic medicin citat need ann gener adversari network gan transform use content creation across numer industri deep learn model abl learn style artist musician huge dataset gener complet new artwork music composit instanc deep neural network train million pair imag text across internet creat artwork base text enter user field music transform use creat origin music commerci documentari compani aiva jukedeck market industri gener model use creat person advertis consum addit major film compani partner technolog compani analyz financi success film partnership warner bro technolog compani cinelyt establish furthermor neural network found use video game creation non player charact npc make decis base charact current game
Logistic regression,https://en.wikipedia.org/wiki/Logistic_regression,"In statistics, the logistic model (or logit model) is a statistical model that models the log-odds of an event as a linear combination of one or more independent variables. In regression analysis, logistic regression[1] (or logit regression) estimates the parameters of a logistic model (the coefficients in the linear or non linear combinations). In binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled ""0"" and ""1"", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled ""1"" can vary between 0 (certainly the value ""0"") and 1 (certainly the value ""1""), hence the labeling;[2] the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. See § Background and § Definition for formal mathematics, and § Example for a worked example.
 Binary variables are widely used in statistics to model the probability of a certain class or event taking place, such as the probability of a team winning, of a patient being healthy, etc. (see § Applications), and the logistic model has been the most commonly used model for binary regression since about 1970.[3] Binary variables can be generalized to categorical variables when there are more than two possible values (e.g. whether an image is of a cat, dog, lion, etc.), and the binary logistic regression generalized to multinomial logistic regression. If the multiple categories are ordered, one can use the ordinal logistic regression (for example the proportional odds ordinal logistic model[4]). See § Extensions for further extensions. The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier.
 Analogous linear models for binary variables with a different sigmoid function instead of the logistic function (to convert the linear combination to a probability) can also be used, most notably the probit model; see § Alternatives. The defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. More abstractly, the logistic function is the natural parameter for the Bernoulli distribution, and in this sense is the ""simplest"" way to convert a real number to a probability. In particular, it maximizes entropy (minimizes added information), and in this sense makes the fewest assumptions of the data being modeled; see § Maximum entropy.
 The parameters of a logistic regression are most commonly estimated by maximum-likelihood estimation (MLE). This does not have a closed-form expression, unlike linear least squares; see § Model fitting. Logistic regression by MLE plays a similarly basic role for binary or categorical responses as linear regression by ordinary least squares (OLS) plays for scalar responses: it is a simple, well-analyzed baseline model; see § Comparison with linear regression for discussion. The logistic regression as a general statistical model was originally developed and popularized primarily by Joseph Berkson,[5] beginning in Berkson (1944), where he coined ""logit""; see § History.
 Logistic regression is used in various fields, including machine learning, most medical fields, and social sciences. For example, the Trauma and Injury Severity Score (TRISS), which is widely used to predict mortality in injured patients, was originally developed by Boyd et al. using logistic regression.[6]  Many other medical scales used to assess severity of a patient have been developed using logistic regression.[7][8][9][10] Logistic regression may be used to predict the risk of developing a given disease (e.g. diabetes; coronary heart disease), based on observed characteristics of the patient (age, sex, body mass index, results of various blood tests, etc.).[11][12]  Another example might be to predict whether a Nepalese voter will vote Nepali Congress or Communist Party of Nepal or Any Other Party, based on age, income, sex, race, state of residence, votes in previous elections, etc.[13] The technique can also be used in engineering, especially for predicting the probability of failure of a given process, system or product.[14][15] It is also used in marketing applications such as prediction of a customer's propensity to purchase a product or halt a subscription, etc.[16] In economics, it can be used to predict the likelihood of a person ending up in the labor force, and a business application would be to predict the likelihood of a homeowner defaulting on a mortgage. Conditional random fields, an extension of logistic regression to sequential data, are used in natural language processing. Disaster planners and engineers  rely on these models to predict decision take by householders or building occupants in small-scale and large-scales evacuations, such as building fires, wildfires, hurricanes among others.[17][18][19] These models help in the development of reliable disaster managing plans and safer design for the built environment.
 Logistic regression is a supervised machine learning algorithm widely used for binary classification tasks, such as identifying whether an email is spam or not and diagnosing diseases by assessing the presence or absence of specific conditions based on patient test results. This approach utilizes the logistic (or sigmoid) function to transform a linear combination of input features into a probability value ranging between 0 and 1. This probability indicates the likelihood that a given input corresponds to one of two predefined categories. The essential mechanism of logistic regression is grounded in the logistic function's ability to model the probability of binary outcomes accurately. With its distinctive S-shaped curve, the logistic function effectively maps any real-valued number to a value within the 0 to 1 interval. This feature renders it particularly suitable for binary classification tasks, such as sorting emails into ""spam"" or ""not spam"". By calculating the probability that the dependent variable will be categorized into a specific group, logistic regression provides a probabilistic framework that supports informed decision-making.[20]
 As a simple example, we can use a logistic regression with one explanatory variable and two categories to answer the following question:
 A group of 20 students spends between 0 and 6 hours studying for an exam. How does the number of hours spent studying affect the probability of the student passing the exam?
 The reason for using logistic regression for this problem is that the values of the dependent variable, pass and fail, while represented by ""1"" and ""0"", are not cardinal numbers. If the problem was changed so that pass/fail was replaced with the grade 0–100 (cardinal numbers), then simple regression analysis could be used.
 The table shows the number of hours each student spent studying, and whether they passed (1) or failed (0).
 We wish to fit a logistic function to the data consisting of the hours studied (xk) and the outcome of the test (yk =1 for pass, 0 for fail). The data points are indexed by the subscript k which runs from 



k
=
1


{\displaystyle k=1}

 to 



k
=
K
=
20


{\displaystyle k=K=20}

. The x variable is called the ""explanatory variable"", and the y variable is called the ""categorical variable"" consisting of two categories: ""pass"" or ""fail"" corresponding to the categorical values 1 and 0 respectively.
 The logistic function is of the form:
 where μ is a location parameter (the midpoint of the curve, where 



p
(
μ
)
=
1

/

2


{\displaystyle p(\mu )=1/2}

) and s is a scale parameter. This expression may be rewritten as:
 where 




β

0


=
−
μ

/

s


{\displaystyle \beta _{0}=-\mu /s}

 and is known as the intercept (it is the vertical intercept or y-intercept of the line 



y
=

β

0


+

β

1


x


{\displaystyle y=\beta _{0}+\beta _{1}x}

), and 




β

1


=
1

/

s


{\displaystyle \beta _{1}=1/s}

 (inverse scale parameter or rate parameter): these are the y-intercept and slope of the log-odds as a function of x. Conversely, 



μ
=
−

β

0



/


β

1




{\displaystyle \mu =-\beta _{0}/\beta _{1}}

 and 



s
=
1

/


β

1




{\displaystyle s=1/\beta _{1}}

.
 Remark: This model is actually an oversimplification, since it assumes everybody will pass if they learn long enough (limit = 1). The limit value should be a variable parameter too, if you want to make it more realistic.
 The usual measure of goodness of fit for a logistic regression uses logistic loss (or log loss), the negative log-likelihood. For a given xk and yk, write 




p

k


=
p
(

x

k


)


{\displaystyle p_{k}=p(x_{k})}

. The ⁠




p

k




{\displaystyle p_{k}}

⁠ are the probabilities that the corresponding ⁠




y

k




{\displaystyle y_{k}}

⁠ will equal one and ⁠



1
−

p

k




{\displaystyle 1-p_{k}}

⁠ are the probabilities that they will be zero (see Bernoulli distribution). We wish to find the values of ⁠




β

0




{\displaystyle \beta _{0}}

⁠ and ⁠




β

1




{\displaystyle \beta _{1}}

⁠ which give the ""best fit"" to the data. In the case of linear regression, the sum of the squared deviations of the fit from the data points (yk), the squared error loss, is taken as a measure of the goodness of fit, and the best fit is obtained when that function is minimized.
 The log loss for the k-th point ⁠




ℓ

k




{\displaystyle \ell _{k}}

⁠ is:
 The log loss can be interpreted as the ""surprisal"" of the actual outcome ⁠




y

k




{\displaystyle y_{k}}

⁠ relative to the prediction ⁠




p

k




{\displaystyle p_{k}}

⁠, and is a measure of information content. Log loss is always greater than or equal to 0, equals 0 only in case of a perfect prediction (i.e., when 




p

k


=
1


{\displaystyle p_{k}=1}

 and 




y

k


=
1


{\displaystyle y_{k}=1}

, or 




p

k


=
0


{\displaystyle p_{k}=0}

 and 




y

k


=
0


{\displaystyle y_{k}=0}

), and approaches infinity as the prediction gets worse (i.e., when 




y

k


=
1


{\displaystyle y_{k}=1}

 and 




p

k


→
0


{\displaystyle p_{k}\to 0}

 or 




y

k


=
0


{\displaystyle y_{k}=0}

 and 




p

k


→
1


{\displaystyle p_{k}\to 1}

), meaning the actual outcome is ""more surprising"". Since the value of the logistic function is always strictly between zero and one, the log loss is always greater than zero and less than infinity. Unlike in a linear regression, where the model can have zero loss at a point by passing through a data point (and zero loss overall if all points are on a line), in a logistic regression it is not possible to have zero loss at any points, since ⁠




y

k




{\displaystyle y_{k}}

⁠ is either 0 or 1, but ⁠



0
<

p

k


<
1


{\displaystyle 0<p_{k}<1}

⁠.
 These can be combined into a single expression:
 This expression is more formally known as the cross-entropy of the predicted distribution 





(



p

k


,
(
1
−

p

k


)


)




{\displaystyle {\big (}p_{k},(1-p_{k}){\big )}}

 from the actual distribution 





(



y

k


,
(
1
−

y

k


)


)




{\displaystyle {\big (}y_{k},(1-y_{k}){\big )}}

, as probability distributions on the two-element space of (pass, fail).
 The sum of these, the total loss, is the overall negative log-likelihood ⁠



−
ℓ


{\displaystyle -\ell }

⁠, and the best fit is obtained for those choices of ⁠




β

0




{\displaystyle \beta _{0}}

⁠ and ⁠




β

1




{\displaystyle \beta _{1}}

⁠ for which ⁠



−
ℓ


{\displaystyle -\ell }

⁠ is minimized.
 Alternatively, instead of minimizing the loss, one can maximize its inverse, the (positive) log-likelihood:
 or equivalently maximize the likelihood function itself, which is the probability that the given data set is produced by a particular logistic function:
 This method is known as maximum likelihood estimation.
 Since ℓ is nonlinear in ⁠




β

0




{\displaystyle \beta _{0}}

⁠ and ⁠




β

1




{\displaystyle \beta _{1}}

⁠, determining their optimum values will require numerical methods. One method of maximizing  ℓ is to require the derivatives of ℓ with respect to ⁠




β

0




{\displaystyle \beta _{0}}

⁠ and ⁠




β

1




{\displaystyle \beta _{1}}

⁠ to be zero:
 and the maximization procedure can be accomplished by solving the above two equations for ⁠




β

0




{\displaystyle \beta _{0}}

⁠ and ⁠




β

1




{\displaystyle \beta _{1}}

⁠, which, again, will generally require the use of numerical methods.
 The values of ⁠




β

0




{\displaystyle \beta _{0}}

⁠ and ⁠




β

1




{\displaystyle \beta _{1}}

⁠ which maximize ℓ and L using the above data are found to be:
 which yields a value for μ and s of:
 The ⁠




β

0




{\displaystyle \beta _{0}}

⁠ and ⁠




β

1




{\displaystyle \beta _{1}}

⁠ coefficients may be entered into the logistic regression equation to estimate the probability of passing the exam.
 For example, for a student who studies 2 hours, entering the value 



x
=
2


{\displaystyle x=2}

 into the equation gives the estimated probability of passing the exam of 0.25:
 Similarly, for a student who studies 4 hours, the estimated probability of passing the exam is 0.87:
 This table shows the estimated probability of passing the exam for several values of hours studying.
 The logistic regression analysis gives the following output.
 By the Wald test, the output indicates that hours studying is significantly associated with the probability of passing the exam (



p
=
0.017


{\displaystyle p=0.017}

). Rather than the Wald method, the recommended method[21] to calculate the p-value for logistic regression is the likelihood-ratio test (LRT), which for these data give 



p
≈
0.00064


{\displaystyle p\approx 0.00064}

 (see § Deviance and likelihood ratio tests below).
 This simple model is an example of binary logistic regression, and has one explanatory variable and a binary categorical variable which can assume one of two categorical values. Multinomial logistic regression is the generalization of binary logistic regression to include any number of explanatory variables and any number of categories.
 An explanation of logistic regression can begin with an explanation of the standard logistic function. The logistic function is a sigmoid function, which takes any real input 



t


{\displaystyle t}

, and outputs a value between zero and one.[2] For the logit, this is interpreted as taking input log-odds and having output probability. The standard logistic function 



σ
:

R

→
(
0
,
1
)


{\displaystyle \sigma :\mathbb {R} \rightarrow (0,1)}

 is defined as follows:
 A graph of the logistic function on the t-interval (−6,6) is shown in Figure 1.
 Let us assume that 



t


{\displaystyle t}

 is a linear function of a single explanatory variable 



x


{\displaystyle x}

 (the case where 



t


{\displaystyle t}

 is a linear combination of multiple explanatory variables is treated similarly). We can then express 



t


{\displaystyle t}

 as follows:
 And the general logistic function 



p
:

R

→
(
0
,
1
)


{\displaystyle p:\mathbb {R} \rightarrow (0,1)}

 can now be written as:
 In the logistic model, 



p
(
x
)


{\displaystyle p(x)}

 is interpreted as the probability of the dependent variable 



Y


{\displaystyle Y}

 equaling a success/case rather than a failure/non-case. It is clear that the response variables 




Y

i




{\displaystyle Y_{i}}

 are not identically distributed: 



P
(

Y

i


=
1
∣
X
)


{\displaystyle P(Y_{i}=1\mid X)}

 differs from one data point 




X

i




{\displaystyle X_{i}}

 to another, though they are independent given design matrix 



X


{\displaystyle X}

 and shared parameters 



β


{\displaystyle \beta }

.[11]
 We can now define the logit (log odds) function as the inverse 



g
=

σ

−
1




{\displaystyle g=\sigma ^{-1}}

 of the standard logistic function. It is easy to see that it satisfies:
 and equivalently, after exponentiating both sides we have the odds:
 In the above equations, the terms are as follows:
 The odds of the dependent variable equaling a case (given some linear combination 



x


{\displaystyle x}

 of the predictors) is equivalent to the exponential function of the linear regression expression. This illustrates how the logit serves as a link function between the probability and the linear regression expression. Given that the logit ranges between negative and positive infinity, it provides an adequate criterion upon which to conduct linear regression and the logit is easily converted back into the odds.[2]
 So we define odds of the dependent variable equaling a case (given some linear combination 



x


{\displaystyle x}

 of the predictors) as follows:
 For a continuous independent variable the odds ratio can be defined as:
 This exponential relationship provides an interpretation for 




β

1




{\displaystyle \beta _{1}}

: The odds multiply by 




e


β

1






{\displaystyle e^{\beta _{1}}}

 for every 1-unit increase in x.[22]
 For a binary independent variable the odds ratio is defined as 






a
d


b
c





{\displaystyle {\frac {ad}{bc}}}

 where a, b, c and d are cells in a 2×2 contingency table.[23]
 If there are multiple explanatory variables, the above expression 




β

0


+

β

1


x


{\displaystyle \beta _{0}+\beta _{1}x}

 can be revised to 




β

0


+

β

1



x

1


+

β

2



x

2


+
⋯
+

β

m



x

m


=

β

0


+

∑

i
=
1


m



β

i



x

i




{\displaystyle \beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}+\cdots +\beta _{m}x_{m}=\beta _{0}+\sum _{i=1}^{m}\beta _{i}x_{i}}

. Then when this is used in the equation relating the log odds of a success to the values of the predictors, the linear regression will be a multiple regression with m explanators; the parameters 




β

i




{\displaystyle \beta _{i}}

 for all 



i
=
0
,
1
,
2
,
…
,
m


{\displaystyle i=0,1,2,\dots ,m}

 are all estimated.
 Again, the more traditional equations are:
 and
 where usually 



b
=
e


{\displaystyle b=e}

.
 A dataset contains N points. Each point i consists of a set of m input variables x1,i ... xm,i (also called independent variables, explanatory variables, predictor variables, features, or attributes), and a binary outcome variable Yi (also known as a dependent variable, response variable, output variable, or class), i.e. it can assume only the two possible values 0 (often meaning ""no"" or ""failure"") or 1 (often meaning ""yes"" or ""success""). The goal of logistic regression is to use the dataset to create a predictive model of the outcome variable.
 As in linear regression, the outcome variables Yi are assumed to depend on the explanatory variables x1,i ... xm,i.
 The explanatory variables may be of any type: real-valued, binary, categorical, etc.  The main distinction is between continuous variables and discrete variables.
 (Discrete variables referring to more than two possible choices are typically coded using dummy variables (or indicator variables), that is, separate explanatory variables taking the value 0 or 1 are created for each possible value of the discrete variable, with a 1 meaning ""variable does have the given value"" and a 0 meaning ""variable does not have that value"".)
 Formally, the outcomes Yi are described as being Bernoulli-distributed data, where each outcome is determined by an unobserved probability pi that is specific to the outcome at hand, but related to the explanatory variables.  This can be expressed in any of the following equivalent forms:
 The meanings of these four lines are:
 The basic idea of logistic regression is to use the mechanism already developed for linear regression by modeling the probability pi using a linear predictor function, i.e. a linear combination of the explanatory variables and a set of regression coefficients that are specific to the model at hand but the same for all trials.  The linear predictor function 



f
(
i
)


{\displaystyle f(i)}

 for a particular data point i is written as:
 where 




β

0


,
…
,

β

m




{\displaystyle \beta _{0},\ldots ,\beta _{m}}

 are regression coefficients indicating the relative effect of a particular explanatory variable on the outcome.
 The model is usually put into a more compact form as follows:
 This makes it possible to write the linear predictor function as follows:
 using the notation for a dot product between two vectors.
 The above example of binary logistic regression on one explanatory variable can be generalized to binary logistic regression on any number of explanatory variables x1, x2,... and any number of categorical values 



y
=
0
,
1
,
2
,
…


{\displaystyle y=0,1,2,\dots }

.
 To begin with, we may consider a logistic model with M explanatory variables, x1, x2 ... xM and, as in the example above, two categorical values (y = 0 and 1). For the simple binary logistic regression model, we assumed a linear relationship between the predictor variable and the log-odds (also called logit) of the event that 



y
=
1


{\displaystyle y=1}

. This linear relationship may be extended to the case of M explanatory variables:
 where t is the log-odds and 




β

i




{\displaystyle \beta _{i}}

 are parameters of the model. An additional generalization has been introduced in which the base of the model (b) is not restricted to Euler's number e. In most applications, the base 



b


{\displaystyle b}

 of the logarithm is usually taken to be e. However, in some cases it can be easier to communicate results by working in base 2 or base 10.
 For a more compact notation, we will specify the explanatory variables and the β coefficients as ⁠



(
M
+
1
)


{\displaystyle (M+1)}

⁠-dimensional vectors:
 with an added explanatory variable x0 =1. The logit may now be written as:
 Solving for the probability p that 



y
=
1


{\displaystyle y=1}

 yields:
 where 




S

b




{\displaystyle S_{b}}

 is the sigmoid function with base 



b


{\displaystyle b}

. The above formula shows that once the 




β

m




{\displaystyle \beta _{m}}

 are fixed, we can easily compute either the log-odds that 



y
=
1


{\displaystyle y=1}

 for a given observation, or the probability that 



y
=
1


{\displaystyle y=1}

 for a given observation. The main use-case of a logistic model is to be given an observation 




x



{\displaystyle {\boldsymbol {x}}}

, and estimate the probability 



p
(

x

)


{\displaystyle p({\boldsymbol {x}})}

 that 



y
=
1


{\displaystyle y=1}

. The optimum beta coefficients may again be found by maximizing the log-likelihood. For K measurements, defining 





x


k




{\displaystyle {\boldsymbol {x}}_{k}}

 as the explanatory vector of the k-th measurement, and 




y

k




{\displaystyle y_{k}}

 as the categorical outcome of that measurement, the log likelihood may be written in a form very similar to the simple 



M
=
1


{\displaystyle M=1}

 case above:
 As in the simple example above, finding the optimum β parameters will require numerical methods. One useful technique is to equate the derivatives of the log likelihood with respect to each of the β parameters to zero yielding a set of equations which will hold at the maximum of the log likelihood:
 where xmk is the value of the xm explanatory variable from the k-th measurement.
 Consider an example with 



M
=
2


{\displaystyle M=2}

 explanatory variables, 



b
=
10


{\displaystyle b=10}

, and coefficients 




β

0


=
−
3


{\displaystyle \beta _{0}=-3}

, 




β

1


=
1


{\displaystyle \beta _{1}=1}

, and 




β

2


=
2


{\displaystyle \beta _{2}=2}

 which have been determined by the above method. To be concrete, the model is:
 where p is the probability of the event that 



y
=
1


{\displaystyle y=1}

. This can be interpreted as follows:
 In the above cases of two categories (binomial logistic regression), the categories were indexed by ""0"" and ""1"", and we had two probabilities: The probability that the outcome was in category 1 was given by 



p
(

x

)


{\displaystyle p({\boldsymbol {x}})}

and the probability that the outcome was in category 0 was given by 



1
−
p
(

x

)


{\displaystyle 1-p({\boldsymbol {x}})}

. The sum of these probabilities equals 1, which must be true, since ""0"" and ""1"" are the only possible categories in this setup.
 In general, if we have ⁠



M
+
1


{\displaystyle M+1}

⁠ explanatory variables (including x0) and ⁠



N
+
1


{\displaystyle N+1}

⁠ categories, we will need ⁠



N
+
1


{\displaystyle N+1}

⁠ separate probabilities,  one for each category, indexed by n, which describe the probability that the categorical outcome y will be in category y=n, conditional on the vector of covariates x. The sum of these probabilities over all categories must equal 1. Using the mathematically convenient base e, these probabilities are:
 Each of the probabilities except 




x

2




{\displaystyle x_{2}}

 will have their own set of regression coefficients 





β


n




{\displaystyle {\boldsymbol {\beta }}_{n}}

.  It can be seen that, as required, the sum of the 




p

n


(

x

)


{\displaystyle p_{n}({\boldsymbol {x}})}

 over all categories n is 1. The selection of 




p

0


(

x

)


{\displaystyle p_{0}({\boldsymbol {x}})}

 to be defined in terms of the other probabilities is artificial. Any of the probabilities could have been selected to be so defined. This special value of n is termed the ""pivot index"", and the log-odds (tn) are expressed in terms of the pivot probability and are again expressed as a linear combination of the explanatory variables:
 Note also that for the simple case of 



N
=
1


{\displaystyle N=1}

, the two-category case is recovered, with 



p
(

x

)
=

p

1


(

x

)


{\displaystyle p({\boldsymbol {x}})=p_{1}({\boldsymbol {x}})}

 and 




p

0


(

x

)
=
1
−

p

1


(

x

)


{\displaystyle p_{0}({\boldsymbol {x}})=1-p_{1}({\boldsymbol {x}})}

.
 The log-likelihood that a particular set of K measurements or data points will be generated by the above probabilities can now be calculated. Indexing each measurement by k, let the k-th set of measured explanatory variables be denoted by 





x


k




{\displaystyle {\boldsymbol {x}}_{k}}

 and their categorical outcomes be denoted by 




y

k




{\displaystyle y_{k}}

 which can be equal to any integer in [0,N]. The log-likelihood is then:
 where 



Δ
(
n
,

y

k


)


{\displaystyle \Delta (n,y_{k})}

 is an indicator function which equals 1 if yk = n and zero otherwise. In the case of two explanatory variables, this indicator function was defined as yk when n = 1 and 1-yk when n = 0. This was convenient, but not necessary.[24] Again, the optimum beta coefficients may be found by maximizing the log-likelihood function generally using numerical methods. A possible method of solution is to set the derivatives of the log-likelihood with respect to each beta coefficient equal to zero and solve for the beta coefficients:
 where 




β

n
m




{\displaystyle \beta _{nm}}

 is the m-th coefficient of the 





β


n




{\displaystyle {\boldsymbol {\beta }}_{n}}

 vector and 




x

m
k




{\displaystyle x_{mk}}

 is the m-th explanatory variable of the k-th measurement. Once the beta coefficients have been estimated from the data, we will be able to estimate the probability that any subsequent set of explanatory variables will result in any of the possible outcome categories.
 There are various equivalent specifications and interpretations of logistic regression, which fit into different types of more general models, and allow different generalizations.
 The particular model used by logistic regression, which distinguishes it from standard linear regression and from other types of regression analysis used for binary-valued outcomes, is the way the probability of a particular outcome is linked to the linear predictor function:
 Written using the more compact notation described above, this is:
 This formulation expresses logistic regression as a type of generalized linear model, which predicts variables with various types of probability distributions by fitting a linear predictor function of the above form to some sort of arbitrary transformation of the expected value of the variable.
 The intuition for transforming using the logit function (the natural log of the odds) was explained above[clarification needed].  It also has the practical effect of converting the probability (which is bounded to be between 0 and 1) to a variable that ranges over 



(
−
∞
,
+
∞
)


{\displaystyle (-\infty ,+\infty )}

 — thereby matching the potential range of the linear prediction function on the right side of the equation.
 Both the probabilities pi and the regression coefficients are unobserved, and the means of determining them is not part of the model itself.  They are typically determined by some sort of optimization procedure, e.g. maximum likelihood estimation, that finds values that best fit the observed data (i.e. that give the most accurate predictions for the data already observed), usually subject to regularization conditions that seek to exclude unlikely values, e.g. extremely large values for any of the regression coefficients.  The use of a regularization condition is equivalent to doing maximum a posteriori (MAP) estimation, an extension of maximum likelihood.  (Regularization is most commonly done using a squared regularizing function, which is equivalent to placing a zero-mean Gaussian prior distribution on the coefficients, but other regularizers are also possible.)  Whether or not regularization is used, it is usually not possible to find a closed-form solution; instead, an iterative numerical method must be used, such as iteratively reweighted least squares (IRLS) or, more commonly these days, a quasi-Newton method such as the L-BFGS method.[25]
 The interpretation of the βj parameter estimates is as the additive effect on the log of the odds for a unit change in the j the explanatory variable.  In the case of a dichotomous explanatory variable, for instance, gender 




e

β




{\displaystyle e^{\beta }}

 is the estimate of the odds of having the outcome for, say, males compared with females.
 An equivalent formula uses the inverse of the logit function, which is the logistic function, i.e.:
 The formula can also be written as a probability distribution (specifically, using a probability mass function):
 The logistic model has an equivalent formulation as a latent-variable model.  This formulation is common in the theory of discrete choice models and makes it easier to extend to certain more complicated models with multiple, correlated choices, as well as to compare logistic regression to the closely related probit model.
 Imagine that, for each trial i, there is a continuous latent variable Yi* (i.e. an unobserved random variable) that is distributed as follows:
 where
 i.e. the latent variable can be written directly in terms of the linear predictor function and an additive random error variable that is distributed according to a standard logistic distribution.
 Then Yi can be viewed as an indicator for whether this latent variable is positive:
 The choice of modeling the error variable specifically with a standard logistic distribution, rather than a general logistic distribution with the location and scale set to arbitrary values, seems restrictive, but in fact, it is not.  It must be kept in mind that we can choose the regression coefficients ourselves, and very often can use them to offset changes in the parameters of the error variable's distribution.  For example, a logistic error-variable distribution with a non-zero location parameter μ (which sets the mean) is equivalent to a distribution with a zero location parameter, where μ has been added to the intercept coefficient.  Both situations produce the same value for Yi* regardless of settings of explanatory variables.  Similarly, an arbitrary scale parameter s is equivalent to setting the scale parameter to 1 and then dividing all regression coefficients by s.  In the latter case, the resulting value of Yi* will be smaller by a factor of s than in the former case, for all sets of explanatory variables — but critically, it will always remain on the same side of 0, and hence lead to the same Yi choice.
 (This predicts that the irrelevancy of the scale parameter may not carry over into more complex models where more than two choices are available.)
 It turns out that this formulation is exactly equivalent to the preceding one, phrased in terms of the generalized linear model and without any latent variables.  This can be shown as follows, using the fact that the cumulative distribution function (CDF) of the standard logistic distribution is the logistic function, which is the inverse of the logit function, i.e.
 Then:
 This formulation—which is standard in discrete choice models—makes clear the relationship between logistic regression (the ""logit model"") and the probit model, which uses an error variable distributed according to a standard normal distribution instead of a standard logistic distribution.  Both the logistic and normal distributions are symmetric with a basic unimodal, ""bell curve"" shape.  The only difference is that the logistic distribution has somewhat heavier tails, which means that it is less sensitive to outlying data (and hence somewhat more robust to model mis-specifications or erroneous data).
 Yet another formulation uses two separate latent variables:
 where
 where EV1(0,1) is a standard type-1 extreme value distribution: i.e.
 Then
 This model has a separate latent variable and a separate set of regression coefficients for each possible outcome of the dependent variable.  The reason for this separation is that it makes it easy to extend logistic regression to multi-outcome categorical variables, as in the multinomial logit model. In such a model, it is natural to model each possible outcome using a different set of regression coefficients.  It is also possible to motivate each of the separate latent variables as the theoretical utility associated with making the associated choice, and thus motivate logistic regression in terms of utility theory. (In terms of utility theory, a rational actor always chooses the choice with the greatest associated utility.) This is the approach taken by economists when formulating discrete choice models, because it both provides a theoretically strong foundation and facilitates intuitions about the model, which in turn makes it easy to consider various sorts of extensions. (See the example below.)
 The choice of the type-1 extreme value distribution seems fairly arbitrary, but it makes the mathematics work out, and it may be possible to justify its use through rational choice theory.
 It turns out that this model is equivalent to the previous model, although this seems non-obvious, since there are now two sets of regression coefficients and error variables, and the error variables have a different distribution.  In fact, this model reduces directly to the previous one with the following substitutions:
 An intuition for this comes from the fact that, since we choose based on the maximum of two values, only their difference matters, not the exact values — and this effectively removes one degree of freedom. Another critical fact is that the difference of two type-1 extreme-value-distributed variables is a logistic distribution, i.e. 



ε
=

ε

1


−

ε

0


∼
Logistic
⁡
(
0
,
1
)
.


{\displaystyle \varepsilon =\varepsilon _{1}-\varepsilon _{0}\sim \operatorname {Logistic} (0,1).}

 We can demonstrate the equivalent as follows:
 As an example, consider a province-level election where the choice is between a right-of-center party, a left-of-center party, and a secessionist party (e.g. the Parti Québécois, which wants Quebec to secede from Canada).  We would then use three latent variables, one for each choice.  Then, in accordance with utility theory, we can then interpret the latent variables as expressing the utility that results from making each of the choices.  We can also interpret the regression coefficients as indicating the strength that the associated factor (i.e. explanatory variable) has in contributing to the utility — or more correctly, the amount by which a unit change in an explanatory variable changes the utility of a given choice.  A voter might expect that the right-of-center party would lower taxes, especially on rich people.  This would give low-income people no benefit, i.e. no change in utility (since they usually don't pay taxes); would cause moderate benefit (i.e. somewhat more money, or moderate utility increase) for middle-incoming people; would cause significant benefits for high-income people.  On the other hand, the left-of-center party might be expected to raise taxes and offset it with increased welfare and other assistance for the lower and middle classes.  This would cause significant positive benefit to low-income people, perhaps a weak benefit to middle-income people, and significant negative benefit to high-income people.  Finally, the secessionist party would take no direct actions on the economy, but simply secede. A low-income or middle-income voter might expect basically no clear utility gain or loss from this, but a high-income voter might expect negative utility since he/she is likely to own companies, which will have a harder time doing business in such an environment and probably lose money.
 These intuitions can be expressed as follows:
 This clearly shows that
 Yet another formulation combines the two-way latent variable formulation above with the original formulation higher up without latent variables, and in the process provides a link to one of the standard formulations of the multinomial logit.
 Here, instead of writing the logit of the probabilities pi as a linear predictor, we separate the linear predictor into two, one for each of the two outcomes:
 Two separate sets of regression coefficients have been introduced, just as in the two-way latent variable model, and the two equations appear a form that writes the logarithm of the associated probability as a linear predictor, with an extra term 



−
ln
⁡
Z


{\displaystyle -\ln Z}

 at the end.  This term, as it turns out, serves as the normalizing factor ensuring that the result is a distribution.  This can be seen by exponentiating both sides:
 In this form it is clear that the purpose of Z is to ensure that the resulting distribution over Yi is in fact a probability distribution, i.e. it sums to 1.  This means that Z is simply the sum of all un-normalized probabilities, and by dividing each probability by Z, the probabilities become ""normalized"".  That is:
 and the resulting equations are
 Or generally:
 This shows clearly how to generalize this formulation to more than two outcomes, as in multinomial logit.
This general formulation is exactly the softmax function as in
 In order to prove that this is equivalent to the previous model, the above model is overspecified, in that 



Pr
(

Y

i


=
0
)


{\displaystyle \Pr(Y_{i}=0)}

 and 



Pr
(

Y

i


=
1
)


{\displaystyle \Pr(Y_{i}=1)}

 cannot be independently specified: rather 



Pr
(

Y

i


=
0
)
+
Pr
(

Y

i


=
1
)
=
1


{\displaystyle \Pr(Y_{i}=0)+\Pr(Y_{i}=1)=1}

 so knowing one automatically determines the other.  As a result, the model is nonidentifiable, in that multiple combinations of β0 and β1 will produce the same probabilities for all possible explanatory variables.  In fact, it can be seen that adding any constant vector to both of them will produce the same probabilities:
 As a result, we can simplify matters, and restore identifiability, by picking an arbitrary value for one of the two vectors.  We choose to set 





β


0


=

0

.


{\displaystyle {\boldsymbol {\beta }}_{0}=\mathbf {0} .}

  Then,
 and so
 which shows that this formulation is indeed equivalent to the previous formulation. (As in the two-way latent variable formulation, any settings where 




β

=


β


1


−


β


0




{\displaystyle {\boldsymbol {\beta }}={\boldsymbol {\beta }}_{1}-{\boldsymbol {\beta }}_{0}}

 will produce equivalent results.)
 Most treatments of the multinomial logit model start out either by extending the ""log-linear"" formulation presented here or the two-way latent variable formulation presented above, since both clearly show the way that the model could be extended to multi-way outcomes.  In general, the presentation with latent variables is more common in econometrics and political science, where discrete choice models and utility theory reign, while the ""log-linear"" formulation here is more common in computer science, e.g. machine learning and natural language processing.
 The model has an equivalent formulation
 This functional form is commonly called a single-layer perceptron or single-layer artificial neural network. A single-layer neural network computes a continuous output instead of a step function. The derivative of pi with respect to  X = (x1, ..., xk) is computed from the general form:
 where f(X) is an analytic function in X. With this choice, the single-layer neural network is identical to the logistic regression model. This function has a continuous derivative, which allows it to be used in backpropagation. This function is also preferred because its derivative is easily calculated:
 A closely related model assumes that each i is associated not with a single Bernoulli trial but with ni independent identically distributed trials, where the observation Yi is the number of successes observed (the sum of the individual Bernoulli-distributed random variables), and hence follows a binomial distribution:
 An example of this distribution is the fraction of seeds (pi) that germinate after ni are planted.
 In terms of expected values, this model is expressed as follows:
 so that
 Or equivalently:
 This model can be fit using the same sorts of methods as the above more basic model.
 The regression coefficients are usually estimated using maximum likelihood estimation.[26][27] Unlike linear regression with normally distributed residuals, it is not possible to find a closed-form expression for the coefficient values that maximize the likelihood function so an iterative process must be used instead; for example Newton's method. This process begins with a tentative solution, revises it slightly to see if it can be improved, and repeats this revision until no more improvement is made, at which point the process is said to have converged.[26]
 In some instances, the model may not reach convergence. Non-convergence of a model indicates that the coefficients are not meaningful because the iterative process was unable to find appropriate solutions. A failure to converge may occur for a number of reasons: having a large ratio of predictors to cases, multicollinearity, sparseness, or complete separation.
 Binary logistic regression (



y
=
0


{\displaystyle y=0}

 or 



y
=
1


{\displaystyle y=1}

) can, for example, be calculated using iteratively reweighted least squares (IRLS), which is equivalent to maximizing the log-likelihood of a Bernoulli distributed  process using Newton's method. If the problem is written in vector matrix form, with parameters 





w


T


=
[

β

0


,

β

1


,

β

2


,
…
]


{\displaystyle \mathbf {w} ^{T}=[\beta _{0},\beta _{1},\beta _{2},\ldots ]}

, explanatory variables 




x

(
i
)
=
[
1
,

x

1


(
i
)
,

x

2


(
i
)
,
…

]

T




{\displaystyle \mathbf {x} (i)=[1,x_{1}(i),x_{2}(i),\ldots ]^{T}}

 and expected value of the Bernoulli distribution 



μ
(
i
)
=


1

1
+

e

−


w


T



x

(
i
)







{\displaystyle \mu (i)={\frac {1}{1+e^{-\mathbf {w} ^{T}\mathbf {x} (i)}}}}

, the parameters 




w



{\displaystyle \mathbf {w} }

 can be found using the following iterative algorithm:
 where 




S

=
diag
⁡
(
μ
(
i
)
(
1
−
μ
(
i
)
)
)


{\displaystyle \mathbf {S} =\operatorname {diag} (\mu (i)(1-\mu (i)))}

 is a diagonal weighting matrix, 




μ

=
[
μ
(
1
)
,
μ
(
2
)
,
…
]


{\displaystyle {\boldsymbol {\mu }}=[\mu (1),\mu (2),\ldots ]}

 the vector of expected values,
 The regressor matrix and 




y

(
i
)
=
[
y
(
1
)
,
y
(
2
)
,
…

]

T




{\displaystyle \mathbf {y} (i)=[y(1),y(2),\ldots ]^{T}}

 the vector of response variables. More details can be found in the literature.[29]
 In a Bayesian statistics context, prior distributions are normally placed on the regression coefficients, for example in the form of Gaussian distributions.  There is no conjugate prior of the likelihood function in logistic regression.  When Bayesian inference was performed analytically, this made the posterior distribution difficult to calculate except in very low dimensions.  Now, though, automatic software such as OpenBUGS, JAGS, PyMC, Stan or Turing.jl allows these posteriors to be computed using simulation, so lack of conjugacy is not a concern.  However, when the sample size or the number of parameters is large, full Bayesian simulation can be slow, and people often use approximate methods such as variational Bayesian methods and expectation propagation.
 Widely used, the ""one in ten rule"", states that logistic regression models give stable values for the explanatory variables if based on a minimum of about 10 events per explanatory variable (EPV); where event denotes the cases belonging to the less frequent category in the dependent variable. Thus a study designed to use 



k


{\displaystyle k}

 explanatory variables for an event (e.g. myocardial infarction) expected to occur in a proportion 



p


{\displaystyle p}

 of participants in the study will require a total of 



10
k

/

p


{\displaystyle 10k/p}

 participants. However, there is considerable debate about the reliability of this rule, which is based on simulation studies and lacks a secure theoretical underpinning.[30] According to some authors[31] the rule is overly conservative in some circumstances, with the authors stating, ""If we (somewhat subjectively) regard confidence interval coverage less than 93 percent, type I error greater than 7 percent, or relative bias greater than 15 percent as problematic, our results indicate that problems are fairly frequent with 2–4 EPV, uncommon with 5–9 EPV, and still observed with 10–16 EPV. The worst instances of each problem were not severe with 5–9 EPV and usually comparable to those with 10–16 EPV"".[32]
 Others have found results that are not consistent with the above, using different criteria.  A useful criterion is whether the fitted model will be expected to achieve the same predictive discrimination in a new sample as it appeared to achieve in the model development sample.  For that criterion, 20 events per candidate variable may be required.[33]  Also, one can argue that 96 observations are needed only to estimate the model's intercept precisely enough that the margin of error in predicted probabilities is ±0.1 with a 0.95 confidence level.[13]
 In any fitting procedure, the addition of another fitting parameter to a model (e.g. the beta parameters in a logistic regression model) will almost always improve the ability of the model to predict the measured outcomes. This will be true even if the additional term has no predictive value, since the model will simply be ""overfitting"" to the noise in the data.  The question arises as to whether the improvement gained by the addition of another fitting parameter is significant enough to recommend the inclusion of the additional term, or whether the improvement is simply that which may be expected from overfitting.
 In short, for logistic regression, a statistic known as the deviance is defined which is a measure of the error between the logistic model fit and the outcome data. In the limit of a large number of data points, the deviance is chi-squared distributed, which allows a chi-squared test to be implemented in order to determine the significance of the explanatory variables.
 Linear regression and logistic regression have many similarities. For example, in simple linear regression, a set of K data points (xk, yk) are fitted to a proposed model function of the form 



y
=

b

0


+

b

1


x


{\displaystyle y=b_{0}+b_{1}x}

. The fit is obtained by choosing the b parameters which minimize the sum of the squares of the residuals (the squared error term) for each data point:
 The minimum value which constitutes the fit will be denoted by 







ε
^




2




{\displaystyle {\hat {\varepsilon }}^{2}}


 The idea of a null model may be introduced, in which it is assumed that the x variable is of no use in predicting the yk outcomes: The data points are fitted to a null model function of the form y = b0 with a squared error term:
 The fitting process consists of choosing a value of b0 which minimizes 




ε

2




{\displaystyle \varepsilon ^{2}}

 of the fit to the null model, denoted by  




ε

φ


2




{\displaystyle \varepsilon _{\varphi }^{2}}

 where the 



φ


{\displaystyle \varphi }

 subscript denotes the null model. It is seen that the null model is optimized by 




b

0


=


y
¯




{\displaystyle b_{0}={\overline {y}}}

 where 





y
¯




{\displaystyle {\overline {y}}}

 is the mean of the yk values, and the optimized 




ε

φ


2




{\displaystyle \varepsilon _{\varphi }^{2}}

 is:
 which is proportional to the square of the (uncorrected) sample standard deviation of the yk data points.
 We can imagine a case where the yk data points are randomly assigned to the various xk, and then fitted using the proposed model. Specifically, we can consider the fits of the proposed model to every permutation of the yk outcomes. It can be shown that the optimized error of any of these fits will never be less than the optimum error of the null model, and that the difference between these minimum error will follow a chi-squared distribution, with degrees of freedom equal those of the proposed model minus those of the null model which, in this case, will be 



2
−
1
=
1


{\displaystyle 2-1=1}

. Using the chi-squared test, we may then estimate how many of these permuted sets of yk will yield an minimum error less than or equal to the minimum  error using the original yk, and so we can estimate how significant an improvement is given by the inclusion of the x variable in the proposed model.
 For logistic regression, the measure of goodness-of-fit is the likelihood function L, or its logarithm, the log-likelihood ℓ. The likelihood function L is analogous to the 




ε

2




{\displaystyle \varepsilon ^{2}}

 in the linear regression case, except that the likelihood is maximized rather than minimized. Denote the maximized log-likelihood of the proposed model by 






ℓ
^





{\displaystyle {\hat {\ell }}}

.
 In the case of simple binary logistic regression, the set of K data points are fitted in a probabilistic sense to a function of the form:
 where ⁠



p
(
x
)


{\displaystyle p(x)}

⁠ is the probability that 



y
=
1


{\displaystyle y=1}

. The log-odds are given by:
 and the log-likelihood is:
 For the null model, the probability that 



y
=
1


{\displaystyle y=1}

 is given by:
 The log-odds for the null model are given by:
 and the log-likelihood is:
 Since we have 




p

φ


=


y
¯




{\displaystyle p_{\varphi }={\overline {y}}}

 at the maximum of L, the maximum log-likelihood for the null model is
 The optimum 




β

0




{\displaystyle \beta _{0}}

 is:
 where 





y
¯




{\displaystyle {\overline {y}}}

 is again the mean of the yk values. Again, we can conceptually consider the fit of the proposed model to every permutation of the yk and it can be shown that the maximum log-likelihood of these permutation fits will never be smaller than that of the null model:
 Also, as an analog to the error of the linear regression case, we may define the deviance of a logistic regression fit as:
 which will always be positive or zero. The reason for this choice is that not only is the deviance a good measure of the goodness of fit, it is also approximately chi-squared distributed, with the approximation improving as the number of data points (K) increases, becoming exactly chi-square distributed in the limit of an infinite number of data points. As in the case of linear regression, we may use this fact to estimate the probability that a random set of data points will give a better fit than the fit obtained by the proposed model, and so have an estimate how significantly the model is improved by including the xk data points in the proposed model.
 For the simple model of student test scores described above, the maximum value of the log-likelihood of the null model is 







ℓ
^




φ


=
−
13.8629
…


{\displaystyle {\hat {\ell }}_{\varphi }=-13.8629\ldots }

 The maximum value of the log-likelihood for the simple model is 






ℓ
^



=
−
8.02988
…


{\displaystyle {\hat {\ell }}=-8.02988\ldots }

 so that the deviance is 



D
=
2
(



ℓ
^



−




ℓ
^




φ


)
=
11.6661
…


{\displaystyle D=2({\hat {\ell }}-{\hat {\ell }}_{\varphi })=11.6661\ldots }


 Using the chi-squared test of significance, the integral of the chi-squared distribution with one degree of freedom from 11.6661... to infinity is equal to 0.00063649...
 This effectively means that about 6 out of a 10,000 fits to random yk can be expected to have a better fit (smaller deviance) than the given yk and so we can conclude that the inclusion of the x variable and data in the proposed model is a very significant improvement over the null model. In other words, we reject the null hypothesis with 



1
−
D
≈
99.94
%


{\displaystyle 1-D\approx 99.94\%}

 confidence.
 Goodness of fit in linear regression models is generally measured using R2. Since this has no direct analog in logistic regression, various methods[34]: ch.21  including the following can be used instead.
 In linear regression analysis, one is concerned with partitioning variance via the sum of squares calculations – variance in the criterion is essentially divided into variance accounted for by the predictors and residual variance. In logistic regression analysis, deviance is used in lieu of a sum of squares calculations.[35] Deviance is analogous to the sum of squares calculations in linear regression[2]  and is a measure of the lack of fit to the data in a logistic regression model.[35] When a ""saturated"" model is available (a model with a theoretically perfect fit), deviance is calculated by comparing a given model with the saturated model.[2]  This computation gives the likelihood-ratio test:[2]
 In the above equation, D represents the deviance and ln represents the natural logarithm. The log of this likelihood ratio (the ratio of the fitted model to the saturated model) will produce a negative value, hence the need for a negative sign. D can be shown to follow an approximate chi-squared distribution.[2]  Smaller values indicate better fit as the fitted model deviates less from the saturated model. When assessed upon a chi-square distribution, nonsignificant chi-square values indicate very little unexplained variance and thus, good model fit. Conversely, a significant chi-square value indicates that a significant amount of the variance is unexplained.
 When the saturated model is not available (a common case), deviance is calculated simply as −2·(log likelihood of the fitted model), and the reference to the saturated model's log likelihood can be removed from all that follows without harm.
 Two measures of deviance are particularly important in logistic regression: null deviance and model deviance. The null deviance represents the difference between a model with only the intercept (which means ""no predictors"") and the saturated model. The model deviance represents the difference between a model with at least one predictor and the saturated model.[35] In this respect, the null model provides a baseline upon which to compare predictor models. Given that deviance is a measure of the difference between a given model and the saturated model, smaller values indicate better fit. Thus, to assess the contribution of a predictor or set of predictors, one can subtract the model deviance from the null deviance and assess the difference on a 




χ

s
−
p


2


,


{\displaystyle \chi _{s-p}^{2},}

  chi-square distribution with degrees of freedom[2] equal to the difference in the number of parameters estimated.
 Let
 Then the difference of both is:
 If the model deviance is significantly smaller than the null deviance then one can conclude that the predictor or set of predictors significantly improve the model's fit. This is analogous to the F-test used in linear regression analysis to assess the significance of prediction.[35]
 In linear regression the squared multiple correlation, R2 is used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors.[35] In logistic regression analysis, there is no agreed upon analogous measure, but there are several competing measures each with limitations.[35][36]
 Four of the most commonly used indices and one less commonly used one are examined on this page:
 The Hosmer–Lemeshow test uses a test statistic that asymptotically follows a 




χ

2




{\displaystyle \chi ^{2}}

 distribution to assess whether or not the observed event rates match expected event rates in subgroups of the model population.  This test is considered to be obsolete by some statisticians because of its dependence on arbitrary binning of predicted probabilities and relative low power.[37]
 After fitting the model, it is likely that researchers will want to examine the contribution of individual predictors. To do so, they will want to examine the regression coefficients. In linear regression, the regression coefficients represent the change in the criterion for each unit change in the predictor.[35] In logistic regression, however, the regression coefficients represent the change in the logit for each unit change in the predictor. Given that the logit is not intuitive, researchers are likely to focus on a predictor's effect on the exponential function of the regression coefficient – the odds ratio (see definition). In linear regression, the significance of a regression coefficient is assessed by computing a t test. In logistic regression, there are several different tests designed to assess the significance of an individual predictor, most notably the likelihood ratio test and the Wald statistic.
 The likelihood-ratio test discussed above to assess model fit is also the recommended procedure to assess the contribution of individual ""predictors"" to a given model.[2][26][35] In the case of a single predictor model, one simply compares the deviance of the predictor model with that of the null model on a chi-square distribution with a single degree of freedom. If the predictor model has significantly smaller deviance (c.f. chi-square using the difference in degrees of freedom of the two models), then one can conclude that there is a significant association between the ""predictor"" and the outcome. Although some common statistical packages (e.g. SPSS) do provide likelihood ratio test statistics, without this computationally intensive test it would be more difficult to assess the contribution of individual predictors in the multiple logistic regression case.[citation needed] To assess the contribution of individual predictors one can enter the predictors hierarchically, comparing each new model with the previous to determine the contribution of each predictor.[35] There is some debate among statisticians about the appropriateness of so-called ""stepwise"" procedures.[weasel words] The fear is that they may not preserve nominal statistical properties and may become misleading.[38]
 Alternatively, when assessing the contribution of individual predictors in a given model, one may examine the significance of the Wald statistic. The Wald statistic, analogous to the t-test in linear regression, is used to assess the significance of coefficients. The Wald statistic is the ratio of the square of the regression coefficient to the square of the standard error of the coefficient and is asymptotically distributed as a chi-square distribution.[26]
 Although several statistical packages (e.g., SPSS, SAS) report the Wald statistic to assess the contribution of individual predictors, the Wald statistic has limitations. When the regression coefficient is large, the standard error of the regression coefficient also tends to be larger increasing the probability of Type-II error. The Wald statistic also tends to be biased when data are sparse.[35]
 Suppose cases are rare. Then we might wish to sample them more frequently than their prevalence in the population. For example, suppose there is a disease that affects 1 person in 10,000 and to collect our data we need to do a complete physical. It may be too expensive to do thousands of physicals of healthy people in order to obtain data for only a few diseased individuals. Thus, we may evaluate more diseased individuals, perhaps all of the rare outcomes. This is also retrospective sampling, or equivalently it is called unbalanced data. As a rule of thumb, sampling controls at a rate of five times the number of cases will produce sufficient control data.[39]
 Logistic regression is unique in that it may be estimated on unbalanced data, rather than randomly sampled data, and still yield correct coefficient estimates of the effects of each independent variable on the outcome.  That is to say, if we form a logistic model from such data, if the model is correct in the general population, the 




β

j




{\displaystyle \beta _{j}}

 parameters are all correct except for 




β

0




{\displaystyle \beta _{0}}

. We can correct 




β

0




{\displaystyle \beta _{0}}

 if we know the true prevalence as follows:[39]
 where 



π


{\displaystyle \pi }

 is the true prevalence and 






π
~





{\displaystyle {\tilde {\pi }}}

 is the prevalence in the sample.
 Like other forms of regression analysis, logistic regression makes use of one or more predictor variables that may be either continuous or categorical. Unlike ordinary linear regression, however, logistic regression is used for predicting dependent variables that take membership in one of a limited number of categories (treating the dependent variable in the binomial case as the outcome of a Bernoulli trial) rather than a continuous outcome. Given this difference, the assumptions of linear regression are violated. In particular, the residuals cannot be normally distributed. In addition, linear regression may make nonsensical predictions for a binary dependent variable. What is needed is a way to convert a binary variable into a continuous one that can take on any real value (negative or positive). To do that, binomial logistic regression first calculates the odds of the event happening for different levels of each independent variable, and then takes its logarithm to create a continuous criterion as a transformed version of the dependent variable. The logarithm of the odds is the logit of the probability, the logit is defined as follows:




logit
⁡
p
=
ln
⁡


p

1
−
p





for 

0
<
p
<
1

.


{\displaystyle \operatorname {logit} p=\ln {\frac {p}{1-p}}\quad {\text{for }}0<p<1\,.}


 Although the dependent variable in logistic regression is Bernoulli, the logit is on an unrestricted scale.[2] The logit function is the link function in this kind of generalized linear model, i.e.




logit
⁡


E


⁡
(
Y
)
=

β

0


+

β

1


x


{\displaystyle \operatorname {logit} \operatorname {\mathcal {E}} (Y)=\beta _{0}+\beta _{1}x}


 Y is the Bernoulli-distributed response variable and x is the predictor variable; the β values are the linear parameters.
 The logit of the probability of success is then fitted to the predictors. The predicted value of the logit is converted back into predicted odds, via the inverse of the natural logarithm – the exponential function. Thus, although the observed dependent variable in binary logistic regression is a 0-or-1 variable, the logistic regression estimates the odds, as a continuous variable, that the dependent variable is a 'success'. In some applications, the odds are all that is needed. In others, a specific yes-or-no prediction is needed for whether the dependent variable is or is not a 'success'; this categorical prediction can be based on the computed odds of success, with predicted odds above some chosen cutoff value being translated into a prediction of success.
 Of all the functional forms used for estimating the probabilities of a particular categorical outcome which optimize the fit by maximizing the likelihood function (e.g. probit regression, Poisson regression, etc.), the logistic regression solution is unique in that it is a maximum entropy solution.[40] This is a case of a general property: an exponential family of distributions maximizes entropy, given an expected value. In the case of the logistic model, the logistic function is the natural parameter of the Bernoulli distribution (it is in ""canonical form"", and the logistic function is the canonical link function), while other sigmoid functions are non-canonical link functions; this underlies its mathematical elegance and ease of optimization. See Exponential family § Maximum entropy derivation for details.
 In order to show this, we use the method of Lagrange multipliers. The Lagrangian is equal to the entropy plus the sum of the products of Lagrange multipliers times various constraint expressions. The general multinomial case will be considered, since the proof is not made that much simpler by considering simpler cases. Equating the derivative of the Lagrangian with respect to the various probabilities to zero yields a functional form for those probabilities which corresponds to those used in logistic regression.[40]
 As in the above section on multinomial logistic regression, we will consider ⁠



M
+
1


{\displaystyle M+1}

⁠ explanatory variables denoted ⁠




x

m




{\displaystyle x_{m}}

⁠ and which include 




x

0


=
1


{\displaystyle x_{0}=1}

. There will be a total of K data points, indexed by 



k
=
{
1
,
2
,
…
,
K
}


{\displaystyle k=\{1,2,\dots ,K\}}

, and the data points are given by 




x

m
k




{\displaystyle x_{mk}}

 and ⁠




y

k




{\displaystyle y_{k}}

⁠. The xmk will also be represented as an ⁠



(
M
+
1
)


{\displaystyle (M+1)}

⁠-dimensional vector  





x


k


=
{

x

0
k


,

x

1
k


,
…
,

x

M
k


}


{\displaystyle {\boldsymbol {x}}_{k}=\{x_{0k},x_{1k},\dots ,x_{Mk}\}}

. There will be ⁠



N
+
1


{\displaystyle N+1}

⁠ possible values of the categorical variable y ranging from 0 to N.
 Let pn(x) be the probability, given explanatory variable vector x, that the outcome will be 



y
=
n


{\displaystyle y=n}

. Define 




p

n
k


=

p

n


(


x


k


)


{\displaystyle p_{nk}=p_{n}({\boldsymbol {x}}_{k})}

 which is the probability that for the k-th measurement, the categorical outcome is n.
 The Lagrangian will be expressed as a function of the probabilities pnk and will minimized by equating the derivatives of the Lagrangian with respect to these probabilities to zero. An important point is that the probabilities are treated equally and the fact that they sum to 1 is part of the Lagrangian formulation, rather than being assumed from the beginning.
 The first contribution to the Lagrangian is the entropy:
 The log-likelihood is:
 Assuming the multinomial logistic function, the derivative of the log-likelihood with respect the beta coefficients was found to be:
 A very important point here is that this expression is (remarkably) not an explicit function of the beta coefficients. It is only a function of the probabilities pnk and the data. Rather than being specific to the assumed multinomial logistic case, it is taken to be a general statement of the condition at which the log-likelihood is maximized and makes no reference to the functional form of pnk. There are then (M+1)(N+1) fitting constraints and the fitting constraint term in the Lagrangian is then:
 where the λnm are the appropriate Lagrange multipliers. There are K normalization constraints which may be written:
 so that the normalization term in the Lagrangian is:
 where the αk are the appropriate Lagrange multipliers. The Lagrangian is then the sum of the above three terms:
 Setting the derivative of the Lagrangian with respect to one of the probabilities to zero yields:
 Using the more condensed vector notation:
 and dropping the primes on the n and k indices, and then solving for 




p

n
k




{\displaystyle p_{nk}}

 yields:
 where:
 Imposing the normalization constraint, we can solve for the Zk and write the probabilities as:
 The 





λ


n




{\displaystyle {\boldsymbol {\lambda }}_{n}}

 are not all independent. We can add any constant ⁠



(
M
+
1
)


{\displaystyle (M+1)}

⁠-dimensional vector to each of the 





λ


n




{\displaystyle {\boldsymbol {\lambda }}_{n}}

 without changing the value of the 




p

n
k




{\displaystyle p_{nk}}

 probabilities so that there are only N rather than ⁠



N
+
1


{\displaystyle N+1}

⁠ independent 





λ


n




{\displaystyle {\boldsymbol {\lambda }}_{n}}

. In the multinomial logistic regression section above, the 





λ


0




{\displaystyle {\boldsymbol {\lambda }}_{0}}

 was subtracted from each 





λ


n




{\displaystyle {\boldsymbol {\lambda }}_{n}}

 which set the exponential term involving 





λ


0




{\displaystyle {\boldsymbol {\lambda }}_{0}}

 to 1, and the beta coefficients were given by 





β


n


=


λ


n


−


λ


0




{\displaystyle {\boldsymbol {\beta }}_{n}={\boldsymbol {\lambda }}_{n}-{\boldsymbol {\lambda }}_{0}}

.
 In machine learning applications where logistic regression is used for binary classification, the MLE minimises the cross-entropy loss function.
 Logistic regression is an important machine learning algorithm. The goal is to model the probability of a random variable 



Y


{\displaystyle Y}

 being 0 or 1 given experimental data.[41]
 Consider a generalized linear model function parameterized by 



θ


{\displaystyle \theta }

,
 Therefore,
 and since 



Y
∈
{
0
,
1
}


{\displaystyle Y\in \{0,1\}}

, we see that 



Pr
(
y
∣
X
;
θ
)


{\displaystyle \Pr(y\mid X;\theta )}

 is given by 



Pr
(
y
∣
X
;
θ
)
=

h

θ


(
X

)

y


(
1
−

h

θ


(
X
)

)

(
1
−
y
)


.


{\displaystyle \Pr(y\mid X;\theta )=h_{\theta }(X)^{y}(1-h_{\theta }(X))^{(1-y)}.}

 We now calculate the likelihood function assuming that all the observations in the sample are independently Bernoulli distributed,
 Typically, the log likelihood is maximized,
 which is maximized using optimization techniques such as gradient descent.
 Assuming the 



(
x
,
y
)


{\displaystyle (x,y)}

 pairs are drawn uniformly from the underlying distribution, then in the limit of large N,
 where 



H
(
Y
∣
X
)


{\displaystyle H(Y\mid X)}

 is the conditional entropy and 




D

KL




{\displaystyle D_{\text{KL}}}

 is the Kullback–Leibler divergence. This leads to the intuition that by maximizing the log-likelihood of a model, you are minimizing the KL divergence of your model from the maximal entropy distribution. Intuitively searching for the model that makes the fewest assumptions in its parameters.
 Logistic regression can be seen as a special case of the generalized linear model and thus analogous to linear regression. The model of logistic regression, however, is based on quite different assumptions (about the relationship between the dependent and independent variables) from those of linear regression. In particular, the key differences between these two models can be seen in the following two features of logistic regression. First, the conditional distribution 



y
∣
x


{\displaystyle y\mid x}

 is a Bernoulli distribution rather than a Gaussian distribution, because the dependent variable is binary. Second, the predicted values are probabilities and are therefore restricted to (0,1) through the logistic distribution function because logistic regression predicts the probability of particular outcomes rather than the outcomes themselves.
 A common alternative to the logistic model (logit model) is the probit model, as the related names suggest. From the perspective of generalized linear models, these differ in the choice of link function: the logistic model uses the logit function (inverse logistic function), while the probit model uses the probit function (inverse error function). Equivalently, in the latent variable interpretations of these two methods, the first assumes a standard logistic distribution of errors and the second a standard normal distribution of errors.[42] Other sigmoid functions or error distributions can be used instead.
 Logistic regression is an alternative to Fisher's 1936 method, linear discriminant analysis.[43]  If the assumptions of linear discriminant analysis hold, the conditioning can be reversed to produce logistic regression.  The converse is not true, however, because logistic regression does not require the multivariate normal assumption of discriminant analysis.[44]
 The assumption of linear predictor effects can easily be relaxed using techniques such as spline functions.[13]
 A detailed history of the logistic regression is given in Cramer (2002). The logistic function was developed as a model of population growth and named ""logistic"" by Pierre François Verhulst in the 1830s and 1840s, under the guidance of Adolphe Quetelet; see Logistic function § History for details.[45] In his earliest paper (1838), Verhulst did not specify how he fit the curves to the data.[46][47] In his more detailed paper (1845), Verhulst determined the three parameters of the model by making the curve pass through three observed points, which yielded poor predictions.[48][49]
 The logistic function was independently developed in chemistry as a model of autocatalysis (Wilhelm Ostwald, 1883).[50] An autocatalytic reaction is one in which one of the products is itself a catalyst for the same reaction, while the supply of one of the reactants is fixed. This naturally gives rise to the logistic equation for the same reason as population growth: the reaction is self-reinforcing but constrained.
 The logistic function was independently rediscovered as a model of population growth in 1920 by Raymond Pearl and Lowell Reed, published as Pearl & Reed (1920), which led to its use in modern statistics. They were initially unaware of Verhulst's work and presumably learned about it from L. Gustave du Pasquier, but they gave him little credit and did not adopt his terminology.[51] Verhulst's priority was acknowledged and the term ""logistic"" revived by Udny Yule in 1925 and has been followed since.[52] Pearl and Reed first applied the model to the population of the United States, and also initially fitted the curve by making it pass through three points; as with Verhulst, this again yielded poor results.[53]
 In the 1930s, the probit model was developed and systematized by Chester Ittner Bliss, who coined the term ""probit"" in Bliss (1934), and by John Gaddum in Gaddum (1933), and the model fit by maximum likelihood estimation by Ronald A. Fisher in Fisher (1935), as an addendum to Bliss's work. The probit model was principally used in bioassay, and had been preceded by earlier work dating to 1860; see Probit model § History. The probit model influenced the subsequent development of the logit model and these models competed with each other.[54]
 The logistic model was likely first used as an alternative to the probit model in bioassay by Edwin Bidwell Wilson and his student Jane Worcester in Wilson & Worcester (1943).[55] However, the development of the logistic model as a general alternative to the probit model was principally due to the work of Joseph Berkson over many decades, beginning in Berkson (1944), where he coined ""logit"", by analogy with ""probit"", and continuing through Berkson (1951) and following years.[56] The logit model was initially dismissed as inferior to the probit model, but ""gradually achieved an equal footing with the probit"",[57] particularly between 1960 and 1970. By 1970, the logit model achieved parity with the probit model in use in statistics journals and thereafter surpassed it. This relative popularity was due to the adoption of the logit outside of bioassay, rather than displacing the probit within bioassay, and its informal use in practice; the logit's popularity is credited to the logit model's computational simplicity, mathematical properties, and generality, allowing its use in varied fields.[3]
 Various refinements occurred during that time, notably by David Cox, as in Cox (1958).[4]
 The multinomial logit model was introduced independently in Cox (1966) and Theil (1969), which greatly increased the scope of application and the popularity of the logit model.[58] In 1973 Daniel McFadden linked the multinomial logit to the theory of discrete choice, specifically Luce's choice axiom, showing that the multinomial logit followed from the assumption of independence of irrelevant alternatives and interpreting odds of alternatives as relative preferences;[59] this gave a theoretical foundation for the logistic regression.[58]
 There are large numbers of extensions:
",statist logist model logit model statist model model event linear combin one independ variabl regress analysi logist regress logit regress estim paramet logist model coeffici linear non linear combin binari logist regress singl binari depend variabl code indic variabl two valu label independ variabl binari variabl two class code indic variabl continu variabl real valu correspond probabl valu label vari certainli valu certainli valu henc label function convert probabl logist function henc name unit measur scale call logit logist unit henc altern name see background definit formal mathemat exampl work exampl binari variabl wide use statist model probabl certain class event take place probabl team win patient healthi etc see applic logist model commonli use model binari regress sinc binari variabl gener categor variabl two possibl valu whether imag cat dog lion etc binari logist regress gener multinomi logist regress multipl categori order one use ordin logist regress exampl proport odd ordin logist model see extens extens logist regress model simpli model probabl output term input perform statist classif classifi though use make classifi instanc choos cutoff valu classifi input probabl greater cutoff one class cutoff common way make binari classifi analog linear model binari variabl differ sigmoid function instead logist function convert linear combin probabl also use notabl probit model see altern defin characterist logist model increas one independ variabl multipl scale odd given outcom constant rate independ variabl paramet binari depend variabl gener odd ratio abstractli logist function natur paramet bernoulli distribut sens simplest way convert real number probabl particular maxim entropi minim ad inform sens make fewest assumpt data model see maximum entropi paramet logist regress commonli estim estim mle express unlik linear least squar see model fit logist regress mle play similarli basic role binari categor respons linear regress ordinari least squar ol play scalar respons simpl baselin model see comparison linear regress discuss logist regress gener statist model origin develop popular primarili joseph berkson begin berkson coin logit see histori logist regress use variou field includ machin learn medic field social scienc exampl trauma injuri sever score triss wide use predict mortal injur patient origin develop boyd et al use logist regress mani medic scale use assess sever patient develop use logist regress logist regress may use predict risk develop given diseas diabet coronari heart diseas base observ characterist patient age sex bodi mass index result variou blood test anoth exampl might predict whether nepales voter vote nepali congress communist parti nepal parti base age incom sex race state resid vote previou elect etc techniqu also use engin especi predict probabl failur given process system product also use market applic predict custom propens purchas product halt subscript etc econom use predict likelihood person end labor forc busi applic would predict likelihood homeown default mortgag condit random field extens logist regress sequenti data use natur languag process disast planner engin reli model predict decis take household build occup evacu build fire wildfir hurrican among other model help develop reliabl disast manag plan safer design built environ logist regress supervis machin learn algorithm wide use binari classif task identifi whether email spam diagnos diseas assess presenc absenc specif condit base patient test result approach util logist sigmoid function transform linear combin input featur probabl valu rang probabl indic likelihood given input correspond one two predefin categori essenti mechan logist regress ground logist function abil model probabl binari outcom accur distinct curv logist function effect map number valu within interv featur render particularli suitabl binari classif task sort email spam spam calcul probabl depend variabl categor specif group logist regress provid probabilist framework support inform simpl exampl use logist regress one explanatori variabl two categori answer follow question group student spend hour studi exam number hour spent studi affect probabl student pass exam reason use logist regress problem valu depend variabl pass fail repres cardin number problem chang replac grade cardin number simpl regress analysi could use tabl show number hour student spent studi whether pass fail wish fit logist function data consist hour studi xk outcom test yk pass fail data point index subscript k run k k k x variabl call explanatori variabl variabl call categor variabl consist two categori pass fail correspond categor valu respect logist function form μ locat paramet midpoint curv p μ p scale paramet express may rewritten β μ known intercept vertic intercept line β β x x β invers scale paramet rate paramet slope function convers μ β β β remark model actual oversimplif sinc assum everybodi pass learn long enough limit limit valu variabl paramet want make realist usual measur good fit logist regress use logist loss log loss neg given xk yk write p k p x k k k p k k probabl correspond k k equal one p k k probabl zero see bernoulli distribut wish find valu β β give best fit data case linear regress sum squar deviat fit data point yk squar error loss taken measur good fit best fit obtain function minim log loss point ℓ k k log loss interpret surpris actual outcom k k rel predict p k k measur inform content log loss alway greater equal equal case perfect predict p k k k k p k k k k approach infin predict get wors k k p k k k k p k k mean actual outcom surpris sinc valu logist function alway strictli zero one log loss alway greater zero less infin unlik linear regress model zero loss point pass data point zero loss overal point line logist regress possibl zero loss point sinc k k either p k k combin singl express express formal known predict distribut p k p k k k actual distribut k k k k probabl distribut space pass fail sum total loss overal neg ℓ best fit obtain choic β β ℓ minim altern instead minim loss one maxim invers posit equival maxim likelihood function probabl given data set produc particular logist function method known maximum likelihood estim sinc ℓ nonlinear β β determin optimum valu requir numer method one method maxim ℓ requir deriv ℓ respect β β zero maxim procedur accomplish solv two equat β β gener requir use numer method valu β β maxim ℓ l use data found yield valu μ β β coeffici may enter logist regress equat estim probabl pass exam exampl student studi hour enter valu x equat give estim probabl pass exam similarli student studi hour estim probabl pass exam tabl show estim probabl pass exam sever valu hour studi logist regress analysi give follow output wald test output indic hour studi significantli associ probabl pass exam p rather wald method recommend method calcul logist regress test lrt data give p see devianc likelihood ratio test simpl model exampl binari logist regress one explanatori variabl binari categor variabl assum one two categor valu multinomi logist regress gener binari logist regress includ number explanatori variabl number categori explan logist regress begin explan standard logist function logist function sigmoid function take real input output valu zero one logit interpret take input output probabl standard logist function σ r r defin follow graph logist function shown figur let us assum linear function singl explanatori variabl x x case linear combin multipl explanatori variabl treat similarli express follow gener logist function p r p r written logist model p x p x interpret probabl depend variabl equal rather clear respons variabl ident distribut p x p x differ one data point x anoth though independ given design matrix x x share paramet β defin logit log odd function invers g σ standard logist function easi see satisfi equival exponenti side odd equat term follow odd depend variabl equal case given linear combin x x predictor equival exponenti function linear regress express illustr logit serv link function probabl linear regress express given logit rang neg posit infin provid adequ criterion upon conduct linear regress logit easili convert back odd defin odd depend variabl equal case given linear combin x x predictor follow continu independ variabl odd ratio defin exponenti relationship provid interpret β odd multipli e β everi increas x binari independ variabl odd ratio defin b c ad bc b c cell conting tabl multipl explanatori variabl express β β x x revis β β x β x β x β β x use equat relat log odd success valu predictor linear regress multipl regress explan paramet β estim tradit equat usual b e dataset contain n point point consist set input variabl xm also call independ variabl explanatori variabl predictor variabl featur attribut binari outcom variabl yi also known depend variabl respons variabl output variabl class assum two possibl valu often mean failur often mean ye success goal logist regress use dataset creat predict model outcom variabl linear regress outcom variabl yi assum depend explanatori variabl xm explanatori variabl may type binari categor etc main distinct continu variabl discret variabl discret variabl refer two possibl choic typic code use dummi variabl indic variabl separ explanatori variabl take valu creat possibl valu discret variabl mean variabl given valu mean variabl valu formal outcom yi describ data outcom determin unobserv probabl pi specif outcom hand relat explanatori variabl express follow equival form mean four line basic idea logist regress use mechan alreadi develop linear regress model probabl pi use linear predictor function linear combin explanatori variabl set regress coeffici specif model hand trial linear predictor function f f particular data point written β β regress coeffici indic rel effect particular explanatori variabl outcom model usual put compact form follow make possibl write linear predictor function follow use notat dot product two vector exampl binari logist regress one explanatori variabl gener binari logist regress number explanatori variabl number categor valu begin may consid logist model explanatori variabl xm exampl two categor valu simpl binari logist regress model assum linear relationship predictor variabl also call logit event linear relationship may extend case explanatori variabl β paramet model addit gener introduc base model b restrict euler number applic base b b logarithm usual taken howev case easier commun result work base base compact notat specifi explanatori variabl β coeffici vector ad explanatori variabl logit may written solv probabl p yield b b sigmoid function base b b formula show β fix easili comput either given observ probabl given observ main logist model given observ x x estim probabl p x p x optimum beta coeffici may found maxim k measur defin x k x k explanatori vector measur k k categor outcom measur log likelihood may written form similar simpl case simpl exampl find optimum β paramet requir numer method one use techniqu equat deriv log likelihood respect β paramet zero yield set equat hold maximum log likelihood xmk valu xm explanatori variabl measur consid exampl explanatori variabl b coeffici β β β determin method concret model p probabl event interpret follow case two categori binomi logist regress categori index two probabl probabl outcom categori given p x p x probabl outcom categori given p x x sum probabl equal must true sinc possibl categori setup gener explanatori variabl includ n categori need n separ probabl one categori index n describ probabl categor outcom categori condit vector covari sum probabl categori must equal use mathemat conveni base e probabl probabl except x set regress coeffici β n n seen requir sum p n x n x categori n select p x x defin term probabl artifici probabl could select defin special valu n term pivot index tn express term pivot probabl express linear combin explanatori variabl note also simpl case n case recov p x p x p x x p x p x x x particular set k measur data point gener probabl calcul index measur k let set measur explanatori variabl denot x k x k categor outcom denot k k equal integ n δ n k n k indic function equal yk n zero otherwis case two explanatori variabl indic function defin yk n n conveni necessari optimum beta coeffici may found maxim function gener use numer method possibl method solut set deriv respect beta coeffici equal zero solv beta coeffici β n nm coeffici β n n vector x k mk explanatori variabl measur beta coeffici estim data abl estim probabl subsequ set explanatori variabl result possibl outcom categori variou equival specif interpret logist regress fit differ type gener model allow differ gener particular model use logist regress distinguish standard linear regress type regress analysi use outcom way probabl particular outcom link linear predictor function written use compact notat describ formul express logist regress type gener linear model predict variabl variou type probabl distribut fit linear predictor function form sort arbitrari transform expect valu variabl intuit transform use logit function natur log odd explain clarif need also practic effect convert probabl bound variabl rang therebi match potenti rang linear predict function right side equat probabl pi regress coeffici unobserv mean determin part model typic determin sort optim procedur maximum likelihood estim find valu best fit observ data give accur predict data alreadi observ usual subject regular condit seek exclud unlik valu extrem larg valu regress coeffici use regular condit equival maximum posteriori map estim extens maximum likelihood regular commonli done use squar regular function equival place gaussian prior distribut coeffici regular also possibl whether regular use usual possibl find solut instead iter numer method must use iter reweight least squar irl commonli day method method interpret βj paramet estim addit effect log odd unit chang j explanatori variabl case dichotom explanatori variabl instanc gender e β estim odd outcom say male compar femal equival formula use invers logit function logist function formula also written probabl distribut specif use probabl mass function logist model equival formul model formul common theori discret choic model make easier extend certain complic model multipl correl choic well compar logist regress close relat probit model imagin trial continu latent variabl yi unobserv random variabl distribut follow latent variabl written directli term linear predictor function addit random error variabl distribut accord standard logist distribut yi view indic whether latent variabl posit choic model error variabl specif standard logist distribut rather gener logist distribut locat scale set arbitrari valu seem restrict fact must kept mind choos regress coeffici often use offset chang paramet error variabl distribut exampl logist distribut locat paramet μ set mean equival distribut zero locat paramet μ ad intercept coeffici situat produc valu yi regardless set explanatori variabl similarli arbitrari scale paramet equival set scale paramet divid regress coeffici latter case result valu yi smaller factor former case set explanatori variabl critic alway remain side henc lead yi choic predict irrelev scale paramet may carri complex model two choic avail turn formul exactli equival preced one phrase term gener linear model without latent variabl shown follow use fact cumul distribut function cdf standard logist distribut logist function invers logit function standard discret choic clear relationship logist regress logit model probit model use error variabl distribut accord standard normal distribut instead standard logist distribut logist normal distribut symmetr basic unimod bell curv shape differ logist distribut somewhat heavier tail mean less sensit outli data henc somewhat robust model erron data yet anoth formul use two separ latent variabl standard extrem valu distribut model separ latent variabl separ set regress coeffici possibl outcom depend variabl reason separ make easi extend logist regress categor variabl multinomi logit model model natur model possibl outcom use differ set regress coeffici also possibl motiv separ latent variabl theoret util associ make associ choic thu motiv logist regress term util theori term util theori ration actor alway choos choic greatest associ util approach taken economist formul discret choic model provid theoret strong foundat facilit intuit model turn make easi consid variou sort extens see exampl choic extrem valu distribut seem fairli arbitrari make mathemat work may possibl justifi use ration choic theori turn model equival previou model although seem sinc two set regress coeffici error variabl error variabl differ distribut fact model reduc directli previou one follow substitut intuit come fact sinc choos base maximum two valu differ matter exact valu effect remov one degre freedom anoth critic fact differ two variabl logist distribut ε ε ε logist logist demonstr equival follow exampl consid elect choic parti parti secessionist parti parti québécoi want quebec seced canada would use three latent variabl one choic accord util theori interpret latent variabl express util result make choic also interpret regress coeffici indic strength associ factor explanatori variabl contribut util correctli amount unit chang explanatori variabl chang util given choic voter might expect parti would lower tax especi rich peopl would give peopl benefit chang util sinc usual pay tax would caus moder benefit somewhat money moder util increas peopl would caus signific benefit peopl hand parti might expect rais tax offset increas welfar assist lower middl class would caus signific posit benefit peopl perhap weak benefit peopl signific neg benefit peopl final secessionist parti would take direct action economi simpli seced voter might expect basic clear util gain loss voter might expect neg util sinc like compani harder time busi environ probabl lose money intuit express follow clearli show yet anoth formul combin latent variabl formul origin formul higher without latent variabl process provid link one standard formul multinomi logit instead write logit probabl pi linear predictor separ linear predictor two one two outcom two separ set regress coeffici introduc latent variabl model two equat appear form write logarithm associ probabl linear predictor extra term ln z z end term turn serv normal factor ensur result distribut seen exponenti side form clear purpos z ensur result distribut yi fact probabl distribut sum mean z simpli sum probabl divid probabl z probabl becom normal result equat gener show clearli gener formul two outcom multinomi logit gener formul exactli softmax function order prove equival previou model model overspecifi pr pr independ specifi rather pr pr know one automat determin result model nonidentifi multipl combin produc probabl possibl explanatori variabl fact seen ad constant vector produc probabl result simplifi matter restor identifi pick arbitrari valu one two vector choos set β show formul inde equival previou formul latent variabl formul set β β β produc equival result treatment multinomi logit model start either extend formul present latent variabl formul present sinc clearli show way model could extend outcom gener present latent variabl common econometr polit scienc discret choic model util theori reign formul common comput scienc machin learn natur languag process model equival formul function form commonli call perceptron artifici neural network neural network comput continu output instead step function deriv pi respect x xk comput gener form f x analyt function choic neural network ident logist regress model function continu deriv allow use backpropag function also prefer deriv easili calcul close relat model assum associ singl bernoulli trial ni independ ident distribut trial observ yi number success observ sum individu random variabl henc follow binomi distribut exampl distribut fraction seed pi germin ni plant term expect valu model express follow equival model fit use sort method basic model regress coeffici usual estim use maximum likelihood estim unlik linear regress normal distribut residu possibl find express coeffici valu maxim likelihood function iter process must use instead exampl newton method process begin tent solut revis slightli see improv repeat revis improv made point process said converg instanc model may reach converg model indic coeffici meaning iter process unabl find appropri solut failur converg may occur number reason larg ratio predictor case multicollinear spars complet separ binari logist regress exampl calcul use iter reweight least squar irl equival maxim bernoulli distribut process use newton method problem written vector matrix form paramet w β β β w explanatori variabl x x x x expect valu bernoulli distribut μ e w x w x paramet w w found use follow iter algorithm diag μ μ diag diagon weight matrix μ μ μ vector expect valu regressor matrix vector respons variabl detail found literatur bayesian statist context prior distribut normal place regress coeffici exampl form gaussian distribut conjug prior likelihood function logist regress bayesian infer perform analyt made posterior distribut difficult calcul except low dimens though automat softwar openbug jag pymc stan allow posterior comput use simul lack conjugaci concern howev sampl size number paramet larg full bayesian simul slow peopl often use approxim method variat bayesian method expect propag wide use one ten rule state logist regress model give stabl valu explanatori variabl base minimum event per explanatori variabl epv event denot case belong less frequent categori depend variabl thu studi design use k k explanatori variabl event myocardi infarct expect occur proport p p particip studi requir total k p particip howev consider debat reliabl rule base simul studi lack secur theoret underpin accord author rule overli conserv circumst author state somewhat subject regard confid interv coverag less percent type error greater percent rel bia greater percent problemat result indic problem fairli frequent epv uncommon epv still observ epv worst instanc problem sever epv usual compar epv other found result consist use differ criteria use criterion whether fit model expect achiev predict discrimin new sampl appear achiev model develop sampl criterion event per candid variabl may requir also one argu observ need estim model intercept precis enough margin error predict probabl confid level fit procedur addit anoth fit paramet model beta paramet logist regress model almost alway improv abil model predict measur outcom true even addit term predict valu sinc model simpli overfit nois data question aris whether improv gain addit anoth fit paramet signific enough recommend inclus addit term whether improv simpli may expect overfit short logist regress statist known devianc defin measur error logist model fit outcom data limit larg number data point devianc distribut allow test implement order determin signific explanatori variabl linear regress logist regress mani similar exampl simpl linear regress set k data point xk yk fit propos model function form b b x x fit obtain choos b paramet minim sum squar residu squar error term data point minimum valu constitut fit denot ε idea null model may introduc assum x variabl use predict yk outcom data point fit null model function form squar error term fit process consist choos valu minim ε fit null model denot ε φ φ subscript denot null model seen null model optim b mean yk valu optim ε φ proport squar uncorrect sampl standard deviat yk data point imagin case yk data point randomli assign variou xk fit use propos model specif consid fit propos model everi permut yk outcom shown optim error fit never less optimum error null model differ minimum error follow distribut degre freedom equal propos model minu null model case use test may estim mani permut set yk yield minimum error less equal minimum error use origin yk estim signific improv given inclus x variabl propos model logist regress measur likelihood function l logarithm likelihood function l analog ε linear regress case except likelihood maxim rather minim denot maxim propos model ℓ case simpl binari logist regress set k data point fit probabilist sens function form p x p x probabl given null model probabl given null model given sinc p φ maximum l maximum null model optimum β mean yk valu conceptu consid fit propos model everi permut yk shown maximum permut fit never smaller null model also analog error linear regress case may defin devianc logist regress fit alway posit zero reason choic devianc good measur good fit also approxim distribut approxim improv number data point k increas becom exactli distribut limit infinit number data point case linear regress may use fact estim probabl random set data point give better fit fit obtain propos model estim significantli model improv includ xk data point propos model simpl model student test score describ maximum valu null model ℓ φ maximum valu simpl model ℓ devianc ℓ ℓ φ use test signific integr distribut one degre freedom infin equal effect mean fit random yk expect better fit smaller devianc given yk conclud inclus x variabl data propos model signific improv null model word reject null hypothesi confid good fit linear regress model gener measur use sinc direct analog logist regress variou method includ follow use instead linear regress analysi one concern partit varianc via sum squar calcul varianc criterion essenti divid varianc account predictor residu varianc logist regress analysi devianc use lieu sum squar calcul devianc analog sum squar calcul linear regress measur lack fit data logist regress model satur model avail model theoret perfect fit devianc calcul compar given model satur model comput give test equat repres devianc ln repres natur logarithm log likelihood ratio ratio fit model satur model produc neg valu henc need neg sign shown follow approxim distribut smaller valu indic better fit fit model deviat less satur model assess upon distribut nonsignific valu indic littl unexplain varianc thu good model fit convers signific valu indic signific amount varianc unexplain satur model avail common case devianc calcul simpli log likelihood fit model refer satur model log likelihood remov follow without harm two measur devianc particularli import logist regress null devianc model devianc null devianc repres differ model intercept mean predictor satur model model devianc repres differ model least one predictor satur model respect null model provid baselin upon compar predictor model given devianc measur differ given model satur model smaller valu indic better fit thu assess contribut predictor set predictor one subtract model devianc null devianc assess differ χ p distribut degre freedom equal differ number paramet estim let differ model devianc significantli smaller null devianc one conclud predictor set predictor significantli improv model fit analog use linear regress analysi assess signific predict linear regress squar multipl correl use assess good fit repres proport varianc criterion explain predictor logist regress analysi agre upon analog measur sever compet measur limit four commonli use indic one less commonli use one examin page test use test statist asymptot follow χ distribut assess whether observ event rate match expect event rate subgroup model popul test consid obsolet statistician depend arbitrari bin predict probabl rel low power fit model like research want examin contribut individu predictor want examin regress coeffici linear regress regress coeffici repres chang criterion unit chang predictor logist regress howev regress coeffici repres chang logit unit chang predictor given logit intuit research like focu predictor effect exponenti function regress coeffici odd ratio see definit linear regress signific regress coeffici assess comput test logist regress sever differ test design assess signific individu predictor notabl likelihood ratio test wald statist test discuss assess model fit also recommend procedur assess contribut individu predictor given model case singl predictor model one simpli compar devianc predictor model null model distribut singl degre freedom predictor model significantli smaller devianc use differ degre freedom two model one conclud signific associ predictor outcom although common statist packag spss provid likelihood ratio test statist without comput intens test would difficult assess contribut individu predictor multipl logist regress case citat need assess contribut individu predictor one enter predictor hierarch compar new model previou determin contribut predictor debat among statistician appropri stepwis procedur weasel word fear may preserv nomin statist properti may becom mislead altern assess contribut individu predictor given model one may examin signific wald statist wald statist analog linear regress use assess signific coeffici wald statist ratio squar regress coeffici squar standard error coeffici asymptot distribut distribut although sever statist packag spss sa report wald statist assess contribut individu predictor wald statist limit regress coeffici larg standard error regress coeffici also tend larger increas probabl error wald statist also tend bias data spars suppos case rare might wish sampl frequent preval popul exampl suppos diseas affect person collect data need complet physic may expens thousand physic healthi peopl order obtain data diseas individu thu may evalu diseas individu perhap rare outcom also retrospect sampl equival call unbalanc data rule thumb sampl control rate five time number case produc suffici control data logist regress uniqu may estim unbalanc data rather randomli sampl data still yield correct coeffici estim effect independ variabl outcom say form logist model data model correct gener popul β j j paramet correct except β correct β know true preval follow π true preval π preval sampl like form regress analysi logist regress make use one predictor variabl may either continu categor unlik ordinari linear regress howev logist regress use predict depend variabl take membership one limit number categori treat depend variabl binomi case outcom bernoulli trial rather continu outcom given differ assumpt linear regress violat particular residu normal distribut addit linear regress may make nonsens predict binari depend variabl need way convert binari variabl continu one take real valu neg posit binomi logist regress first calcul odd event happen differ level independ variabl take logarithm creat continu criterion transform version depend variabl logarithm odd logit probabl logit defin follow logit p ln p p p logit p p although depend variabl logist regress bernoulli logit unrestrict scale logit function link function kind gener linear model logit e β β x logit e x respons variabl x predictor variabl β valu linear paramet logit probabl success fit predictor predict valu logit convert back predict odd via invers natur logarithm exponenti function thu although observ depend variabl binari logist regress variabl logist regress estim odd continu variabl depend variabl applic odd need other specif predict need whether depend variabl categor predict base comput odd success predict odd chosen cutoff valu translat predict success function form use estim probabl particular categor outcom optim fit maxim likelihood function probit regress poisson regress etc logist regress solut uniqu maximum entropi solut case gener properti exponenti famili distribut maxim entropi given expect valu case logist model logist function natur paramet bernoulli distribut canon form logist function canon link function sigmoid function link function underli mathemat eleg eas optim see exponenti famili maximum entropi deriv detail order show use method lagrang multipli lagrangian equal entropi plu sum product lagrang multipli time variou constraint express gener multinomi case consid sinc proof made much simpler consid simpler case equat deriv lagrangian respect variou probabl zero yield function form probabl correspond use logist regress section multinomi logist regress consid explanatori variabl denot x includ x total k data point index k k data point given x k mk k k xmk also repres vector x k x k x k x k x k mk n possibl valu categor variabl rang let pn x probabl given explanatori variabl vector x outcom n defin p n k p n x k nk n x k probabl measur categor outcom lagrangian express function probabl pnk minim equat deriv lagrangian respect probabl zero import point probabl treat equal fact sum part lagrangian formul rather assum begin first contribut lagrangian entropi assum multinomi logist function deriv respect beta coeffici found import point express remark explicit function beta coeffici function probabl pnk data rather specif assum multinomi logist case taken gener statement condit maxim make refer function form pnk fit constraint fit constraint term lagrangian λnm appropri lagrang multipli k normal constraint may written normal term lagrangian αk appropri lagrang multipli lagrangian sum three term set deriv lagrangian respect one probabl zero yield use condens vector notat drop prime n k indic solv p n k nk yield impos normal constraint solv zk write probabl λ n n independ add constant vector λ n n without chang valu p n k nk probabl n rather n independ λ n n multinomi logist regress section λ subtract λ n n set exponenti term involv λ beta coeffici given β n λ n λ n n machin learn applic logist regress use binari classif mle minimis loss function logist regress import machin learn algorithm goal model probabl random variabl given experiment data consid gener linear model function parameter θ therefor sinc see pr x θ x given pr x θ h θ x h θ x x x x calcul likelihood function assum observ sampl independ bernoulli distribut typic log likelihood maxim maxim use optim techniqu gradient descent assum x x pair drawn uniformli underli distribut limit larg n h x h x condit entropi kl kl diverg lead intuit maxim model minim kl diverg model maxim entropi distribut intuit search model make fewest assumpt paramet logist regress seen special case gener linear model thu analog linear regress model logist regress howev base quit differ assumpt relationship depend independ variabl linear regress particular key differ two model seen follow two featur logist regress first condit distribut x x bernoulli distribut rather gaussian distribut depend variabl binari second predict valu probabl therefor restrict logist distribut function logist regress predict probabl particular outcom rather outcom common altern logist model logit model probit model relat name suggest perspect gener linear model differ choic link function logist model use logit function invers logist function probit model use probit function invers error function equival latent variabl interpret two method first assum standard logist distribut error second standard normal distribut error sigmoid function error distribut use instead logist regress altern fisher method linear discrimin analysi assumpt linear discrimin analysi hold condit revers produc logist regress convers true howev logist regress requir multivari normal assumpt discrimin analysi assumpt linear predictor effect easili relax use techniqu spline function detail histori logist regress given cramer logist function develop model popul growth name logist pierr françoi verhulst guidanc adolph quetelet see logist function histori detail earliest paper verhulst specifi fit curv data detail paper verhulst determin three paramet model make curv pass three observ point yield poor predict logist function independ develop chemistri model autocatalysi wilhelm ostwald autocatalyt reaction one one product catalyst reaction suppli one reactant fix natur give rise logist equat reason popul growth reaction constrain logist function independ rediscov model popul growth raymond pearl lowel reed publish pearl reed led use modern statist initi unawar verhulst work presum learn gustav du pasquier gave littl credit adopt terminolog verhulst prioriti acknowledg term logist reviv udni yule follow sinc pearl reed first appli model popul unit state also initi fit curv make pass three point verhulst yield poor result probit model develop systemat chester ittner bliss coin term probit bliss john gaddum gaddum model fit maximum likelihood estim ronald fisher fisher addendum bliss work probit model princip use bioassay preced earlier work date see probit model histori probit model influenc subsequ develop logit model model compet logist model like first use altern probit model bioassay edwin bidwel wilson student jane worcest wilson worcest howev develop logist model gener altern probit model princip due work joseph berkson mani decad begin berkson coin logit analog probit continu berkson follow year logit model initi dismiss inferior probit model gradual achiev equal foot probit particularli logit model achiev pariti probit model use statist journal thereaft surpass rel popular due adopt logit outsid bioassay rather displac probit within bioassay inform use practic logit popular credit logit model comput simplic mathemat properti gener allow use vari field variou refin occur time notabl david cox cox multinomi logit model introduc independ cox theil greatli increas scope applic popular logit model daniel mcfadden link multinomi logit theori discret choic specif luce choic axiom show multinomi logit follow assumpt independ irrelev altern interpret odd altern rel prefer gave theoret foundat logist regress larg number extens
Perceptron,https://en.wikipedia.org/wiki/Perceptron,"In machine learning, the perceptron (or McCulloch–Pitts neuron) is an algorithm for supervised learning of binary classifiers.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.[1]  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.
 The artificial neuron network was invented in 1943 by Warren McCulloch and Walter Pitts in A logical calculus of the ideas immanent in nervous activity.[5]
 In 1957, Frank Rosenblatt was at the Cornell Aeronautical Laboratory. He simulated the perceptron on an IBM 704.[6][7] Later, he obtained funding by the Information Systems Branch of the United States Office of Naval Research and the Rome Air Development Center, to build a custom-made computer, the Mark I Perceptron. It was first publicly demonstrated on 23 June 1960.[8] The machine was ""part of a previously secret four-year NPIC [the US' National Photographic Interpretation Center] effort from 1963 through 1966 to develop this algorithm into a useful tool for photo-interpreters"".[9]
 Rosenblatt described the details of the perceptron in a 1958 paper.[10] His organization of a perceptron is constructed of three kinds of cells (""units""): AI, AII, R, which stand for ""projection"", ""association"" and ""response"". He presented at the first international symposium on AI, Mechanisation of Thought Processes, which took place in 1958 November.[11]
 Rosenblatt's project was funded under Contract Nonr-401(40) ""Cognitive Systems Research Program"", which lasted from 1959 to 1970,[12] and Contract Nonr-2381(00) ""Project PARA"" (""PARA"" means ""Perceiving and Recognition Automata""), which lasted from 1957[6] to 1963.[13]
 In 1959, the Institute for Defense Analysis awarded his group a $10,000 contract. By September 1961, the ONR awarded further $153,000 worth of contracts, with $108,000 committed for 1962.[14]
 The ONR research manager, Marvin Denicoff, stated that ONR, instead of ARPA, funded the Perceptron project, because the project was unlikely to produce technological results in the near or medium term. Funding from ARPA go up to the order of millions dollars, while from ONR are on the order of 10,000 dollars. Meanwhile, the head of IPTO at ARPA, J.C.R. Licklider, was interested in 'self-organizing', 'adaptive' and other biologically-inspired methods in the 1950s; but by the mid-1960s he was openly critical of these, including the perceptron. Instead he strongly favored the logical AI approach of Simon and Newell.[15]
 The perceptron was intended to be a machine, rather than a program, and while its first implementation was in software for the IBM 704, it was subsequently implemented in custom-built hardware as the Mark I Perceptron with the project name ""Project PARA"",[16] designed for image recognition. The machine is currently in Smithsonian National Museum of American History.[17]
 The Mark I Perceptron had 3 layers. One version was implemented as follows:
 Rosenblatt called this three-layered perceptron network the alpha-perceptron, to distinguish it from other perceptron models he experimented with.[8]
 The S-units are connected to the A-units randomly (according to a table of random numbers) via a plugboard (see photo), to ""eliminate any particular intentional bias in the perceptron"". The connection weights are fixed, not learned. Rosenblatt was adamant about the random connections, as he believed the retina was randomly connected to the visual cortex, and he wanted his perceptron machine to resemble human visual perception.[18]
 The A-units are connected to the R-units, with adjustable weights encoded in potentiometers, and weight updates during learning were performed by electric motors.[2]: 193 The hardware details are in an operators' manual.[16]
 In a 1958 press conference organized by the US Navy, Rosenblatt made statements about the perceptron that caused a heated controversy among the fledgling AI community; based on Rosenblatt's statements, The New York Times reported the perceptron to be ""the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.""[19]
 Central Intelligence Agency’s (CIA) Photo Division, from 1960 to 1964, studied the use of Mark I Perceptron machine for recognizing militarily interesting silhouetted targets (such as planes and ships) in aerial photos.[20][21]
 Rosenblatt described his experiments with many variants of the Perceptron machine in a book Principles of Neurodynamics (1962). The book is a published version of the 1961 report.[22]
 Among the variants are:
 The machine was shipped from Cornell to Smithsonian in 1967, under a government transfer administered by the Office of Naval Research.[9]
 Although the perceptron initially seemed promising, it was quickly proved that perceptrons could not be trained to recognise many classes of patterns. This caused the field of neural network research to stagnate for many years, before it was recognised that a feedforward neural network with two or more layers (also called a multilayer perceptron) had greater processing power than perceptrons with one layer (also called a single-layer perceptron).
 Single-layer perceptrons are only capable of learning linearly separable patterns.[23] For a classification task with some step activation function, a single node will have a single line dividing the data points forming the patterns. More nodes can create more dividing lines, but those lines must somehow be combined to form more complex classifications. A second layer of perceptrons, or even linear nodes, are sufficient to solve many otherwise non-separable problems.
 In 1969, a famous book entitled Perceptrons by Marvin Minsky and Seymour Papert showed that it was impossible for these classes of network to learn an XOR function. It is often incorrectly believed that they also conjectured that a similar result would hold for a multi-layer perceptron network. However, this is not true, as both Minsky and Papert already knew that multi-layer perceptrons were capable of producing an XOR function. (See the page on Perceptrons (book) for more information.) Nevertheless, the often-miscited Minsky and Papert text caused a significant decline in interest and funding of neural network research. It took ten more years until neural network research experienced a resurgence in the 1980s.[23][verification needed] This text was reprinted in 1987 as ""Perceptrons - Expanded Edition"" where some errors in the original text are shown and corrected.
 Rosenblatt continued working on perceptrons despite diminishing funding. The last attempt was Tobermory, built between 1961 and 1967, built for speech recognition.[24] It occupied an entire room.[25] It had 4 layers with 12,000 weights implemented by toroidal magnetic cores. By the time of its completion, simulation on digital computers had become faster than purpose-built perceptron machines.[26] He died in a boating accident in 1971.
 The kernel perceptron algorithm was already introduced in 1964 by Aizerman et al.[27] Margin bounds guarantees were given for the Perceptron algorithm in the general non-separable case first by Freund and Schapire (1998),[1] and more recently by Mohri and Rostamizadeh (2013) who extend previous results and give new and more favorable L1 bounds.[28][29]
 The perceptron is a simplified model of a biological neuron. While the complexity of biological neuron models is often required to fully understand neural behavior, research suggests a perceptron-like linear model can produce some behavior seen in real neurons.[30]
 The solution spaces of decision boundaries for all binary functions and learning behaviors are studied in.[31]
 In the modern sense, the perceptron is an algorithm for learning a binary classifier called a threshold function: a function that maps its input 




x



{\displaystyle \mathbf {x} }

 (a real-valued vector) to an output value 



f
(

x

)


{\displaystyle f(\mathbf {x} )}

 (a single binary value):
 



f
(

x

)
=
h
(

w

⋅

x

+
b
)


{\displaystyle f(\mathbf {x} )=h(\mathbf {w} \cdot \mathbf {x} +b)}


 where 



h


{\displaystyle h}

 is the Heaviside step-function, 




w



{\displaystyle \mathbf {w} }

 is a vector of real-valued weights, 




w

⋅

x



{\displaystyle \mathbf {w} \cdot \mathbf {x} }

 is the dot product 




∑

i
=
1


m



w

i



x

i




{\textstyle \sum _{i=1}^{m}w_{i}x_{i}}

, where m is the number of inputs to the perceptron, and b is the bias. The bias shifts the decision boundary away from the origin and does not depend on any input value.
 Equivalently, since 




w

⋅

x

+
b
=
(

w

,
b
)
⋅
(

x

,
1
)


{\displaystyle \mathbf {w} \cdot \mathbf {x} +b=(\mathbf {w} ,b)\cdot (\mathbf {x} ,1)}

, we can add the bias term 



b


{\displaystyle b}

 as another weight 





w


m
+
1




{\displaystyle \mathbf {w} _{m+1}}

 and add a coordinate 



1


{\displaystyle 1}

 to each input 




x



{\displaystyle \mathbf {x} }

, and then write it as a linear classifier that passes the origin:



f
(

x

)
=
h
(

w

⋅

x

)


{\displaystyle f(\mathbf {x} )=h(\mathbf {w} \cdot \mathbf {x} )}


 The binary value of 



f
(

x

)


{\displaystyle f(\mathbf {x} )}

 (0 or 1) is used to perform binary classification on 




x



{\displaystyle \mathbf {x} }

 as either a positive or a negative instance. Spatially, the bias shifts the position (though not the orientation) of the planar decision boundary.
 In the context of neural networks, a perceptron is an artificial neuron using the Heaviside step function as the activation function. The perceptron algorithm is also termed the single-layer perceptron, to distinguish it from a multilayer perceptron, which is a misnomer for a more complicated neural network.  As a linear classifier, the single-layer perceptron is the simplest feedforward neural network.
 From an information theory point of view, a single perceptron with K inputs has a capacity of 2K bits of information.[32] This result is due to Thomas Cover.[33]
 Specifically let 



T
(
N
,
K
)


{\displaystyle T(N,K)}

 be the number of ways to linearly separate N points in K dimensions, then



T
(
N
,
K
)
=

{





2

N




K
≥
N




2

∑

k
=
0


K
−
1



(




N
−
1




k




)



K
<
N








{\displaystyle T(N,K)=\left\{{\begin{array}{cc}2^{N}&K\geq N\\2\sum _{k=0}^{K-1}\left({\begin{array}{c}N-1\\k\end{array}}\right)&K<N\end{array}}\right.}

When K is large, 



T
(
N
,
K
)

/


2

N




{\displaystyle T(N,K)/2^{N}}

 is very close to one when 



N
≤
2
K


{\displaystyle N\leq 2K}

, but very close to zero when 



N
>
2
K


{\displaystyle N>2K}

. In words, one perceptron unit can almost certainly memorize a random assignment of binary labels on N points when 



N
≤
2
K


{\displaystyle N\leq 2K}

, but almost certainly not when 



N
>
2
K


{\displaystyle N>2K}

.
 When operating on only binary inputs, a perceptron is called a linearly separable Boolean function, or threshold Boolean function. The sequence of numbers of threshold Boolean functions on n inputs is OEIS A000609. The value is only known exactly up to 



n
=
9


{\displaystyle n=9}

 case, but the order of magnitude is known quite exactly: it has upper bound 




2


n

2


−
n

log

2


⁡
n
+
O
(
n
)




{\displaystyle 2^{n^{2}-n\log _{2}n+O(n)}}

 and lower bound 




2


n

2


−
n

log

2


⁡
n
−
O
(
n
)




{\displaystyle 2^{n^{2}-n\log _{2}n-O(n)}}

.[34]
 Any Boolean linear threshold function can be implemented with only integer weights. Furthermore, the number of bits necessary and sufficient for representing a single integer weight parameter is 



Θ
(
n
ln
⁡
n
)


{\displaystyle \Theta (n\ln n)}

.[34]
 A single perceptron can learn to classify any half-space. It cannot solve any linearly nonseparable vectors, such as the Boolean exclusive-or problem (the famous ""XOR problem"").
 A perceptron network with one hidden layer can learn to classify any compact subset arbitrarily closely. Similarly, it can also approximate any compactly-supported continuous function arbitrarily closely. This is essentially a special case of the theorems by George Cybenko and Kurt Hornik.
 Perceptrons (Minsky and Papert, 1969) studied the kind of perceptron networks necessary to learn various Boolean functions.
 Consider a perceptron network with 



n


{\displaystyle n}

 input units, one hidden layer, and one output, similar to the Mark I Perceptron machine. It computes a Boolean function of type 



f
:

2

n


→
2


{\displaystyle f:2^{n}\to 2}

. They call a function conjuctively local of order 



k


{\displaystyle k}

, iff there exists a perceptron network such that each unit in the hidden layer connects to at most 



k


{\displaystyle k}

 input units.
 Theorem. (Theorem 3.1.1): The parity function is conjuctively local of order 



n


{\displaystyle n}

.
 Theorem. (Section 5.5): The connectedness function is conjuctively local of order 



Ω
(

n

1

/

2


)


{\displaystyle \Omega (n^{1/2})}

.
 Below is an example of a learning algorithm for a single-layer perceptron with a single output unit. For a single-layer perceptron with multiple output units, since the weights of one output unit are completely separate from all the others', the same algorithm can be run for each output unit.
 For multilayer perceptrons, where a hidden layer exists, more sophisticated algorithms such as backpropagation must be used. If the activation function or the underlying process being modeled by the perceptron is nonlinear, alternative learning algorithms such as the delta rule can be used as long as the activation function is differentiable. Nonetheless, the learning algorithm described in the steps below will often work, even for multilayer perceptrons with nonlinear activation functions.
 When multiple perceptrons are combined in an artificial neural network, each output neuron operates independently of all the others; thus, learning each output can be considered in isolation.
 We first define some variables:
 We show the values of the features as follows:
 To represent the weights: 
 To show the time-dependence of 




w



{\displaystyle \mathbf {w} }

, we use:
 The algorithm updates the weights after every training sample in step 2b.
 A single perceptron is a linear classifier. It can only reach a stable state if all input vectors are classified correctly. In case the training set D is not linearly separable, i.e. if the positive examples cannot be separated from the negative examples by a hyperplane, then the algorithm would not converge since there is no solution. Hence, if linear separability of the training set is not known a priori, one of the training variants below should be used. Detailed analysis and extensions to the convergence theorem are in Chapter 11 of Perceptrons (1969).
 Linear separability is testable in time 



min
(
O
(

n

d

/

2


)
,
O
(

d

2
n


)
,
O
(

n

d
−
1


ln
⁡
n
)
)


{\displaystyle \min(O(n^{d/2}),O(d^{2n}),O(n^{d-1}\ln n))}

, where 



n


{\displaystyle n}

 is the number of data points, and 



d


{\displaystyle d}

 is the dimension of each point.[35]
 If the training set is linearly separable, then the perceptron is guaranteed to converge after making finitely many mistakes.[36] The theorem is proved by Rosenblatt et al.
 Perceptron convergence theorem — Given a dataset 



D


{\textstyle D}

, such that 




max

(
x
,
y
)
∈
D


‖
x

‖

2


=
R


{\textstyle \max _{(x,y)\in D}\|x\|_{2}=R}

, and it is linearly separable by some unit vector 




w

∗




{\textstyle w^{*}}

, with margin 



γ


{\textstyle \gamma }

: 



γ
:=

min

(
x
,
y
)
∈
D


y
(

w

∗


⋅
x
)


{\displaystyle \gamma :=\min _{(x,y)\in D}y(w^{*}\cdot x)}


 Then the perceptron 0-1 learning algorithm converges after making at most 



(
R

/

γ

)

2




{\textstyle (R/\gamma )^{2}}

 mistakes, for any learning rate, and any method of sampling from the dataset.
 The following simple proof is due to Novikoff (1962). The idea of the proof is that the weight vector is always adjusted by a bounded amount in a direction with which it has a negative dot product, and thus can be bounded above by O(√t), where t is the number of changes to the weight vector. However, it can also be bounded below by O(t) because if there exists an (unknown) satisfactory weight vector, then every change makes progress in this (unknown) direction by a positive amount that depends only on the input vector. Suppose at step 



t


{\textstyle t}

, the perceptron with weight 




w

t




{\textstyle w_{t}}

 makes a mistake on data point 



(
x
,
y
)


{\textstyle (x,y)}

, then it updates to 




w

t
+
1


=

w

t


+
r
(
y
−

f


w

t




(
x
)
)
x


{\textstyle w_{t+1}=w_{t}+r(y-f_{w_{t}}(x))x}

.
 If 



y
=
0


{\textstyle y=0}

, the argument is symmetric, so we omit it.
 WLOG, 



y
=
1


{\textstyle y=1}

, then 




f


w

t




(
x
)
=
0


{\textstyle f_{w_{t}}(x)=0}

, 




f


w

∗




(
x
)
=
1


{\textstyle f_{w^{*}}(x)=1}

, and 




w

t
+
1


=

w

t


+
r
x


{\textstyle w_{t+1}=w_{t}+rx}

.
 By assumption, we have separation with margins: 




w

∗


⋅
x
≥
γ


{\displaystyle w^{*}\cdot x\geq \gamma }

 Thus,





w

∗


⋅

w

t
+
1


−

w

∗


⋅

w

t


=

w

∗


⋅
(
r
x
)
≥
r
γ


{\displaystyle w^{*}\cdot w_{t+1}-w^{*}\cdot w_{t}=w^{*}\cdot (rx)\geq r\gamma }


 Also 



‖

w

t
+
1



‖

2


2


−
‖

w

t



‖

2


2


=
‖

w

t


+
r
x

‖

2


2


−
‖

w

t



‖

2


2


=
2
r
(

w

t


⋅
x
)
+

r

2


‖
x

‖

2


2




{\displaystyle \|w_{t+1}\|_{2}^{2}-\|w_{t}\|_{2}^{2}=\|w_{t}+rx\|_{2}^{2}-\|w_{t}\|_{2}^{2}=2r(w_{t}\cdot x)+r^{2}\|x\|_{2}^{2}}

 and since the perceptron made a mistake, 




w

t


⋅
x
≤
0


{\textstyle w_{t}\cdot x\leq 0}

, and so




‖

w

t
+
1



‖

2


2


−
‖

w

t



‖

2


2


≤
‖
x

‖

2


2


≤

r

2



R

2




{\displaystyle \|w_{t+1}\|_{2}^{2}-\|w_{t}\|_{2}^{2}\leq \|x\|_{2}^{2}\leq r^{2}R^{2}}


 Since we started with 




w

0


=
0


{\textstyle w_{0}=0}

, after making 



N


{\textstyle N}

 mistakes, 



‖
w

‖

2


≤


N

r

2



R

2






{\displaystyle \|w\|_{2}\leq {\sqrt {Nr^{2}R^{2}}}}

 but also




‖
w

‖

2


≥
w
⋅

w

∗


≥
N
r
γ


{\displaystyle \|w\|_{2}\geq w\cdot w^{*}\geq Nr\gamma }


 Combining the two, we have 



N
≤
(
R

/

γ

)

2




{\textstyle N\leq (R/\gamma )^{2}}


 While the perceptron algorithm is guaranteed to converge on some solution in the case of a linearly separable training set, it may still pick any solution and problems may admit many solutions of varying quality.[37] The perceptron of optimal stability, nowadays better known as the linear support-vector machine, was designed to solve this problem (Krauth and Mezard, 1987).[38]
 When the dataset is not linearly separable, then there is no way for a single perceptron to converge. However, we still have[39]
 Perceptron cycling theorem — If the dataset 



D


{\displaystyle D}

 has only finitely many points, then there exists an upper bound number 



M


{\displaystyle M}

, such that for any starting weight vector 




w

0




{\displaystyle w_{0}}

 all weight vector 




w

t




{\displaystyle w_{t}}

 has norm bounded by 



‖

w

t


‖
≤
‖

w

0


‖
+
M


{\displaystyle \|w_{t}\|\leq \|w_{0}\|+M}


 This is proved first by Bradley Efron.[40]
 Consider a dataset where the 



x


{\displaystyle x}

 are from 



{
−
1
,
+
1

}

n




{\displaystyle \{-1,+1\}^{n}}

, that is, the vertices of an n-dimensional hypercube centered at origin, and 



y
=
θ
(

x

i


)


{\displaystyle y=\theta (x_{i})}

. That is, all data points with positive 




x

i




{\displaystyle x_{i}}

 have 



y
=
1


{\displaystyle y=1}

, and vice versa. By the perceptron convergence theorem, a perceptron would converge after making at most 



n


{\displaystyle n}

 mistakes.
 If we were to write a logical program to perform the same task, each positive example shows that one of the coordinates is the right one, and each negative example shows that its complement is a positive example. By collecting all the known positive examples, we eventually eliminate all but one coordinate, at which point the dataset is learned.[41]
 This bound is asymptotically tight in terms of the worst-case. In the worst-case, the first presented example is entirely new, and gives 



n


{\displaystyle n}

 bits of information, but each subsequent example would differ minimally from previous examples, and gives 1 bit each. After 



n
+
1


{\displaystyle n+1}

 examples, there are 



2
n


{\displaystyle 2n}

 bits of information, which is sufficient for the perceptron (with 



2
n


{\displaystyle 2n}

 bits of information).[32]
 However, it is not tight in terms of expectation if the examples are presented uniformly at random, since the first would give 



n


{\displaystyle n}

 bits, the second 



n

/

2


{\displaystyle n/2}

 bits, and so on, taking 



O
(
ln
⁡
n
)


{\displaystyle O(\ln n)}

 examples in total.[41]
 The pocket algorithm with ratchet (Gallant, 1990) solves the stability problem of perceptron learning by keeping the best solution seen so far ""in its pocket"". The pocket algorithm then returns the solution in the pocket, rather than the last solution. It can be used also for non-separable data sets, where the aim is to find a perceptron with a small number of misclassifications. However, these solutions appear purely stochastically and hence the pocket algorithm neither approaches them gradually in the course of learning, nor are they guaranteed to show up within a given number of learning steps.
 The Maxover algorithm (Wendemuth, 1995) is ""robust"" in the sense that it will converge regardless of (prior) knowledge of linear separability of the data set.[42] In the linearly separable case, it will solve the training problem – if desired, even with optimal stability (maximum margin between the classes). For non-separable data sets, it will return a solution with a small number of misclassifications. In all cases, the algorithm gradually approaches the solution in the course of learning, without memorizing previous states and without stochastic jumps. Convergence is to global optimality for separable data sets and to local optimality for non-separable data sets.
 The Voted Perceptron (Freund and Schapire, 1999), is a variant using multiple weighted perceptrons. The algorithm starts a new perceptron every time an example is wrongly classified, initializing the weights vector with the final weights of the last perceptron. Each perceptron will also be given another weight corresponding to how many examples do they correctly classify before wrongly classifying one, and at the end the output will be a weighted vote on all perceptrons.
 In separable problems, perceptron training can also aim at finding the largest separating margin between the classes. The so-called perceptron of optimal stability can be determined by means of iterative training and optimization schemes, such as the Min-Over algorithm (Krauth and Mezard, 1987)[38]  or the AdaTron (Anlauf and Biehl, 1989)).[43] AdaTron uses the fact that the corresponding quadratic optimization problem is convex. The perceptron of optimal stability, together with the kernel trick, are the conceptual foundations of the support-vector machine.
 The 



α


{\displaystyle \alpha }

-perceptron further used a pre-processing layer of fixed random weights, with thresholded output units. This enabled the perceptron to classify analogue patterns, by projecting them into a binary space. In fact, for a projection space of sufficiently high dimension, patterns can become linearly separable.
 Another way to solve nonlinear problems without using multiple layers is to use higher order networks (sigma-pi unit). In this type of network, each element in the input vector is extended with each pairwise combination of multiplied inputs (second order). This can be extended to an n-order network.
 It should be kept in mind, however, that the best classifier is not necessarily that which classifies all the training data perfectly. Indeed, if we had the prior constraint that the data come from equi-variant Gaussian distributions, the linear separation in the input space is optimal, and the nonlinear solution is overfitted.
 Other linear classification algorithms include Winnow, support-vector machine, and logistic regression.
 Like most other techniques for training linear classifiers, the perceptron generalizes naturally to multiclass classification.  Here, the input 



x


{\displaystyle x}

 and the output 



y


{\displaystyle y}

 are drawn from arbitrary sets. A feature representation function 



f
(
x
,
y
)


{\displaystyle f(x,y)}

 maps each possible input/output pair to a finite-dimensional real-valued feature vector.  As before, the feature vector is multiplied by a weight vector 



w


{\displaystyle w}

, but now the resulting score is used to choose among many possible outputs:
 Learning again iterates over the examples, predicting an output for each, leaving the weights unchanged when the predicted output matches the target, and changing them when it does not.  The update becomes:
 This multiclass feedback formulation reduces to the original perceptron when 



x


{\displaystyle x}

 is a real-valued vector, 



y


{\displaystyle y}

 is chosen from 



{
0
,
1
}


{\displaystyle \{0,1\}}

, and 



f
(
x
,
y
)
=
y
x


{\displaystyle f(x,y)=yx}

.
 For certain problems, input/output representations and features can be chosen so that 





a
r
g
m
a
x


y


f
(
x
,
y
)
⋅
w


{\displaystyle \mathrm {argmax} _{y}f(x,y)\cdot w}

 can be found efficiently even though 



y


{\displaystyle y}

 is chosen from a very large or even infinite set.
 Since 2002, perceptron training has become popular in the field of natural language processing for such tasks as part-of-speech tagging and syntactic parsing (Collins, 2002). It has also been applied to large-scale machine learning problems in a distributed computing setting.[44]
",machin learn perceptron neuron algorithm supervis learn binari classifi binari classifi function decid whether input repres vector number belong specif class type linear classifi classif algorithm make predict base linear predictor function combin set weight featur vector artifici neuron network invent warren mcculloch walter pitt logic calculu idea imman nervou activ frank rosenblatt cornel aeronaut laboratori simul perceptron ibm later obtain fund inform system branch unit state offic naval research rome air develop center build comput mark perceptron first publicli demonstr june machin part previous secret npic us nation photograph interpret center effort develop algorithm use tool rosenblatt describ detail perceptron paper organ perceptron construct three kind cell unit ai aii r stand project associ respons present first intern symposium ai mechanis thought process took place novemb rosenblatt project fund contract cognit system research program last contract project para para mean perceiv recognit automata last institut defens analysi award group contract septemb onr award worth contract commit onr research manag marvin denicoff state onr instead arpa fund perceptron project project unlik produc technolog result near medium term fund arpa go order million dollar onr order dollar meanwhil head ipto arpa licklid interest method openli critic includ perceptron instead strongli favor logic ai approach simon newel perceptron intend machin rather program first implement softwar ibm subsequ implement hardwar mark perceptron project name project para design imag recognit machin current smithsonian nation museum american histori mark perceptron layer one version implement follow rosenblatt call perceptron network distinguish perceptron model experi connect randomli accord tabl random number via plugboard see photo elimin particular intent bia perceptron connect weight fix learn rosenblatt adam random connect believ retina randomli connect visual cortex want perceptron machin resembl human visual percept connect adjust weight encod potentiomet weight updat learn perform electr motor hardwar detail oper manual press confer organ us navi rosenblatt made statement perceptron caus heat controversi among fledgl ai commun base rosenblatt statement new york time report perceptron embryo electron comput navi expect abl walk talk see write reproduc consciou exist central intellig agenc cia photo divis studi use mark perceptron machin recogn militarili interest silhouet target plane ship aerial photo rosenblatt describ experi mani variant perceptron machin book principl neurodynam book publish version report among variant machin ship cornel smithsonian govern transfer administ offic naval research although perceptron initi seem promis quickli prove perceptron could train recognis mani class pattern caus field neural network research stagnat mani year recognis feedforward neural network two layer also call multilay perceptron greater process power perceptron one layer also call perceptron perceptron capabl learn linearli separ pattern classif task step activ function singl node singl line divid data point form pattern node creat divid line line must somehow combin form complex classif second layer perceptron even linear node suffici solv mani otherwis problem famou book entitl perceptron marvin minski seymour papert show imposs class network learn xor function often incorrectli believ also conjectur similar result would hold perceptron network howev true minski papert alreadi knew perceptron capabl produc xor function see page perceptron book inform nevertheless minski papert text caus signific declin interest fund neural network research took ten year neural network research experienc resurg verif need text reprint perceptron expand edit error origin text shown correct rosenblatt continu work perceptron despit diminish fund last attempt tobermori built built speech recognit occupi entir room layer weight implement toroid magnet core time complet simul digit comput becom faster perceptron machin die boat accid kernel perceptron algorithm alreadi introduc aizerman et al margin bound guarante given perceptron algorithm gener case first freund schapir recent mohri rostamizadeh extend previou result give new favor bound perceptron simplifi model biolog neuron complex biolog neuron model often requir fulli understand neural behavior research suggest linear model produc behavior seen real neuron solut space decis boundari binari function learn behavior studi modern sens perceptron algorithm learn binari classifi call threshold function function map input x x vector output valu f x f x singl binari valu f x h w x b f x w x h h heavisid w w vector weight w x w x dot product w x number input perceptron b bia bia shift decis boundari away origin depend input valu equival sinc w x b w b x w x w b x add bia term b b anoth weight w w add coordin input x x write linear classifi pass origin f x h w x f x w x binari valu f x f x use perform binari classif x x either posit neg instanc spatial bia shift posit though orient planar decis boundari context neural network perceptron artifici neuron use heavisid step function activ function perceptron algorithm also term perceptron distinguish multilay perceptron misnom complic neural network linear classifi perceptron simplest feedforward neural network inform theori point view singl perceptron k input capac bit inform result due thoma cover specif let n k n k number way linearli separ n point k dimens n k n k n k k n k k n n k array cc n array c array k array k larg n k n n k n close one n k close zero n k n word one perceptron unit almost certainli memor random assign binari label n point n k almost certainli n k n oper binari input perceptron call linearli separ boolean function threshold boolean function sequenc number threshold boolean function n input oei valu known exactli n case order magnitud known quit exactli upper bound n n log n n n lower bound n n log n n n boolean linear threshold function implement integ weight furthermor number bit necessari suffici repres singl integ weight paramet θ n ln n n singl perceptron learn classifi solv linearli nonsepar vector boolean problem famou xor problem perceptron network one hidden layer learn classifi compact subset arbitrarili close similarli also approxim continu function arbitrarili close essenti special case theorem georg cybenko kurt hornik perceptron minski papert studi kind perceptron network necessari learn variou boolean function consid perceptron network n n input unit one hidden layer one output similar mark perceptron machin comput boolean function type f n n call function conjuct local order k k iff exist perceptron network unit hidden layer connect k k input unit theorem theorem pariti function conjuct local order n n theorem section connected function conjuct local order ω n exampl learn algorithm perceptron singl output unit perceptron multipl output unit sinc weight one output unit complet separ other algorithm run output unit multilay perceptron hidden layer exist sophist algorithm backpropag must use activ function underli process model perceptron nonlinear altern learn algorithm delta rule use long activ function differenti nonetheless learn algorithm describ step often work even multilay perceptron nonlinear activ function multipl perceptron combin artifici neural network output neuron oper independ other thu learn output consid isol first defin variabl show valu featur follow repres weight show w w use algorithm updat weight everi train sampl step singl perceptron linear classifi reach stabl state input vector classifi correctli case train set linearli separ posit exampl separ neg exampl hyperplan algorithm would converg sinc solut henc linear separ train set known priori one train variant use detail analysi extens converg theorem chapter perceptron linear separ testabl time min n n n ln n n n n number data point dimens point train set linearli separ perceptron guarante converg make finit mani mistak theorem prove rosenblatt et al perceptron converg theorem given dataset max x x r x linearli separ unit vector w margin γ γ min x w x x x perceptron learn algorithm converg make r γ mistak learn rate method sampl dataset follow simpl proof due novikoff idea proof weight vector alway adjust bound amount direct neg dot product thu bound number chang weight vector howev also bound exist unknown satisfactori weight vector everi chang make progress unknown direct posit amount depend input vector suppos step perceptron weight w make mistak data point x x updat w w r f w x x x x argument symmetr omit wlog f w x x f w x x w w r x assumpt separ margin w x γ thu w w w w w r x r γ rx also w w w r x w r w x r x x sinc perceptron made mistak w x w w x r r sinc start w make n n mistak w n r r also w w w n r γ combin two n r γ perceptron algorithm guarante converg solut case linearli separ train set may still pick solut problem may admit mani solut vari qualiti perceptron optim stabil nowaday better known linear machin design solv problem krauth mezard dataset linearli separ way singl perceptron converg howev still perceptron cycl theorem dataset finit mani point exist upper bound number start weight vector w weight vector w norm bound w w prove first bradley efron consid dataset x x n n vertic hypercub center origin θ x data point posit x vice versa perceptron converg theorem perceptron would converg make n n mistak write logic program perform task posit exampl show one coordin right one neg exampl show complement posit exampl collect known posit exampl eventu elimin one coordin point dataset learn bound asymptot tight term first present exampl entir new give n n bit inform subsequ exampl would differ minim previou exampl give bit n exampl n bit inform suffici perceptron n bit inform howev tight term expect exampl present uniformli random sinc first would give n n bit second n bit take ln n n exampl total pocket algorithm ratchet gallant solv stabil problem perceptron learn keep best solut seen far pocket pocket algorithm return solut pocket rather last solut use also data set aim find perceptron small number misclassif howev solut appear pure stochast henc pocket algorithm neither approach gradual cours learn guarante show within given number learn step maxov algorithm wendemuth robust sens converg regardless prior knowledg linear separ data set linearli separ case solv train problem desir even optim stabil maximum margin class data set return solut small number misclassif case algorithm gradual approach solut cours learn without memor previou state without stochast jump converg global optim separ data set local optim data set vote perceptron freund schapir variant use multipl weight perceptron algorithm start new perceptron everi time exampl wrongli classifi initi weight vector final weight last perceptron perceptron also given anoth weight correspond mani exampl correctli classifi wrongli classifi one end output weight vote perceptron separ problem perceptron train also aim find largest separ margin class perceptron optim stabil determin mean iter train optim scheme algorithm krauth mezard adatron anlauf biehl adatron use fact correspond quadrat optim problem convex perceptron optim stabil togeth kernel trick conceptu foundat machin α use layer fix random weight threshold output unit enabl perceptron classifi analogu pattern project binari space fact project space suffici high dimens pattern becom linearli separ anoth way solv nonlinear problem without use multipl layer use higher order network unit type network element input vector extend pairwis combin multipli input second order extend network kept mind howev best classifi necessarili classifi train data perfectli inde prior constraint data come gaussian distribut linear separ input space optim nonlinear solut overfit linear classif algorithm includ winnow machin logist regress like techniqu train linear classifi perceptron gener natur multiclass classif input x x output drawn arbitrari set featur represent function f x f x map possibl pair featur vector featur vector multipli weight vector w w result score use choos among mani possibl output learn iter exampl predict output leav weight unchang predict output match target chang updat becom multiclass feedback formul reduc origin perceptron x x vector chosen f x x f x certain problem represent featur chosen r g x f x w argmax f x w found effici even though chosen larg even infinit set sinc perceptron train becom popular field natur languag process task tag syntact pars collin also appli machin learn problem distribut comput set
Relevance vector machine,https://en.wikipedia.org/wiki/Relevance_vector_machine,"In mathematics, a Relevance Vector Machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification.[1] A greedy optimisation procedure and thus fast version were subsequently developed.[2][3]
The RVM has an identical functional form to the support vector machine, but provides probabilistic classification.
 It is actually equivalent to a Gaussian process model with covariance function:
 where 



φ


{\displaystyle \varphi }

 is the kernel function (usually Gaussian), 




α

j




{\displaystyle \alpha _{j}}

 are the variances of the prior on the weight vector




w
∼
N
(
0
,

α

−
1


I
)


{\displaystyle w\sim N(0,\alpha ^{-1}I)}

, and 





x


1


,
…
,


x


N




{\displaystyle \mathbf {x} _{1},\ldots ,\mathbf {x} _{N}}

 are the input vectors of the training set.[4]
 Compared to that of support vector machines (SVM), the Bayesian formulation of the RVM avoids the set of free parameters of the SVM (that usually require cross-validation-based post-optimizations). However RVMs use an expectation maximization (EM)-like learning method and are therefore at risk of local minima. This is unlike the standard sequential minimal optimization (SMO)-based algorithms employed by SVMs, which are guaranteed to find a global optimum (of the convex problem).
 The relevance vector machine was patented in the United States by Microsoft (patent expired September 4, 2019).[5]
",mathemat relev vector machin rvm machin learn techniqu use bayesian infer obtain parsimoni solut regress probabilist classif greedi optimis procedur thu fast version subsequ develop rvm ident function form support vector machin provid probabilist classif actual equival gaussian process model covari function φ kernel function usual gaussian α j j varianc prior weight vector w n α n x x n x x n input vector train set compar support vector machin svm bayesian formul rvm avoid set free paramet svm usual requir howev rvm use expect maxim em learn method therefor risk local minima unlik standard sequenti minim optim smo algorithm employ svm guarante find global optimum convex problem relev vector machin patent unit state microsoft patent expir septemb
Support vector machine,https://en.wikipedia.org/wiki/Support_vector_machine,"In machine learning, support vector machines (SVMs, also support vector networks[1]) are supervised max-margin models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories,[1][2] SVMs are one of the most studied models, being based on statistical learning frameworks of VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974).
 In addition to performing linear classification, SVMs can efficiently perform non-linear classification using the kernel trick, representing the data only through a set of pairwise similarity comparisons between the original data points using a kernel function, which transforms them into coordinates in a higher-dimensional feature space. Thus, SVMs use the kernel trick to implicitly map their inputs into high-dimensional feature spaces, where linear classification can be performed.[3]  Being max-margin models, SVMs are resilient to noisy data (e.g., misclassified examples). SVMs can also be used for regression tasks, where the objective becomes 



ϵ


{\displaystyle \epsilon }

-sensitive.
 The support vector clustering[4] algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data.[citation needed] These data sets require unsupervised learning approaches, which attempt to find natural clustering of the data into groups, and then to map new data according to these clusters.
 The popularity of SVMs is likely due to their amenability to theoretical analysis, and their flexibility in being applied to a wide variety of tasks, including structured prediction problems. It is not clear that SVMs have better predictive performance than other linear models, such as logistic regression and linear regression.[citation needed]
 Classifying data is a common task in machine learning.
Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support vector machines, a data point is viewed as a 



p


{\displaystyle p}

-dimensional vector (a list of 



p


{\displaystyle p}

 numbers), and we want to know whether we can separate such points with a 



(
p
−
1
)


{\displaystyle (p-1)}

-dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum-margin classifier; or equivalently, the perceptron of optimal stability.[citation needed]
 More formally, a support vector machine constructs a hyperplane or set of hyperplanes in a high or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection.[5] Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier.[6] A lower generalization error means that the implementer is less likely to experience overfitting.
 Whereas the original problem may be stated in a finite-dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed[7] that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space. To keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products of pairs of input data vectors may be computed easily in terms of the variables in the original space, by defining them in terms of a kernel function 



k
(
x
,
y
)


{\displaystyle k(x,y)}

 selected to suit the problem.[8] The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant, where such a set of vectors is an orthogonal (and thus minimal) set of vectors that defines a hyperplane. The vectors defining the hyperplanes can be chosen to be linear combinations with parameters 




α

i




{\displaystyle \alpha _{i}}

 of images of feature vectors 




x

i




{\displaystyle x_{i}}

 that occur in the data base. With this choice of a hyperplane, the points 



x


{\displaystyle x}

 in the feature space that are mapped into the hyperplane are defined by the relation 





∑

i



α

i


k
(

x

i


,
x
)
=

constant

.



{\displaystyle \textstyle \sum _{i}\alpha _{i}k(x_{i},x)={\text{constant}}.}

  Note that if 



k
(
x
,
y
)


{\displaystyle k(x,y)}

 becomes small as 



y


{\displaystyle y}

 grows further away from 



x


{\displaystyle x}

, each term in the sum measures the degree of closeness of the test point 



x


{\displaystyle x}

 to the corresponding data base point 




x

i




{\displaystyle x_{i}}

. In this way, the sum of kernels above can be used to measure the relative nearness of each test point to the data points originating in one or the other of the sets to be discriminated. Note the fact that the set of points 



x


{\displaystyle x}

 mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination between sets that are not convex at all in the original space.
 SVMs can be used to solve various real-world problems:
 The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1964.[citation needed] In 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes.[7] The ""soft margin"" incarnation, as is commonly used in software packages, was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995.[1]
 We are given a training dataset of 



n


{\displaystyle n}

 points of the form




(


x


1


,

y

1


)
,
…
,
(


x


n


,

y

n


)
,


{\displaystyle (\mathbf {x} _{1},y_{1}),\ldots ,(\mathbf {x} _{n},y_{n}),}


where the 




y

i




{\displaystyle y_{i}}

 are either 1 or −1, each indicating the class to which the point 





x


i




{\displaystyle \mathbf {x} _{i}}

 belongs. Each 





x


i




{\displaystyle \mathbf {x} _{i}}

 is a 



p


{\displaystyle p}

-dimensional real vector. We want to find the ""maximum-margin hyperplane"" that divides the group of points 





x


i




{\displaystyle \mathbf {x} _{i}}

 for which 




y

i


=
1


{\displaystyle y_{i}=1}

 from the group of points for which 




y

i


=
−
1


{\displaystyle y_{i}=-1}

, which is defined so that the distance between the hyperplane and the nearest point 





x


i




{\displaystyle \mathbf {x} _{i}}

 from either group is maximized.
 Any hyperplane can be written as the set of points 




x



{\displaystyle \mathbf {x} }

 satisfying






w



T




x

−
b
=
0
,


{\displaystyle \mathbf {w} ^{\mathsf {T}}\mathbf {x} -b=0,}


where 




w



{\displaystyle \mathbf {w} }

 is the (not necessarily normalized) normal vector to the hyperplane. This is much like Hesse normal form, except that 




w



{\displaystyle \mathbf {w} }

 is not necessarily a unit vector. The parameter 






b

‖

w

‖






{\displaystyle {\tfrac {b}{\|\mathbf {w} \|}}}

 determines the offset of the hyperplane from the origin along the normal vector 




w



{\displaystyle \mathbf {w} }

.
 Warning: most of the literature on the subject defines the bias so that






w



T




x

+
b
=
0.


{\displaystyle \mathbf {w} ^{\mathsf {T}}\mathbf {x} +b=0.}


 If the training data is linearly separable, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called the ""margin"", and the maximum-margin hyperplane is the hyperplane that lies halfway between them. With a normalized or standardized dataset, these hyperplanes can be described by the equations
 and
 Geometrically, the distance between these two hyperplanes is 






2

‖

w

‖






{\displaystyle {\tfrac {2}{\|\mathbf {w} \|}}}

,[19] so to maximize the distance between the planes we want to minimize 



‖

w

‖


{\displaystyle \|\mathbf {w} \|}

. The distance is computed using the distance from a point to a plane equation. We also have to prevent data points from falling into the margin, we add the following constraint: for each 



i


{\displaystyle i}

 either






w



T





x


i


−
b
≥
1

,

 if 


y

i


=
1
,


{\displaystyle \mathbf {w} ^{\mathsf {T}}\mathbf {x} _{i}-b\geq 1\,,{\text{ if }}y_{i}=1,}


or






w



T





x


i


−
b
≤
−
1

,

 if 


y

i


=
−
1.


{\displaystyle \mathbf {w} ^{\mathsf {T}}\mathbf {x} _{i}-b\leq -1\,,{\text{ if }}y_{i}=-1.}


These constraints state that each data point must lie on the correct side of the margin.
 This can be rewritten as
 We can put this together to get the optimization problem:
 










minimize


w

,

b








1
2


‖

w


‖

2








subject to





y

i


(


w


⊤




x


i


−
b
)
≥
1

∀
i
∈
{
1
,
…
,
n
}






{\displaystyle {\begin{aligned}&{\underset {\mathbf {w} ,\;b}{\operatorname {minimize} }}&&{\frac {1}{2}}\|\mathbf {w} \|^{2}\\&{\text{subject to}}&&y_{i}(\mathbf {w} ^{\top }\mathbf {x} _{i}-b)\geq 1\quad \forall i\in \{1,\dots ,n\}\end{aligned}}}


 The 




w



{\displaystyle \mathbf {w} }

 and 



b


{\displaystyle b}

 that solve this problem determine the final classifier, 




x

↦
sgn
⁡
(


w



T




x

−
b
)


{\displaystyle \mathbf {x} \mapsto \operatorname {sgn}(\mathbf {w} ^{\mathsf {T}}\mathbf {x} -b)}

, where 



sgn
⁡
(
⋅
)


{\displaystyle \operatorname {sgn}(\cdot )}

 is the sign function.
 An important consequence of this geometric description is that the max-margin hyperplane is completely determined by those 





x


i




{\displaystyle \mathbf {x} _{i}}

 that lie nearest to it (explained below). These 





x


i




{\displaystyle \mathbf {x} _{i}}

 are called support vectors.
 To extend SVM to cases in which the data are not linearly separable, the hinge loss function is helpful




max

(

0
,
1
−

y

i


(


w



T





x


i


−
b
)

)

.


{\displaystyle \max \left(0,1-y_{i}(\mathbf {w} ^{\mathsf {T}}\mathbf {x} _{i}-b)\right).}


 Note that 




y

i




{\displaystyle y_{i}}

 is the i-th target (i.e., in this case, 1 or −1), and 





w



T





x


i


−
b


{\displaystyle \mathbf {w} ^{\mathsf {T}}\mathbf {x} _{i}-b}

 is the i-th output.
 This function is zero if the constraint in (1) is satisfied, in other words, if 





x


i




{\displaystyle \mathbf {x} _{i}}

 lies on the correct side of the margin. For data on the wrong side of the margin, the function's value is proportional to the distance from the margin.
 The goal of the optimization then is to minimize:
 



‖

w


‖

2


+
C

[



1
n



∑

i
=
1


n


max

(

0
,
1
−

y

i


(


w



T





x


i


−
b
)

)


]

,


{\displaystyle \lVert \mathbf {w} \rVert ^{2}+C\left[{\frac {1}{n}}\sum _{i=1}^{n}\max \left(0,1-y_{i}(\mathbf {w} ^{\mathsf {T}}\mathbf {x} _{i}-b)\right)\right],}


 where the parameter 



C
>
0


{\displaystyle C>0}

 determines the trade-off between increasing the margin size and ensuring that the 





x


i




{\displaystyle \mathbf {x} _{i}}

 lie on the correct side of the margin (Note we can add a weight to either term in the equation above). By deconstructing the hinge loss, this optimization problem can be massaged into the following:
 










minimize


w

,

b
,


ζ








‖

w


‖

2


2


+
C

∑

i
=
1


n



ζ

i








subject to





y

i


(


w


⊤




x


i


−
b
)
≥
1
−

ζ

i


,


ζ

i


≥
0

∀
i
∈
{
1
,
…
,
n
}






{\displaystyle {\begin{aligned}&{\underset {\mathbf {w} ,\;b,\;\mathbf {\zeta } }{\operatorname {minimize} }}&&\|\mathbf {w} \|_{2}^{2}+C\sum _{i=1}^{n}\zeta _{i}\\&{\text{subject to}}&&y_{i}(\mathbf {w} ^{\top }\mathbf {x} _{i}-b)\geq 1-\zeta _{i},\quad \zeta _{i}\geq 0\quad \forall i\in \{1,\dots ,n\}\end{aligned}}}


 Thus, for large values of 



C


{\displaystyle C}

, it will behave similar to the hard-margin SVM, if the input data are linearly classifiable, but will still learn if a classification rule is viable or not.
 The original maximum-margin hyperplane algorithm proposed by Vapnik in 1963 constructed a linear classifier. However, in 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick (originally proposed by Aizerman et al.[20]) to maximum-margin hyperplanes.[7] The kernel trick, where dot products are replaced by kernels, is easily derived in the dual representation of the SVM problem. This allows the algorithm to fit the maximum-margin hyperplane in a transformed feature space. The transformation may be nonlinear and the transformed space high-dimensional; although the classifier is a hyperplane in the transformed feature space, it may be nonlinear in the original input space.
 It is noteworthy that working in a higher-dimensional feature space increases the generalization error of support vector machines, although given enough samples the algorithm still performs well.[21]
 Some common kernels include:
 The kernel is related to the transform 



φ
(


x


i


)


{\displaystyle \varphi (\mathbf {x} _{i})}

 by the equation 



k
(


x


i


,


x


j


)
=
φ
(


x


i


)
⋅
φ
(


x


j


)


{\displaystyle k(\mathbf {x} _{i},\mathbf {x} _{j})=\varphi (\mathbf {x} _{i})\cdot \varphi (\mathbf {x} _{j})}

. The value w is also in the transformed space, with 




w

=

∑

i



α

i



y

i


φ
(


x


i


)


{\textstyle \mathbf {w} =\sum _{i}\alpha _{i}y_{i}\varphi (\mathbf {x} _{i})}

. Dot products with w for classification can again be computed by the kernel trick, i.e. 




w

⋅
φ
(

x

)
=

∑

i



α

i



y

i


k
(


x


i


,

x

)


{\textstyle \mathbf {w} \cdot \varphi (\mathbf {x} )=\sum _{i}\alpha _{i}y_{i}k(\mathbf {x} _{i},\mathbf {x} )}

.
 Computing the (soft-margin) SVM classifier amounts to minimizing an expression of the form
 We focus on the soft-margin classifier since, as noted above, choosing a sufficiently small value for 



λ


{\displaystyle \lambda }

 yields the hard-margin classifier for linearly classifiable input data. The classical approach, which involves reducing (2) to a quadratic programming problem, is detailed below. Then, more recent approaches such as sub-gradient descent and coordinate descent will be discussed.
 Minimizing (2) can be rewritten as a constrained optimization problem with a differentiable objective function in the following way.
 For each 



i
∈
{
1
,

…
,

n
}


{\displaystyle i\in \{1,\,\ldots ,\,n\}}

 we introduce a variable 




ζ

i


=
max

(

0
,
1
−

y

i


(


w



T





x


i


−
b
)

)



{\displaystyle \zeta _{i}=\max \left(0,1-y_{i}(\mathbf {w} ^{\mathsf {T}}\mathbf {x} _{i}-b)\right)}

. Note that 




ζ

i




{\displaystyle \zeta _{i}}

 is the smallest nonnegative number satisfying 




y

i


(


w



T





x


i


−
b
)
≥
1
−

ζ

i


.


{\displaystyle y_{i}(\mathbf {w} ^{\mathsf {T}}\mathbf {x} _{i}-b)\geq 1-\zeta _{i}.}


 Thus we can rewrite the optimization problem as follows
 









minimize 



1
n



∑

i
=
1


n



ζ

i


+
λ
‖

w


‖

2








subject to 


y

i



(



w



T





x


i


−
b

)

≥
1
−

ζ

i




 and 



ζ

i


≥
0
,


for all 

i
.






{\displaystyle {\begin{aligned}&{\text{minimize }}{\frac {1}{n}}\sum _{i=1}^{n}\zeta _{i}+\lambda \|\mathbf {w} \|^{2}\\[0.5ex]&{\text{subject to }}y_{i}\left(\mathbf {w} ^{\mathsf {T}}\mathbf {x} _{i}-b\right)\geq 1-\zeta _{i}\,{\text{ and }}\,\zeta _{i}\geq 0,\,{\text{for all }}i.\end{aligned}}}


 This is called the primal problem.
 By solving for the Lagrangian dual of the above problem, one obtains the simplified problem
 









maximize



f
(

c

1


…

c

n


)
=

∑

i
=
1


n



c

i


−


1
2



∑

i
=
1


n



∑

j
=
1


n



y

i



c

i


(


x


i



T





x


j


)

y

j



c

j


,






subject to 


∑

i
=
1


n



c

i



y

i


=
0
,


and 

0
≤

c

i


≤


1

2
n
λ





for all 

i
.






{\displaystyle {\begin{aligned}&{\text{maximize}}\,\,f(c_{1}\ldots c_{n})=\sum _{i=1}^{n}c_{i}-{\frac {1}{2}}\sum _{i=1}^{n}\sum _{j=1}^{n}y_{i}c_{i}(\mathbf {x} _{i}^{\mathsf {T}}\mathbf {x} _{j})y_{j}c_{j},\\&{\text{subject to }}\sum _{i=1}^{n}c_{i}y_{i}=0,\,{\text{and }}0\leq c_{i}\leq {\frac {1}{2n\lambda }}\;{\text{for all }}i.\end{aligned}}}


 This is called the dual problem. Since the dual maximization problem is a quadratic function of the 




c

i




{\displaystyle c_{i}}

 subject to linear constraints, it is efficiently solvable by quadratic programming algorithms.
 Here, the variables 




c

i




{\displaystyle c_{i}}

 are defined such that
 




w

=

∑

i
=
1


n



c

i



y

i




x


i


.


{\displaystyle \mathbf {w} =\sum _{i=1}^{n}c_{i}y_{i}\mathbf {x} _{i}.}


 Moreover, 




c

i


=
0


{\displaystyle c_{i}=0}

 exactly when 





x


i




{\displaystyle \mathbf {x} _{i}}

 lies on the correct side of the margin, and 



0
<

c

i


<
(
2
n
λ

)

−
1




{\displaystyle 0<c_{i}<(2n\lambda )^{-1}}

  when 





x


i




{\displaystyle \mathbf {x} _{i}}

 lies on the margin's boundary. It follows that 




w



{\displaystyle \mathbf {w} }

 can be written as a linear combination of the support vectors.
 The offset, 



b


{\displaystyle b}

, can be recovered by finding an 





x


i




{\displaystyle \mathbf {x} _{i}}

 on the margin's boundary and solving





y

i


(


w



T





x


i


−
b
)
=
1

⟺

b
=


w



T





x


i


−

y

i


.


{\displaystyle y_{i}(\mathbf {w} ^{\mathsf {T}}\mathbf {x} _{i}-b)=1\iff b=\mathbf {w} ^{\mathsf {T}}\mathbf {x} _{i}-y_{i}.}


 (Note that 




y

i


−
1


=

y

i




{\displaystyle y_{i}^{-1}=y_{i}}

 since 




y

i


=
±
1


{\displaystyle y_{i}=\pm 1}

.)
 Suppose now that we would like to learn a nonlinear classification rule which corresponds to a linear classification rule for the transformed data points 



φ
(


x


i


)
.


{\displaystyle \varphi (\mathbf {x} _{i}).}

 Moreover, we are given a kernel function 



k


{\displaystyle k}

 which satisfies 



k
(


x


i


,


x


j


)
=
φ
(


x


i


)
⋅
φ
(


x


j


)


{\displaystyle k(\mathbf {x} _{i},\mathbf {x} _{j})=\varphi (\mathbf {x} _{i})\cdot \varphi (\mathbf {x} _{j})}

.
 We know the classification vector 




w



{\displaystyle \mathbf {w} }

 in the transformed space satisfies
 




w

=

∑

i
=
1


n



c

i



y

i


φ
(


x


i


)
,


{\displaystyle \mathbf {w} =\sum _{i=1}^{n}c_{i}y_{i}\varphi (\mathbf {x} _{i}),}


 where, the 




c

i




{\displaystyle c_{i}}

 are obtained by solving the optimization problem
 








maximize



f
(

c

1


…

c

n


)



=

∑

i
=
1


n



c

i


−


1
2



∑

i
=
1


n



∑

j
=
1


n



y

i



c

i


(
φ
(


x


i


)
⋅
φ
(


x


j


)
)

y

j



c

j








=

∑

i
=
1


n



c

i


−


1
2



∑

i
=
1


n



∑

j
=
1


n



y

i



c

i


k
(


x


i


,


x


j


)

y

j



c

j







subject to 


∑

i
=
1


n



c

i



y

i





=
0
,


and 

0
≤

c

i


≤


1

2
n
λ





for all 

i
.






{\displaystyle {\begin{aligned}{\text{maximize}}\,\,f(c_{1}\ldots c_{n})&=\sum _{i=1}^{n}c_{i}-{\frac {1}{2}}\sum _{i=1}^{n}\sum _{j=1}^{n}y_{i}c_{i}(\varphi (\mathbf {x} _{i})\cdot \varphi (\mathbf {x} _{j}))y_{j}c_{j}\\&=\sum _{i=1}^{n}c_{i}-{\frac {1}{2}}\sum _{i=1}^{n}\sum _{j=1}^{n}y_{i}c_{i}k(\mathbf {x} _{i},\mathbf {x} _{j})y_{j}c_{j}\\{\text{subject to }}\sum _{i=1}^{n}c_{i}y_{i}&=0,\,{\text{and }}0\leq c_{i}\leq {\frac {1}{2n\lambda }}\;{\text{for all }}i.\end{aligned}}}


 The coefficients 




c

i




{\displaystyle c_{i}}

 can be solved for using quadratic programming, as before. Again, we can find some index 



i


{\displaystyle i}

 such that 



0
<

c

i


<
(
2
n
λ

)

−
1




{\displaystyle 0<c_{i}<(2n\lambda )^{-1}}

, so that 



φ
(


x


i


)


{\displaystyle \varphi (\mathbf {x} _{i})}

 lies on the boundary of the margin in the transformed space, and then solve
 







b
=


w



T



φ
(


x


i


)
−

y

i





=

[


∑

j
=
1


n



c

j



y

j


φ
(


x


j


)
⋅
φ
(


x


i


)

]

−

y

i








=

[


∑

j
=
1


n



c

j



y

j


k
(


x


j


,


x


i


)

]

−

y

i


.






{\displaystyle {\begin{aligned}b=\mathbf {w} ^{\mathsf {T}}\varphi (\mathbf {x} _{i})-y_{i}&=\left[\sum _{j=1}^{n}c_{j}y_{j}\varphi (\mathbf {x} _{j})\cdot \varphi (\mathbf {x} _{i})\right]-y_{i}\\&=\left[\sum _{j=1}^{n}c_{j}y_{j}k(\mathbf {x} _{j},\mathbf {x} _{i})\right]-y_{i}.\end{aligned}}}


 Finally,
 




z

↦
sgn
⁡
(


w



T



φ
(

z

)
−
b
)
=
sgn
⁡

(


[


∑

i
=
1


n



c

i



y

i


k
(


x


i


,

z

)

]

−
b

)

.


{\displaystyle \mathbf {z} \mapsto \operatorname {sgn}(\mathbf {w} ^{\mathsf {T}}\varphi (\mathbf {z} )-b)=\operatorname {sgn} \left(\left[\sum _{i=1}^{n}c_{i}y_{i}k(\mathbf {x} _{i},\mathbf {z} )\right]-b\right).}


 Recent algorithms for finding the SVM classifier include sub-gradient descent and coordinate descent. Both techniques have proven to offer significant advantages over the traditional approach when dealing with large, sparse datasets—sub-gradient methods are especially efficient when there are many training examples, and coordinate descent when the dimension of the feature space is high.
 Sub-gradient descent algorithms for the SVM work directly with the expression
 



f
(

w

,
b
)
=

[



1
n



∑

i
=
1


n


max

(

0
,
1
−

y

i


(


w



T





x


i


−
b
)

)


]

+
λ
‖

w


‖

2


.


{\displaystyle f(\mathbf {w} ,b)=\left[{\frac {1}{n}}\sum _{i=1}^{n}\max \left(0,1-y_{i}(\mathbf {w} ^{\mathsf {T}}\mathbf {x} _{i}-b)\right)\right]+\lambda \|\mathbf {w} \|^{2}.}


 Note that 



f


{\displaystyle f}

 is a convex function of 




w



{\displaystyle \mathbf {w} }

 and 



b


{\displaystyle b}

. As such, traditional gradient descent (or SGD) methods can be adapted, where instead of taking a step in the direction of the function's gradient, a step is taken in the direction of a vector selected from the function's sub-gradient. This approach has the advantage that, for certain implementations, the number of iterations does not scale with 



n


{\displaystyle n}

, the number of data points.[22]
 Coordinate descent algorithms for the SVM work from the dual problem
 









maximize



f
(

c

1


…

c

n


)
=

∑

i
=
1


n



c

i


−


1
2



∑

i
=
1


n



∑

j
=
1


n



y

i



c

i


(

x

i


⋅

x

j


)

y

j



c

j


,






subject to 


∑

i
=
1


n



c

i



y

i


=
0
,


and 

0
≤

c

i


≤


1

2
n
λ





for all 

i
.






{\displaystyle {\begin{aligned}&{\text{maximize}}\,\,f(c_{1}\ldots c_{n})=\sum _{i=1}^{n}c_{i}-{\frac {1}{2}}\sum _{i=1}^{n}\sum _{j=1}^{n}y_{i}c_{i}(x_{i}\cdot x_{j})y_{j}c_{j},\\&{\text{subject to }}\sum _{i=1}^{n}c_{i}y_{i}=0,\,{\text{and }}0\leq c_{i}\leq {\frac {1}{2n\lambda }}\;{\text{for all }}i.\end{aligned}}}


 For each 



i
∈
{
1
,

…
,

n
}


{\displaystyle i\in \{1,\,\ldots ,\,n\}}

, iteratively, the coefficient 




c

i




{\displaystyle c_{i}}

 is adjusted in the direction of 



∂
f

/

∂

c

i




{\displaystyle \partial f/\partial c_{i}}

. Then, the resulting vector of coefficients 



(

c

1

′

,

…
,


c

n

′

)


{\displaystyle (c_{1}',\,\ldots ,\,c_{n}')}

 is projected onto the nearest vector of coefficients that satisfies the given constraints. (Typically Euclidean distances are used.) The process is then repeated until a near-optimal vector of coefficients is obtained. The resulting algorithm is extremely fast in practice, although few performance guarantees have been proven.[23]
 The soft-margin support vector machine described above is an example of an empirical risk minimization (ERM) algorithm for the hinge loss. Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss. This perspective can provide further insight into how and why SVMs work, and allow us to better analyze their statistical properties.
 In supervised learning, one is given a set of training examples 




X

1


…

X

n




{\displaystyle X_{1}\ldots X_{n}}

 with labels 




y

1


…

y

n




{\displaystyle y_{1}\ldots y_{n}}

, and wishes to predict 




y

n
+
1




{\displaystyle y_{n+1}}

 given 




X

n
+
1




{\displaystyle X_{n+1}}

. To do so one forms a hypothesis, 



f


{\displaystyle f}

, such that 



f
(

X

n
+
1


)


{\displaystyle f(X_{n+1})}

 is a ""good"" approximation of 




y

n
+
1




{\displaystyle y_{n+1}}

. A ""good"" approximation is usually defined with the help of a loss function, 



ℓ
(
y
,
z
)


{\displaystyle \ell (y,z)}

, which characterizes how bad 



z


{\displaystyle z}

 is as a prediction of 



y


{\displaystyle y}

. We would then like to choose a hypothesis that minimizes the expected risk:
 



ε
(
f
)
=

E


[

ℓ
(

y

n
+
1


,
f
(

X

n
+
1


)
)

]

.


{\displaystyle \varepsilon (f)=\mathbb {E} \left[\ell (y_{n+1},f(X_{n+1}))\right].}


 In most cases, we don't know the joint distribution of 




X

n
+
1


,


y

n
+
1




{\displaystyle X_{n+1},\,y_{n+1}}

 outright. In these cases, a common strategy is to choose the hypothesis that minimizes the empirical risk:
 






ε
^



(
f
)
=


1
n



∑

k
=
1


n


ℓ
(

y

k


,
f
(

X

k


)
)
.


{\displaystyle {\hat {\varepsilon }}(f)={\frac {1}{n}}\sum _{k=1}^{n}\ell (y_{k},f(X_{k})).}


 Under certain assumptions about the sequence of random variables 




X

k


,


y

k




{\displaystyle X_{k},\,y_{k}}

 (for example, that they are generated by a finite Markov process), if the set of hypotheses being considered is small enough, the minimizer of the empirical risk will closely approximate the minimizer of the expected risk as 



n


{\displaystyle n}

 grows large. This approach is called empirical risk minimization, or ERM.
 In order for the minimization problem to have a well-defined solution, we have to place constraints on the set 





H




{\displaystyle {\mathcal {H}}}

 of hypotheses being considered. If 





H




{\displaystyle {\mathcal {H}}}

 is a normed space (as is the case for SVM), a particularly effective technique is to consider only those hypotheses 



f


{\displaystyle f}

 for which 



‖
f

‖


H



<
k


{\displaystyle \lVert f\rVert _{\mathcal {H}}<k}

 . This is equivalent to imposing a regularization penalty 





R


(
f
)
=

λ

k


‖
f

‖


H





{\displaystyle {\mathcal {R}}(f)=\lambda _{k}\lVert f\rVert _{\mathcal {H}}}

, and solving the new optimization problem
 






f
^



=

a
r
g


min

f
∈


H







ε
^



(
f
)
+


R


(
f
)
.


{\displaystyle {\hat {f}}=\mathrm {arg} \min _{f\in {\mathcal {H}}}{\hat {\varepsilon }}(f)+{\mathcal {R}}(f).}


 This approach is called Tikhonov regularization.
 More generally, 





R


(
f
)


{\displaystyle {\mathcal {R}}(f)}

 can be some measure of the complexity of the hypothesis 



f


{\displaystyle f}

, so that simpler hypotheses are preferred.
 Recall that the (soft-margin) SVM classifier 







w

^



,
b
:

x

↦
sgn
⁡
(





w

^





T




x

−
b
)


{\displaystyle {\hat {\mathbf {w} }},b:\mathbf {x} \mapsto \operatorname {sgn}({\hat {\mathbf {w} }}^{\mathsf {T}}\mathbf {x} -b)}

 is chosen to minimize the following expression:
 




[



1
n



∑

i
=
1


n


max

(

0
,
1
−

y

i


(


w



T




x

−
b
)

)


]

+
λ
‖

w


‖

2


.


{\displaystyle \left[{\frac {1}{n}}\sum _{i=1}^{n}\max \left(0,1-y_{i}(\mathbf {w} ^{\mathsf {T}}\mathbf {x} -b)\right)\right]+\lambda \|\mathbf {w} \|^{2}.}


 In light of the above discussion, we see that the SVM technique is equivalent to empirical risk minimization with Tikhonov regularization, where in this case the loss function is the hinge loss
 



ℓ
(
y
,
z
)
=
max

(

0
,
1
−
y
z

)

.


{\displaystyle \ell (y,z)=\max \left(0,1-yz\right).}


 From this perspective, SVM is closely related to other fundamental classification algorithms such as regularized least-squares and logistic regression. The difference between the three lies in the choice of loss function: regularized least-squares amounts to empirical risk minimization with the square-loss,  




ℓ

s
q


(
y
,
z
)
=
(
y
−
z

)

2




{\displaystyle \ell _{sq}(y,z)=(y-z)^{2}}

; logistic regression employs the log-loss,
 




ℓ

log


(
y
,
z
)
=
ln
⁡
(
1
+

e

−
y
z


)
.


{\displaystyle \ell _{\log }(y,z)=\ln(1+e^{-yz}).}


 The difference between the hinge loss and these other loss functions is best stated in terms of target functions - the function that minimizes expected risk for a given pair of random variables 



X
,

y


{\displaystyle X,\,y}

.
 In particular, let 




y

x




{\displaystyle y_{x}}

 denote 



y


{\displaystyle y}

 conditional on the event that 



X
=
x


{\displaystyle X=x}

.  In the classification setting, we have:
 




y

x


=


{



1



with probability 


p

x






−
1



with probability 

1
−

p

x










{\displaystyle y_{x}={\begin{cases}1&{\text{with probability }}p_{x}\\-1&{\text{with probability }}1-p_{x}\end{cases}}}


 The optimal classifier is therefore:
 




f

∗


(
x
)
=


{



1



if 


p

x


≥
1

/

2




−
1



otherwise









{\displaystyle f^{*}(x)={\begin{cases}1&{\text{if }}p_{x}\geq 1/2\\-1&{\text{otherwise}}\end{cases}}}


 For the square-loss, the target function is the conditional expectation function, 




f

s
q


(
x
)
=

E


[

y

x


]



{\displaystyle f_{sq}(x)=\mathbb {E} \left[y_{x}\right]}

; For the logistic loss, it's the logit function, 




f

log


(
x
)
=
ln
⁡

(


p

x



/

(

1
−

p

x



)

)



{\displaystyle f_{\log }(x)=\ln \left(p_{x}/({1-p_{x}})\right)}

. While both of these target functions yield the correct classifier, as 



sgn
⁡
(

f

s
q


)
=
sgn
⁡
(

f

log


)
=

f

∗




{\displaystyle \operatorname {sgn}(f_{sq})=\operatorname {sgn}(f_{\log })=f^{*}}

, they give us more information than we need. In fact, they give us enough information to completely describe the distribution of 




y

x




{\displaystyle y_{x}}

.
 On the other hand, one can check that the target function for the hinge loss is exactly 




f

∗




{\displaystyle f^{*}}

. Thus, in a sufficiently rich hypothesis space—or equivalently, for an appropriately chosen kernel—the SVM classifier will converge to the simplest function (in terms of 





R




{\displaystyle {\mathcal {R}}}

) that correctly classifies the data. This extends the geometric interpretation of SVM—for linear classification, the empirical risk is minimized by any function whose margins lie between the support vectors, and the simplest of these is the max-margin classifier.[24]
 SVMs belong to a family of generalized linear classifiers and can be interpreted as an extension of the perceptron.[25] They can also be considered a special case of Tikhonov regularization. A special property is that they simultaneously minimize the empirical classification error and maximize the geometric margin; hence they are also known as maximum margin classifiers.
 A comparison of the SVM to other classifiers has been made by Meyer, Leisch and Hornik.[26]
 The effectiveness of SVM depends on the selection of kernel, the kernel's parameters, and soft margin parameter 



λ


{\displaystyle \lambda }

.
A common choice is a Gaussian kernel, which has a single parameter 



γ


{\displaystyle \gamma }

. The best combination of 



λ


{\displaystyle \lambda }

 and 



γ


{\displaystyle \gamma }

 is often selected by a grid search with exponentially growing sequences of 



λ


{\displaystyle \lambda }

 and 



γ


{\displaystyle \gamma }

, for example, 



λ
∈
{

2

−
5


,

2

−
3


,
…
,

2

13


,

2

15


}


{\displaystyle \lambda \in \{2^{-5},2^{-3},\dots ,2^{13},2^{15}\}}

; 



γ
∈
{

2

−
15


,

2

−
13


,
…
,

2

1


,

2

3


}


{\displaystyle \gamma \in \{2^{-15},2^{-13},\dots ,2^{1},2^{3}\}}

. Typically, each combination of parameter choices is checked using cross validation, and the parameters with best cross-validation accuracy are picked. Alternatively, recent work in Bayesian optimization can be used to select 



λ


{\displaystyle \lambda }

 and 



γ


{\displaystyle \gamma }

 , often requiring the evaluation of far fewer parameter combinations than grid search. The final model, which is used for testing and for classifying new data, is then trained on the whole training set using the selected parameters.[27]
 Potential drawbacks of the SVM include the following aspects:
 Multiclass SVM aims to assign labels to instances by using support vector machines, where the labels are drawn from a finite set of several elements.
 The dominant approach for doing so is to reduce the single multiclass problem into multiple binary classification problems.[28] Common methods for such reduction include:[28][29]
 Crammer and Singer proposed a multiclass SVM method which casts the multiclass classification problem into a single optimization problem, rather than decomposing it into multiple binary classification problems.[32] See also Lee, Lin and Wahba[33][34] and Van den Burg and Groenen.[35]
 Transductive support vector machines extend SVMs in that they could also treat partially labeled data in semi-supervised learning by following the principles of transduction. Here, in addition to the training set 





D




{\displaystyle {\mathcal {D}}}

, the learner is also given a set
 






D



⋆


=
{


x


i


⋆


∣


x


i


⋆


∈


R


p



}

i
=
1


k




{\displaystyle {\mathcal {D}}^{\star }=\{\mathbf {x} _{i}^{\star }\mid \mathbf {x} _{i}^{\star }\in \mathbb {R} ^{p}\}_{i=1}^{k}}


 of test examples to be classified. Formally, a transductive support vector machine is defined by the following primal optimization problem:[36]
 Minimize (in 




w

,
b
,


y


⋆




{\displaystyle \mathbf {w} ,b,\mathbf {y} ^{\star }}

)
 





1
2


‖

w


‖

2




{\displaystyle {\frac {1}{2}}\|\mathbf {w} \|^{2}}


 subject to (for any 



i
=
1
,
…
,
n


{\displaystyle i=1,\dots ,n}

 and any 



j
=
1
,
…
,
k


{\displaystyle j=1,\dots ,k}

)
 









y

i


(

w

⋅


x


i


−
b
)
≥
1
,






y

j


⋆


(

w

⋅


x


j


⋆


−
b
)
≥
1
,






{\displaystyle {\begin{aligned}&y_{i}(\mathbf {w} \cdot \mathbf {x} _{i}-b)\geq 1,\\&y_{j}^{\star }(\mathbf {w} \cdot \mathbf {x} _{j}^{\star }-b)\geq 1,\end{aligned}}}


 and
 




y

j


⋆


∈
{
−
1
,
1
}
.


{\displaystyle y_{j}^{\star }\in \{-1,1\}.}


 Transductive support vector machines were introduced by Vladimir N. Vapnik in 1998.
 Structured support-vector machine is an extension of the traditional SVM model. While the SVM model is primarily designed for binary classification, multiclass classification, and regression tasks, structured SVM broadens its application to handle general structured output labels, for example parse trees, classification with taxonomies, sequence alignment and many more.[37]
 A version of SVM for regression was proposed in 1996 by Vladimir N. Vapnik, Harris Drucker, Christopher J. C. Burges, Linda Kaufman and Alexander J. Smola.[38] This method is called support vector regression (SVR). The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by SVR depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction. Another SVM version known as least-squares support vector machine (LS-SVM) has been proposed by Suykens and Vandewalle.[39]
 Training the original SVR means solving[40]
 where 




x

i




{\displaystyle x_{i}}

 is a training sample with target value 




y

i




{\displaystyle y_{i}}

. The inner product plus intercept 



⟨
w
,

x

i


⟩
+
b


{\displaystyle \langle w,x_{i}\rangle +b}

 is the prediction for that sample, and 



ε


{\displaystyle \varepsilon }

 is a free parameter that serves as a threshold: all predictions have to be within an 



ε


{\displaystyle \varepsilon }

 range of the true predictions. Slack variables are usually added into the above to allow for errors and to allow approximation in the case the above problem is infeasible.
 In 2011 it was shown by Polson and Scott that the SVM admits a Bayesian interpretation through the technique of data augmentation.[41] In this approach the SVM is viewed as a graphical model (where the parameters are connected via probability distributions). This extended view allows the application of Bayesian techniques to SVMs, such as flexible feature modeling, automatic hyperparameter tuning, and predictive uncertainty quantification. Recently, a scalable version of the Bayesian SVM was developed by Florian Wenzel, enabling the application of Bayesian SVMs to big data.[42] Florian Wenzel developed two different versions, a variational inference (VI) scheme for the Bayesian kernel support vector machine (SVM) and a stochastic version (SVI) for the linear Bayesian SVM.[43]
 The parameters of the maximum-margin hyperplane are derived by solving the optimization. There exist several specialized algorithms for quickly solving the quadratic programming (QP) problem that arises from SVMs, mostly relying on heuristics for breaking the problem down into smaller, more manageable chunks.
 Another approach is to use an interior-point method that uses Newton-like iterations to find a solution of the Karush–Kuhn–Tucker conditions of the primal and dual problems.[44]
Instead of solving a sequence of broken-down problems, this approach directly solves the problem altogether. To avoid solving a linear system involving the large kernel matrix, a low-rank approximation to the matrix is often used in the kernel trick.
 Another common method is Platt's sequential minimal optimization (SMO) algorithm, which breaks the problem down into 2-dimensional sub-problems that are solved analytically, eliminating the need for a numerical optimization algorithm and matrix storage. This algorithm is conceptually simple, easy to implement, generally faster, and has better scaling properties for difficult SVM problems.[45]
 The special case of linear support vector machines can be solved more efficiently by the same kind of algorithms used to optimize its close cousin, logistic regression; this class of algorithms includes sub-gradient descent (e.g., PEGASOS[46]) and coordinate descent (e.g., LIBLINEAR[47]). LIBLINEAR has some attractive training-time properties. Each convergence iteration takes time linear in the time taken to read the train data, and the iterations also have a Q-linear convergence property, making the algorithm extremely fast.
 The general kernel SVMs can also be solved more efficiently using sub-gradient descent (e.g. P-packSVM[48]), especially when parallelization is allowed.
 Kernel SVMs are available in many machine-learning toolkits, including LIBSVM, MATLAB, SAS, SVMlight, kernlab, scikit-learn, Shogun, Weka, Shark, JKernelMachines, OpenCV and others.
 Preprocessing of data (standardization) is highly recommended to enhance accuracy of classification.[49] There are a few methods of standardization, such as min-max, normalization by decimal scaling, Z-score.[50] Subtraction of mean and division by variance of each feature is usually used for SVM.[51]
",machin learn support vector machin svm also support vector network supervis model associ learn algorithm analyz data classif regress analysi develop bell laboratori svm one studi model base statist learn framework vc theori propos vapnik chervonenki addit perform linear classif svm effici perform classif use kernel trick repres data set pairwis similar comparison origin data point use kernel function transform coordin featur space thu svm use kernel trick implicitli map input featur space linear classif perform model svm resili noisi data misclassifi exampl svm also use regress task object becom ϵ support vector cluster algorithm creat hava siegelmann vladimir vapnik appli statist support vector develop support vector machin algorithm categor unlabel data citat need data set requir unsupervis learn approach attempt find natur cluster data group map new data accord cluster popular svm like due amen theoret analysi flexibl appli wide varieti task includ structur predict problem clear svm better predict perform linear model logist regress linear regress citat need classifi data common task machin learn suppos given data point belong one two class goal decid class new data point case support vector machin data point view p p vector list p p number want know whether separ point p hyperplan call linear classifi mani hyperplan might classifi data one reason choic best hyperplan one repres largest separ margin two class choos hyperplan distanc nearest data point side maxim hyperplan exist known hyperplan linear classifi defin known classifi equival perceptron optim stabil citat need formal support vector machin construct hyperplan set hyperplan high space use classif regress task like outlier detect intuit good separ achiev hyperplan largest distanc nearest point class function margin sinc gener larger margin lower gener error classifi lower gener error mean implement less like experi overfit wherea origin problem may state space often happen set discrimin linearli separ space reason propos origin space map much space presum make separ easier space keep comput load reason map use svm scheme design ensur dot product pair input data vector may comput easili term variabl origin space defin term kernel function k x k x select suit problem hyperplan space defin set point whose dot product vector space constant set vector orthogon thu minim set vector defin hyperplan vector defin hyperplan chosen linear combin paramet α imag featur vector x occur data base choic hyperplan point x x featur space map hyperplan defin relat α k x x constant k x constant note k x k x becom small grow away x x term sum measur degre close test point x x correspond data base point x way sum kernel use measur rel near test point data point origin one set discrimin note fact set point x x map hyperplan quit convolut result allow much complex discrimin set convex origin space svm use solv variou problem origin svm algorithm invent vladimir vapnik alexey ya chervonenki citat need bernhard boser isabel guyon vladimir vapnik suggest way creat nonlinear classifi appli kernel trick hyperplan soft margin incarn commonli use softwar packag propos corinna cort vapnik publish given train dataset n n point form x x n n x x n n either indic class point x x belong x x p p real vector want find hyperplan divid group point x x group point defin distanc hyperplan nearest point x x either group maxim hyperplan written set point x x satisfi w x b w x w w necessarili normal normal vector hyperplan much like hess normal form except w w necessarili unit vector paramet b w b w determin offset hyperplan origin along normal vector w w warn literatur subject defin bia w x b w x train data linearli separ select two parallel hyperplan separ two class data distanc larg possibl region bound two hyperplan call margin hyperplan hyperplan lie halfway normal standard dataset hyperplan describ equat geometr distanc two hyperplan w w maxim distanc plane want minim w w distanc comput use distanc point plane equat also prevent data point fall margin add follow constraint either w x b w x w x b w x constraint state data point must lie correct side margin rewritten put togeth get optim problem minim w b w subject w x b n align w b minim w subject w x align w w b b solv problem determin final classifi x sgn w x b x sgn w x sgn sgn sign function import consequ geometr descript hyperplan complet determin x x lie nearest explain x x call support vector extend svm case data linearli separ hing loss function help max w x b w x note target case w x b w x output function zero constraint satisfi word x x lie correct side margin data wrong side margin function valu proport distanc margin goal optim minim w c n n max w x b w n n w x paramet c c determin increas margin size ensur x x lie correct side margin note add weight either term equat deconstruct hing loss optim problem massag follow minim w b ζ w c n ζ subject w x b ζ ζ n align w b minim w n subject w x align thu larg valu c c behav similar svm input data linearli classifi still learn classif rule viabl origin hyperplan algorithm propos vapnik construct linear classifi howev bernhard boser isabel guyon vladimir vapnik suggest way creat nonlinear classifi appli kernel trick origin propos aizerman et al hyperplan kernel trick dot product replac kernel easili deriv dual represent svm problem allow algorithm fit hyperplan transform featur space transform may nonlinear transform space although classifi hyperplan transform featur space may nonlinear origin input space noteworthi work featur space increas gener error support vector machin although given enough sampl algorithm still perform well common kernel includ kernel relat transform φ x x equat k x x j φ x φ x j k x x j x x j valu w also transform space w α φ x w x dot product w classif comput kernel trick w φ x α k x x w x k x x comput svm classifi amount minim express form focu classifi sinc note choos suffici small valu λ yield classifi linearli classifi input data classic approach involv reduc quadrat program problem detail recent approach descent coordin descent discuss minim rewritten constrain optim problem differenti object function follow way n introduc variabl ζ max w x b w x note ζ smallest nonneg number satisfi w x b ζ w x thu rewrit optim problem follow minim n n ζ λ w subject w x b ζ ζ align minim n n w subject w x align call primal problem solv lagrangian dual problem one obtain simplifi problem maxim f c c n n c n j n c x x j j c j subject n c c n λ align maxim f n n n n x x j j j subject n align call dual problem sinc dual maxim problem quadrat function c subject linear constraint effici solvabl quadrat program algorithm variabl c defin w n c x w n x moreov c exactli x x lie correct side margin c n λ x x lie margin boundari follow w w written linear combin support vector offset b b recov find x x margin boundari solv w x b b w x w x w x note sinc suppos would like learn nonlinear classif rule correspond linear classif rule transform data point φ x x moreov given kernel function k k satisfi k x x j φ x φ x j k x x j x x j know classif vector w w transform space satisfi w n c φ x w n x c obtain solv optim problem maxim f c c n n c n j n c φ x φ x j j c j n c n j n c k x x j j c j subject n c c n λ align maxim f n n n n x x j j j n n n k x x j j j subject n align coeffici c solv use quadrat program find index c n λ φ x x lie boundari margin transform space solv b w φ x j n c j j φ x j φ x j n c j j k x j x align w x n j j x j x n j j k x j x align final z sgn w φ z b sgn n c k x z b z sgn w z sgn n k x z recent algorithm find svm classifi includ descent coordin descent techniqu proven offer signific advantag tradit approach deal larg spars method especi effici mani train exampl coordin descent dimens featur space high descent algorithm svm work directli express f w b n n max w x b λ w f w b n n w x w note f f convex function w w b b tradit gradient descent sgd method adapt instead take step direct function gradient step taken direct vector select function approach advantag certain implement number iter scale n n number data point coordin descent algorithm svm work dual problem maxim f c c n n c n j n c x x j j c j subject n c c n λ align maxim f n n n n j j j subject n align n iter coeffici c adjust direct f c result vector coeffici c c n n project onto nearest vector coeffici satisfi given constraint typic euclidean distanc use process repeat vector coeffici obtain result algorithm extrem fast practic although perform guarante proven support vector machin describ exampl empir risk minim erm algorithm hing loss seen way support vector machin belong natur class algorithm statist infer mani uniqu featur due behavior hing loss perspect provid insight svm work allow us better analyz statist properti supervis learn one given set train exampl x x n n label n n wish predict n given x n one form hypothesi f f f x n f good approxim n good approxim usual defin help loss function ℓ z z character bad z z predict would like choos hypothesi minim expect risk ε f e ℓ n f x n f e f case know joint distribut x n n outright case common strategi choos hypothesi minim empir risk ε f n k n ℓ k f x k f n n k f k certain assumpt sequenc random variabl x k k k k exampl gener finit markov process set hypothes consid small enough minim empir risk close approxim minim expect risk n n grow larg approach call empir risk minim erm order minim problem solut place constraint set h h hypothes consid h h norm space case svm particularli effect techniqu consid hypothes f f f h k h k equival impos regular penalti r f λ k f h r f k h solv new optim problem f r g min f h ε f r f f arg h f r f approach call tikhonov regular gener r f r f measur complex hypothesi f f simpler hypothes prefer recal svm classifi w b x sgn w x b w b x sgn w x chosen minim follow express n n max w x b λ w n n w x w light discuss see svm techniqu equival empir risk minim tikhonov regular case loss function hing loss ℓ z max z z perspect svm close relat fundament classif algorithm regular logist regress differ three lie choic loss function regular amount empir risk minim ℓ q z z sq z logist regress employ ℓ log z ln e z z differ hing loss loss function best state term target function function minim expect risk given pair random variabl x x particular let x x denot condit event x x classif set x probabl p x probabl p x x case probabl x probabl x case optim classifi therefor f x p x otherwis x case x otherwis case target function condit expect function f q x e x sq x e x logist loss logit function f log x ln p x p x x x x target function yield correct classifi sgn f q sgn f log f sgn sq sgn give us inform need fact give us enough inform complet describ distribut x x hand one check target function hing loss exactli f thu suffici rich hypothesi equival appropri chosen svm classifi converg simplest function term r r correctli classifi data extend geometr interpret linear classif empir risk minim function whose margin lie support vector simplest classifi svm belong famili gener linear classifi interpret extens perceptron also consid special case tikhonov regular special properti simultan minim empir classif error maxim geometr margin henc also known maximum margin classifi comparison svm classifi made meyer leisch hornik effect svm depend select kernel kernel paramet soft margin paramet λ common choic gaussian kernel singl paramet γ best combin λ γ often select grid search exponenti grow sequenc λ γ exampl λ γ typic combin paramet choic check use cross valid paramet best accuraci pick altern recent work bayesian optim use select λ γ often requir evalu far fewer paramet combin grid search final model use test classifi new data train whole train set use select paramet potenti drawback svm includ follow aspect multiclass svm aim assign label instanc use support vector machin label drawn finit set sever element domin approach reduc singl multiclass problem multipl binari classif problem common method reduct includ crammer singer propos multiclass svm method cast multiclass classif problem singl optim problem rather decompos multipl binari classif problem see also lee lin wahba van den burg groenen transduct support vector machin extend svm could also treat partial label data learn follow principl transduct addit train set learner also given set x x r p k x x r p k test exampl classifi formal transduct support vector machin defin follow primal optim problem minim w b w b w w subject n n j k k w x b j w x j b align w x j w x j align j j transduct support vector machin introduc vladimir vapnik structur machin extens tradit svm model svm model primarili design binari classif multiclass classif regress task structur svm broaden applic handl gener structur output label exampl pars tree classif taxonomi sequenc align mani version svm regress propos vladimir vapnik harri drucker christoph burg linda kaufman alexand j smola method call support vector regress svr model produc support vector classif describ depend subset train data cost function build model care train point lie beyond margin analog model produc svr depend subset train data cost function build model ignor train data close model predict anoth svm version known support vector machin propos suyken vandewal train origin svr mean solv x train sampl target valu inner product plu intercept w x b w predict sampl ε free paramet serv threshold predict within ε rang true predict slack variabl usual ad allow error allow approxim case problem infeas shown polson scott svm admit bayesian interpret techniqu data augment approach svm view graphic model paramet connect via probabl distribut extend view allow applic bayesian techniqu svm flexibl featur model automat hyperparamet tune predict uncertainti quantif recent scalabl version bayesian svm develop florian wenzel enabl applic bayesian svm big data florian wenzel develop two differ version variat infer vi scheme bayesian kernel support vector machin svm stochast version svi linear bayesian svm paramet hyperplan deriv solv optim exist sever special algorithm quickli solv quadrat program qp problem aris svm mostli reli heurist break problem smaller manag chunk anoth approach use method use iter find solut condit primal dual problem instead solv sequenc problem approach directli solv problem altogeth avoid solv linear system involv larg kernel matrix approxim matrix often use kernel trick anoth common method platt sequenti minim optim smo algorithm break problem solv analyt elimin need numer optim algorithm matrix storag algorithm conceptu simpl easi implement gener faster better scale properti difficult svm problem special case linear support vector machin solv effici kind algorithm use optim close cousin logist regress class algorithm includ descent pegaso coordin descent liblinear liblinear attract properti converg iter take time linear time taken read train data iter also converg properti make algorithm extrem fast gener kernel svm also solv effici use descent especi parallel allow kernel svm avail mani toolkit includ libsvm matlab sa svmlight kernlab shogun weka shark jkernelmachin opencv other preprocess data standard highli recommend enhanc accuraci classif method standard normal decim scale subtract mean divis varianc featur usual use svm
BIRCH,https://en.wikipedia.org/wiki/BIRCH,"BIRCH (balanced iterative reducing and clustering using hierarchies) is an unsupervised data mining algorithm used to perform hierarchical clustering over particularly large data-sets.[1] With modifications it can also be used to accelerate k-means clustering and Gaussian mixture modeling with the expectation–maximization algorithm.[2] An advantage of BIRCH is its ability to incrementally and dynamically cluster incoming, multi-dimensional metric data points in an attempt to produce the best quality clustering for a given set of resources (memory and time constraints). In most cases, BIRCH only requires a single scan of the database.
 Its inventors claim BIRCH to be the ""first clustering algorithm proposed in the database area to handle 'noise' (data points that are not part of the underlying pattern) effectively"",[1] beating DBSCAN by two months. The BIRCH algorithm received the SIGMOD 10 year test of time award in 2006.[3]
 Previous clustering algorithms performed less effectively over very large databases and did not adequately consider the case wherein a data-set was too large to fit in main memory. As a result, there was a lot of overhead maintaining high clustering quality while minimizing the cost of additional IO (input/output) operations. Furthermore, most of BIRCH's predecessors inspect all data points (or all currently existing clusters) equally for each 'clustering decision' and do not perform heuristic weighting based on the distance between these data points.
 It is local in that each clustering decision is made without scanning all data points and currently existing clusters.
It exploits the observation that the data space is not usually uniformly occupied and not every data point is equally important.
It makes full use of available memory to derive the finest possible sub-clusters while minimizing I/O costs.
It is also an incremental method that does not require the whole data set in advance.
 The BIRCH algorithm takes as input a set of N data points, represented as real-valued vectors, and a desired number of clusters K. It operates in four phases, the second of which is optional.
 The first phase builds a clustering feature (



C
F


{\displaystyle CF}

) tree out of the data points, a height-balanced tree data structure, defined as follows:
 In the second step, the algorithm scans all the leaf entries in the initial 



C
F


{\displaystyle CF}

 tree to rebuild a smaller 



C
F


{\displaystyle CF}

 tree, while removing outliers and grouping crowded subclusters into larger ones. This step is marked optional in the original presentation of BIRCH.
 In step three an existing clustering algorithm is used to cluster all leaf entries. Here an agglomerative hierarchical clustering algorithm is applied directly to the subclusters represented by their 



C
F


{\displaystyle CF}

 vectors. It also provides the flexibility of allowing the user to specify either the desired number of clusters or the desired diameter threshold for clusters. After this step a set of clusters is obtained that captures major distribution pattern in the data. However, there might exist minor and localized inaccuracies which can be handled by an optional step 4. In step 4 the centroids of the clusters produced in step 3 are used as seeds and redistribute the data points to its closest seeds to obtain a new set of clusters. Step 4 also provides us with an option of discarding outliers. That is a point which is too far from its closest seed can be treated as an outlier.
 Given only the clustering feature 



C
F
=
[
N
,



L
S

→


,
S
S
]


{\displaystyle CF=[N,{\overrightarrow {LS}},SS]}

, the same measures can be calculated without the knowledge of the underlying actual values.
 In multidimensional cases the square root should be replaced with a suitable norm.
 BIRCH uses the distances DO to D3 to find the nearest leaf, then the radius R or the diameter D to decide whether to absorb the data into the existing leaf or whether to add a new leaf.
 Unfortunately, there are numerical issues associated with the use of the term 



S
S


{\displaystyle SS}

 in BIRCH. When subtracting 






S
S

N


−


(







L
S

→


N





)



2




{\displaystyle {\frac {SS}{N}}-{\big (}{\frac {\vec {LS}}{N}}{\big )}^{2}}

 or similar in the other distances such as 




D

2




{\displaystyle D_{2}}

, catastrophic cancellation can occur and yield a poor precision, and which can in some cases even cause the result to be negative (and the square root then become undefined).[2] This can be resolved by using BETULA cluster features 



C
F
=
(
N
,
μ
,
S
)


{\displaystyle CF=(N,\mu ,S)}

 instead, which store the count 



N


{\displaystyle N}

, mean 



μ


{\displaystyle \mu }

, and sum of squared deviations instead based on numerically more reliable online algorithms to calculate variance. For these features, a similar additivity theorem holds. When storing a vector respectively a matrix for the squared deviations, the resulting BIRCH CF-tree can also be used to accelerate Gaussian Mixture Modeling with the expectation–maximization algorithm, besides k-means clustering and hierarchical agglomerative clustering.
 Instead of storing the linear sum and the sum of squares, we can instead store the mean and the squared deviation from the mean in each cluster feature 



C

F
′

=
(
N
,
μ
,
S
)


{\displaystyle CF'=(N,\mu ,S)}

,[4] where
 The main difference here is that S is computed relative to the center, instead of relative to the origin.
 A single point 



x


{\displaystyle x}

 can be cast into a cluster feature 



C

F

x


=
(
1
,
x
,
0
)


{\displaystyle CF_{x}=(1,x,0)}

. In order to combine two cluster features 



C

F

A
B


=
C

F

A


+
C

F

B




{\displaystyle CF_{AB}=CF_{A}+CF_{B}}

, we use
 These computations use numerically more reliable computations (c.f. online computation of the variance) that avoid the subtraction of two similar squared values. The centroid is simply the node center vector 



μ


{\displaystyle \mu }

, and can directly be used for distance computations using, e.g., the Euclidean or Manhattan distances. The radius simplifies to 



R
=




1
N


S




{\displaystyle R={\sqrt {{\frac {1}{N}}S}}}

 and the diameter to 



D
=




2

N
−
1



S




{\displaystyle D={\sqrt {{\frac {2}{N-1}}S}}}

.
 We can now compute the different distances D0 to D4 used in the BIRCH algorithm as:[4]
 These distances can also be used to initialize the distance matrix for hierarchical clustering, depending on the chosen linkage. For accurate hierarchical clustering and k-means clustering, we also need to use the node weight 



N


{\displaystyle N}

.
 The CF-tree provides a compressed summary of the data set, but the leaves themselves only provide a very poor data clustering.
In a second step, the leaves can be clustered using, e.g.,
",birch balanc iter reduc cluster use hierarchi unsupervis data mine algorithm use perform hierarch cluster particularli larg modif also use acceler cluster gaussian mixtur model algorithm advantag birch abil increment dynam cluster incom metric data point attempt produc best qualiti cluster given set resourc memori time constraint case birch requir singl scan databas inventor claim birch first cluster algorithm propos databas area handl data point part underli pattern effect beat dbscan two month birch algorithm receiv sigmod year test time award previou cluster algorithm perform less effect larg databas adequ consid case wherein larg fit main memori result lot overhead maintain high cluster qualiti minim cost addit io oper furthermor birch predecessor inspect data point current exist cluster equal decis perform heurist weight base distanc data point local cluster decis made without scan data point current exist cluster exploit observ data space usual uniformli occupi everi data point equal import make full use avail memori deriv finest possibl minim cost also increment method requir whole data set advanc birch algorithm take input set n data point repres vector desir number cluster oper four phase second option first phase build cluster featur c f cf tree data point tree data structur defin follow second step algorithm scan leaf entri initi c f cf tree rebuild smaller c f cf tree remov outlier group crowd subclust larger one step mark option origin present birch step three exist cluster algorithm use cluster leaf entri agglom hierarch cluster algorithm appli directli subclust repres c f cf vector also provid flexibl allow user specifi either desir number cluster desir diamet threshold cluster step set cluster obtain captur major distribut pattern data howev might exist minor local inaccuraci handl option step step centroid cluster produc step use seed redistribut data point closest seed obtain new set cluster step also provid us option discard outlier point far closest seed treat outlier given cluster featur c f n l n ls ss measur calcul without knowledg underli actual valu multidimension case squar root replac suitabl norm birch use distanc find nearest leaf radiu r diamet decid whether absorb data exist leaf whether add new leaf unfortun numer issu associ use term ss birch subtract n l n ss n ls n similar distanc catastroph cancel occur yield poor precis case even caus result neg squar root becom undefin resolv use betula cluster featur c f n μ n instead store count n n mean μ sum squar deviat instead base numer reliabl onlin algorithm calcul varianc featur similar addit theorem hold store vector respect matrix squar deviat result birch also use acceler gaussian mixtur model algorithm besid cluster hierarch agglom cluster instead store linear sum sum squar instead store mean squar deviat mean cluster featur c f n μ n main differ comput rel center instead rel origin singl point x x cast cluster featur c f x x x order combin two cluster featur c f b c f c f b ab b use comput use numer reliabl comput onlin comput varianc avoid subtract two similar squar valu centroid simpli node center vector μ directli use distanc comput use euclidean manhattan distanc radiu simplifi r n n diamet n comput differ distanc use birch algorithm distanc also use initi distanc matrix hierarch cluster depend chosen linkag accur hierarch cluster cluster also need use node weight n n provid compress summari data set leav provid poor data cluster second step leav cluster use
CURE algorithm,https://en.wikipedia.org/wiki/CURE_algorithm,"CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases[citation needed]. Compared with K-means clustering it is more robust to outliers and able to identify clusters having non-spherical shapes and size variances.
 The popular K-means clustering algorithm minimizes the sum of squared errors criterion:
 Given large differences in sizes or geometries of different clusters, the square error method could split the large clusters to minimize the square error, which is not always correct. Also, with hierarchic clustering algorithms these problems exist as none of the distance measures between clusters (




d

m
i
n


,

d

m
e
a
n




{\displaystyle d_{min},d_{mean}}

) tend to work with different cluster shapes.  Also the running time is high when n is large.
 The problem with the BIRCH algorithm is that once the clusters are generated after step 3, it uses centroids of the clusters and assigns each data point to the cluster with the closest centroid.[citation needed] Using only the centroid to redistribute the data has problems when clusters lack uniform sizes and shapes.
 To avoid the problems with non-uniform sized or shaped clusters, CURE employs a hierarchical clustering algorithm that adopts a middle ground between the centroid based and all point extremes. In CURE, a constant number c of well scattered points of a cluster are chosen and they are shrunk towards the centroid of the cluster by a fraction α. The scattered points after shrinking are used as representatives of the cluster. The clusters with the closest pair of representatives are the clusters that are merged at each step of CURE's hierarchical clustering algorithm. This enables CURE to correctly identify the clusters and makes it less sensitive to outliers.
 Running time is O(n2 log n), making it rather expensive, and space complexity is O(n).
 The algorithm cannot be directly applied to large databases because of the high runtime complexity. Enhancements address this requirement.
 CURE (no. of points,k)
 Input : A set of points S
 Output : k clusters
",cure cluster use repres effici data cluster algorithm larg databas citat need compar cluster robust outlier abl identifi cluster shape size varianc popular cluster algorithm minim sum squar error criterion given larg differ size geometri differ cluster squar error method could split larg cluster minim squar error alway correct also hierarch cluster algorithm problem exist none distanc measur cluster n e n min mean tend work differ cluster shape also run time high n larg problem birch algorithm cluster gener step use centroid cluster assign data point cluster closest centroid citat need use centroid redistribut data problem cluster lack uniform size shape avoid problem size shape cluster cure employ hierarch cluster algorithm adopt middl ground centroid base point extrem cure constant number c well scatter point cluster chosen shrunk toward centroid cluster fraction scatter point shrink use repres cluster cluster closest pair repres cluster merg step cure hierarch cluster algorithm enabl cure correctli identifi cluster make less sensit outlier run time log n make rather expens space complex n algorithm directli appli larg databas high runtim complex enhanc address requir cure point k input set point output k cluster
Hierarchical clustering,https://en.wikipedia.org/wiki/Hierarchical_clustering,"In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis that seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two categories:
 In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering[1] are usually presented in a dendrogram.
 Hierarchical clustering has the distinct advantage that any valid measure of distance can be used. In fact, the observations themselves are not required: all that is used is a matrix of distances. On the other hand, except for the special case of single-linkage distance, none of the algorithms (except exhaustive search in 





O


(

2

n


)


{\displaystyle {\mathcal {O}}(2^{n})}

) can be guaranteed to find the optimum solution.[citation needed]
 The standard algorithm for hierarchical agglomerative clustering (HAC) has a time complexity of 





O


(

n

3


)


{\displaystyle {\mathcal {O}}(n^{3})}

 and requires 



Ω
(

n

2


)


{\displaystyle \Omega (n^{2})}

 memory, which makes it too slow for even medium data sets. However, for some special cases, optimal efficient agglomerative methods (of complexity 





O


(

n

2


)


{\displaystyle {\mathcal {O}}(n^{2})}

) are known: SLINK[2] for single-linkage and CLINK[3] for complete-linkage clustering. With a heap, the runtime of the general case can be reduced to 





O


(

n

2


log
⁡
n
)


{\displaystyle {\mathcal {O}}(n^{2}\log n)}

, an improvement on the aforementioned bound of 





O


(

n

3


)


{\displaystyle {\mathcal {O}}(n^{3})}

, at the cost of further increasing the memory requirements. In many cases, the memory overheads of this approach are too large to make it practically usable. Methods exist which use quadtrees that demonstrate 





O


(

n

2


)


{\displaystyle {\mathcal {O}}(n^{2})}

 total running time with 





O


(
n
)


{\displaystyle {\mathcal {O}}(n)}

 space.[4]
 Divisive clustering with an exhaustive search is 





O


(

2

n


)


{\displaystyle {\mathcal {O}}(2^{n})}

, but it is common to use faster heuristics to choose splits, such as k-means.
 In order to decide which clusters should be combined (for agglomerative), or where a cluster should be split (for divisive), a measure of dissimilarity between sets of observations is required. In most methods of hierarchical clustering, this is achieved by use of an appropriate distance d, such as the Euclidean distance, between single observations of the data set, and a linkage criterion, which specifies the dissimilarity of sets as a function of the pairwise distances of observations in the sets. The choice of metric as well as linkage can have a major impact on the result of the clustering, where the lower level metric determines which objects are most similar, whereas the linkage criterion influences the shape of the clusters. For example, complete-linkage tends to produce more spherical clusters than single-linkage.
 The linkage criterion determines the distance between sets of observations as a function of the pairwise distances between observations.
 Some commonly used linkage criteria between two sets of observations A and B and a distance d are:[5][6]
 Some of these can only be recomputed recursively (WPGMA, WPGMC), for many a recursive computation with Lance-Williams-equations is more efficient, while for other (Hausdorff, Medoid) the distances have to be computed with the slower full formula. Other linkage criteria include:
 For example, suppose this data is to be clustered, and the Euclidean distance is the distance metric.
 The hierarchical clustering dendrogram would be:
 Cutting the tree at a given height will give a partitioning clustering at a selected precision. In this example, cutting after the second row (from the top) of the dendrogram will yield clusters {a} {b c} {d e} {f}. Cutting after the third row will yield clusters {a} {b c} {d e f}, which is a coarser clustering, with a smaller number but larger clusters.
 This method builds the hierarchy from the individual elements by progressively merging clusters. In our example, we have six elements {a} {b} {c} {d} {e} and {f}. The first step is to determine which elements to merge in a cluster. Usually, we want to take the two closest elements, according to the chosen distance.
 Optionally, one can also construct a distance matrix at this stage, where the number in the i-th row j-th column is the distance between the i-th and j-th elements. Then, as clustering progresses, rows and columns are merged as the clusters are merged and the distances updated. This is a common way to implement this type of clustering, and has the benefit of caching distances between clusters. A simple agglomerative clustering algorithm is described in the single-linkage clustering page; it can easily be adapted to different types of linkage (see below).
 Suppose we have merged the two closest elements b and c, we now have the following clusters {a}, {b, c}, {d}, {e} and {f}, and want to merge them further. To do that, we need to take the distance between {a} and {b c}, and therefore define the distance between two clusters.
Usually the distance between two clusters 





A




{\displaystyle {\mathcal {A}}}

 and 





B




{\displaystyle {\mathcal {B}}}

 is one of the following:
 In case of tied minimum distances, a pair is randomly chosen, thus being able to generate several structurally different dendrograms. Alternatively, all tied pairs may be joined at the same time, generating a unique dendrogram.[18]
 One can always decide to stop clustering when there is a sufficiently small number of clusters (number criterion). Some linkages may also guarantee that agglomeration occurs at a greater distance between clusters than the previous agglomeration, and then one can stop clustering when the clusters are too far apart to be merged (distance criterion). However, this is not the case of, e.g., the centroid linkage where the so-called reversals[19] (inversions, departures from ultrametricity) may occur.
 The basic principle of divisive clustering was published as the DIANA (DIvisive ANAlysis clustering) algorithm.[20] Initially, all data is in the same cluster, and the largest cluster is split until every object is separate.
Because there exist 



O
(

2

n


)


{\displaystyle O(2^{n})}

 ways of splitting each cluster, heuristics are needed. DIANA chooses the object with the maximum average dissimilarity and then moves all objects to this cluster that are more similar to the new cluster than to the remainder.
 Informally, DIANA is not so much a process of ""dividing"" as it is of ""hollowing out"": each iteration, an existing cluster (e.g. the initial cluster of the entire dataset) is chosen to form a new cluster inside of it. Objects progressively move to this nested cluster, and hollow out the existing cluster. Eventually, all that's left inside a cluster is nested clusters that grew there, without it owning any loose objects by itself.
 Formally, DIANA operates in the following steps:
 Intuitively, 



D
(
i
)


{\displaystyle D(i)}

 above measures how strongly an object wants to leave its current cluster, but it is attenuated when the object wouldn't fit in the splinter group either. Such objects will likely start their own splinter group eventually.
 The dendrogram of DIANA can be constructed by letting the splinter group 




C


new





{\displaystyle C_{\textrm {new}}}

 be a child of the hollowed-out cluster 




C

∗




{\displaystyle C_{*}}

 each time. This constructs a tree with 




C

0




{\displaystyle C_{0}}

 as its root and 



n


{\displaystyle n}

 unique single-object clusters as its leaves.
",data mine statist hierarch cluster also call hierarch cluster analysi hca method cluster analysi seek build hierarchi cluster strategi hierarch cluster gener fall two categori gener merg split determin greedi manner result hierarch cluster usual present dendrogram hierarch cluster distinct advantag valid measur distanc use fact observ requir use matrix distanc hand except special case distanc none algorithm except exhaust search n n guarante find optimum solut citat need standard algorithm hierarch agglom cluster hac time complex n requir ω n memori make slow even medium data set howev special case optim effici agglom method complex n known slink clink cluster heap runtim gener case reduc n log n n improv aforement bound n cost increas memori requir mani case memori overhead approach larg make practic usabl method exist use quadtre demonstr n total run time n n space divis cluster exhaust search n n common use faster heurist choos split order decid cluster combin agglom cluster split divis measur dissimilar set observ requir method hierarch cluster achiev use appropri distanc euclidean distanc singl observ data set linkag criterion specifi dissimilar set function pairwis distanc observ set choic metric well linkag major impact result cluster lower level metric determin object similar wherea linkag criterion influenc shape cluster exampl tend produc spheric cluster linkag criterion determin distanc set observ function pairwis distanc observ commonli use linkag criteria two set observ b distanc recomput recurs wpgma wpgmc mani recurs comput effici hausdorff medoid distanc comput slower full formula linkag criteria includ exampl suppos data cluster euclidean distanc distanc metric hierarch cluster dendrogram would cut tree given height give partit cluster select precis exampl cut second row top dendrogram yield cluster b c e f cut third row yield cluster b c e f coarser cluster smaller number larger cluster method build hierarchi individu element progress merg cluster exampl six element b c e f first step determin element merg cluster usual want take two closest element accord chosen distanc option one also construct distanc matrix stage number row column distanc element cluster progress row column merg cluster merg distanc updat common way implement type cluster benefit cach distanc cluster simpl agglom cluster algorithm describ cluster page easili adapt differ type linkag see suppos merg two closest element b c follow cluster b c e f want merg need take distanc b c therefor defin distanc two cluster usual distanc two cluster b b one follow case tie minimum distanc pair randomli chosen thu abl gener sever structur differ dendrogram altern tie pair may join time gener uniqu dendrogram one alway decid stop cluster suffici small number cluster number criterion linkag may also guarante agglomer occur greater distanc cluster previou agglomer one stop cluster cluster far apart merg distanc criterion howev case centroid linkag revers invers departur ultrametr may occur basic principl divis cluster publish diana divis analysi cluster algorithm initi data cluster largest cluster split everi object separ exist n n way split cluster heurist need diana choos object maximum averag dissimilar move object cluster similar new cluster remaind inform diana much process divid hollow iter exist cluster initi cluster entir dataset chosen form new cluster insid object progress move nest cluster hollow exist cluster eventu left insid cluster nest cluster grew without own loos object formal diana oper follow step intuit measur strongli object want leav current cluster attenu object would fit splinter group either object like start splinter group eventu dendrogram diana construct let splinter group c new new child cluster c time construct tree c root n n uniqu cluster leav
k-means clustering,https://en.wikipedia.org/wiki/K-means_clustering,"k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.
 The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation–maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.
 The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.
 Given a set of observations (x1, x2, ..., xn), where each observation is a 



d


{\displaystyle d}

-dimensional real vector, k-means clustering aims to partition the n observations into k (≤ n) sets S = {S1, S2, ..., Sk} so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find:







a
r
g

m
i
n




S



⁡

∑

i
=
1


k



∑


x

∈

S

i






‖


x

−


μ


i



‖


2


=



a
r
g

m
i
n




S



⁡

∑

i
=
1


k



|


S

i



|

Var
⁡

S

i




{\displaystyle \mathop {\operatorname {arg\,min} } _{\mathbf {S} }\sum _{i=1}^{k}\sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}=\mathop {\operatorname {arg\,min} } _{\mathbf {S} }\sum _{i=1}^{k}|S_{i}|\operatorname {Var} S_{i}}


where μi is the mean (also called centroid) of points in 




S

i




{\displaystyle S_{i}}

, i.e.






μ

i



=


1


|


S

i



|





∑


x

∈

S

i





x

,


{\displaystyle {\boldsymbol {\mu _{i}}}={\frac {1}{|S_{i}|}}\sum _{\mathbf {x} \in S_{i}}\mathbf {x} ,}







|


S

i



|



{\displaystyle |S_{i}|}

 is the size of 




S

i




{\displaystyle S_{i}}

, and 



‖
⋅
‖


{\displaystyle \|\cdot \|}

 is the usual L2 norm . This is equivalent to minimizing the pairwise squared deviations of points in the same cluster:







a
r
g

m
i
n




S



⁡

∑

i
=
1


k





1


|


S

i



|






∑


x

,

y

∈

S

i






‖


x

−

y


‖


2




{\displaystyle \mathop {\operatorname {arg\,min} } _{\mathbf {S} }\sum _{i=1}^{k}\,{\frac {1}{|S_{i}|}}\,\sum _{\mathbf {x} ,\mathbf {y} \in S_{i}}\left\|\mathbf {x} -\mathbf {y} \right\|^{2}}


The equivalence can be deduced from identity 




|


S

i



|


∑


x

∈

S

i






‖


x

−


μ


i



‖


2


=


1
2



∑


x

,

y

∈

S

i






‖


x

−

y


‖


2




{\textstyle |S_{i}|\sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}={\frac {1}{2}}\sum _{\mathbf {x} ,\mathbf {y} \in S_{i}}\left\|\mathbf {x} -\mathbf {y} \right\|^{2}}

. Since the total variance is constant, this is equivalent to maximizing the sum of squared deviations between points in different clusters (between-cluster sum of squares, BCSS).[1] This deterministic relationship is also related to the law of total variance in probability theory.
 The term ""k-means"" was first used by James MacQueen in 1967,[2] though the idea goes back to Hugo Steinhaus in 1956.[3] The standard algorithm was first proposed by Stuart Lloyd of Bell Labs in 1957 as a technique for pulse-code modulation, although it was not published as a journal article until 1982.[4] In 1965, Edward W. Forgy published essentially the same method, which is why it is sometimes referred to as the Lloyd–Forgy algorithm.[5]
 The most common algorithm uses an iterative refinement technique. Due to its ubiquity, it is often called ""the k-means algorithm""; it is also referred to as Lloyd's algorithm, particularly in the computer science community. It is sometimes also referred to as ""naïve k-means"", because there exist much faster alternatives.[6]
 Given an initial set of k means m1(1), ..., mk(1) (see below), the algorithm proceeds by alternating between two steps:[7]
 The objective function in k-means is the WCSS (within cluster sum of squares). After each iteration, the WCSS decreases and so we have a nonnegative monotonically decreasing sequence. This guarantees that the k-means always converges, but not necessarily to the global optimum.
 The algorithm has converged when the assignments no longer change or equivalently, when the WCSS has become stable. The algorithm is not guaranteed to find the optimum.[9]
 The algorithm is often presented as assigning objects to the nearest cluster by distance. Using a different distance function other than (squared) Euclidean distance may prevent the algorithm from converging. Various modifications of k-means such as spherical k-means and k-medoids have been proposed to allow using other distance measures.
 The below pseudocode outlines the implementation of the standard k-means clustering algorithm. Initialization of centroids, distance metric between points and centroids, and the calculation of new centroids are design choices and will vary with different implementations. In this example pseudocode, argmin is used to find the index of the minimum value.
 Commonly used initialization methods are Forgy and Random Partition.[10] The Forgy method randomly chooses k observations from the dataset and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster's randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al.,[10] the Random Partition method is generally preferable for algorithms such as the k-harmonic means and fuzzy k-means. For expectation maximization and standard k-means algorithms, the Forgy method of initialization is preferable. A comprehensive study by Celebi et al.,[11] however, found that popular initialization methods such as Forgy, Random Partition, and Maximin often perform poorly, whereas Bradley and Fayyad's approach[12] performs ""consistently"" in ""the best group"" and k-means++ performs ""generally well"".
 The algorithm does not guarantee convergence to the global optimum. The result may depend on the initial clusters. As the algorithm is usually fast, it is common to run it multiple times with different starting conditions. However, worst-case performance can be slow: in particular certain point sets, even in two dimensions, converge in exponential time, that is 2Ω(n).[13] These point sets do not seem to arise in practice: this is corroborated by the fact that the smoothed running time of k-means is polynomial.[14]
 The ""assignment"" step is referred to as the ""expectation step"", while the ""update step"" is a maximization step, making this algorithm a variant of the generalized expectation–maximization algorithm.
 Finding the optimal solution to the k-means clustering problem for observations in d dimensions is:
 Thus, a variety of heuristic algorithms such as Lloyd's algorithm given above are generally used.
 The running time of Lloyd's algorithm (and most variants) is 



O
(
n
k
d
i
)


{\displaystyle O(nkdi)}

,[9][19] where:
 On data that does have a clustering structure, the number of iterations until convergence is often small, and results only improve slightly after the first dozen iterations. Lloyd's algorithm is therefore often considered to be of ""linear"" complexity in practice, although it is in the worst case superpolynomial when performed until convergence.[20]
 Lloyd's algorithm is the standard approach for this problem. However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the naïve implementation very inefficient. Some implementations use caching and the triangle inequality in order to create bounds and accelerate Lloyd's algorithm.[9][22][23][24][25]
 Finding the optimal number of clusters (k) for k-means clustering is a crucial step to ensure that the clustering results are meaningful and useful.[26] Several techniques are available to determine a suitable number of clusters. Here are some of commonly used methods:
 Hartigan and Wong's method[9] provides a variation of k-means algorithm which progresses towards a local minimum of the minimum sum-of-squares problem with different solution updates. The method is a local search that iteratively attempts to relocate a sample into a different cluster as long as this process improves the objective function. When no sample can be relocated into a different cluster with an improvement of the objective, the method stops (in a local minimum). In a similar way as the classical k-means, the approach remains a heuristic since it does not necessarily guarantee that the final solution is globally optimum.
 Let 



φ
(

S

j


)


{\displaystyle \varphi (S_{j})}

 be the individual cost of 




S

j




{\displaystyle S_{j}}

 defined by 




∑

x
∈

S

j




(
x
−

μ

j



)

2




{\textstyle \sum _{x\in S_{j}}(x-\mu _{j})^{2}}

, with 




μ

j




{\displaystyle \mu _{j}}

 the center of the cluster.
 Different move acceptance strategies can be used. In a first-improvement strategy, any improving relocation can be applied, whereas in a best-improvement strategy, all possible relocations are iteratively tested and only the best is applied at each iteration. The former approach favors speed, whether the latter approach generally favors solution quality at the expense of additional computational time. The function 



Δ


{\displaystyle \Delta }

 used to calculate the result of a relocation can also be efficiently evaluated by using equality[44]
 



Δ
(
x
,
n
,
m
)
=



∣

S

n


∣


∣

S

n


∣
−
1



⋅
‖

μ

n


−
x

‖

2


−



∣

S

m


∣


∣

S

m


∣
+
1



⋅
‖

μ

m


−
x

‖

2


.


{\displaystyle \Delta (x,n,m)={\frac {\mid S_{n}\mid }{\mid S_{n}\mid -1}}\cdot \lVert \mu _{n}-x\rVert ^{2}-{\frac {\mid S_{m}\mid }{\mid S_{m}\mid +1}}\cdot \lVert \mu _{m}-x\rVert ^{2}.}


 The classical k-means algorithm and its variations are known to only converge to local minima of the minimum-sum-of-squares clustering problem defined as







a
r
g

m
i
n




S



⁡

∑

i
=
1


k



∑


x

∈

S

i






‖


x

−


μ


i



‖


2


.


{\displaystyle \mathop {\operatorname {arg\,min} } _{\mathbf {S} }\sum _{i=1}^{k}\sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}.}


Many studies have attempted to improve the convergence behavior of the algorithm and maximize the chances of attaining the global optimum (or at least, local minima of better quality). Initialization and restart techniques discussed in the previous sections are one alternative to find better solutions. More recently, global optimization algorithms based on branch-and-bound and semidefinite programming have produced ‘’provenly optimal’’ solutions for datasets with up to 4,177 entities and 20,531 features.[45] As expected, due to the NP-hardness of the subjacent optimization problem, the computational time of optimal algorithms for k-means quickly increases beyond this size. Optimal solutions for small- and medium-scale still remain valuable as a benchmark tool, to evaluate the quality of other heuristics. To find high-quality local minima within a controlled computational time but without optimality guarantees, other works have explored metaheuristics and other global optimization techniques, e.g., based on incremental approaches and convex optimization,[46] random swaps[47] (i.e., iterated local search), variable neighborhood search[48] and genetic algorithms.[49][50] It is indeed known that finding better local minima of the minimum sum-of-squares clustering problem can make the difference between failure and success to recover cluster structures in feature spaces of high dimension.[50]
 Three key features of k-means that make it efficient are often regarded as its biggest drawbacks:
 A key limitation of k-means is its cluster model. The concept is based on spherical clusters that are separable so that the mean converges towards the cluster center. The clusters are expected to be of similar size, so that the assignment to the nearest cluster center is the correct assignment. When for example applying k-means with a value of 



k
=
3


{\displaystyle k=3}

 onto the well-known Iris flower data set, the result often fails to separate the three Iris species contained in the data set. With 



k
=
2


{\displaystyle k=2}

, the two visible clusters (one containing two species) will be discovered, whereas with 



k
=
3


{\displaystyle k=3}

 one of the two clusters will be split into two even parts. In fact, 



k
=
2


{\displaystyle k=2}

 is more appropriate for this data set, despite the data set's containing 3 classes. As with any other clustering algorithm, the k-means result makes assumptions that the data satisfy certain criteria. It works well on some data sets, and fails on others.
 The result of k-means can be seen as the Voronoi cells of the cluster means. Since data is split halfway between cluster means, this can lead to suboptimal splits as can be seen in the ""mouse"" example. The Gaussian models used by the expectation–maximization algorithm (arguably a generalization of k-means) are more flexible by having both variances and covariances. The EM result is thus able to accommodate clusters of variable size much better than k-means as well as correlated clusters (not in this example). In counterpart, EM requires the optimization of a larger number of free parameters and poses some methodological issues due to vanishing clusters or badly-conditioned covariance matrices. k-means is closely related to nonparametric Bayesian modeling.[52]
 k-means clustering is rather easy to apply to even large data sets, particularly when using heuristics such as Lloyd's algorithm. It has been successfully used in market segmentation, computer vision, and astronomy among many other domains. It often is used as a preprocessing step for other algorithms, for example to find a starting configuration.
 Vector quantization, a technique commonly used in signal processing and computer graphics, involves reducing the color palette of an image to a fixed number of colors, known as k. One popular method for achieving vector quantization is through k-means clustering. In this process, k-means is applied to the color space of an image to partition it into k clusters, with each cluster representing a distinct color in the image. This technique is particularly useful in image segmentation tasks, where it helps identify and group similar colors together. Example: In the field of computer graphics, k-means clustering is often employed for color quantization in image compression. By reducing the number of colors used to represent an image, file sizes can be significantly reduced without significant loss of visual quality. For instance, consider an image with millions of colors. By applying k-means clustering with k set to a smaller number, the image can be represented using a more limited color palette, resulting in a compressed version that consumes less storage space and bandwidth. Other uses of vector quantization include non-random sampling, as k-means can easily be used to choose k different but prototypical objects from a large data set for further analysis.
 Cluster analysis, a fundamental task in data mining and machine learning, involves grouping a set of data points into clusters based on their similarity. k-means clustering is a popular algorithm used for partitioning data into k clusters, where each cluster is represented by its centroid.
 However, the pure k-means algorithm is not very flexible, and as such is of limited use (except for when vector quantization as above is actually the desired use case). In particular, the parameter k is known to be hard to choose (as discussed above) when not given by external constraints. Another limitation is that it cannot be used with arbitrary distance functions or on non-numerical data. For these use cases, many other algorithms are superior.
 Example: In marketing, k-means clustering is frequently employed for market segmentation, where customers with similar characteristics or behaviors are grouped together. For instance, a retail company may use k-means clustering to segment its customer base into distinct groups based on factors such as purchasing behavior, demographics, and geographic location. These customer segments can then be targeted with tailored marketing strategies and product offerings to maximize sales and customer satisfaction.
 k-means clustering has been used as a feature learning (or dictionary learning) step, in either (semi-)supervised learning or unsupervised learning.[53] The basic approach is first to train a k-means clustering representation, using the input training data (which need not be labelled). Then, to project any input datum into the new feature space, an ""encoding"" function, such as the thresholded matrix-product of the datum with the centroid locations, computes the distance from the datum to each centroid, or simply an indicator function for the nearest centroid,[53][54] or some smooth transformation of the distance.[55] Alternatively, transforming the sample-cluster distance through a Gaussian RBF, obtains the hidden layer of a radial basis function network.[56]
 This use of k-means has been successfully combined with simple, linear classifiers for semi-supervised learning in NLP (specifically for named-entity recognition)[57] and in computer vision. On an object recognition task, it was found to exhibit comparable performance with more sophisticated feature learning approaches such as autoencoders and restricted Boltzmann machines.[55] However, it generally requires more data, for equivalent performance, because each data point only contributes to one ""feature"".[53]
 Example: In natural language processing (NLP), k-means clustering has been integrated with simple linear classifiers for semi-supervised learning tasks such as named-entity recognition (NER). By first clustering unlabeled text data using k-means, meaningful features can be extracted to improve the performance of NER models. For instance, k-means clustering can be applied to identify clusters of words or phrases that frequently co-occur in the input text, which can then be used as features for training the NER model. This approach has been shown to achieve comparable performance with more complex feature learning techniques such as autoencoders and restricted Boltzmann machines, albeit with a greater requirement for labeled data.
 Recent advancements in the application of k-means clustering include improvements in initialization techniques, such as the use of k-means++ initialization to select initial cluster centroids in a more effective manner. Additionally, researchers have explored the integration of k-means clustering with deep learning methods, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), to enhance the performance of various tasks in computer vision, natural language processing, and other domains.
 The slow ""standard algorithm"" for k-means clustering, and its associated expectation–maximization algorithm, is a special case of a Gaussian mixture model, specifically, the limiting case when fixing all covariances to be diagonal, equal and have infinitesimal small variance.[58]: 850  Instead of small variances, a hard cluster assignment can also be used to show another equivalence of k-means clustering to a special case of ""hard"" Gaussian mixture modelling.[59]: 354, 11.4.2.5  This does not mean that it is efficient to use Gaussian mixture modelling to compute k-means, but just that there is a theoretical relationship, and that Gaussian mixture modelling can be interpreted as a generalization of k-means; on the contrary, it has been suggested to use k-means clustering to find starting points for Gaussian mixture modelling on difficult data.[58]: 849 
 Another generalization of the k-means algorithm is the k-SVD algorithm, which estimates data points as a sparse linear combination of ""codebook vectors"". k-means corresponds to the special case of using a single codebook vector, with a weight of 1.[60]
 The relaxed solution of k-means clustering, specified by the cluster indicators, is given by principal component analysis (PCA).[61][62]  The intuition is that k-means describe spherically shaped (ball-like) clusters. If the data has 2 clusters, the line connecting the two centroids is the best 1-dimensional projection direction, which is also the first PCA direction. Cutting the line at the center of mass separates the clusters (this is the continuous relaxation of the discrete cluster indicator). If the data have three clusters, the 2-dimensional plane spanned by three cluster centroids is the best 2-D projection. This plane is also defined by the first two PCA dimensions. Well-separated clusters are effectively modelled by ball-shaped clusters and thus discovered by k-means. Non-ball-shaped clusters are hard to separate when they are close. For example, two half-moon shaped clusters intertwined in space do not separate well when projected onto PCA subspace. k-means should not be expected to do well on this data.[63] It is straightforward to produce counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.[64]
 Basic mean shift clustering algorithms maintain a set of data points the same size as the input data set. Initially, this set is copied from the input set. All points are then iteratively moved towards the mean of the points surrounding them. By contrast, k-means restricts the set of clusters to k clusters, usually much less than the number of points in the input data set, using the mean of all points in the prior cluster that are closer to that point than any other for the centroid (e.g. within the Voronoi partition of each updating point). A mean shift algorithm that is similar then to k-means, called likelihood mean shift, replaces the set of points undergoing replacement by the mean of all points in the input set that are within a given distance of the changing set.[65] An advantage of mean shift clustering over k-means is the detection of an arbitrary number of clusters in the data set, as there is not a parameter determining the number of clusters. Mean shift can be much slower than k-means, and still requires selection of a bandwidth parameter.
 Under sparsity assumptions and when input data is pre-processed with the whitening transformation, k-means produces the solution to the linear independent component analysis (ICA) task. This aids in explaining the successful application of k-means to feature learning.[66]
 k-means implicitly assumes that the ordering of the input data set does not matter. The bilateral filter is similar to k-means and mean shift in that it maintains a set of data points that are iteratively replaced by means. However, the bilateral filter restricts the calculation of the (kernel weighted) mean to include only points that are close in the ordering of the input data.[65] This makes it applicable to problems such as image denoising, where the spatial arrangement of pixels in an image is of critical importance.
 The set of squared error minimizing cluster functions also includes the k-medoids algorithm, an approach which forces the center point of each cluster to be one of the actual points, i.e., it uses medoids in place of centroids.
 Different implementations of the algorithm exhibit performance differences, with the fastest on a test data set finishing in 10 seconds, the slowest taking 25,988 seconds (~7 hours).[1] The differences can be attributed to implementation quality, language and compiler differences, different termination criteria and precision levels, and the use of indexes for acceleration.
 The following implementations are available under Free/Open Source Software licenses, with publicly available source code.
 The following implementations are available under proprietary license terms, and may not have publicly available source code.
",cluster method vector quantiz origin signal process aim partit n observ k cluster observ belong cluster nearest mean cluster center cluster centroid serv prototyp cluster result partit data space voronoi cell cluster minim varianc squar euclidean distanc regular euclidean distanc would difficult weber problem mean optim squar error wherea geometr median minim euclidean distanc instanc better euclidean solut found use problem comput difficult howev effici heurist algorithm converg quickli local optimum usual similar algorithm mixtur gaussian distribut via iter refin approach employ gaussian mixtur model use cluster center model data howev cluster tend find cluster compar spatial extent gaussian mixtur model allow cluster differ shape unsupervis algorithm loos relationship neighbor classifi popular supervis machin learn techniqu classif often confus due name appli neighbor classifi cluster center obtain classifi new data exist cluster known nearest centroid classifi rocchio algorithm given set observ xn observ real vector cluster aim partit n observ k n set sk minim sum squar wcss varianc formal object find r g n k x x μ r g n k var min k x x min k var μi mean also call centroid point μ x x x x size usual norm equival minim pairwis squar deviat point cluster r g n k x x min k x x equival deduc ident x x μ x x x x x x sinc total varianc constant equival maxim sum squar deviat point differ cluster sum squar bcss determinist relationship also relat law total varianc probabl theori term first use jame macqueen though idea goe back hugo steinhau standard algorithm first propos stuart lloyd bell lab techniqu modul although publish journal articl edward forgi publish essenti method sometim refer algorithm common algorithm use iter refin techniqu due ubiqu often call algorithm also refer lloyd algorithm particularli comput scienc commun sometim also refer naïv exist much faster altern given initi set k mean mk see algorithm proce altern two step object function wcss within cluster sum squar iter wcss decreas nonneg monoton decreas sequenc guarante alway converg necessarili global optimum algorithm converg assign longer chang equival wcss becom stabl algorithm guarante find optimum algorithm often present assign object nearest cluster distanc use differ distanc function squar euclidean distanc may prevent algorithm converg variou modif spheric propos allow use distanc measur pseudocod outlin implement standard cluster algorithm initi centroid distanc metric point centroid calcul new centroid design choic vari differ implement exampl pseudocod argmin use find index minimum valu commonli use initi method forgi random partit forgi method randomli choos k observ dataset use initi mean random partit method first randomli assign cluster observ proce updat step thu comput initi mean centroid cluster randomli assign point forgi method tend spread initi mean random partit place close center data set accord hamerli et random partit method gener prefer algorithm mean fuzzi expect maxim standard algorithm forgi method initi prefer comprehens studi celebi et howev found popular initi method forgi random partit maximin often perform poorli wherea bradley fayyad approach perform consist best group perform gener well algorithm guarante converg global optimum result may depend initi cluster algorithm usual fast common run multipl time differ start condit howev perform slow particular certain point set even two dimens converg exponenti time n point set seem aris practic corrobor fact smooth run time polynomi assign step refer expect step updat step maxim step make algorithm variant gener algorithm find optim solut cluster problem observ dimens thu varieti heurist algorithm lloyd algorithm given gener use run time lloyd algorithm variant n k nkdi data cluster structur number iter converg often small result improv slightli first dozen iter lloyd algorithm therefor often consid linear complex practic although worst case superpolynomi perform converg lloyd algorithm standard approach problem howev spend lot process time comput distanc k cluster center n data point sinc point usual stay cluster iter much work unnecessari make naïv implement ineffici implement use cach triangl inequ order creat bound acceler lloyd algorithm find optim number cluster k cluster crucial step ensur cluster result meaning use sever techniqu avail determin suitabl number cluster commonli use method hartigan wong method provid variat algorithm progress toward local minimum minimum problem differ solut updat method local search iter attempt reloc sampl differ cluster long process improv object function sampl reloc differ cluster improv object method stop local minimum similar way classic approach remain heurist sinc necessarili guarante final solut global optimum let φ j j individu cost j j defin x j x μ j j j μ j j center cluster differ move accept strategi use strategi improv reloc appli wherea strategi possibl reloc iter test best appli iter former approach favor speed whether latter approach gener favor solut qualiti expens addit comput time function δ use calcul result reloc also effici evalu use equal δ x n n n μ n x μ x x n n n n classic algorithm variat known converg local minima cluster problem defin r g n k x x μ min k x x mani studi attempt improv converg behavior algorithm maxim chanc attain global optimum least local minima better qualiti initi restart techniqu discuss previou section one altern find better solut recent global optim algorithm base semidefinit program produc provenli optim solut dataset entiti featur expect due subjac optim problem comput time optim algorithm quickli increas beyond size optim solut still remain valuabl benchmark tool evalu qualiti heurist find local minima within control comput time without optim guarante work explor metaheurist global optim techniqu base increment approach convex optim random swap iter local search variabl neighborhood search genet algorithm inde known find better local minima minimum cluster problem make differ failur success recov cluster structur featur space high dimens three key featur make effici often regard biggest drawback key limit cluster model concept base spheric cluster separ mean converg toward cluster center cluster expect similar size assign nearest cluster center correct assign exampl appli valu k onto iri flower data set result often fail separ three iri speci contain data set k two visibl cluster one contain two speci discov wherea k one two cluster split two even part fact k appropri data set despit data set contain class cluster algorithm result make assumpt data satisfi certain criteria work well data set fail other result seen voronoi cell cluster mean sinc data split halfway cluster mean lead suboptim split seen mous exampl gaussian model use algorithm arguabl gener flexibl varianc covari em result thu abl accommod cluster variabl size much better well correl cluster exampl counterpart em requir optim larger number free paramet pose methodolog issu due vanish cluster covari matric close relat nonparametr bayesian model cluster rather easi appli even larg data set particularli use heurist lloyd algorithm success use market segment comput vision astronomi among mani domain often use preprocess step algorithm exampl find start configur vector quantiz techniqu commonli use signal process comput graphic involv reduc color palett imag fix number color known one popular method achiev vector quantiz cluster process appli color space imag partit k cluster cluster repres distinct color imag techniqu particularli use imag segment task help identifi group similar color togeth exampl field comput graphic cluster often employ color quantiz imag compress reduc number color use repres imag file size significantli reduc without signific loss visual qualiti instanc consid imag million color appli cluster k set smaller number imag repres use limit color palett result compress version consum less storag space bandwidth use vector quantiz includ sampl easili use choos k differ prototyp object larg data set analysi cluster analysi fundament task data mine machin learn involv group set data point cluster base similar cluster popular algorithm use partit data k cluster cluster repres centroid howev pure algorithm flexibl limit use except vector quantiz actual desir use case particular paramet k known hard choos discuss given extern constraint anoth limit use arbitrari distanc function data use case mani algorithm superior exampl market cluster frequent employ market segment custom similar characterist behavior group togeth instanc retail compani may use cluster segment custom base distinct group base factor purchas behavior demograph geograph locat custom segment target tailor market strategi product offer maxim sale custom satisfact cluster use featur learn dictionari learn step either supervis learn unsupervis learn basic approach first train cluster represent use input train data need label project input datum new featur space encod function threshold datum centroid locat comput distanc datum centroid simpli indic function nearest centroid smooth transform distanc altern transform distanc gaussian rbf obtain hidden layer radial basi function network use success combin simpl linear classifi learn nlp specif recognit comput vision object recognit task found exhibit compar perform sophist featur learn approach autoencod restrict boltzmann machin howev gener requir data equival perform data point contribut one featur exampl natur languag process nlp cluster integr simpl linear classifi learn task recognit ner first cluster unlabel text data use meaning featur extract improv perform ner model instanc cluster appli identifi cluster word phrase frequent input text use featur train ner model approach shown achiev compar perform complex featur learn techniqu autoencod restrict boltzmann machin albeit greater requir label data recent advanc applic cluster includ improv initi techniqu use initi select initi cluster centroid effect manner addit research explor integr cluster deep learn method convolut neural network cnn recurr neural network rnn enhanc perform variou task comput vision natur languag process domain slow standard algorithm cluster associ algorithm special case gaussian mixtur model specif limit case fix covari diagon equal infinitesim small varianc instead small varianc hard cluster assign also use show anoth equival cluster special case hard gaussian mixtur model mean effici use gaussian mixtur model comput theoret relationship gaussian mixtur model interpret gener contrari suggest use cluster find start point gaussian mixtur model difficult data anoth gener algorithm algorithm estim data point spars linear combin codebook vector correspond special case use singl codebook vector weight relax solut cluster specifi cluster indic given princip compon analysi pca intuit describ spheric shape cluster data cluster line connect two centroid best project direct also first pca direct cut line center mass separ cluster continu relax discret cluster indic data three cluster plane span three cluster centroid best project plane also defin first two pca dimens cluster effect model cluster thu discov cluster hard separ close exampl two shape cluster intertwin space separ well project onto pca subspac expect well data straightforward produc counterexampl statement cluster centroid subspac span princip direct basic mean shift cluster algorithm maintain set data point size input data set initi set copi input set point iter move toward mean point surround contrast restrict set cluster k cluster usual much less number point input data set use mean point prior cluster closer point centroid within voronoi partit updat point mean shift algorithm similar call likelihood mean shift replac set point undergo replac mean point input set within given distanc chang set advantag mean shift cluster detect arbitrari number cluster data set paramet determin number cluster mean shift much slower still requir select bandwidth paramet sparsiti assumpt input data whiten transform produc solut linear independ compon analysi ica task aid explain success applic featur learn implicitli assum order input data set matter bilater filter similar mean shift maintain set data point iter replac mean howev bilater filter restrict calcul kernel weight mean includ point close order input data make applic problem imag denois spatial arrang pixel imag critic import set squar error minim cluster function also includ algorithm approach forc center point cluster one actual point use medoid place centroid differ implement algorithm exhibit perform differ fastest test data set finish second slowest take second hour differ attribut implement qualiti languag compil differ differ termin criteria precis level use index acceler follow implement avail sourc softwar licens publicli avail sourc code follow implement avail proprietari licens term may publicli avail sourc code
Fuzzy clustering,https://en.wikipedia.org/wiki/Fuzzy_clustering,"Fuzzy clustering (also referred to as soft clustering or soft k-means) is a form of clustering in which each data point can belong to more than one cluster.
 Clustering or cluster analysis involves assigning data points to clusters such that items in the same cluster are as similar as possible, while items belonging to different clusters are as dissimilar as possible. Clusters are identified via similarity measures. These similarity measures include distance, connectivity, and intensity. Different similarity measures may be chosen based on the data or the application.[1]
 In non-fuzzy clustering (also known as hard clustering), data are divided into distinct clusters, where each data point can only belong to exactly one cluster. In fuzzy clustering, data points can potentially belong to multiple clusters. For example, an apple can be red or green (hard clustering), but an apple can also be red AND green (fuzzy clustering). Here, the apple can be red to a certain degree as well as green to a certain degree. Instead of the apple belonging to green [green = 1] and not red [red = 0], the apple can belong to green [green = 0.5] and red [red = 0.5]. These value are normalized between 0 and 1; however, they do not represent probabilities, so the two values do not need to add up to 1.
 Membership grades are assigned to each of the data points (tags). These membership grades indicate the degree to which data points belong to each cluster. Thus, points on the edge of a cluster, with lower membership grades, may be in the cluster to a lesser degree than points in the center of cluster.
 One of the most widely used fuzzy clustering algorithms is the Fuzzy C-means clustering (FCM) algorithm.
 Fuzzy c-means (FCM) clustering was developed by J.C. Dunn in 1973,[2] and improved by J.C. Bezdek in 1981.[3]
 The fuzzy c-means algorithm is very similar to the k-means algorithm:
 Any point x has a set of coefficients giving the degree of being in the kth cluster wk(x). With fuzzy c-means, the centroid of a cluster is the mean of all points, weighted by their degree of belonging to the cluster, or, mathematically,
 




c

k


=




∑

x





w

k


(
x
)


m


x



∑

x





w

k


(
x
)


m





,


{\displaystyle c_{k}={{\sum _{x}{w_{k}(x)}^{m}x} \over {\sum _{x}{w_{k}(x)}^{m}}},}


 where m is the hyper- parameter that controls how fuzzy the cluster will be. The higher it is, the fuzzier the cluster will be in the end.
 The FCM algorithm attempts to partition a finite collection of 



n


{\displaystyle n}

 elements 




X
=
{


x


1


,
.
.
.
,


x


n


}


{\displaystyle X=\{\mathbf {x} _{1},...,\mathbf {x} _{n}\}}

 into a collection of c fuzzy clusters with respect to some given criterion.
 Given a finite set of data, the algorithm returns a list of  



c


{\displaystyle c}

  cluster centres  



C
=
{


c


1


,
.
.
.
,


c


c


}


{\displaystyle C=\{\mathbf {c} _{1},...,\mathbf {c} _{c}\}}

  and a partition matrix
 



W
=

w

i
,
j


∈
[
0
,
1
]
,

i
=
1
,
.
.
.
,
n
,

j
=
1
,
.
.
.
,
c


{\displaystyle W=w_{i,j}\in [0,1],\;i=1,...,n,\;j=1,...,c}

, where each element, 




w

i
j




{\displaystyle w_{ij}}

 , tells
the degree to which element, 





x


i




{\displaystyle \mathbf {x} _{i}}

, belongs to cluster 





c


j




{\displaystyle \mathbf {c} _{j}}

.
 The FCM aims to minimize an objective function:
 where:
 
 K-means clustering also attempts to minimize the objective function shown above, except that in K-means, the membership values are either zero or one, and cannot take values in between, i.e. 




w

i
j


∈
{
0
,
1
}


{\displaystyle w_{ij}\in \{0,1\}}

. In Fuzzy C-means, the degree of fuzziness is parametrized by 



m
∈
(
1
,
∞
)


{\displaystyle m\in (1,\infty )}

, where a larger 



m


{\displaystyle m}

 results in fuzzier clusters. In the limit 



m
→
1


{\displaystyle m\rightarrow 1}

, the memberships, 




w

i
j




{\displaystyle w_{ij}}

 , converge to 0 or 1, and the Fuzzy C-means objective coincides with that of K-means. In the absence of experimentation or domain knowledge, 



m


{\displaystyle m}

 is commonly set to 2. The algorithm minimizes intra-cluster variance as well, but has the same problems as 'k'-means; the minimum is a local minimum, and the results depend on the initial choice of weights.
 There are several implementations of this algorithm that are publicly available.[4][5]
 Fuzzy C-means (FCM) with automatically determined for the number of clusters could enhance the detection accuracy.[6] Using a mixture of Gaussians along with the expectation-maximization algorithm is a more statistically formalized method which includes some of these ideas: partial membership in classes.
 To better understand this principle, a classic example of mono-dimensional data is given below on an x axis.
 This data set can be traditionally grouped into two clusters. By selecting a threshold on the x-axis, the data is separated into two clusters.  The resulting clusters are labelled 'A' and 'B', as seen in the following image.  Each point belonging to the data set would therefore have a membership coefficient of 1 or 0. This membership coefficient of each corresponding data point is represented by the inclusion of the y-axis.   
 In fuzzy clustering, each data point can have membership to multiple clusters.  By relaxing the definition of membership coefficients from strictly 1 or 0, these values can range from any value from 1 to 0. The following image shows the data set from the previous clustering, but now fuzzy c-means clustering is applied. First, a new threshold value defining two clusters may be generated. Next, new membership coefficients for each data point are generated based on clusters centroids, as well as distance from each cluster centroid.
 As one can see, the middle data point belongs to cluster A and cluster B. the value of 0.3 is this data point's membership coefficient for cluster A .[7]
 Clustering problems have applications in surface science, biology, medicine, psychology, economics, and many other disciplines.[8]
 In the field of bioinformatics, clustering is used for a number of applications. One use is as a pattern recognition technique to analyze gene expression data from RNA-sequencing data or other technologies.[9] In this case, genes with similar expression patterns are grouped into the same cluster, and different clusters display distinct, well-separated patterns of expression. Use of clustering can provide insight into gene function and regulation.[8] Because fuzzy clustering allows genes to belong to more than one cluster, it allows for the identification of genes that are conditionally co-regulated or co-expressed.[10] For example, one gene may be acted on by more than one transcription factor, and one gene may encode a protein that has more than one function. Thus, fuzzy clustering is more appropriate than hard clustering.
 Fuzzy c-means has been a very important tool for image processing in clustering objects in an image. In the 1970s, mathematicians introduced the spatial term into the FCM algorithm to improve the accuracy of clustering under noise.[11] Furthermore, FCM algorithms have been used to distinguish between different activities using image-based features such as the Hu and the Zernike Moments.[12] Alternatively, A fuzzy logic model can be described on fuzzy sets that are defined on three components of the HSL color space HSL and HSV; The membership functions aim to describe colors follow the human intuition of color identification.[13]
 In marketing, customers can be grouped into fuzzy clusters based on their needs, brand choices, psycho-graphic profiles, or other marketing related partitions.[citation needed]
 Image segmentation using k-means clustering algorithms has long been used for pattern recognition, object detection, and medical imaging. However, due to real world limitations such as noise, shadowing, and variations in cameras, traditional hard clustering is often unable to reliably perform image processing tasks as stated above.[citation needed]  Fuzzy clustering has been proposed as a more applicable algorithm in the performance to these tasks.  Given is gray scale image that has undergone fuzzy clustering in Matlab.[14]  The original image is seen next to a clustered image.  Colors are used to give a visual representation of the three distinct clusters used to identify the membership of each pixel. Below, a chart is given that defines the fuzzy membership coefficients of their corresponding intensity values.
 Depending on the application for which the fuzzy clustering coefficients are to be used, different pre-processing techniques can be applied to RGB images.  RGB to HCL conversion is common practice.[15]
",fuzzi cluster also refer soft cluster soft form cluster data point belong one cluster cluster cluster analysi involv assign data point cluster item cluster similar possibl item belong differ cluster dissimilar possibl cluster identifi via similar measur similar measur includ distanc connect intens differ similar measur may chosen base data applic cluster also known hard cluster data divid distinct cluster data point belong exactli one cluster fuzzi cluster data point potenti belong multipl cluster exampl appl red green hard cluster appl also red green fuzzi cluster appl red certain degre well green certain degre instead appl belong green green red red appl belong green green red red valu normal howev repres probabl two valu need add membership grade assign data point tag membership grade indic degre data point belong cluster thu point edg cluster lower membership grade may cluster lesser degre point center cluster one wide use fuzzi cluster algorithm fuzzi cluster fcm algorithm fuzzi fcm cluster develop dunn improv bezdek fuzzi algorithm similar algorithm point x set coeffici give degre kth cluster wk x fuzzi centroid cluster mean point weight degre belong cluster mathemat c k x w k x x x w k x k x k x x x k x paramet control fuzzi cluster higher fuzzier cluster end fcm algorithm attempt partit finit collect n n element x x x n x x n collect c fuzzi cluster respect given criterion given finit set data algorithm return list c c cluster centr c c c c c c c partit matrix w w j n j c j n c element w j ij tell degre element x x belong cluster c j c j fcm aim minim object function cluster also attempt minim object function shown except membership valu either zero one take valu w j ij fuzzi degre fuzzi parametr larger result fuzzier cluster limit membership w j ij converg fuzzi object coincid absenc experiment domain knowledg commonli set algorithm minim varianc well problem minimum local minimum result depend initi choic weight sever implement algorithm publicli avail fuzzi fcm automat determin number cluster could enhanc detect accuraci use mixtur gaussian along algorithm statist formal method includ idea partial membership class better understand principl classic exampl data given x axi data set tradit group two cluster select threshold data separ two cluster result cluster label b seen follow imag point belong data set would therefor membership coeffici membership coeffici correspond data point repres inclus fuzzi cluster data point membership multipl cluster relax definit membership coeffici strictli valu rang valu follow imag show data set previou cluster fuzzi cluster appli first new threshold valu defin two cluster may gener next new membership coeffici data point gener base cluster centroid well distanc cluster centroid one see middl data point belong cluster cluster valu data point membership coeffici cluster cluster problem applic surfac scienc biolog medicin psycholog econom mani disciplin field bioinformat cluster use number applic one use pattern recognit techniqu analyz gene express data data technolog case gene similar express pattern group cluster differ cluster display distinct pattern express use cluster provid insight gene function regul fuzzi cluster allow gene belong one cluster allow identif gene condit exampl one gene may act one transcript factor one gene may encod protein one function thu fuzzi cluster appropri hard cluster fuzzi import tool imag process cluster object imag mathematician introduc spatial term fcm algorithm improv accuraci cluster nois furthermor fcm algorithm use distinguish differ activ use featur hu zernik moment altern fuzzi logic model describ fuzzi set defin three compon hsl color space hsl hsv membership function aim describ color follow human intuit color identif market custom group fuzzi cluster base need brand choic profil market relat partit citat need imag segment use cluster algorithm long use pattern recognit object detect medic imag howev due real world limit nois shadow variat camera tradit hard cluster often unabl reliabl perform imag process task state citat need fuzzi cluster propos applic algorithm perform task given gray scale imag undergon fuzzi cluster matlab origin imag seen next cluster imag color use give visual represent three distinct cluster use identifi membership pixel chart given defin fuzzi membership coeffici correspond intens valu depend applic fuzzi cluster coeffici use differ techniqu appli rgb imag rgb hcl convers common practic
Expectation–maximization algorithm,https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm,"In statistics, an expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables.[1] The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step. It can be used, for example, to estimate a mixture of gaussians, or to solve the multiple linear regression problem.[2]
 The EM algorithm was explained and given its name in a classic 1977 paper by Arthur Dempster, Nan Laird, and Donald Rubin.[3] They pointed out that the method had been ""proposed many times in special circumstances"" by earlier authors. One of the earliest is the gene-counting method for estimating allele frequencies by Cedric Smith.[4] Another was proposed by H.O. Hartley in 1958, and Hartley and Hocking in 1977, from which many of the ideas in the Dempster–Laird–Rubin paper originated.[5] Another one by S.K Ng, Thriyambakam Krishnan and G.J McLachlan in 1977.[6] Hartley’s ideas can be broadened to any grouped discrete distribution. A very detailed treatment of the EM method for exponential families was published by Rolf Sundberg in his thesis and several papers,[7][8][9] following his collaboration with Per Martin-Löf and Anders Martin-Löf.[10][11][12][13][14] The Dempster–Laird–Rubin paper in 1977 generalized the method and sketched a convergence analysis for a wider class of problems. The Dempster–Laird–Rubin paper established the EM method as an important tool of statistical analysis. See also Meng and van Dyk (1997).
 The convergence analysis of the Dempster–Laird–Rubin algorithm was flawed and a correct convergence analysis was published by C. F. Jeff Wu in 1983.[15]
Wu's proof established the EM method's convergence also outside of the exponential family, as claimed by Dempster–Laird–Rubin.[15]
 The EM algorithm is used to find (local) maximum likelihood parameters of a statistical model in cases where the equations cannot be solved directly. Typically these models involve latent variables in addition to unknown parameters and known data observations. That is, either missing values exist among the data, or the model can be formulated more simply by assuming the existence of further unobserved data points. For example, a mixture model can be described more simply by assuming that each observed data point has a corresponding unobserved data point, or latent variable, specifying the mixture component to which each data point belongs.
 Finding a maximum likelihood solution typically requires taking the derivatives of the likelihood function with respect to all the unknown values, the parameters and the latent variables, and simultaneously solving the resulting equations. In statistical models with latent variables, this is usually impossible. Instead, the result is typically a set of interlocking equations in which the solution to the parameters requires the values of the latent variables and vice versa, but substituting one set of equations into the other produces an unsolvable equation.
 The EM algorithm proceeds from the observation that there is a way to solve these two sets of equations numerically. One can simply pick arbitrary values for one of the two sets of unknowns, use them to estimate the second set, then use these new values to find a better estimate of the first set, and then keep alternating between the two until the resulting values both converge to fixed points. It's not obvious that this will work, but it can be proven in this context. Additionally, it can be proven that the derivative of the likelihood is (arbitrarily close to) zero at that point, which in turn means that the point is either a local maximum or a saddle point.[15] In general, multiple maxima may occur, with no guarantee that the global maximum will be found. Some likelihoods also have singularities in them, i.e., nonsensical maxima. For example, one of the solutions that may be found by EM in a mixture model involves setting one of the components to have zero variance and the mean parameter for the same component to be equal to one of the data points. The convergence of expectation-maximization (EM)-based algorithms typically requires continuity of the likelihood function with respect to all the unknown parameters (referred to as optimization variables).[16]
 Given the statistical model which generates a set 




X



{\displaystyle \mathbf {X} }

 of observed data, a set of unobserved latent data or missing values 




Z



{\displaystyle \mathbf {Z} }

, and a vector of unknown parameters 




θ



{\displaystyle {\boldsymbol {\theta }}}

, along with a likelihood function 



L
(

θ

;

X

,

Z

)
=
p
(

X

,

Z

∣

θ

)


{\displaystyle L({\boldsymbol {\theta }};\mathbf {X} ,\mathbf {Z} )=p(\mathbf {X} ,\mathbf {Z} \mid {\boldsymbol {\theta }})}

, the maximum likelihood estimate (MLE) of the unknown parameters is determined by maximizing the marginal likelihood of the observed data
 However, this quantity is often intractable since 




Z



{\displaystyle \mathbf {Z} }

 is unobserved and the distribution of 




Z



{\displaystyle \mathbf {Z} }

 is unknown before attaining 




θ



{\displaystyle {\boldsymbol {\theta }}}

.
 The EM algorithm seeks to find the maximum likelihood estimate of the marginal likelihood by iteratively applying these two steps:
 More succinctly, we can write it as one equation:





θ


(
t
+
1
)


=



a
r
g

m
a
x

θ



E


Z

∼
p
(
⋅

|


X

,


θ


(
t
)


)


⁡

[

log
⁡
p
(

X

,

Z


|


θ

)

]




{\displaystyle {\boldsymbol {\theta }}^{(t+1)}={\underset {\boldsymbol {\theta }}{\operatorname {arg\,max} }}\operatorname {E} _{\mathbf {Z} \sim p(\cdot |\mathbf {X} ,{\boldsymbol {\theta }}^{(t)})}\left[\log p(\mathbf {X} ,\mathbf {Z} |{\boldsymbol {\theta }})\right]\,}


 The typical models to which EM is applied use 




Z



{\displaystyle \mathbf {Z} }

 as a latent variable indicating membership in one of a set of groups:
 However, it is possible to apply EM to other sorts of models.
 The motivation is as follows. If the value of the parameters 




θ



{\displaystyle {\boldsymbol {\theta }}}

 is known, usually the value of the latent variables 




Z



{\displaystyle \mathbf {Z} }

 can be found by maximizing the log-likelihood over all possible values of 




Z



{\displaystyle \mathbf {Z} }

, either simply by iterating over 




Z



{\displaystyle \mathbf {Z} }

 or through an algorithm such as the Viterbi algorithm for hidden Markov models. Conversely, if we know the value of the latent variables 




Z



{\displaystyle \mathbf {Z} }

, we can find an estimate of the parameters 




θ



{\displaystyle {\boldsymbol {\theta }}}

 fairly easily, typically by simply grouping the observed data points according to the value of the associated latent variable and averaging the values, or some function of the values, of the points in each group. This suggests an iterative algorithm, in the case where both 




θ



{\displaystyle {\boldsymbol {\theta }}}

 and 




Z



{\displaystyle \mathbf {Z} }

 are unknown:
 The algorithm as just described monotonically approaches a local minimum of the cost function.
 Although an EM iteration does increase the observed data (i.e., marginal) likelihood function, no guarantee exists that the sequence converges to a maximum likelihood estimator. For multimodal distributions, this means that an EM algorithm may converge to a local maximum of the observed data likelihood function, depending on starting values. A variety of heuristic or metaheuristic approaches exist to escape a local maximum, such as random-restart hill climbing (starting with several different random initial estimates 





θ


(
t
)




{\displaystyle {\boldsymbol {\theta }}^{(t)}}

), or applying simulated annealing methods.
 EM is especially useful when the likelihood is an exponential family, see Sundberg (2019, Ch. 8) for a comprehensive treatment:[17] the E step becomes the sum of expectations of sufficient statistics, and the M step involves maximizing a linear function. In such a case, it is usually possible to derive closed-form expression updates for each step, using the Sundberg formula[18] (proved and published by Rolf Sundberg, based on unpublished results of Per Martin-Löf and Anders Martin-Löf).[8][9][11][12][13][14]
 The EM method was modified to compute maximum a posteriori (MAP) estimates for Bayesian inference in the original paper by Dempster, Laird, and Rubin.
 Other methods exist to find maximum likelihood estimates, such as gradient descent, conjugate gradient, or variants of the Gauss–Newton algorithm. Unlike EM, such methods typically require the evaluation of first and/or second derivatives of the likelihood function.
 Expectation-Maximization works to improve 



Q
(

θ

∣


θ


(
t
)


)


{\displaystyle Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})}

 rather than directly improving 



log
⁡
p
(

X

∣

θ

)


{\displaystyle \log p(\mathbf {X} \mid {\boldsymbol {\theta }})}

. Here it is shown that improvements to the former imply improvements to the latter.[19][20]
 For any 




Z



{\displaystyle \mathbf {Z} }

 with non-zero probability 



p
(

Z

∣

X

,

θ

)


{\displaystyle p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }})}

, we can write
 We take the expectation over possible values of the unknown data 




Z



{\displaystyle \mathbf {Z} }

 under the current parameter estimate 




θ

(
t
)




{\displaystyle \theta ^{(t)}}

 by multiplying both sides by 



p
(

Z

∣

X

,


θ


(
t
)


)


{\displaystyle p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)})}

 and summing (or integrating) over 




Z



{\displaystyle \mathbf {Z} }

. The left-hand side is the expectation of a constant, so we get:
 where 



H
(

θ

∣


θ


(
t
)


)


{\displaystyle H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})}

 is defined by the negated sum it is replacing.
This last equation holds for every value of 




θ



{\displaystyle {\boldsymbol {\theta }}}

 including 




θ

=


θ


(
t
)




{\displaystyle {\boldsymbol {\theta }}={\boldsymbol {\theta }}^{(t)}}

,
 and subtracting this last equation from the previous equation gives
 However, Gibbs' inequality tells us that 



H
(

θ

∣


θ


(
t
)


)
≥
H
(


θ


(
t
)


∣


θ


(
t
)


)


{\displaystyle H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})\geq H({\boldsymbol {\theta }}^{(t)}\mid {\boldsymbol {\theta }}^{(t)})}

, so we can conclude that
 In words, choosing 




θ



{\displaystyle {\boldsymbol {\theta }}}

 to improve 



Q
(

θ

∣


θ


(
t
)


)


{\displaystyle Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})}

 causes 



log
⁡
p
(

X

∣

θ

)


{\displaystyle \log p(\mathbf {X} \mid {\boldsymbol {\theta }})}

 to improve at least as much.
 The EM algorithm can be viewed as two alternating maximization steps, that is, as an example of coordinate descent.[21][22] Consider the function:
 where q is an arbitrary probability distribution over the unobserved data z and H(q) is the entropy of the distribution q. This function can be written as
 where  




p

Z
∣
X


(
⋅
∣
x
;
θ
)


{\displaystyle p_{Z\mid X}(\cdot \mid x;\theta )}

 is the conditional distribution of the unobserved data given the observed data 



x


{\displaystyle x}

 and 




D

K
L




{\displaystyle D_{KL}}

 is the Kullback–Leibler divergence.
 Then the steps in the EM algorithm may be viewed as:
 A Kalman filter is typically used for on-line state estimation and a minimum-variance smoother may be employed for off-line or batch state estimation. However, these minimum-variance solutions require estimates of the state-space model parameters. EM algorithms can be used for solving joint state and parameter estimation problems.
 Filtering and smoothing EM algorithms arise by repeating this two-step procedure:
 Suppose that a Kalman filter or minimum-variance smoother operates on measurements of a single-input-single-output system that possess additive white noise. An updated measurement noise variance estimate can be obtained from the maximum likelihood calculation
 where 







x
^




k




{\displaystyle {\widehat {x}}_{k}}

 are scalar output estimates calculated by a filter or a smoother from N scalar measurements 




z

k




{\displaystyle z_{k}}

. The above update can also be applied to updating a Poisson measurement noise intensity. Similarly, for a first-order auto-regressive process, an updated process noise variance estimate can be calculated by
 where 







x
^




k




{\displaystyle {\widehat {x}}_{k}}

 and 







x
^




k
+
1




{\displaystyle {\widehat {x}}_{k+1}}

 are scalar state estimates calculated by a filter or a smoother. The updated model coefficient estimate is obtained via
 The convergence of parameter estimates such as those above are well studied.[28][29][30][31]
 A number of methods have been proposed to accelerate the sometimes slow convergence of the EM algorithm, such as those using conjugate gradient and modified Newton's methods (Newton–Raphson).[32] Also, EM can be used with constrained estimation methods.
 Parameter-expanded expectation maximization (PX-EM) algorithm often provides speed up by ""us[ing] a `covariance adjustment' to correct the analysis of the M step, capitalising on extra information captured in the imputed complete data"".[33]
 Expectation conditional maximization (ECM) replaces each M step with a sequence of conditional maximization (CM) steps in which each parameter θi is maximized individually, conditionally on the other parameters remaining fixed.[34] Itself can be extended into the Expectation conditional maximization either (ECME) algorithm.[35]
 This idea is further extended in generalized expectation maximization (GEM) algorithm, in which is sought only an increase in the objective function F for both the E step and M step as described in the As a maximization–maximization procedure section.[21] GEM is further developed in a distributed environment and shows promising results.[36]
 It is also possible to consider the EM algorithm as a subclass of the MM (Majorize/Minimize or Minorize/Maximize, depending on context) algorithm,[37] and therefore use any machinery developed in the more general case.
 The Q-function used in the EM algorithm is based on the log likelihood. Therefore, it is regarded as the log-EM algorithm. The use of the log likelihood can be generalized to that of the α-log likelihood ratio. Then, the α-log likelihood ratio of the observed data can be exactly expressed as equality by using the Q-function of the α-log likelihood ratio and the α-divergence. Obtaining this Q-function is a generalized E step. Its maximization is a generalized M step. This pair is called the α-EM algorithm[38]
which contains the log-EM algorithm as its subclass. Thus, the α-EM algorithm by Yasuo Matsuyama is an exact generalization of the log-EM algorithm. No computation of gradient or Hessian matrix is needed. The α-EM shows faster convergence than the log-EM algorithm by choosing an appropriate α. The α-EM algorithm leads to a faster version of the Hidden Markov model estimation algorithm α-HMM.
[39]
 EM is a partially non-Bayesian, maximum likelihood method. Its final result gives a probability distribution over the latent variables (in the Bayesian style) together with a point estimate for θ (either a maximum likelihood estimate or a posterior mode). A fully Bayesian version of this may be wanted, giving a probability distribution over θ and the latent variables. The Bayesian approach to inference is simply to treat θ as another latent variable. In this paradigm, the distinction between the E and M steps disappears. If using the factorized Q approximation as described above (variational Bayes), solving can iterate over each latent variable (now including θ) and optimize them one at a time. Now, k steps per iteration are needed, where k is the number of latent variables. For graphical models this is easy to do as each variable's new Q depends only on its Markov blanket, so local message passing can be used for efficient inference.
 In information geometry, the E step and the M step are interpreted as projections under dual affine connections, called the e-connection and the m-connection; the Kullback–Leibler divergence can also be understood in these terms.
 Let 




x

=
(


x


1


,


x


2


,
…
,


x


n


)


{\displaystyle \mathbf {x} =(\mathbf {x} _{1},\mathbf {x} _{2},\ldots ,\mathbf {x} _{n})}

 be a sample of 



n


{\displaystyle n}

 independent observations from a mixture of two multivariate normal distributions of dimension 



d


{\displaystyle d}

, and let 




z

=
(

z

1


,

z

2


,
…
,

z

n


)


{\displaystyle \mathbf {z} =(z_{1},z_{2},\ldots ,z_{n})}

 be the latent variables that determine the component from which the observation originates.[22]
 where
 The aim is to estimate the unknown parameters representing the mixing value between the Gaussians and the means and covariances of each:
 where the incomplete-data likelihood function is
 and the complete-data likelihood function is
 or
 where 




I



{\displaystyle \mathbb {I} }

 is an indicator function and 



f


{\displaystyle f}

 is the probability density function of a multivariate normal.
 In the last equality, for each i, one indicator 




I

(

z

i


=
j
)


{\displaystyle \mathbb {I} (z_{i}=j)}

 is equal to zero, and one indicator is equal to one. The inner sum thus reduces to one term.
 Given our current estimate of the parameters θ(t), the conditional distribution of the Zi is determined by Bayes theorem to be the proportional height of the normal density weighted by τ:
 These are called the ""membership probabilities"", which are normally considered the output of the E step (although this is not the Q function of below).
 This E step corresponds with setting up this function for Q:
 The expectation of 



log
⁡
L
(
θ
;


x


i


,

Z

i


)


{\displaystyle \log L(\theta ;\mathbf {x} _{i},Z_{i})}

 inside the sum is taken with respect to the probability density function 



P
(

Z

i


∣

X

i


=


x


i


;

θ

(
t
)


)


{\displaystyle P(Z_{i}\mid X_{i}=\mathbf {x} _{i};\theta ^{(t)})}

, which might be different for each  





x


i




{\displaystyle \mathbf {x} _{i}}

 of the training set. Everything in the E step is known before the step is taken except 




T

j
,
i




{\displaystyle T_{j,i}}

, which is computed according to the equation at the beginning of the E step section.
 This full conditional expectation does not need to be calculated in one step, because τ and μ/Σ appear in separate linear terms and can thus be maximized independently.
 



Q
(
θ
∣

θ

(
t
)


)


{\displaystyle Q(\theta \mid \theta ^{(t)})}

 being quadratic in form means that determining the maximizing values of 



θ


{\displaystyle \theta }

 is relatively straightforward. Also, 



τ


{\displaystyle \tau }

, 



(


μ


1


,

Σ

1


)


{\displaystyle ({\boldsymbol {\mu }}_{1},\Sigma _{1})}

 and 



(


μ


2


,

Σ

2


)


{\displaystyle ({\boldsymbol {\mu }}_{2},\Sigma _{2})}

 may all be maximized independently since they all appear in separate linear terms.
 To begin, consider 



τ


{\displaystyle \tau }

, which has the constraint 




τ

1


+

τ

2


=
1


{\displaystyle \tau _{1}+\tau _{2}=1}

:
 This has the same form as the maximum likelihood estimate for the binomial distribution, so
 For the next estimates of 



(


μ


1


,

Σ

1


)


{\displaystyle ({\boldsymbol {\mu }}_{1},\Sigma _{1})}

:
 This has the same form as a weighted maximum likelihood estimate for a normal distribution, so
 and, by symmetry,
 Conclude the iterative process if 




E

Z
∣

θ

(
t
)


,

x



[
log
⁡
L
(

θ

(
t
)


;

x

,

Z

)
]
≤

E

Z
∣

θ

(
t
−
1
)


,

x



[
log
⁡
L
(

θ

(
t
−
1
)


;

x

,

Z

)
]
+
ε


{\displaystyle E_{Z\mid \theta ^{(t)},\mathbf {x} }[\log L(\theta ^{(t)};\mathbf {x} ,\mathbf {Z} )]\leq E_{Z\mid \theta ^{(t-1)},\mathbf {x} }[\log L(\theta ^{(t-1)};\mathbf {x} ,\mathbf {Z} )]+\varepsilon }

 for 



ε


{\displaystyle \varepsilon }

 below some preset threshold.
 The algorithm illustrated above can be generalized for mixtures of more than two multivariate normal distributions.
 The EM algorithm has been implemented in the case where an underlying linear regression model exists explaining the variation of some quantity, but where the values actually observed are censored or truncated versions of those represented in the model.[40] Special cases of this model include censored or truncated observations from one normal distribution.[40]
 EM typically converges to a local optimum, not necessarily the global optimum, with no bound on the convergence rate in general. It is possible that it can be arbitrarily poor in high dimensions and there can be an exponential number of local optima. Hence, a need exists for alternative methods for guaranteed learning, especially in the high-dimensional setting. Alternatives to EM exist with better guarantees for consistency, which are termed moment-based approaches[41] or the so-called spectral techniques.[42][43] Moment-based approaches to learning the parameters of a probabilistic model enjoy guarantees such as global convergence under certain conditions unlike EM which is often plagued by the issue of getting stuck in local optima. Algorithms with guarantees for learning can be derived for a number of important models such as mixture models, HMMs etc. For these spectral methods, no spurious local optima occur, and the true parameters can be consistently estimated under some regularity conditions.[citation needed]
",statist em algorithm iter method find local maximum likelihood maximum posteriori map estim paramet statist model model depend unobserv latent variabl em iter altern perform expect e step creat function expect evalu use current estim paramet maxim step comput paramet maxim expect found e step use determin distribut latent variabl next e step use exampl estim mixtur gaussian solv multipl linear regress problem em algorithm explain given name classic paper arthur dempster nan laird donald rubin point method propos mani time special circumst earlier author one earliest method estim allel frequenc cedric smith anoth propos hartley hartley hock mani idea paper origin anoth one ng thriyambakam krishnan mclachlan hartley idea broaden group discret distribut detail treatment em method exponenti famili publish rolf sundberg thesi sever paper follow collabor per ander paper gener method sketch converg analysi wider class problem paper establish em method import tool statist analysi see also meng van dyk converg analysi algorithm flaw correct converg analysi publish jeff wu wu proof establish em method converg also outsid exponenti famili claim em algorithm use find local maximum likelihood paramet statist model case equat solv directli typic model involv latent variabl addit unknown paramet known data observ either miss valu exist among data model formul simpli assum exist unobserv data point exampl mixtur model describ simpli assum observ data point correspond unobserv data point latent variabl specifi mixtur compon data point belong find maximum likelihood solut typic requir take deriv likelihood function respect unknown valu paramet latent variabl simultan solv result equat statist model latent variabl usual imposs instead result typic set interlock equat solut paramet requir valu latent variabl vice versa substitut one set equat produc unsolv equat em algorithm proce observ way solv two set equat numer one simpli pick arbitrari valu one two set unknown use estim second set use new valu find better estim first set keep altern two result valu converg fix point obviou work proven context addit proven deriv likelihood arbitrarili close zero point turn mean point either local maximum saddl point gener multipl maxima may occur guarante global maximum found likelihood also singular nonsens maxima exampl one solut may found em mixtur model involv set one compon zero varianc mean paramet compon equal one data point converg em algorithm typic requir continu likelihood function respect unknown paramet refer optim variabl given statist model gener set x x observ data set unobserv latent data miss valu z z vector unknown paramet θ along likelihood function l θ x z p x z θ l x z x z maximum likelihood estim mle unknown paramet determin maxim margin likelihood observ data howev quantiti often intract sinc z z unobserv distribut z z unknown attain θ em algorithm seek find maximum likelihood estim margin likelihood iter appli two step succinctli write one equat θ r g x θ e z p x θ log p x z θ max e z p x p x z typic model em appli use z z latent variabl indic membership one set group howev possibl appli em sort model motiv follow valu paramet θ known usual valu latent variabl z z found maxim possibl valu z z either simpli iter z z algorithm viterbi algorithm hidden markov model convers know valu latent variabl z z find estim paramet θ fairli easili typic simpli group observ data point accord valu associ latent variabl averag valu function valu point group suggest iter algorithm case θ z z unknown algorithm describ monoton approach local minimum cost function although em iter increas observ data margin likelihood function guarante exist sequenc converg maximum likelihood estim multimod distribut mean em algorithm may converg local maximum observ data likelihood function depend start valu varieti heurist metaheurist approach exist escap local maximum hill climb start sever differ random initi estim θ appli simul anneal method em especi use likelihood exponenti famili see sundberg ch comprehens treatment e step becom sum expect suffici statist step involv maxim linear function case usual possibl deriv express updat step use sundberg formula prove publish rolf sundberg base unpublish result per ander em method modifi comput maximum posteriori map estim bayesian infer origin paper dempster laird rubin method exist find maximum likelihood estim gradient descent conjug gradient variant algorithm unlik em method typic requir evalu first second deriv likelihood function work improv q θ θ q rather directli improv log p x θ p x shown improv former impli improv latter z z probabl p z x θ p z x write take expect possibl valu unknown data z z current paramet estim θ multipli side p z x θ p z x sum integr z z side expect constant get h θ θ h defin negat sum replac last equat hold everi valu θ includ θ θ subtract last equat previou equat give howev gibb inequ tell us h θ θ h θ θ h h conclud word choos θ improv q θ θ q caus log p x θ p x improv least much em algorithm view two altern maxim step exampl coordin descent consid function q arbitrari probabl distribut unobserv data z h q entropi distribut function written p z x x θ x x condit distribut unobserv data given observ data x x k l kl diverg step em algorithm may view kalman filter typic use state estim smoother may employ batch state estim howev solut requir estim model paramet em algorithm use solv joint state paramet estim problem filter smooth em algorithm aris repeat procedur suppos kalman filter smoother oper measur system possess addit white nois updat measur nois varianc estim obtain maximum likelihood calcul x k x k scalar output estim calcul filter smoother n scalar measur z k k updat also appli updat poisson measur nois intens similarli process updat process nois varianc estim calcul x k x k x k x scalar state estim calcul filter smoother updat model coeffici estim obtain via converg paramet estim well studi number method propos acceler sometim slow converg em algorithm use conjug gradient modifi newton method also em use constrain estim method expect maxim algorithm often provid speed us ing covari adjust correct analysi step capitalis extra inform captur imput complet data expect condit maxim ecm replac step sequenc condit maxim cm step paramet θi maxim individu condit paramet remain fix extend expect condit maxim either ecm algorithm idea extend gener expect maxim gem algorithm sought increas object function f e step step describ procedur section gem develop distribut environ show promis result also possibl consid em algorithm subclass mm depend context algorithm therefor use machineri develop gener case use em algorithm base log likelihood therefor regard algorithm use log likelihood gener likelihood ratio likelihood ratio observ data exactli express equal use likelihood ratio obtain gener e step maxim gener step pair call algorithm contain algorithm subclass thu algorithm yasuo matsuyama exact gener algorithm comput gradient hessian matrix need show faster converg algorithm choos appropri algorithm lead faster version hidden markov model estim algorithm em partial maximum likelihood method final result give probabl distribut latent variabl bayesian style togeth point estim θ either maximum likelihood estim posterior mode fulli bayesian version may want give probabl distribut θ latent variabl bayesian approach infer simpli treat θ anoth latent variabl paradigm distinct e step disappear use factor q approxim describ variat bay solv iter latent variabl includ θ optim one time k step per iter need k number latent variabl graphic model easi variabl new q depend markov blanket local messag pass use effici infer inform geometri e step step interpret project dual affin connect call diverg also understood term let x x x x n x x x x n sampl n n independ observ mixtur two multivari normal distribut dimens let z z z z n z n latent variabl determin compon observ origin aim estim unknown paramet repres mix valu gaussian mean covari likelihood function likelihood function indic function f f probabl densiti function multivari normal last equal one indic z j equal zero one indic equal one inner sum thu reduc one term given current estim paramet θ condit distribut zi determin bay theorem proport height normal densiti weight τ call membership probabl normal consid output e step although q function e step correspond set function q expect log l θ x z l x insid sum taken respect probabl densiti function p z x x θ p x might differ x x train set everyth e step known step taken except j j comput accord equat begin e step section full condit expect need calcul one step τ appear separ linear term thu maxim independ q θ θ q quadrat form mean determin maxim valu θ rel straightforward also τ μ σ μ σ may maxim independ sinc appear separ linear term begin consid τ constraint τ τ form maximum likelihood estim binomi distribut next estim μ σ form weight maximum likelihood estim normal distribut symmetri conclud iter process e z θ x log l θ x z e z θ x log l θ x z ε x l x z x l x z ε preset threshold algorithm illustr gener mixtur two multivari normal distribut em algorithm implement case underli linear regress model exist explain variat quantiti valu actual observ censor truncat version repres model special case model includ censor truncat observ one normal distribut em typic converg local optimum necessarili global optimum bound converg rate gener possibl arbitrarili poor high dimens exponenti number local optima henc need exist altern method guarante learn especi set altern em exist better guarante consist term approach spectral techniqu approach learn paramet probabilist model enjoy guarante global converg certain condit unlik em often plagu issu get stuck local optima algorithm guarante learn deriv number import model mixtur model hmm etc spectral method spuriou local optima occur true paramet consist estim regular condit citat need
DBSCAN,https://en.wikipedia.org/wiki/DBSCAN,"Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu in 1996.[1]
It is a density-based clustering non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed (points with many nearby neighbors), and marks as outliers points that lie alone in low-density regions (those whose nearest neighbors are too far away).
DBSCAN is one of the most commonly used and cited clustering algorithms.[2]
 In 2014, the algorithm was awarded the Test of Time Award (an award given to algorithms which have received substantial attention in theory and practice) at the leading data mining conference, ACM SIGKDD.[3] As of July 2020[update], the follow-up paper ""DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN""[4] appears in the list of the 8 most downloaded articles of the prestigious ACM Transactions on Database Systems (TODS) journal.[5]
 Another follow-up, HDBSCAN*, was initially published by Ricardo J. G. Campello, David Moulavi, and Jörg Sander in 2013,[6] then expanded upon with Arthur Zimek in 2015.[7] It revises some of the original decisions such as the border points, and produces a hierarchical instead of a flat result.
 In 1972, Robert F. Ling published a closely related algorithm in ""The Theory and Construction of k-Clusters""[8] in The Computer Journal with an estimated runtime complexity of O(n³).[8] DBSCAN has a worst-case of O(n²), and the database-oriented range-query formulation of DBSCAN allows for index acceleration. The algorithms slightly differ in their handling of border points.
 Consider a set of points in some space to be clustered. Let ε be a parameter specifying the radius of a neighborhood with respect to some point. For the purpose of DBSCAN clustering, the points are classified as core points, (directly-) reachable points and outliers, as follows:
 Now if p is a core point, then it forms a cluster together with all points (core or non-core) that are reachable from it. Each cluster contains at least one core point; non-core points can be part of a cluster, but they form its ""edge"", since they cannot be used to reach more points.
 Reachability is not a symmetric relation: by definition, only core points can reach non-core points. The opposite is not true, so a non-core point may be reachable, but nothing can be reached from it. Therefore, a further notion of connectedness is needed to formally define the extent of the clusters found by DBSCAN. Two points p and q are density-connected if there is a point o such that both p and q are reachable from o. Density-connectedness is symmetric.
 A cluster then satisfies two properties:
 DBSCAN requires two parameters: ε (eps) and the minimum number of points required to form a dense region[a] (minPts). It starts with an arbitrary starting point that has not been visited. This point's ε-neighborhood is retrieved, and if it contains sufficiently many points, a cluster is started. Otherwise, the point is labeled as noise. Note that this point might later be found in a sufficiently sized ε-environment of a different point and hence be made part of a cluster.
 If a point is found to be a dense part of a cluster, its ε-neighborhood is also part of that cluster. Hence, all points that are found within the ε-neighborhood are added, as is their own ε-neighborhood when they are also dense. This process continues until the density-connected cluster is completely found. Then, a new unvisited point is retrieved and processed, leading to the discovery of a further cluster or noise.
 DBSCAN can be used with any distance function[1][4] (as well as similarity functions or other predicates).[9] The distance function (dist) can therefore be seen as an additional parameter.
 The algorithm can be expressed in pseudocode as follows:[4]
 where RangeQuery can be implemented using a database index for better performance, or using a slow linear scan:
 The DBSCAN algorithm can be abstracted into the following steps:[4]
 A naive implementation of this requires storing the neighborhoods in step 1, thus requiring substantial memory. The original DBSCAN algorithm does not require this by performing these steps for one point at a time.
 DBSCAN optimizes the following loss function:[10]
For any possible clustering 



C
=
{

C

1


,
…
,

C

l


}


{\displaystyle C=\{C_{1},\ldots ,C_{l}\}}

 out of the set of all clusterings 





C




{\displaystyle {\mathcal {C}}}

, it minimizes the number of clusters under the condition that every pair of points in a cluster is density-reachable, which corresponds to the original two properties ""maximality"" and ""connectivity"" of a cluster:[1]
 




min

C
⊂


C


,
 

d

d
b


(
p
,
q
)
≤
ε
 
∀
p
,
q
∈

C

i


 
∀

C

i


∈
C



|

C

|



{\displaystyle \min _{C\subset {\mathcal {C}},~d_{db}(p,q)\leq \varepsilon ~\forall p,q\in C_{i}~\forall C_{i}\in C}|C|}


 where 




d

d
b


(
p
,
q
)


{\displaystyle d_{db}(p,q)}

 gives the smallest 



ε


{\displaystyle \varepsilon }

 such that two points p and q are density-connected.
 DBSCAN visits each point of the database, possibly multiple times (e.g., as candidates to different clusters). For practical considerations, however, the time complexity is mostly governed by the number of regionQuery invocations. DBSCAN executes exactly one such query for each point, and if an indexing structure is used that executes a neighborhood query in O(log n), an overall average runtime complexity of O(n log n) is obtained (if parameter ε is chosen in a meaningful way, i.e. such that on average only O(log n) points are returned). Without the use of an accelerating index structure, or on degenerated data (e.g. all points within a distance less than ε), the worst case run time complexity remains O(n²). The 







(


n
2


)






{\displaystyle \textstyle {\binom {n}{2}}}

 - n = (n²-n)/2-sized upper triangle of the distance matrix can be materialized to avoid distance recomputations, but this needs O(n²) memory, whereas a non-matrix based implementation of DBSCAN only needs O(n) memory.
 See the section below on extensions for algorithmic modifications to handle these issues.
 Every data mining task has the problem of parameters. Every parameter influences the algorithm in specific ways. For DBSCAN, the parameters ε and minPts are needed. The parameters must be specified by the user. Ideally, the value of ε is given by the problem to solve (e.g. a physical distance), and minPts is then the desired minimum cluster size.[a]
 OPTICS can be seen as a generalization of DBSCAN that replaces the ε parameter with a maximum value that mostly affects performance. MinPts then essentially becomes the minimum cluster size to find. While the algorithm is much easier to parameterize than DBSCAN, the results are a bit more difficult to use, as it will usually produce a hierarchical clustering instead of the simple data partitioning that DBSCAN produces.
 Recently, one of the original authors of DBSCAN has revisited DBSCAN and OPTICS, and published a refined version of hierarchical DBSCAN (HDBSCAN*),[6][7] which no longer has the notion of border points. Instead, only the core points form the cluster.
 A spectral implementation of DBSCAN is related to spectral clustering in the trivial case of determining connected graph components — the optimal clusters with no edges cut.[12] However, it can be computationally intensive, up to 



O
(

n

3


)


{\displaystyle O(n^{3})}

. Additionally, one has to choose the number of eigenvectors to compute. For performance reasons, the original DBSCAN algorithm remains preferable to its spectral implementation.
 Generalized DBSCAN (GDBSCAN)[9][13] is a generalization by the same authors to arbitrary ""neighborhood"" and ""dense"" predicates. The ε and minPts parameters are removed from the original algorithm and moved to the predicates. For example, on polygon data, the ""neighborhood"" could be any intersecting polygon, whereas the density predicate uses the polygon areas instead of just the object count.
 Various extensions to the DBSCAN algorithm have been proposed, including methods for parallelization, parameter estimation, and support for uncertain data. The basic idea has been extended to hierarchical clustering by the OPTICS algorithm. DBSCAN is also used as part of subspace clustering algorithms like PreDeCon and SUBCLU. HDBSCAN*[6][7] is a hierarchical version of DBSCAN which is also faster than OPTICS, from which a flat partition consisting of the most prominent clusters can be extracted from the hierarchy.[14]
 Different implementations of the same algorithm were found to exhibit enormous performance differences, with the fastest on a test data set finishing in 1.4 seconds, the slowest taking 13803 seconds.[15] The differences can be attributed to implementation quality, language and compiler differences, and the use of indexes for acceleration.
",spatial cluster applic nois dbscan data cluster algorithm propos martin ester kriegel jörg sander xiaowei xu cluster algorithm given set point space group togeth point close pack point mani nearbi neighbor mark outlier point lie alon region whose nearest neighbor far away dbscan one commonli use cite cluster algorithm algorithm award test time award award given algorithm receiv substanti attent theori practic lead data mine confer acm sigkdd juli updat paper dbscan revisit revisit still use dbscan appear list download articl prestigi acm transact databas system tod journal anoth hdbscan initi publish ricardo campello david moulavi jörg sander expand upon arthur zimek revis origin decis border point produc hierarch instead flat result robert ling publish close relat algorithm theori construct comput journal estim runtim complex dbscan formul dbscan allow index acceler algorithm slightli differ handl border point consid set point space cluster let ε paramet specifi radiu neighborhood respect point purpos dbscan cluster point classifi core point reachabl point outlier follow p core point form cluster togeth point core reachabl cluster contain least one core point point part cluster form edg sinc use reach point reachabl symmetr relat definit core point reach point opposit true point may reachabl noth reach therefor notion connected need formal defin extent cluster found dbscan two point p q point p q reachabl symmetr cluster satisfi two properti dbscan requir two paramet ε ep minimum number point requir form dens region minpt start arbitrari start point visit point retriev contain suffici mani point cluster start otherwis point label nois note point might later found suffici size differ point henc made part cluster point found dens part cluster also part cluster henc point found within ad also dens process continu cluster complet found new unvisit point retriev process lead discoveri cluster nois dbscan use distanc function well similar function predic distanc function dist therefor seen addit paramet algorithm express pseudocod follow rangequeri implement use databas index better perform use slow linear scan dbscan algorithm abstract follow step naiv implement requir store neighborhood step thu requir substanti memori origin dbscan algorithm requir perform step one point time dbscan optim follow loss function possibl cluster c c c l l set cluster c c minim number cluster condit everi pair point cluster correspond origin two properti maxim connect cluster min c c b p q ε p q c c c c c db p q p c b p q db p q give smallest ε two point p q dbscan visit point databas possibl multipl time candid differ cluster practic consider howev time complex mostli govern number regionqueri invoc dbscan execut exactli one queri point index structur use execut neighborhood queri log n overal averag runtim complex n log n obtain paramet ε chosen meaning way averag log n point return without use acceler index structur degener data point within distanc less ε worst case run time complex remain n n n upper triangl distanc matrix materi avoid distanc recomput need memori wherea base implement dbscan need n memori see section extens algorithm modif handl issu everi data mine task problem paramet everi paramet influenc algorithm specif way dbscan paramet ε minpt need paramet must specifi user ideal valu ε given problem solv physic distanc minpt desir minimum cluster size optic seen gener dbscan replac ε paramet maximum valu mostli affect perform minpt essenti becom minimum cluster size find algorithm much easier parameter dbscan result bit difficult use usual produc hierarch cluster instead simpl data partit dbscan produc recent one origin author dbscan revisit dbscan optic publish refin version hierarch dbscan hdbscan longer notion border point instead core point form cluster spectral implement dbscan relat spectral cluster trivial case determin connect graph compon optim cluster edg cut howev comput intens n addit one choos number eigenvector comput perform reason origin dbscan algorithm remain prefer spectral implement gener dbscan gdbscan gener author arbitrari neighborhood dens predic ε minpt paramet remov origin algorithm move predic exampl polygon data neighborhood could intersect polygon wherea densiti predic use polygon area instead object count variou extens dbscan algorithm propos includ method parallel paramet estim support uncertain data basic idea extend hierarch cluster optic algorithm dbscan also use part subspac cluster algorithm like predecon subclu hdbscan hierarch version dbscan also faster optic flat partit consist promin cluster extract hierarchi differ implement algorithm found exhibit enorm perform differ fastest test data set finish second slowest take second differ attribut implement qualiti languag compil differ use index acceler
OPTICS algorithm,https://en.wikipedia.org/wiki/OPTICS_algorithm,"Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based[1] clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and Jörg Sander.[2]
Its basic idea is similar to DBSCAN,[3] but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. To do so, the points of the database are (linearly) ordered such that spatially closest points become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that must be accepted for a cluster so that both points belong to the same cluster. This is represented as a dendrogram.
 Like DBSCAN, OPTICS requires two parameters: ε, which describes the maximum distance (radius) to consider, and MinPts, describing the number of points required to form a cluster. A point p is a core point if at least MinPts points are found within its ε-neighborhood 




N

ε


(
p
)


{\displaystyle N_{\varepsilon }(p)}

 (including point p itself). In contrast to DBSCAN, OPTICS also considers points that are part of a more densely packed cluster, so each point is assigned a core distance that describes the distance to the MinPtsth closest point:
 The reachability-distance of another point o from a point p is either the distance between o and p, or the core distance of p, whichever is bigger:
 If p and o are nearest neighbors, this is the 




ε
′

<
ε


{\displaystyle \varepsilon '<\varepsilon }

 we need to assume to have p and o belong to the same cluster.
 Both core-distance and reachability-distance are undefined if no sufficiently dense cluster (w.r.t. ε) is available. Given a sufficiently large ε, this never happens, but then every ε-neighborhood query returns the entire database, resulting in 



O
(

n

2


)


{\displaystyle O(n^{2})}

 runtime. Hence, the ε parameter is required to cut off the density of clusters that are no longer interesting, and to speed up the algorithm.
 The parameter ε is, strictly speaking, not necessary. It can simply be set to the maximum possible value. When a spatial index is available, however, it does play a practical role with regards to complexity. OPTICS abstracts from DBSCAN by removing this parameter, at least to the extent of only having to give the maximum value.
 The basic approach of OPTICS is similar to DBSCAN, but instead of maintaining known, but so far unprocessed cluster members in a set, they are maintained in a priority queue (e.g. using an indexed heap).
 In update(), the priority queue Seeds is updated with the 



ε


{\displaystyle \varepsilon }

-neighborhood of 



p


{\displaystyle p}

 and 



q


{\displaystyle q}

, respectively:
 OPTICS hence outputs the points in a particular ordering, annotated with their smallest reachability distance (in the original algorithm, the core distance is also exported, but this is not required for further processing).
 
 Using a reachability-plot (a special kind of dendrogram), the hierarchical structure of the clusters can be obtained easily. It is a 2D plot, with the ordering of the points as processed by OPTICS on the x-axis and the reachability distance on the y-axis. Since points belonging to a cluster have a low reachability distance to their nearest neighbor, the clusters show up as valleys in the reachability plot. The deeper the valley, the denser the cluster.
 The image above illustrates this concept. In its upper left area, a synthetic example data set is shown. The upper right part visualizes the spanning tree produced by OPTICS, and the lower part shows the reachability plot as computed by OPTICS. Colors in this plot are labels, and not computed by the algorithm; but it is well visible how the valleys in the plot correspond to the clusters in above data set. The yellow points in this image are considered noise, and no valley is found in their reachability plot. They are usually not assigned to clusters, except the omnipresent ""all data"" cluster in a hierarchical result.
 Extracting clusters from this plot can be done manually by selecting ranges on the x-axis after visual inspection, by selecting a threshold on the y-axis (the result is then similar to a DBSCAN clustering result with the same 



ε


{\displaystyle \varepsilon }

 and minPts parameters; here a value of 0.1 may yield good results), or by different algorithms that try to detect the valleys by steepness, knee detection, or local maxima. A range of the plot beginning with a steep descent and ending with a steep ascent is considered a valley, and corresponds to a contiguous area of high density. Additional care must be taken to the last points in a valley to assign them to the inner or outer cluster, this can be achieved by considering the predecessor.[4] Clusterings obtained this way usually are hierarchical, and cannot be achieved by a single DBSCAN run.
 Like DBSCAN, OPTICS processes each point once, and performs one 



ε


{\displaystyle \varepsilon }

-neighborhood query during this processing. Given a spatial index that grants a neighborhood query in 



O
(
log
⁡
n
)


{\displaystyle O(\log n)}

 runtime, an overall runtime of 



O
(
n
⋅
log
⁡
n
)


{\displaystyle O(n\cdot \log n)}

 is obtained. The worst case however is 



O
(

n

2


)


{\displaystyle O(n^{2})}

, as with DBSCAN. The authors of the original OPTICS paper report an actual constant slowdown factor of 1.6 compared to DBSCAN. Note that the value of 



ε


{\displaystyle \varepsilon }

 might heavily influence the cost of the algorithm, since a value too large might raise the cost of a neighborhood query to linear complexity.
 In particular, choosing 



ε
>

max

x
,
y


d
(
x
,
y
)


{\displaystyle \varepsilon >\max _{x,y}d(x,y)}

 (larger than the maximum distance in the data set) is possible, but leads to quadratic complexity, since every neighborhood query returns the full data set. Even when no spatial index is available, this comes at additional cost in managing the heap. Therefore, 



ε


{\displaystyle \varepsilon }

 should be chosen appropriately for the data set.
 OPTICS-OF[5] is an outlier detection algorithm based on OPTICS. The main use is the extraction of outliers from an existing run of OPTICS at low cost compared to using a different outlier detection method. The better known version LOF is based on the same concepts.
 DeLi-Clu,[6] Density-Link-Clustering combines ideas from single-linkage clustering and OPTICS, eliminating the 



ε


{\displaystyle \varepsilon }

 parameter and offering performance improvements over OPTICS.
 HiSC[7] is a hierarchical subspace clustering (axis-parallel) method based on OPTICS.
 HiCO[8] is a hierarchical correlation clustering algorithm based on OPTICS.
 DiSH[9] is an improvement over HiSC that can find more complex hierarchies.
 FOPTICS[10] is a faster implementation using random projections.
 HDBSCAN*[11] is based on a refinement of DBSCAN, excluding border-points from the clusters and thus following more strictly the basic definition of density-levels by Hartigan.[12]
 Java implementations of OPTICS, OPTICS-OF, DeLi-Clu, HiSC, HiCO and DiSH are available in the ELKI data mining framework (with index acceleration for several distance functions, and with automatic cluster extraction using the ξ extraction method). Other Java implementations include the Weka extension (no support for ξ cluster extraction).
 The R package ""dbscan"" includes a C++ implementation of OPTICS (with both traditional dbscan-like and ξ cluster extraction) using a k-d tree for index acceleration for Euclidean distance only.
 Python implementations of OPTICS are available in the PyClustering library and in scikit-learn. HDBSCAN* is available in the hdbscan library.
",order point identifi cluster structur optic algorithm find cluster spatial data present mihael ankerst marku breunig kriegel jörg sander basic idea similar dbscan address one dbscan major weak problem detect meaning cluster data vari densiti point databas linearli order spatial closest point becom neighbor order addit special distanc store point repres densiti must accept cluster point belong cluster repres dendrogram like dbscan optic requir two paramet ε describ maximum distanc radiu consid minpt describ number point requir form cluster point p core point least minpt point found within n ε p p includ point p contrast dbscan optic also consid point part dens pack cluster point assign core distanc describ distanc minptsth closest point anoth point point p either distanc p core distanc p whichev bigger p nearest neighbor ε ε need assum p belong cluster undefin suffici dens cluster ε avail given suffici larg ε never happen everi queri return entir databas result n runtim henc ε paramet requir cut densiti cluster longer interest speed algorithm paramet ε strictli speak necessari simpli set maximum possibl valu spatial index avail howev play practic role regard complex optic abstract dbscan remov paramet least extent give maximum valu basic approach optic similar dbscan instead maintain known far unprocess cluster member set maintain prioriti queue use index heap updat prioriti queue seed updat ε p p q q respect optic henc output point particular order annot smallest reachabl distanc origin algorithm core distanc also export requir process use special kind dendrogram hierarch structur cluster obtain easili plot order point process optic reachabl distanc sinc point belong cluster low reachabl distanc nearest neighbor cluster show valley reachabl plot deeper valley denser cluster imag illustr concept upper left area synthet exampl data set shown upper right part visual span tree produc optic lower part show reachabl plot comput optic color plot label comput algorithm well visibl valley plot correspond cluster data set yellow point imag consid nois valley found reachabl plot usual assign cluster except omnipres data cluster hierarch result extract cluster plot done manual select rang visual inspect select threshold result similar dbscan cluster result ε minpt paramet valu may yield good result differ algorithm tri detect valley steep knee detect local maxima rang plot begin steep descent end steep ascent consid valley correspond contigu area high densiti addit care must taken last point valley assign inner outer cluster achiev consid predecessor cluster obtain way usual hierarch achiev singl dbscan run like dbscan optic process point perform one ε queri process given spatial index grant neighborhood queri log n n runtim overal runtim n log n n obtain worst case howev n dbscan author origin optic paper report actual constant slowdown factor compar dbscan note valu ε might heavili influenc cost algorithm sinc valu larg might rais cost neighborhood queri linear complex particular choos ε max x x x x larger maximum distanc data set possibl lead quadrat complex sinc everi neighborhood queri return full data set even spatial index avail come addit cost manag heap therefor ε chosen appropri data set outlier detect algorithm base optic main use extract outlier exist run optic low cost compar use differ outlier detect method better known version lof base concept combin idea cluster optic elimin ε paramet offer perform improv optic hisc hierarch subspac cluster method base optic hico hierarch correl cluster algorithm base optic dish improv hisc find complex hierarchi foptic faster implement use random project hdbscan base refin dbscan exclud cluster thu follow strictli basic definit hartigan java implement optic hisc hico dish avail elki data mine framework index acceler sever distanc function automat cluster extract use ξ extract method java implement includ weka extens support ξ cluster extract r packag dbscan includ implement optic tradit ξ cluster extract use tree index acceler euclidean distanc python implement optic avail pyclust librari hdbscan avail hdbscan librari
Mean shift,https://en.wikipedia.org/wiki/Mean_shift,"Mean shift is a non-parametric feature-space mathematical analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm.[1] Application domains include cluster analysis in computer vision and image processing.[2]
 The mean shift procedure is usually credited to work by Fukunaga and Hostetler in 1975.[3] It is, however, reminiscent of earlier work by Schnell in 1964.[4]
 Mean shift is a procedure for locating the maxima—the modes—of a density function given discrete data sampled from that function.[1] This is an iterative method, and we start with an initial estimate 



x


{\displaystyle x}

.  Let a kernel function 



K
(

x

i


−
x
)


{\displaystyle K(x_{i}-x)}

 be given.  This function determines the weight of nearby points for re-estimation of the mean.  Typically a Gaussian kernel on the distance to the current estimate is used, 



K
(

x

i


−
x
)
=

e

−
c

|


|


x

i


−
x

|



|


2






{\displaystyle K(x_{i}-x)=e^{-c||x_{i}-x||^{2}}}

.  The weighted mean of the density in the window determined by 



K


{\displaystyle K}

 is
 where 



N
(
x
)


{\displaystyle N(x)}

 is the neighborhood of 



x


{\displaystyle x}

, a set of points for which 



K
(

x

i


−
x
)
≠
0


{\displaystyle K(x_{i}-x)\neq 0}

.
 The difference 



m
(
x
)
−
x


{\displaystyle m(x)-x}

 is called mean shift in Fukunaga and Hostetler.[3] 
The mean-shift algorithm now sets 



x
←
m
(
x
)


{\displaystyle x\leftarrow m(x)}

, and repeats the estimation until 



m
(
x
)


{\displaystyle m(x)}

 converges.
 Although the mean shift algorithm has been widely used in many applications, a rigid proof for the convergence of the algorithm using a general kernel in a high dimensional space is still not known.[5] Aliyari Ghassabeh showed the convergence of the mean shift algorithm in one dimension with a differentiable, convex, and strictly decreasing profile function.[6] However, the one-dimensional case has limited real world applications. Also, the convergence of the algorithm in higher dimensions with a finite number of the stationary (or isolated) points has been proved.[5][7] However, sufficient conditions for a general kernel function to have finite stationary (or isolated) points have not been provided.
 Gaussian Mean-Shift is an Expectation–maximization algorithm.[8]
 Let data be a finite set 



S


{\displaystyle S}

 embedded in the 



n


{\displaystyle n}

-dimensional Euclidean space, 



X


{\displaystyle X}

. Let 



K


{\displaystyle K}

 be a flat kernel that is the characteristic function of the 



λ


{\displaystyle \lambda }

-ball in 



X


{\displaystyle X}

,
 



K
(
x
)
=


{



1



if

 
‖
x
‖
≤
λ




0



if

 
‖
x
‖
>
λ








{\displaystyle K(x)={\begin{cases}1&{\text{if}}\ \|x\|\leq \lambda \\0&{\text{if}}\ \|x\|>\lambda \\\end{cases}}}


 In each iteration of the algorithm, 



s
←
m
(
s
)


{\displaystyle s\leftarrow m(s)}

 is performed for all 



s
∈
S


{\displaystyle s\in S}

 simultaneously. The first question, then, is how to estimate the density function given a sparse set of samples. One of the simplest approaches is to just smooth the data, e.g., by convolving it with a fixed kernel of width 



h


{\displaystyle h}

,
 



f
(
x
)
=

∑

i


K
(
x
−

x

i


)
=

∑

i


k

(



‖
x
−

x

i



‖

2




h

2




)



{\displaystyle f(x)=\sum _{i}K(x-x_{i})=\sum _{i}k\left({\frac {\|x-x_{i}\|^{2}}{h^{2}}}\right)}


 where 




x

i




{\displaystyle x_{i}}

 are the input samples and 



k
(
r
)


{\displaystyle k(r)}

 is the kernel function (or Parzen window). 



h


{\displaystyle h}

 is the only parameter in the algorithm and is called the bandwidth. This approach is known as kernel density estimation or the Parzen window technique. Once we have computed 



f
(
x
)


{\displaystyle f(x)}

 from the equation above, we can find its local maxima using gradient ascent or some other optimization technique. The problem with this ""brute force"" approach is that, for higher dimensions, it becomes computationally prohibitive to evaluate 



f
(
x
)


{\displaystyle f(x)}

 over the complete search space. Instead, mean shift uses a variant of what is known in the optimization literature as multiple restart gradient descent. Starting at some guess for a local maximum, 




y

k




{\displaystyle y_{k}}

, which can be a random input data point 




x

1




{\displaystyle x_{1}}

, mean shift computes the gradient of the density estimate 



f
(
x
)


{\displaystyle f(x)}

 at 




y

k




{\displaystyle y_{k}}

 and takes an uphill step in that direction.[9]
 Kernel definition: Let 



X


{\displaystyle X}

 be the 



n


{\displaystyle n}

-dimensional Euclidean space, 





R


n




{\displaystyle \mathbb {R} ^{n}}

. The norm of 



x


{\displaystyle x}

 is a non-negative number, 



‖
x

‖

2


=

x

⊤


x
≥
0


{\displaystyle \|x\|^{2}=x^{\top }x\geq 0}

. A function 



K
:
X
→

R



{\displaystyle K:X\rightarrow \mathbb {R} }

 is said to be a kernel if there exists a profile, 



k
:
[
0
,
∞
]
→

R



{\displaystyle k:[0,\infty ]\rightarrow \mathbb {R} }

 , such that
 



K
(
x
)
=
k
(
‖
x

‖

2


)


{\displaystyle K(x)=k(\|x\|^{2})}


and 
 The two most frequently used kernel profiles for mean shift are:
 



k
(
x
)
=


{



1



if

 
x
≤
λ




0



if

 
x
>
λ








{\displaystyle k(x)={\begin{cases}1&{\text{if}}\ x\leq \lambda \\0&{\text{if}}\ x>\lambda \\\end{cases}}}


 



k
(
x
)
=

e

−


x

2

σ

2







,


{\displaystyle k(x)=e^{-{\frac {x}{2\sigma ^{2}}}},}


 where the standard deviation parameter 



σ


{\displaystyle \sigma }

 works as the bandwidth parameter, 



h


{\displaystyle h}

.
 Consider a set of points in two-dimensional space. Assume a circular window centered at 



C


{\displaystyle C}

 and having radius 



r


{\displaystyle r}

 as the kernel. Mean-shift is a hill climbing algorithm which involves shifting this kernel iteratively to a higher density region until convergence. Every shift is defined by a mean shift vector. The mean shift vector always points toward the direction of the maximum increase in the density. At every iteration the kernel is shifted to the centroid or the mean of the points within it. The method of calculating this mean depends on the choice of the kernel. In this case if a Gaussian kernel is chosen instead of a flat kernel, then every point will first be assigned a weight which will decay exponentially as the distance from the kernel's center increases. At convergence, there will be no direction at which a shift can accommodate more points inside the kernel.
 The mean shift algorithm can be used for visual tracking.  The simplest such algorithm would create a confidence map in the new image based on the color histogram of the object in the previous image, and use mean shift to find the peak of a confidence map near the object's old position. The confidence map is a probability density function on the new image, assigning each pixel of the new image a probability, which is the probability of the pixel color occurring in the object in the previous image. A few algorithms, such as kernel-based object tracking,[10] 
ensemble tracking,[11] 
CAMshift [12][13] 
expand on this idea.
 Let 




x

i




{\displaystyle x_{i}}

 and 




z

i


,
i
=
1
,
.
.
.
,
n
,


{\displaystyle z_{i},i=1,...,n,}

 be the 



d


{\displaystyle d}

-dimensional input and filtered image pixels in the joint spatial-range domain. For each pixel,
 Variants of the algorithm can be found in machine learning and image processing packages:
",mean shift mathemat analysi techniqu locat maxima densiti function algorithm applic domain includ cluster analysi comput vision imag process mean shift procedur usual credit work fukunaga hostetl howev reminisc earlier work schnell mean shift procedur locat densiti function given discret data sampl function iter method start initi estim x x let kernel function k x x k given function determin weight nearbi point mean typic gaussian kernel distanc current estim use k x x e c x x k weight mean densiti window determin k k n x n x neighborhood x x set point k x x k differ x x x call mean shift fukunaga hostetl algorithm set x x x repeat estim x x converg although mean shift algorithm wide use mani applic rigid proof converg algorithm use gener kernel high dimension space still known aliyari ghassabeh show converg mean shift algorithm one dimens differenti convex strictli decreas profil function howev case limit real world applic also converg algorithm higher dimens finit number stationari isol point prove howev suffici condit gener kernel function finit stationari isol point provid gaussian algorithm let data finit set embed n n euclidean space x x let k k flat kernel characterist function λ x x k x x λ x λ k x case case iter algorithm perform simultan first question estim densiti function given spars set sampl one simplest approach smooth data convolv fix kernel width h h f x k x x k x x h f x k x input sampl k r k r kernel function parzen window h h paramet algorithm call bandwidth approach known kernel densiti estim parzen window techniqu comput f x f x equat find local maxima use gradient ascent optim techniqu problem brute forc approach higher dimens becom comput prohibit evalu f x f x complet search space instead mean shift use variant known optim literatur multipl restart gradient descent start guess local maximum k k random input data point x mean shift comput gradient densiti estim f x f x k k take uphil step direct kernel definit let x x n n euclidean space r n r n norm x x number x x x function k x r k r said kernel exist profil k r k r k x k x k x two frequent use kernel profil mean shift k x x λ x λ k x case x case k x e x σ k x x standard deviat paramet σ work bandwidth paramet h h consid set point space assum circular window center c c radiu r r kernel hill climb algorithm involv shift kernel iter higher densiti region converg everi shift defin mean shift vector mean shift vector alway point toward direct maximum increas densiti everi iter kernel shift centroid mean point within method calcul mean depend choic kernel case gaussian kernel chosen instead flat kernel everi point first assign weight decay exponenti distanc kernel center increas converg direct shift accommod point insid kernel mean shift algorithm use visual track simplest algorithm would creat confid map new imag base color histogram object previou imag use mean shift find peak confid map near object old posit confid map probabl densiti function new imag assign pixel new imag probabl probabl pixel color occur object previou imag algorithm object track ensembl track camshift expand idea let x z n n input filter imag pixel joint domain pixel variant algorithm found machin learn imag process packag
Factor analysis,https://en.wikipedia.org/wiki/Factor_analysis,"Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors plus ""error"" terms, hence factor analysis can be thought of as a special case of errors-in-variables models.[1]
 Simply put, the factor loading of a variable quantifies the extent to which the variable is related to a given factor.[2]
 A common rationale behind factor analytic methods is that the information gained about the interdependencies between observed variables can be used later to reduce the set of variables in a dataset. Factor analysis is commonly used in psychometrics, personality psychology, biology, marketing, product management, operations research, finance, and machine learning. It may help to deal with data sets where there are large numbers of observed variables that are thought to reflect a smaller number of underlying/latent variables. It is one of the most commonly used inter-dependency techniques and is used when the relevant set of variables shows a systematic inter-dependence and the objective is to find out the latent factors that create a commonality.
 The model attempts to explain a set of 



p


{\displaystyle p}

 observations in each of 



n


{\displaystyle n}

 individuals with a set of 



k


{\displaystyle k}

 common factors (




f

i
,
j




{\displaystyle f_{i,j}}

) where there are fewer factors per unit than observations per unit (



k
<
p


{\displaystyle k<p}

). Each individual has 



k


{\displaystyle k}

 of their own common factors, and these are related to the observations via the factor loading matrix (



L
∈


R


p
×
k




{\displaystyle L\in \mathbb {R} ^{p\times k}}

), for a single observation, according to
 where
 In matrix notation
 where observation matrix 



X
∈


R


p
×
n




{\displaystyle X\in \mathbb {R} ^{p\times n}}

, loading matrix 



L
∈


R


p
×
k




{\displaystyle L\in \mathbb {R} ^{p\times k}}

, factor matrix 



F
∈


R


k
×
n




{\displaystyle F\in \mathbb {R} ^{k\times n}}

, error term matrix 



ε
∈


R


p
×
n




{\displaystyle \varepsilon \in \mathbb {R} ^{p\times n}}

 and mean matrix 




M

∈


R


p
×
n




{\displaystyle \mathrm {M} \in \mathbb {R} ^{p\times n}}

 whereby the 



(
i
,
m
)


{\displaystyle (i,m)}

th element is simply 





M


i
,
m


=

μ

i




{\displaystyle \mathrm {M} _{i,m}=\mu _{i}}

.
 Also we will impose the following assumptions on 



F


{\displaystyle F}

:
 Suppose 




C
o
v

(
X
−

M

)
=
Σ


{\displaystyle \mathrm {Cov} (X-\mathrm {M} )=\Sigma }

. Then
 and therefore, from conditions 1 and 2 imposed on 



F


{\displaystyle F}

 above, 



E
[
L
F
]
=
L
E
[
F
]
=
0


{\displaystyle E[LF]=LE[F]=0}

 and 



C
o
v
(
L
F
+
ϵ
)
=
C
o
v
(
L
F
)
+
C
o
v
(
ϵ
)


{\displaystyle Cov(LF+\epsilon )=Cov(LF)+Cov(\epsilon )}

, giving
 or, setting 



Ψ
:=

C
o
v

(
ε
)


{\displaystyle \Psi :=\mathrm {Cov} (\varepsilon )}

,
 For any orthogonal matrix 



Q


{\displaystyle Q}

, if we set 




L

′


=
 
L
Q


{\displaystyle L^{\prime }=\ LQ}

 and 




F

′


=

Q

T


F


{\displaystyle F^{\prime }=Q^{T}F}

, the criteria for being factors and factor loadings still hold. Hence a set of factors and factor loadings is unique only up to an orthogonal transformation.
 Suppose a psychologist has the hypothesis that there are two kinds of intelligence, ""verbal intelligence"" and ""mathematical intelligence"", neither of which is directly observed.[note 1] Evidence for the hypothesis is sought in the examination scores from each of 10 different academic fields of 1000 students. If each student is chosen randomly from a large population, then each student's 10 scores are random variables. The psychologist's hypothesis may say that for each of the 10 academic fields, the score averaged over the group of all students who share some common pair of values for verbal and mathematical ""intelligences"" is some constant times their level of verbal intelligence plus another constant times their level of mathematical intelligence, i.e., it is a linear combination of those two ""factors"". The numbers for a particular subject, by which the two kinds of intelligence are multiplied to obtain the expected score, are posited by the hypothesis to be the same for all intelligence level pairs, and are called ""factor loading"" for this subject. [clarification needed]  For example, the hypothesis may hold that the predicted average student's aptitude in the field of astronomy is
 The numbers 10 and 6 are the factor loadings associated with astronomy. Other academic subjects may have different factor loadings.
 Two students assumed to have identical degrees of verbal and mathematical intelligence may have different measured aptitudes in astronomy because individual aptitudes differ from average aptitudes (predicted above) and because of measurement error itself. Such differences make up what is collectively called the ""error"" — a statistical term that means the amount by which an individual, as measured, differs from what is average for or predicted by his or her levels of intelligence (see errors and residuals in statistics).
 The observable data that go into factor analysis would be 10 scores of each of the 1000 students, a total of 10,000 numbers. The factor loadings and levels of the two kinds of intelligence of each student must be inferred from the data.
 In the following, matrices will be indicated by indexed variables. ""Subject"" indices will be indicated using letters 



a


{\displaystyle a}

,



b


{\displaystyle b}

 and 



c


{\displaystyle c}

, with values running from 



1


{\displaystyle 1}

 to 



p


{\displaystyle p}

 which is equal to 



10


{\displaystyle 10}

 in the above example.  ""Factor"" indices will be indicated using letters 



p


{\displaystyle p}

, 



q


{\displaystyle q}

 and 



r


{\displaystyle r}

, with values running from 



1


{\displaystyle 1}

 to 



k


{\displaystyle k}

 which is equal to 



2


{\displaystyle 2}

 in the above example. ""Instance"" or ""sample"" indices will be indicated using letters 



i


{\displaystyle i}

,



j


{\displaystyle j}

 and 



k


{\displaystyle k}

, with values running from 



1


{\displaystyle 1}

 to 



N


{\displaystyle N}

. In the example above, if a sample of 



N
=
1000


{\displaystyle N=1000}

 students participated in the 



p
=
10


{\displaystyle p=10}

 exams, the 



i


{\displaystyle i}

th student's score for the 



a


{\displaystyle a}

th exam is given by 




x

a
i




{\displaystyle x_{ai}}

. The purpose of factor analysis is to characterize the correlations between the variables 




x

a




{\displaystyle x_{a}}

 of which the 




x

a
i




{\displaystyle x_{ai}}

 are a particular instance, or set of observations. In order for the variables to be on equal footing, they are normalized into standard scores 



z


{\displaystyle z}

:
 where the sample mean is:
 and the sample variance is given by:
 The factor analysis model for this particular sample is then:
 or, more succinctly:
 where
 In matrix notation, we have
 Observe that by doubling the scale on which ""verbal intelligence""—the first component in each column of 



F


{\displaystyle F}

—is measured, and simultaneously halving the factor loadings for verbal intelligence makes no difference to the model. Thus, no generality is lost by assuming that the standard deviation of the factors for verbal intelligence is 



1


{\displaystyle 1}

. Likewise for mathematical intelligence. Moreover, for similar reasons, no generality is lost by assuming the two factors are uncorrelated with each other. In other words:
 where 




δ

p
q




{\displaystyle \delta _{pq}}

 is the Kronecker delta (



0


{\displaystyle 0}

 when 



p
≠
q


{\displaystyle p\neq q}

 and 



1


{\displaystyle 1}

 when 



p
=
q


{\displaystyle p=q}

).The errors are assumed to be independent of the factors:
 Since any rotation of a solution is also a solution, this makes interpreting the factors difficult. See disadvantages below. In this particular example, if we do not know beforehand that the two types of intelligence are uncorrelated, then we cannot interpret the two factors as the two different types of intelligence. Even if they are uncorrelated, we cannot tell which factor corresponds to verbal intelligence and which corresponds to mathematical intelligence without an outside argument.
 The values of the loadings 



L


{\displaystyle L}

, the averages 



μ


{\displaystyle \mu }

, and the variances of the ""errors"" 



ε


{\displaystyle \varepsilon }

 must be estimated given the observed data 



X


{\displaystyle X}

 and 



F


{\displaystyle F}

 (the assumption about the levels of the factors is fixed for a given 



F


{\displaystyle F}

). 
The ""fundamental theorem"" may be derived from the above conditions:
 The term on the left is the 



(
a
,
b
)


{\displaystyle (a,b)}

-term of the correlation matrix (a 



p
×
p


{\displaystyle p\times p}

 matrix derived as the product of the 



p
×
N


{\displaystyle p\times N}

 matrix of standardized observations with its transpose) of the observed data, and its 



p


{\displaystyle p}

 diagonal elements will be 



1


{\displaystyle 1}

s. The second term on the right will be a diagonal matrix with terms less than unity. The first term on the right is the ""reduced correlation matrix"" and will be equal to the correlation matrix except for its diagonal values which will be less than unity. These diagonal elements of the reduced correlation matrix are called ""communalities"" (which represent the fraction of the variance in the observed variable that is accounted for by the factors):
 The sample data 




z

a
i




{\displaystyle z_{ai}}

 will not exactly obey the fundamental equation given above due to sampling errors, inadequacy of the model, etc. The goal of any analysis of the above model is to find the factors 




F

p
i




{\displaystyle F_{pi}}

 and loadings 




ℓ

a
p




{\displaystyle \ell _{ap}}

 which give a ""best fit"" to the data. In factor analysis, the best fit is defined as the minimum of the mean square error in the off-diagonal residuals of the correlation matrix:[3]
 This is equivalent to minimizing the off-diagonal components of the error covariance which, in the model equations have expected values of zero. This is to be contrasted with principal component analysis which seeks to minimize the mean square error of all residuals.[3] Before the advent of high-speed computers, considerable effort was devoted to finding approximate solutions to the problem, particularly in estimating the communalities by other means, which then simplifies the problem considerably by yielding a known reduced correlation matrix. This was then used to estimate the factors and the loadings. With the advent of high-speed computers, the minimization problem can be solved iteratively with adequate speed, and the communalities are calculated in the process, rather than being needed beforehand. The MinRes algorithm is particularly suited to this problem, but is hardly the only iterative means of finding a solution.
 If the solution factors are allowed to be correlated (as in 'oblimin' rotation, for example), then the corresponding mathematical model uses skew coordinates rather than orthogonal coordinates.
 The parameters and variables of factor analysis can be given a geometrical interpretation. The data (




z

a
i




{\displaystyle z_{ai}}

), the factors (




F

p
i




{\displaystyle F_{pi}}

) and the errors (




ε

a
i




{\displaystyle \varepsilon _{ai}}

) can be viewed as vectors in an 



N


{\displaystyle N}

-dimensional Euclidean space (sample space), represented as 





z


a




{\displaystyle \mathbf {z} _{a}}

, 





F


p




{\displaystyle \mathbf {F} _{p}}

 and 





ε


a




{\displaystyle {\boldsymbol {\varepsilon }}_{a}}

 respectively. Since the data are standardized, the data vectors are of unit length (




|


|



z


a



|


|

=
1


{\displaystyle ||\mathbf {z} _{a}||=1}

). The factor vectors define an 



k


{\displaystyle k}

-dimensional linear subspace (i.e. a hyperplane) in this space, upon which the data vectors are projected orthogonally. This follows from the model equation 
 and the independence of the factors and the errors: 





F


p


⋅


ε


a


=
0


{\displaystyle \mathbf {F} _{p}\cdot {\boldsymbol {\varepsilon }}_{a}=0}

. In the above example, the hyperplane is just a 2-dimensional plane defined by the two factor vectors. The projection of the data vectors onto the hyperplane is given by 
 and the errors are vectors from that projected point to the data point and are perpendicular to the hyperplane. The goal of factor analysis is to find a hyperplane which is a ""best fit"" to the data in some sense, so it doesn't matter how the factor vectors which define this hyperplane are chosen, as long as they are independent and lie in the hyperplane. We are free to specify them as both orthogonal and normal (





F


p


⋅


F


q


=

δ

p
q




{\displaystyle \mathbf {F} _{p}\cdot \mathbf {F} _{q}=\delta _{pq}}

) with no loss of generality. After a suitable set of factors are found, they may also be arbitrarily rotated within the hyperplane, so that any rotation of the factor vectors will define the same hyperplane, and also be a solution. As a result, in the above example, in which the fitting hyperplane is two dimensional, if we do not know beforehand that the two types of intelligence are uncorrelated, then we cannot interpret the two factors as the two different types of intelligence. Even if they are uncorrelated, we cannot tell which factor corresponds to verbal intelligence and which corresponds to mathematical intelligence, or whether the factors are linear combinations of both, without an outside argument.
 The data vectors 





z


a




{\displaystyle \mathbf {z} _{a}}

 have unit length. The entries of the correlation matrix for the data are given by 




r

a
b


=


z


a


⋅


z


b




{\displaystyle r_{ab}=\mathbf {z} _{a}\cdot \mathbf {z} _{b}}

. The correlation matrix can be geometrically interpreted as the cosine of the angle between the two data vectors 





z


a




{\displaystyle \mathbf {z} _{a}}

 and 





z


b




{\displaystyle \mathbf {z} _{b}}

. The diagonal elements will clearly be 



1


{\displaystyle 1}

s and the off diagonal elements will have absolute values less than or equal to unity. The ""reduced correlation matrix"" is defined as 
 The goal of factor analysis is to choose the fitting hyperplane such that the reduced correlation matrix reproduces the correlation matrix as nearly as possible, except for the diagonal elements of the correlation matrix which are known to have unit value.  In other words, the goal is to reproduce as accurately as possible the cross-correlations in the data. Specifically, for the fitting hyperplane, the mean square error in the off-diagonal components 
 is to be minimized, and this is accomplished by minimizing it with respect to a set of orthonormal factor vectors. It can be seen that 
 The term on the right is just the covariance of the errors. In the model, the error covariance is stated to be a diagonal matrix and so the above minimization problem will in fact yield a ""best fit"" to the model: It will yield a sample estimate of the error covariance which has its off-diagonal components minimized in the mean square sense. It can be seen that since the 







z
^




a




{\displaystyle {\hat {z}}_{a}}

 are orthogonal projections of the data vectors, their length will be less than or equal to the length of the projected data vector, which is unity. The square of these lengths are just the diagonal elements of the reduced correlation matrix. These diagonal elements of the reduced correlation matrix are known as ""communalities"":
 Large values of the communalities will indicate that the fitting hyperplane is rather accurately reproducing the correlation matrix. The mean values of the factors must also be constrained to be zero, from which it follows that the mean values of the errors will also be zero.
 Exploratory factor analysis (EFA) is used to identify complex interrelationships among items and group items that are part of unified concepts.[4]  The researcher makes no a priori assumptions about relationships among factors.[4]
 Confirmatory factor analysis (CFA) is a more complex approach that tests the hypothesis that the items are associated with specific factors.[4] CFA uses structural equation modeling to test a measurement model whereby loading on the factors allows for evaluation of relationships between observed variables and unobserved variables.[4]  Structural equation modeling approaches can accommodate measurement error and are less restrictive than least-squares estimation.[4]  Hypothesized models are tested against actual data, and the analysis would demonstrate loadings of observed variables on the latent variables (factors), as well as the correlation between the latent variables.[4]
 Principal component analysis (PCA) is a widely used method for factor extraction, which is the first phase of EFA.[4] Factor weights are computed to extract the maximum possible variance, with successive factoring continuing until there is no further meaningful variance left.[4] The factor model must then be rotated for analysis.[4]
 Canonical factor analysis, also called Rao's canonical factoring, is a different method of computing the same model as PCA, which uses the principal axis method. Canonical factor analysis seeks factors that have the highest canonical correlation with the observed variables. Canonical factor analysis is unaffected by arbitrary rescaling of the data.
 Common factor analysis, also called principal factor analysis (PFA) or principal axis factoring (PAF), seeks the fewest factors which can account for the common variance (correlation) of a set of variables.
 Image factoring is based on the correlation matrix of predicted variables rather than actual variables, where each variable is predicted from the others using multiple regression.
 Alpha factoring is based on maximizing the reliability of factors, assuming variables are randomly sampled from a universe of variables. All other methods assume cases to be sampled and variables fixed.
 Factor regression model is a combinatorial model of factor model and regression model; or alternatively, it can be viewed as the hybrid factor model,[5] whose factors are partially known.
 Explained from PCA perspective, not from Factor Analysis perspective. Researchers wish to avoid such subjective or arbitrary criteria for factor retention as ""it made sense to me"". A number of objective methods have been developed to solve this problem, allowing users to determine an appropriate range of solutions to investigate.[7] However these different methods often disagree with one another as to the number of factors that ought to be retained. For instance, the parallel analysis may suggest 5 factors while Velicer's MAP suggests 6, so the researcher may request both 5 and 6-factor solutions and discuss each in terms of their relation to external data and theory.
 Horn's parallel analysis (PA):[8] A Monte-Carlo based simulation method that compares the observed eigenvalues with those obtained from uncorrelated normal variables. A factor or component is retained if the associated eigenvalue is bigger than the 95th percentile of the distribution of eigenvalues derived from the random data. PA is among the more commonly recommended rules for determining the number of components to retain,[7][9] but many programs fail to include this option (a notable exception being R).[10] However, Formann provided both theoretical and empirical evidence that its application might not be appropriate in many cases since its performance is considerably influenced by sample size, item discrimination, and type of correlation coefficient.[11]
 Velicer's (1976) MAP test[12] as described by Courtney (2013)[13] “involves a complete principal components analysis followed by the examination of a series of matrices of partial correlations” (p. 397 (though this quote does not occur in Velicer (1976) and the cited page number is outside the pages of the citation). The squared correlation for Step “0” (see Figure 4) is the average squared off-diagonal correlation for the unpartialed correlation matrix. On Step 1, the first principal component and its associated items are partialed out. Thereafter, the average squared off-diagonal correlation for the subsequent correlation matrix is then computed for Step 1. On Step 2, the first two principal components are partialed out and the resultant average squared off-diagonal correlation is again computed. The computations are carried out for k minus one step (k representing the total number of variables in the matrix). Thereafter, all of the average squared correlations for each step are lined up and the step number in the analyses that resulted in the lowest average squared partial correlation determines the number of components or factors to retain.[12] By this method, components are maintained as long as the variance in the correlation matrix represents systematic variance, as opposed to residual or error variance. Although methodologically akin to principal components analysis, the MAP technique has been shown to perform quite well in determining the number of factors to retain in multiple simulation studies.[7][14][15][16] This procedure is made available through SPSS's user interface,[13] as well as the psych package for the R programming language.[17][18]
 Kaiser criterion: The Kaiser rule is to drop all components with eigenvalues under 1.0 – this being the eigenvalue equal to the information accounted for by an average single item.[19] The Kaiser criterion is the default in SPSS and most statistical software but is not recommended when used as the sole cut-off criterion for estimating the number of factors as it tends to over-extract factors.[20] A variation of this method has been created where a researcher calculates confidence intervals for each eigenvalue and retains only factors which have the entire confidence interval greater than 1.0.[14][21]
 Scree plot:[22]
The Cattell scree test plots the components as the X-axis and the corresponding eigenvalues as the Y-axis.  As one moves to the right, toward later components, the eigenvalues drop. When the drop ceases and the curve makes an elbow toward less steep decline, Cattell's scree test says to drop all further components after the one starting at the elbow. This rule is sometimes criticised for being amenable to researcher-controlled ""fudging"".  That is, as picking the ""elbow"" can be subjective because the curve has multiple elbows or is a smooth curve, the researcher may be tempted to set the cut-off at the number of factors desired by their research agenda.[citation needed]
 Variance explained criteria: Some researchers simply use the rule of keeping enough factors to account for 90% (sometimes 80%) of the variation.  Where the researcher's goal emphasizes parsimony (explaining variance with as few factors as possible), the criterion could be as low as 50%.
 By placing a prior distribution over the number of latent factors and then applying Bayes' theorem, Bayesian models can return a probability distribution over the number of latent factors. This has been modeled using the Indian buffet process,[23] but can be modeled more simply by placing any discrete prior (e.g. a negative binomial distribution) on the number of components.
 The output of PCA maximizes the variance accounted for by the first factor first, then the second factor, etc. A disadvantage of this procedure is that most items load on the early factors, while very few items load on later variables. This makes interpreting the factors by reading through a list of questions and loadings difficult, as every question is strongly correlated with the first few components, while very few questions are strongly correlated with the last few components.
 Rotation serves to make the output easier to interpret. By choosing a different basis for the same principal components – that is, choosing different factors to express the same correlation structure – it is possible to create variables that are more easily interpretable.
 Rotations can be orthogonal or oblique; oblique rotations allow the factors to correlate.[24] This increased flexibility means that more rotations are possible, some of which may be better at achieving a specified goal. However, this can also make the factors more difficult to interpret, as some information is ""double-counted"" and included multiple times in different components; some factors may even appear to be near-duplicates of each other. 
 Two broad classes of orthogonal rotations exist: those that look for sparse rows (where each row is a case, i.e. subject), and those that look for sparse columns (where each column is a variable).
 It can be difficult to interpret a factor structure when each variable is loading on multiple factors. 
Small changes in the data can sometimes tip a balance in the factor rotation criterion so that a completely different factor rotation is produced. This can make it difficult to compare the results of different experiments. This problem is illustrated by a comparison of different studies of world-wide cultural differences. Each study has used different measures of cultural variables and produced a differently rotated factor analysis result. The authors of each study believed that they had discovered something new, and invented new names for the factors they found. A later comparison of the studies found that the results were rather similar when the unrotated results were compared. The common practice of factor rotation has obscured the similarity between the results of the different studies.[25]
 Higher-order factor analysis is a statistical method consisting of repeating steps factor analysis – oblique rotation – factor analysis of rotated factors.  Its merit is to enable the researcher to see the hierarchical structure of studied phenomena. To interpret the results, one proceeds either by post-multiplying the primary factor pattern matrix by the higher-order factor pattern matrices (Gorsuch, 1983) and perhaps applying a Varimax rotation to the result (Thompson, 1990) or by using a Schmid-Leiman solution (SLS, Schmid & Leiman, 1957, also known as Schmid-Leiman transformation) which attributes the variation from the primary factors to the second-order factors.
 Factor analysis is related to principal component analysis (PCA), but the two are not identical.[26] There has been significant controversy in the field over differences between the two techniques. PCA can be considered as a more basic version of exploratory factor analysis (EFA) that was developed in the early days prior to the advent of high-speed computers. Both PCA and factor analysis aim to reduce the dimensionality of a set of data, but the approaches taken to do so are different for the two techniques. Factor analysis is clearly designed with the objective to identify certain unobservable factors from the observed variables, whereas PCA does not directly address this objective; at best, PCA provides an approximation to the required factors.[27] From the point of view of exploratory analysis, the eigenvalues of PCA are inflated component loadings, i.e., contaminated with error variance.[28][29][30][31][32][33]
 Whilst EFA and PCA are treated as synonymous techniques in some fields of statistics, this has been criticised.[34][35] Factor analysis ""deals with the assumption of an underlying causal structure: [it] assumes that the covariation in the observed variables is due to the presence of one or more latent variables (factors) that exert causal influence on these observed variables"".[36] In contrast, PCA neither assumes nor depends on such an underlying causal relationship. Researchers have argued that the distinctions between the two techniques may mean that there are objective benefits for preferring one over the other based on the analytic goal. If the factor model is incorrectly formulated or the assumptions are not met, then factor analysis will give erroneous results. Factor analysis has been used successfully where adequate understanding of the system permits good initial model formulations. PCA employs a mathematical transformation to the original data with no assumptions about the form of the covariance matrix. The objective of PCA is to determine linear combinations of the original variables and select a few that can be used to summarize the data set without losing much information.[37]
 Fabrigar et al. (1999)[34] address a number of reasons used to suggest that PCA is not equivalent to factor analysis:
 Factor analysis takes into account the random error that is inherent in measurement, whereas PCA fails to do so. This point is exemplified by Brown (2009),[38] who indicated that, in respect to the correlation matrices involved in the calculations:
 ""In PCA, 1.00s are put in the diagonal meaning that all of the variance in the matrix is to be accounted for (including variance unique to each variable, variance common among variables, and error variance). That would, therefore, by definition, include all of the variance in the variables. In contrast, in EFA, the communalities are put in the diagonal meaning that only the variance shared with other variables is to be accounted for (excluding variance unique to each variable and error variance). That would, therefore, by definition, include only variance that is common among the variables."" For this reason, Brown (2009) recommends using factor analysis when theoretical ideas about relationships between variables exist, whereas PCA should be used if the goal of the researcher is to explore patterns in their data.
 The differences between PCA and factor analysis (FA) are further illustrated by Suhr (2009):[35]
 Charles Spearman was the first psychologist to discuss common factor analysis[39] and did so in his 1904 paper.[40] It provided few details about his methods and was concerned with single-factor models.[41] He discovered that school children's scores on a wide variety of seemingly unrelated subjects were positively correlated, which led him to postulate that a single general mental ability, or g, underlies and shapes human cognitive performance.
 The initial development of common factor analysis with multiple factors was given by Louis Thurstone in two papers in the early 1930s,[42][43] summarized in his 1935 book, The Vector of Mind.[44] Thurstone introduced several important factor analysis concepts, including communality, uniqueness, and rotation.[45] He advocated for ""simple structure"", and developed methods of rotation that could be used as a way to achieve such structure.[39]
 In Q methodology, William Stephenson, a student of Spearman, distinguish between R factor analysis, oriented toward the study of inter-individual differences, and Q factor analysis oriented toward subjective intra-individual differences.[46][47]
 Raymond Cattell was a strong advocate of factor analysis and psychometrics and used Thurstone's multi-factor theory to explain intelligence. Cattell also developed the scree test and similarity coefficients.
 Factor analysis is used to identify ""factors"" that explain a variety of results on different tests. For example, intelligence research found that people who get a high score on a test of verbal ability are also good on other tests that require verbal abilities. Researchers explained this by using factor analysis to isolate one factor, often called verbal intelligence, which represents the degree to which someone is able to solve problems involving verbal skills.[citation needed]
 Factor analysis in psychology is most often associated with intelligence research. However, it also has been used to find factors in a broad range of domains such as personality, attitudes, beliefs, etc. It is linked to psychometrics, as it can assess the validity of an instrument by finding if the instrument indeed measures the postulated factors.[citation needed]
 Factor analysis is a frequently used technique in cross-cultural research. It serves the purpose of extracting cultural dimensions. The best known cultural dimensions models are those elaborated by Geert Hofstede, Ronald Inglehart, Christian Welzel, Shalom Schwartz and Michael Minkov. A popular visualization is Inglehart and Welzel's cultural map of the world.[25]
 In an early 1965 study, political systems around the world are examined via factor analysis to construct related theoretical models and research, compare political systems, and create typological categories.[50] For these purposes, in this study seven basic political dimensions are identified, which are related to a wide variety of political behaviour: these dimensions are Access, Differentiation, Consensus, Sectionalism, Legitimation, Interest, and Leadership Theory and Research.
 Other political scientists explore the measurement of internal political efficacy using four new questions added to the 1988 National Election Study. Factor analysis is here used to find that these items measure a single concept distinct from external efficacy and political trust, and that these four questions provided the best measure of internal political efficacy up to that point in time.[51]
 The basic steps are:
 The data collection stage is usually done by marketing research professionals. Survey questions ask the respondent to rate a product sample or descriptions of product concepts on a range of attributes. Anywhere from five to twenty attributes are chosen. They could include things like: ease of use, weight, accuracy, durability, colourfulness, price, or size. The attributes chosen will vary depending on the product being studied. The same question is asked about all the products in the study. The data for multiple products is coded and input into a statistical program such as R, SPSS, SAS, Stata, STATISTICA, JMP, and SYSTAT.
 The analysis will isolate the underlying factors that explain the data using a matrix of associations.[52] Factor analysis is an interdependence technique. The complete set of interdependent relationships is examined. There is no specification of dependent variables, independent variables, or causality. Factor analysis assumes that all the rating data on different attributes can be reduced down to a few important dimensions. This reduction is possible because some attributes may be related to each other. The rating given to any one attribute is partially the result of the influence of other attributes. The statistical algorithm deconstructs the rating (called a raw score) into its various components and reconstructs the partial scores into underlying factor scores. The degree of correlation between the initial raw score and the final factor score is called a factor loading.
 Factor analysis has also been widely used in physical sciences such as geochemistry,  hydrochemistry,[53] astrophysics and cosmology, as well as biological sciences, such as ecology, molecular biology, neuroscience and biochemistry.
 In groundwater quality management, it is important to relate the spatial distribution of different chemical
parameters to different possible sources, which have different chemical signatures. For example, a sulfide mine is likely to be associated with high levels of acidity, dissolved sulfates and transition metals. These signatures can be identified as factors through R-mode factor analysis, and the location of possible sources can be suggested by contouring the factor scores.[54]
 In geochemistry, different factors can correspond to different mineral associations, and thus to mineralisation.[55]
 Factor analysis can be used for summarizing high-density oligonucleotide DNA microarrays data at probe level for Affymetrix GeneChips. In this case, the latent variable corresponds to the RNA concentration in a sample.[56]
 Factor analysis has been implemented in several statistical analysis programs since the 1980s:
",factor analysi statist method use describ variabl among observ correl variabl term potenti lower number unobserv variabl call factor exampl possibl variat six observ variabl mainli reflect variat two unobserv underli variabl factor analysi search joint variat respons unobserv latent variabl observ variabl model linear combin potenti factor plu error term henc factor analysi thought special case model simpli put factor load variabl quantifi extent variabl relat given factor common rational behind factor analyt method inform gain interdepend observ variabl use later reduc set variabl dataset factor analysi commonli use psychometr person psycholog biolog market product manag oper research financ machin learn may help deal data set larg number observ variabl thought reflect smaller number variabl one commonli use techniqu use relev set variabl show systemat object find latent factor creat common model attempt explain set p p observ n n individu set k k common factor f j j fewer factor per unit observ per unit k p k p individu k k common factor relat observ via factor load matrix l r p k r k singl observ accord matrix notat observ matrix x r p n r n load matrix l r p k r k factor matrix f r k n r n error term matrix ε r p n r n mean matrix r p n r n wherebi th element simpli μ also impos follow assumpt f f suppos c v x σ cov therefor condit impos f f e l f l e f e lf f c v l f ϵ c v l f c v ϵ cov lf give set ψ c v ε cov orthogon matrix q q set l l q lq f q f f criteria factor factor load still hold henc set factor factor load uniqu orthogon transform suppos psychologist hypothesi two kind intellig verbal intellig mathemat intellig neither directli observ note evid hypothesi sought examin score differ academ field student student chosen randomli larg popul student score random variabl psychologist hypothesi may say academ field score averag group student share common pair valu verbal mathemat intellig constant time level verbal intellig plu anoth constant time level mathemat intellig linear combin two factor number particular subject two kind intellig multipli obtain expect score posit hypothesi intellig level pair call factor load subject clarif need exampl hypothesi may hold predict averag student aptitud field astronomi number factor load associ astronomi academ subject may differ factor load two student assum ident degre verbal mathemat intellig may differ measur aptitud astronomi individu aptitud differ averag aptitud predict measur error differ make collect call error statist term mean amount individu measur differ averag predict level intellig see error residu statist observ data go factor analysi would score student total number factor load level two kind intellig student must infer data follow matric indic index variabl subject indic indic use letter b b c c valu run p p equal exampl factor indic indic use letter p p q q r r valu run k k equal exampl instanc sampl indic indic use letter j j k k valu run n n exampl sampl n student particip p exam th student score th exam given x ai purpos factor analysi character correl variabl x x ai particular instanc set observ order variabl equal foot normal standard score z z sampl mean sampl varianc given factor analysi model particular sampl succinctli matrix notat observ doubl scale verbal intellig first compon column f f measur simultan halv factor load verbal intellig make differ model thu gener lost assum standard deviat factor verbal intellig likewis mathemat intellig moreov similar reason gener lost assum two factor uncorrel word δ p q pq kroneck delta p q q p q error assum independ factor sinc rotat solut also solut make interpret factor difficult see disadvantag particular exampl know beforehand two type intellig uncorrel interpret two factor two differ type intellig even uncorrel tell factor correspond verbal intellig correspond mathemat intellig without outsid argument valu load l l averag μ varianc error ε must estim given observ data x x f f assumpt level factor fix given f f fundament theorem may deriv condit term left b b correl matrix p p p matrix deriv product p n n matrix standard observ transpos observ data p p diagon element second term right diagon matrix term less uniti first term right reduc correl matrix equal correl matrix except diagon valu less uniti diagon element reduc correl matrix call commun repres fraction varianc observ variabl account factor sampl data z ai exactli obey fundament equat given due sampl error inadequaci model etc goal analysi model find factor f p pi load ℓ p ap give best fit data factor analysi best fit defin minimum mean squar error residu correl matrix equival minim compon error covari model equat expect valu zero contrast princip compon analysi seek minim mean squar error residu advent comput consider effort devot find approxim solut problem particularli estim commun mean simplifi problem consider yield known reduc correl matrix use estim factor load advent comput minim problem solv iter adequ speed commun calcul process rather need beforehand minr algorithm particularli suit problem hardli iter mean find solut solut factor allow correl rotat exampl correspond mathemat model use skew coordin rather orthogon coordin paramet variabl factor analysi given geometr interpret data z ai factor f p pi error ε ai view vector n n euclidean space sampl space repres z z f p f p ε respect sinc data standard data vector unit length z z factor vector defin k k linear subspac hyperplan space upon data vector project orthogon follow model equat independ factor error f p ε f p exampl hyperplan plane defin two factor vector project data vector onto hyperplan given error vector project point data point perpendicular hyperplan goal factor analysi find hyperplan best fit data sens matter factor vector defin hyperplan chosen long independ lie hyperplan free specifi orthogon normal f p f q δ p q f p f q pq loss gener suitabl set factor found may also arbitrarili rotat within hyperplan rotat factor vector defin hyperplan also solut result exampl fit hyperplan two dimension know beforehand two type intellig uncorrel interpret two factor two differ type intellig even uncorrel tell factor correspond verbal intellig correspond mathemat intellig whether factor linear combin without outsid argument data vector z z unit length entri correl matrix data given r b z z b ab z z b correl matrix geometr interpret cosin angl two data vector z z z b z b diagon element clearli diagon element absolut valu less equal uniti reduc correl matrix defin goal factor analysi choos fit hyperplan reduc correl matrix reproduc correl matrix nearli possibl except diagon element correl matrix known unit valu word goal reproduc accur possibl data specif fit hyperplan mean squar error compon minim accomplish minim respect set orthonorm factor vector seen term right covari error model error covari state diagon matrix minim problem fact yield best fit model yield sampl estim error covari compon minim mean squar sens seen sinc z z orthogon project data vector length less equal length project data vector uniti squar length diagon element reduc correl matrix diagon element reduc correl matrix known commun larg valu commun indic fit hyperplan rather accur reproduc correl matrix mean valu factor must also constrain zero follow mean valu error also zero exploratori factor analysi efa use identifi complex interrelationship among item group item part unifi concept research make priori assumpt relationship among factor confirmatori factor analysi cfa complex approach test hypothesi item associ specif factor cfa use structur equat model test measur model wherebi load factor allow evalu relationship observ variabl unobserv variabl structur equat model approach accommod measur error less restrict estim hypothes model test actual data analysi would demonstr load observ variabl latent variabl factor well correl latent variabl princip compon analysi pca wide use method factor extract first phase efa factor weight comput extract maximum possibl varianc success factor continu meaning varianc left factor model must rotat analysi canon factor analysi also call rao canon factor differ method comput model pca use princip axi method canon factor analysi seek factor highest canon correl observ variabl canon factor analysi unaffect arbitrari rescal data common factor analysi also call princip factor analysi pfa princip axi factor paf seek fewest factor account common varianc correl set variabl imag factor base correl matrix predict variabl rather actual variabl variabl predict other use multipl regress alpha factor base maxim reliabl factor assum variabl randomli sampl univers variabl method assum case sampl variabl fix factor regress model combinatori model factor model regress model altern view hybrid factor model whose factor partial known explain pca perspect factor analysi perspect research wish avoid subject arbitrari criteria factor retent made sens number object method develop solv problem allow user determin appropri rang solut investig howev differ method often disagre one anoth number factor ought retain instanc parallel analysi may suggest factor velic map suggest research may request solut discuss term relat extern data theori horn parallel analysi pa base simul method compar observ eigenvalu obtain uncorrel normal variabl factor compon retain associ eigenvalu bigger percentil distribut eigenvalu deriv random data pa among commonli recommend rule determin number compon retain mani program fail includ option notabl except r howev formann provid theoret empir evid applic might appropri mani case sinc perform consider influenc sampl size item discrimin type correl coeffici velic map test describ courtney involv complet princip compon analysi follow examin seri matric partial correl though quot occur velic cite page number outsid page citat squar correl step see figur averag squar correl unparti correl matrix step first princip compon associ item partial thereaft averag squar correl subsequ correl matrix comput step step first two princip compon partial result averag squar correl comput comput carri k minu one step k repres total number variabl matrix thereaft averag squar correl step line step number analys result lowest averag squar partial correl determin number compon factor retain method compon maintain long varianc correl matrix repres systemat varianc oppos residu error varianc although methodolog akin princip compon analysi map techniqu shown perform quit well determin number factor retain multipl simul studi procedur made avail spss user interfac well psych packag r program languag kaiser criterion kaiser rule drop compon eigenvalu eigenvalu equal inform account averag singl item kaiser criterion default spss statist softwar recommend use sole criterion estim number factor tend factor variat method creat research calcul confid interv eigenvalu retain factor entir confid interv greater scree plot cattel scree test plot compon correspond eigenvalu one move right toward later compon eigenvalu drop drop ceas curv make elbow toward less steep declin cattel scree test say drop compon one start elbow rule sometim criticis amen fudg pick elbow subject curv multipl elbow smooth curv research may tempt set number factor desir research agenda citat need varianc explain criteria research simpli use rule keep enough factor account sometim variat research goal emphas parsimoni explain varianc factor possibl criterion could low place prior distribut number latent factor appli bay theorem bayesian model return probabl distribut number latent factor model use indian buffet process model simpli place discret prior neg binomi distribut number compon output pca maxim varianc account first factor first second factor etc disadvantag procedur item load earli factor item load later variabl make interpret factor read list question load difficult everi question strongli correl first compon question strongli correl last compon rotat serv make output easier interpret choos differ basi princip compon choos differ factor express correl structur possibl creat variabl easili interpret rotat orthogon obliqu obliqu rotat allow factor correl increas flexibl mean rotat possibl may better achiev specifi goal howev also make factor difficult interpret inform includ multipl time differ compon factor may even appear two broad class orthogon rotat exist look spars row row case subject look spars column column variabl difficult interpret factor structur variabl load multipl factor small chang data sometim tip balanc factor rotat criterion complet differ factor rotat produc make difficult compar result differ experi problem illustr comparison differ studi cultur differ studi use differ measur cultur variabl produc differ rotat factor analysi result author studi believ discov someth new invent new name factor found later comparison studi found result rather similar unrot result compar common practic factor rotat obscur similar result differ studi factor analysi statist method consist repeat step factor analysi obliqu rotat factor analysi rotat factor merit enabl research see hierarch structur studi phenomena interpret result one proce either primari factor pattern matrix factor pattern matric gorsuch perhap appli varimax rotat result thompson use solut sl schmid leiman also known transform attribut variat primari factor factor factor analysi relat princip compon analysi pca two ident signific controversi field differ two techniqu pca consid basic version exploratori factor analysi efa develop earli day prior advent comput pca factor analysi aim reduc dimension set data approach taken differ two techniqu factor analysi clearli design object identifi certain unobserv factor observ variabl wherea pca directli address object best pca provid approxim requir factor point view exploratori analysi eigenvalu pca inflat compon load contamin error varianc whilst efa pca treat synonym techniqu field statist criticis factor analysi deal assumpt underli causal structur assum covari observ variabl due presenc one latent variabl factor exert causal influenc observ variabl contrast pca neither assum depend underli causal relationship research argu distinct two techniqu may mean object benefit prefer one base analyt goal factor model incorrectli formul assumpt met factor analysi give erron result factor analysi use success adequ understand system permit good initi model formul pca employ mathemat transform origin data assumpt form covari matrix object pca determin linear combin origin variabl select use summar data set without lose much inform fabrigar et al address number reason use suggest pca equival factor analysi factor analysi take account random error inher measur wherea pca fail point exemplifi brown indic respect correl matric involv calcul pca put diagon mean varianc matrix account includ varianc uniqu variabl varianc common among variabl error varianc would therefor definit includ varianc variabl contrast efa commun put diagon mean varianc share variabl account exclud varianc uniqu variabl error varianc would therefor definit includ varianc common among variabl reason brown recommend use factor analysi theoret idea relationship variabl exist wherea pca use goal research explor pattern data differ pca factor analysi fa illustr suhr charl spearman first psychologist discuss common factor analysi paper provid detail method concern model discov school children score wide varieti seemingli unrel subject posit correl led postul singl gener mental abil g underli shape human cognit perform initi develop common factor analysi multipl factor given loui thurston two paper earli summar book vector mind thurston introduc sever import factor analysi concept includ commun uniqu rotat advoc simpl structur develop method rotat could use way achiev structur q methodolog william stephenson student spearman distinguish r factor analysi orient toward studi differ q factor analysi orient toward subject differ raymond cattel strong advoc factor analysi psychometr use thurston theori explain intellig cattel also develop scree test similar coeffici factor analysi use identifi factor explain varieti result differ test exampl intellig research found peopl get high score test verbal abil also good test requir verbal abil research explain use factor analysi isol one factor often call verbal intellig repres degre someon abl solv problem involv verbal skill citat need factor analysi psycholog often associ intellig research howev also use find factor broad rang domain person attitud belief etc link psychometr assess valid instrument find instrument inde measur postul factor citat need factor analysi frequent use techniqu research serv purpos extract cultur dimens best known cultur dimens model elabor geert hofsted ronald inglehart christian welzel shalom schwartz michael minkov popular visual inglehart welzel cultur map world earli studi polit system around world examin via factor analysi construct relat theoret model research compar polit system creat typolog categori purpos studi seven basic polit dimens identifi relat wide varieti polit behaviour dimens access differenti consensu section legitim interest leadership theori research polit scientist explor measur intern polit efficaci use four new question ad nation elect studi factor analysi use find item measur singl concept distinct extern efficaci polit trust four question provid best measur intern polit efficaci point time basic step data collect stage usual done market research profession survey question ask respond rate product sampl descript product concept rang attribut anywher five twenti attribut chosen could includ thing like eas use weight accuraci durabl colour price size attribut chosen vari depend product studi question ask product studi data multipl product code input statist program r spss sa stata statistica jmp systat analysi isol underli factor explain data use matrix associ factor analysi interdepend techniqu complet set interdepend relationship examin specif depend variabl independ variabl causal factor analysi assum rate data differ attribut reduc import dimens reduct possibl attribut may relat rate given one attribut partial result influenc attribut statist algorithm deconstruct rate call raw score variou compon reconstruct partial score underli factor score degre correl initi raw score final factor score call factor load factor analysi also wide use physic scienc geochemistri hydrochemistri astrophys cosmolog well biolog scienc ecolog molecular biolog neurosci biochemistri groundwat qualiti manag import relat spatial distribut differ chemic paramet differ possibl sourc differ chemic signatur exampl sulfid mine like associ high level acid dissolv sulfat transit metal signatur identifi factor factor analysi locat possibl sourc suggest contour factor score geochemistri differ factor correspond differ miner associ thu mineralis factor analysi use summar oligonucleotid dna microarray data probe level affymetrix genechip case latent variabl correspond rna concentr sampl factor analysi implement sever statist analysi program sinc
Canonical correlation,https://en.wikipedia.org/wiki/Canonical_correlation,"In statistics, canonical-correlation analysis (CCA), also called canonical variates analysis, is a way of inferring information from cross-covariance matrices. If we have two vectors X = (X1, ..., Xn) and Y = (Y1, ..., Ym)  of random variables, and there are correlations among the variables, then canonical-correlation analysis will find linear combinations of X and Y that have a maximum correlation with each other.[1] T. R. Knapp notes that ""virtually all of the commonly encountered parametric tests of significance can be treated as special cases of canonical-correlation analysis, which is the general procedure for investigating the relationships between two sets of variables.""[2] The method was first introduced by Harold Hotelling in 1936,[3] although in the context of angles between flats the mathematical concept was published by Camille Jordan in 1875.[4]
 CCA is now a cornerstone of multivariate statistics and multi-view learning, and a great number of interpretations and extensions have been proposed, such as probabilistic CCA, sparse CCA, multi-view CCA, Deep CCA, and DeepGeoCCA.[5] Unfortunately, perhaps because of its popularity, the literature can be inconsistent with notation, we attempt to highlight such inconsistencies in this article to help the reader make best use of the existing literature and techniques available.
 Like its sister method PCA, CCA can be viewed in population form (corresponding to random vectors and their covariance matrices) or in sample form (corresponding to datasets and their sample covariance matrices). These two forms are almost exact analogues of each other, which is why their distinction is often overlooked, but they can behave very differently in high dimensional settings.[6] We next give explicit mathematical definitions for the population problem and highlight the different objects in the so-called canonical decomposition - understanding the differences between these objects is crucial for interpretation of the technique.
 Given two column vectors 



X
=
(

x

1


,
…
,

x

n



)

T




{\displaystyle X=(x_{1},\dots ,x_{n})^{T}}

 and 



Y
=
(

y

1


,
…
,

y

m



)

T




{\displaystyle Y=(y_{1},\dots ,y_{m})^{T}}

 of random variables with finite second moments, one may define the cross-covariance 




Σ

X
Y


=
cov
⁡
(
X
,
Y
)


{\displaystyle \Sigma _{XY}=\operatorname {cov} (X,Y)}

 to be the 



n
×
m


{\displaystyle n\times m}

 matrix whose 



(
i
,
j
)


{\displaystyle (i,j)}

 entry is the covariance 



cov
⁡
(

x

i


,

y

j


)


{\displaystyle \operatorname {cov} (x_{i},y_{j})}

. In practice, we would estimate the covariance matrix based on sampled data from 



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

 (i.e. from a pair of data matrices).
 Canonical-correlation analysis seeks a sequence of vectors 




a

k




{\displaystyle a_{k}}

  (




a

k


∈


R


n




{\displaystyle a_{k}\in \mathbb {R} ^{n}}

) and 




b

k




{\displaystyle b_{k}}

 (




b

k


∈


R


m




{\displaystyle b_{k}\in \mathbb {R} ^{m}}

) such that the random variables 




a

k


T


X


{\displaystyle a_{k}^{T}X}

 and 




b

k


T


Y


{\displaystyle b_{k}^{T}Y}

 maximize the correlation 



ρ
=
corr
⁡
(

a

k


T


X
,

b

k


T


Y
)


{\displaystyle \rho =\operatorname {corr} (a_{k}^{T}X,b_{k}^{T}Y)}

. The (scalar) random variables 



U
=

a

1


T


X


{\displaystyle U=a_{1}^{T}X}

 and 



V
=

b

1


T


Y


{\displaystyle V=b_{1}^{T}Y}

 are the first pair of canonical variables. Then one seeks vectors maximizing the same correlation subject to the constraint that they are to be uncorrelated with the first pair of canonical variables; this gives the second pair of canonical variables. This procedure may be continued up to 



min
{
m
,
n
}


{\displaystyle \min\{m,n\}}

 times.
 The sets of vectors 




a

k


,

b

k




{\displaystyle a_{k},b_{k}}

 are called canonical directions or weight vectors or simply weights. The 'dual' sets of vectors 




Σ

X
X



a

k


,

Σ

Y
Y



b

k




{\displaystyle \Sigma _{XX}a_{k},\Sigma _{YY}b_{k}}

 are called canonical loading vectors or simply loadings; these are often more straightforward to interpret than the weights.[7]
 Let 




Σ

X
Y




{\displaystyle \Sigma _{XY}}

 be the cross-covariance matrix for any pair of (vector-shaped) random variables 



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

. The target function to maximize is
 The first step is to define a change of basis and define
 where 




Σ

X
X


1

/

2




{\displaystyle \Sigma _{XX}^{1/2}}

 and 




Σ

Y
Y


1

/

2




{\displaystyle \Sigma _{YY}^{1/2}}

 can be obtained from the eigen-decomposition (or by diagonalization):
 and
 Thus
 By the Cauchy–Schwarz inequality,
 There is equality if the vectors 



d


{\displaystyle d}

 and 




Σ

Y
Y


−
1

/

2



Σ

Y
X



Σ

X
X


−
1

/

2


c


{\displaystyle \Sigma _{YY}^{-1/2}\Sigma _{YX}\Sigma _{XX}^{-1/2}c}

 are collinear. In addition, the maximum of correlation is attained if 



c


{\displaystyle c}

 is the eigenvector with the maximum eigenvalue for the matrix 




Σ

X
X


−
1

/

2



Σ

X
Y



Σ

Y
Y


−
1



Σ

Y
X



Σ

X
X


−
1

/

2




{\displaystyle \Sigma _{XX}^{-1/2}\Sigma _{XY}\Sigma _{YY}^{-1}\Sigma _{YX}\Sigma _{XX}^{-1/2}}

 (see Rayleigh quotient). The subsequent pairs are found by using eigenvalues of decreasing magnitudes. Orthogonality is guaranteed by the symmetry of the correlation matrices.
 Another way of viewing this computation is that 



c


{\displaystyle c}

 and 



d


{\displaystyle d}

 are the left and right singular vectors of the correlation matrix of X and Y corresponding to the highest singular value.
 The solution is therefore:
 Reciprocally, there is also:
 Reversing the change of coordinates, we have that
 The canonical variables are defined by:
 CCA can be computed using singular value decomposition on a correlation matrix.[8] It is available as a function in[9]
 CCA computation using singular value decomposition on a correlation matrix is related to the cosine of the angles between flats. The cosine function is ill-conditioned for small angles, leading to very inaccurate computation of highly correlated principal vectors in finite precision computer arithmetic. To  fix this trouble, alternative algorithms[11] are available in
 Each row can be tested for significance with the following method. Since the correlations are sorted, saying that row 



i


{\displaystyle i}

 is zero implies all further correlations are also zero.  If we have 



p


{\displaystyle p}

 independent observations in a sample and 







ρ
^




i




{\displaystyle {\widehat {\rho }}_{i}}

 is the estimated correlation for 



i
=
1
,
…
,
min
{
m
,
n
}


{\displaystyle i=1,\dots ,\min\{m,n\}}

. For the 



i


{\displaystyle i}

th row, the test statistic is:
 which is asymptotically distributed as a chi-squared with 



(
m
−
i
+
1
)
(
n
−
i
+
1
)


{\displaystyle (m-i+1)(n-i+1)}

 degrees of freedom for large 



p


{\displaystyle p}

.[12]  Since all the correlations from 



min
{
m
,
n
}


{\displaystyle \min\{m,n\}}

 to 



p


{\displaystyle p}

 are logically zero (and estimated that way also) the product for the terms after this point is irrelevant.
 Note that in the small sample size limit with 



p
<
n
+
m


{\displaystyle p<n+m}

 then we are guaranteed that the top 



m
+
n
−
p


{\displaystyle m+n-p}

 correlations will be identically 1 and hence the test is meaningless.[13]
 A typical use for canonical correlation in the experimental context is to take two sets of variables and see what is common among the two sets.[14] For example, in psychological testing, one could take two well established multidimensional personality tests such as the Minnesota Multiphasic Personality Inventory (MMPI-2) and the NEO. By seeing how the MMPI-2 factors relate to the NEO factors, one could gain insight into what dimensions were common between the tests and how much variance was shared. For example, one might find that an extraversion or neuroticism dimension accounted for a substantial amount of shared variance between the two tests.
 One can also use canonical-correlation analysis to produce a model equation which relates two sets of variables, for example a set of performance measures and a set of explanatory variables, or a set of outputs and set of inputs. Constraint restrictions can be imposed on such a model to ensure it reflects theoretical requirements or intuitively obvious conditions. This type of model is known as a maximum correlation model.[15]
 Visualization of the results of canonical correlation is usually through bar plots of the coefficients of the two sets of variables for the pairs of canonical variates showing significant correlation. Some authors suggest that they are best visualized by plotting them as heliographs, a circular format with ray like bars, with each half representing the two sets of variables.[16]
 Let 



X
=

x

1




{\displaystyle X=x_{1}}

 with zero expected value, i.e., 



E
⁡
(
X
)
=
0


{\displaystyle \operatorname {E} (X)=0}

. 
 We notice that in both cases 



U
=
V


{\displaystyle U=V}

, which illustrates that the canonical-correlation analysis treats correlated and anticorrelated variables similarly.
 Assuming that 



X
=
(

x

1


,
…
,

x

n



)

T




{\displaystyle X=(x_{1},\dots ,x_{n})^{T}}

 and 



Y
=
(

y

1


,
…
,

y

m



)

T




{\displaystyle Y=(y_{1},\dots ,y_{m})^{T}}

 have zero expected values, i.e., 



E
⁡
(
X
)
=
E
⁡
(
Y
)
=
0


{\displaystyle \operatorname {E} (X)=\operatorname {E} (Y)=0}

, their covariance  matrices 




Σ

X
X


=
Cov
⁡
(
X
,
X
)
=
E
⁡
[
X

X

T


]


{\displaystyle \Sigma _{XX}=\operatorname {Cov} (X,X)=\operatorname {E} [XX^{T}]}

 and 




Σ

Y
Y


=
Cov
⁡
(
Y
,
Y
)
=
E
⁡
[
Y

Y

T


]


{\displaystyle \Sigma _{YY}=\operatorname {Cov} (Y,Y)=\operatorname {E} [YY^{T}]}

 can be viewed as Gram matrices in an inner product for the entries of  



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

, correspondingly. In this interpretation, the random variables, entries 




x

i




{\displaystyle x_{i}}

 of  



X


{\displaystyle X}

 and 




y

j




{\displaystyle y_{j}}

 of 



Y


{\displaystyle Y}

 are treated as elements of a vector space with an inner product given by the covariance 



cov
⁡
(

x

i


,

y

j


)


{\displaystyle \operatorname {cov} (x_{i},y_{j})}

; see Covariance#Relationship to inner products.
 The definition of the canonical variables 



U


{\displaystyle U}

 and 



V


{\displaystyle V}

 is then equivalent to the definition of principal vectors for the pair of subspaces spanned by the entries of  



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

 with respect to this  inner product. The canonical correlations 



corr
⁡
(
U
,
V
)


{\displaystyle \operatorname {corr} (U,V)}

 is equal to the cosine of principal angles.
 CCA can also be viewed as a special whitening transformation where the random vectors 



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

 are simultaneously transformed in such a way that the cross-correlation between the whitened vectors 




X

C
C
A




{\displaystyle X^{CCA}}

 and 




Y

C
C
A




{\displaystyle Y^{CCA}}

 is diagonal.[17]
The canonical correlations are then interpreted as regression coefficients linking 




X

C
C
A




{\displaystyle X^{CCA}}

 and 




Y

C
C
A




{\displaystyle Y^{CCA}}

 and may also be negative.  The regression view of CCA also provides a way to construct a latent variable probabilistic generative model for CCA, with uncorrelated hidden variables representing shared and non-shared variability.
 
",statist analysi cca also call canon variat analysi way infer inform matric two vector x xn ym random variabl correl among variabl analysi find linear combin x maximum correl knapp note virtual commonli encount parametr test signific treat special case analysi gener procedur investig relationship two set variabl method first introduc harold hotel although context angl flat mathemat concept publish camil jordan cca cornerston multivari statist learn great number interpret extens propos probabilist cca spars cca cca deep cca deepgeocca unfortun perhap popular literatur inconsist notat attempt highlight inconsist articl help reader make best use exist literatur techniqu avail like sister method pca cca view popul form correspond random vector covari matric sampl form correspond dataset sampl covari matric two form almost exact analogu distinct often overlook behav differ high dimension set next give explicit mathemat definit popul problem highlight differ object canon decomposit understand differ object crucial interpret techniqu given two column vector x x x n n random variabl finit second moment one may defin σ x cov x xy cov x n matrix whose j j entri covari cov x j cov j practic would estim covari matrix base sampl data x x pair data matric analysi seek sequenc vector k k k r n k r n b k k b k r k r random variabl k x k x b k k maxim correl ρ corr k x b k corr k x k scalar random variabl u x x v b first pair canon variabl one seek vector maxim correl subject constraint uncorrel first pair canon variabl give second pair canon variabl procedur may continu min n time set vector k b k k k call canon direct weight vector simpli weight set vector σ x x k σ b k xx k yy k call canon load vector simpli load often straightforward interpret weight let σ x xy matrix pair random variabl x x target function maxim first step defin chang basi defin σ x x xx σ yy obtain diagon thu inequ equal vector σ σ x σ x x c yy yx xx c collinear addit maximum correl attain c c eigenvector maximum eigenvalu matrix σ x x σ x σ σ x σ x x xx xy yy yx xx see rayleigh quotient subsequ pair found use eigenvalu decreas magnitud orthogon guarante symmetri correl matric anoth way view comput c c left right singular vector correl matrix x correspond highest singular valu solut therefor reciproc also revers chang coordin canon variabl defin cca comput use singular valu decomposit correl matrix avail function cca comput use singular valu decomposit correl matrix relat cosin angl flat cosin function small angl lead inaccur comput highli correl princip vector finit precis comput arithmet fix troubl altern algorithm avail row test signific follow method sinc correl sort say row zero impli correl also zero p p independ observ sampl ρ estim correl min n th row test statist asymptot distribut n degre freedom larg p p sinc correl min n p p logic zero estim way also product term point irrelev note small sampl size limit p n p guarante top n p correl ident henc test meaningless typic use canon correl experiment context take two set variabl see common among two set exampl psycholog test one could take two well establish multidimension person test minnesota multiphas person inventori neo see factor relat neo factor one could gain insight dimens common test much varianc share exampl one might find extravers neurotic dimens account substanti amount share varianc two test one also use analysi produc model equat relat two set variabl exampl set perform measur set explanatori variabl set output set input constraint restrict impos model ensur reflect theoret requir intuit obviou condit type model known maximum correl model visual result canon correl usual bar plot coeffici two set variabl pair canon variat show signific correl author suggest best visual plot heliograph circular format ray like bar half repres two set variabl let x x zero expect valu e x e x notic case u v illustr analysi treat correl anticorrel variabl similarli assum x x x n n zero expect valu e x e e x e covari matric σ x x cov x x e x x xx cov x x e σ cov e yy cov e view gram matric inner product entri x x correspondingli interpret random variabl entri x x x j j treat element vector space inner product given covari cov x j cov j see covari relationship inner product definit canon variabl u u v v equival definit princip vector pair subspac span entri x x respect inner product canon correl corr u v corr u v equal cosin princip angl cca also view special whiten transform random vector x x simultan transform way whiten vector x c c cca c c cca diagon canon correl interpret regress coeffici link x c c cca c c cca may also neg regress view cca also provid way construct latent variabl probabilist gener model cca uncorrel hidden variabl repres share variabl
Independent component analysis,https://en.wikipedia.org/wiki/Independent_component_analysis,"In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that at most one subcomponent is Gaussian and that the subcomponents are statistically independent from each other.[1] ICA was invented by Jeanny Hérault and Christian Jutten in 1985.[2] ICA is a special case of blind source separation. A common example application of ICA is the ""cocktail party problem"" of listening in on one person's speech in a noisy room.[3]
 Independent component analysis attempts to decompose a multivariate signal into independent non-Gaussian signals. As an example, sound is usually a signal that is composed of the numerical addition, at each time t, of signals from several sources. The question then is whether it is possible to separate these contributing sources from the observed total signal. When the statistical independence assumption is correct, blind ICA separation of a mixed signal gives very good results.[5] It is also used for signals that are not supposed to be generated by mixing for analysis purposes.
 A simple application of ICA is the ""cocktail party problem"", where the underlying speech signals are separated from a sample data consisting of people talking simultaneously in a room. Usually the problem is simplified by assuming no time delays or echoes. Note that a filtered and delayed signal is a copy of a dependent component, and thus the statistical independence assumption is not violated.
 Mixing weights for constructing the 



M


{\textstyle M}

 observed signals from the 



N


{\textstyle N}

 components can be placed in an 



M
×
N


{\textstyle M\times N}

 matrix. An important thing to consider is that if 



N


{\textstyle N}

 sources are present, at least 



N


{\textstyle N}

 observations (e.g. microphones if the observed signal is audio) are needed to recover the original signals. When there are an equal number of observations and source signals, the mixing matrix is square (



M
=
N


{\textstyle M=N}

). Other cases of underdetermined (



M
<
N


{\textstyle M<N}

) and overdetermined (



M
>
N


{\textstyle M>N}

) have been investigated.
 The success of ICA separation of mixed signals relies on two assumptions and three effects of mixing source signals. Two assumptions:
 Three effects of mixing source signals:
 Those principles contribute to the basic establishment of ICA. If the signals extracted from a set of mixtures are independent and have non-Gaussian distributions or have low complexity, then they must be source signals.[6][7]
 ICA finds the independent components (also called factors, latent variables or sources) by maximizing the statistical independence of the estimated components. We may choose one of many ways to define a proxy for independence, and this choice governs the form of the ICA algorithm. The two broadest definitions of independence for ICA are
 The Minimization-of-Mutual information (MMI) family of ICA algorithms uses measures like Kullback-Leibler Divergence and maximum entropy.  The non-Gaussianity family of ICA algorithms, motivated by the central limit theorem, uses kurtosis and negentropy.[8]
 Typical algorithms for ICA use centering (subtract the mean to create a zero mean signal), whitening (usually with the eigenvalue decomposition), and dimensionality reduction as preprocessing steps in order to simplify and reduce the complexity of the problem for the actual iterative algorithm. Whitening and dimension reduction can be achieved with principal component analysis or singular value decomposition. Whitening ensures that all dimensions are treated equally a priori before the algorithm is run. Well-known algorithms for ICA include infomax, FastICA, JADE, and kernel-independent component analysis, among others. In general, ICA cannot identify the actual number of source signals, a uniquely correct ordering of the source signals, nor the proper scaling (including sign) of the source signals.
 ICA is important to blind signal separation and has many practical applications. It is closely related to (or even a special case of) the search for a factorial code of the data, i.e., a new vector-valued representation of each data vector such that it gets uniquely encoded by the resulting code vector (loss-free coding), but the code components are statistically independent.
 Linear independent component analysis can be divided into noiseless and noisy cases, where noiseless ICA is a special case of noisy ICA. Nonlinear ICA should be considered as a separate case.
 The data are represented by the observed random vector 




x

=
(

x

1


,
…
,

x

m



)

T




{\displaystyle {\boldsymbol {x}}=(x_{1},\ldots ,x_{m})^{T}}

 and the hidden components as the random vector 




s

=
(

s

1


,
…
,

s

n



)

T


.


{\displaystyle {\boldsymbol {s}}=(s_{1},\ldots ,s_{n})^{T}.}

 The task is to transform the observed data 




x

,


{\displaystyle {\boldsymbol {x}},}

 using a linear static transformation 




W



{\displaystyle {\boldsymbol {W}}}

 as 




s

=

W


x

,


{\displaystyle {\boldsymbol {s}}={\boldsymbol {W}}{\boldsymbol {x}},}

 into a vector of maximally independent components 




s



{\displaystyle {\boldsymbol {s}}}

 measured by some function 



F
(

s

1


,
…
,

s

n


)


{\displaystyle F(s_{1},\ldots ,s_{n})}

 of independence.
 The components 




x

i




{\displaystyle x_{i}}

 of the observed random vector 




x

=
(

x

1


,
…
,

x

m



)

T




{\displaystyle {\boldsymbol {x}}=(x_{1},\ldots ,x_{m})^{T}}

 are generated as a sum of the independent components 




s

k




{\displaystyle s_{k}}

, 



k
=
1
,
…
,
n


{\displaystyle k=1,\ldots ,n}

:
 




x

i


=

a

i
,
1



s

1


+
⋯
+

a

i
,
k



s

k


+
⋯
+

a

i
,
n



s

n




{\displaystyle x_{i}=a_{i,1}s_{1}+\cdots +a_{i,k}s_{k}+\cdots +a_{i,n}s_{n}}


 weighted by the mixing weights 




a

i
,
k




{\displaystyle a_{i,k}}

.
 The same generative model can be written in vector form as 




x

=

∑

k
=
1


n



s

k




a


k




{\displaystyle {\boldsymbol {x}}=\sum _{k=1}^{n}s_{k}{\boldsymbol {a}}_{k}}

, where the observed random vector 




x



{\displaystyle {\boldsymbol {x}}}

 is represented by the basis vectors 





a


k


=
(


a


1
,
k


,
…
,


a


m
,
k



)

T




{\displaystyle {\boldsymbol {a}}_{k}=({\boldsymbol {a}}_{1,k},\ldots ,{\boldsymbol {a}}_{m,k})^{T}}

. The basis vectors 





a


k




{\displaystyle {\boldsymbol {a}}_{k}}

 form the columns of the mixing matrix 




A

=
(


a


1


,
…
,


a


n


)


{\displaystyle {\boldsymbol {A}}=({\boldsymbol {a}}_{1},\ldots ,{\boldsymbol {a}}_{n})}

 and the generative formula can be written as 




x

=

A


s



{\displaystyle {\boldsymbol {x}}={\boldsymbol {A}}{\boldsymbol {s}}}

, where 




s

=
(

s

1


,
…
,

s

n



)

T




{\displaystyle {\boldsymbol {s}}=(s_{1},\ldots ,s_{n})^{T}}

.
 Given the model and realizations (samples) 





x


1


,
…
,


x


N




{\displaystyle {\boldsymbol {x}}_{1},\ldots ,{\boldsymbol {x}}_{N}}

 of the random vector 




x



{\displaystyle {\boldsymbol {x}}}

, the task is to estimate both the mixing matrix 




A



{\displaystyle {\boldsymbol {A}}}

 and the sources 




s



{\displaystyle {\boldsymbol {s}}}

. This is done by adaptively calculating the 




w



{\displaystyle {\boldsymbol {w}}}

 vectors and setting up a cost function which either maximizes the non-gaussianity of the calculated 




s

k


=


w


T



x



{\displaystyle s_{k}={\boldsymbol {w}}^{T}{\boldsymbol {x}}}

 or minimizes the mutual information. In some cases, a priori knowledge of the probability distributions of the sources can be used in the cost function.
 The original sources 




s



{\displaystyle {\boldsymbol {s}}}

 can be recovered by multiplying the observed signals 




x



{\displaystyle {\boldsymbol {x}}}

 with the inverse of the mixing matrix 




W

=


A


−
1




{\displaystyle {\boldsymbol {W}}={\boldsymbol {A}}^{-1}}

, also known as the unmixing matrix. Here it is assumed that the mixing matrix is square (



n
=
m


{\displaystyle n=m}

). If the number of basis vectors is greater than the dimensionality of the observed vectors, 



n
>
m


{\displaystyle n>m}

, the task is overcomplete but is still solvable with the pseudo inverse.
 With the added assumption of zero-mean and uncorrelated Gaussian noise 



n
∼
N
(
0
,
diag
⁡
(
Σ
)
)


{\displaystyle n\sim N(0,\operatorname {diag} (\Sigma ))}

, the ICA model takes the form 




x

=

A


s

+
n


{\displaystyle {\boldsymbol {x}}={\boldsymbol {A}}{\boldsymbol {s}}+n}

.
 The mixing of the sources does not need to be linear. Using a nonlinear mixing function 



f
(
⋅

|

θ
)


{\displaystyle f(\cdot |\theta )}

 with parameters 



θ


{\displaystyle \theta }

 the nonlinear ICA model is 



x
=
f
(
s

|

θ
)
+
n


{\displaystyle x=f(s|\theta )+n}

.
 The independent components are identifiable up to a permutation and scaling of the sources.[9] This identifiability requires that:
 A special variant of ICA is binary ICA in which both signal sources and monitors are in binary form and observations from monitors are disjunctive mixtures of binary independent sources. The problem was shown to have applications in many domains including medical diagnosis, multi-cluster assignment, network tomography and internet resource management.
 Let 





x

1


,

x

2


,
…
,

x

m





{\displaystyle {x_{1},x_{2},\ldots ,x_{m}}}

 be the set of binary variables from 



m


{\displaystyle m}

 monitors and 





y

1


,

y

2


,
…
,

y

n





{\displaystyle {y_{1},y_{2},\ldots ,y_{n}}}

 be the set of binary variables from 



n


{\displaystyle n}

 sources. Source-monitor connections are represented by the (unknown) mixing matrix 




G



{\textstyle {\boldsymbol {G}}}

, where 




g

i
j


=
1


{\displaystyle g_{ij}=1}

 indicates that signal from the i-th source can be observed by the j-th monitor. The system works as follows: at any time, if a source 



i


{\displaystyle i}

 is active (




y

i


=
1


{\displaystyle y_{i}=1}

) and it is connected to the monitor 



j


{\displaystyle j}

 (




g

i
j


=
1


{\displaystyle g_{ij}=1}

) then the monitor 



j


{\displaystyle j}

 will observe some activity (




x

j


=
1


{\displaystyle x_{j}=1}

). Formally we have:
 where 



∧


{\displaystyle \wedge }

 is Boolean AND and 



∨


{\displaystyle \vee }

 is Boolean OR. Noise is not explicitly modelled, rather, can be treated as independent sources.
 The above problem can be heuristically solved[10] by assuming variables are continuous and running FastICA on binary observation data to get the mixing matrix 




G



{\textstyle {\boldsymbol {G}}}

 (real values), then apply round number techniques on 




G



{\textstyle {\boldsymbol {G}}}

 to obtain the binary values. This approach has been shown to produce a highly inaccurate result.[citation needed]
 Another method is to use dynamic programming: recursively breaking the observation matrix 




X



{\textstyle {\boldsymbol {X}}}

 into its sub-matrices and run the inference algorithm on these sub-matrices. The key observation which leads to this algorithm is the sub-matrix 





X


0




{\textstyle {\boldsymbol {X}}^{0}}

 of 




X



{\textstyle {\boldsymbol {X}}}

 where 




x

i
j


=
0
,
∀
j


{\textstyle x_{ij}=0,\forall j}

 corresponds to the unbiased observation matrix of hidden components that do not have connection to the 



i


{\displaystyle i}

-th monitor. Experimental results from[11] show that this approach is accurate under moderate noise levels.
 The Generalized Binary ICA framework[12] introduces a broader problem formulation which does not necessitate any knowledge on the generative model. In other words, this method attempts to decompose a source into its independent components (as much as possible, and without losing any information) with no prior assumption on the way it was generated. Although this problem appears quite complex, it can be accurately solved with a branch and bound search tree algorithm or tightly upper bounded with a single multiplication of a matrix with a vector.
 Signal mixtures tend to have Gaussian probability density functions, and source signals tend to have non-Gaussian probability density functions.  Each source signal can be extracted from a set of signal mixtures by taking the inner product of a weight vector and those signal mixtures where this inner product provides an orthogonal projection of the signal mixtures.  The remaining challenge is finding such a weight vector. One type of method for doing so is projection pursuit.[13][14]
 Projection pursuit seeks one projection at a time such that the extracted signal is as non-Gaussian as possible. This contrasts with ICA, which typically extracts M signals simultaneously from M signal mixtures, which requires estimating a M × M unmixing matrix. One practical advantage of projection pursuit over ICA is that fewer than M signals can be extracted if required, where each source signal is extracted from M signal mixtures using an M-element weight vector.
 We can use kurtosis to recover the multiple source signal by finding the correct weight vectors with the use of projection pursuit.
 The kurtosis of the probability density function of a signal, for a finite sample, is computed as
 where 





y
¯




{\displaystyle \mathbf {\overline {y}} }

 is the sample mean of 




y



{\displaystyle \mathbf {y} }

, the extracted signals. The constant 3 ensures that Gaussian signals have zero kurtosis, Super-Gaussian signals have positive kurtosis, and Sub-Gaussian signals have negative kurtosis. The denominator is the variance of 




y



{\displaystyle \mathbf {y} }

, and ensures that the measured kurtosis takes account of signal variance. The goal of projection pursuit is to maximize the kurtosis, and make the extracted signal as non-normal as possible.
 Using kurtosis as a measure of non-normality, we can now examine how the kurtosis of a signal 




y

=


w


T



x



{\displaystyle \mathbf {y} =\mathbf {w} ^{T}\mathbf {x} }

 extracted from a set of M mixtures 




x

=
(

x

1


,

x

2


,
…
,

x

M



)

T




{\displaystyle \mathbf {x} =(x_{1},x_{2},\ldots ,x_{M})^{T}}

 varies as the weight vector 




w



{\displaystyle \mathbf {w} }

 is rotated around the origin. Given our assumption that each source signal 




s



{\displaystyle \mathbf {s} }

 is super-gaussian we would expect:
 For multiple source mixture signals, we can use kurtosis and Gram-Schmidt Orthogonalization (GSO) to recover the signals. Given M signal mixtures in an M-dimensional space, GSO project these data points onto an (M-1)-dimensional space by using the weight vector. We can guarantee the independence of the extracted signals with the use of GSO.
 In order to find the correct value of 




w



{\displaystyle \mathbf {w} }

, we can use gradient descent method.  We first of all whiten the data, and transform 




x



{\displaystyle \mathbf {x} }

 into a new mixture 




z



{\displaystyle \mathbf {z} }

, which has unit variance, and 




z

=
(

z

1


,

z

2


,
…
,

z

M



)

T




{\displaystyle \mathbf {z} =(z_{1},z_{2},\ldots ,z_{M})^{T}}

. This process can be achieved by applying Singular value decomposition to 




x



{\displaystyle \mathbf {x} }

,
 Rescaling each vector 




U

i


=

U

i



/

E
⁡
(

U

i


2


)


{\displaystyle U_{i}=U_{i}/\operatorname {E} (U_{i}^{2})}

, and let 




z

=

U



{\displaystyle \mathbf {z} =\mathbf {U} }

. The signal extracted by a weighted vector 




w



{\displaystyle \mathbf {w} }

 is 




y

=


w


T



z



{\displaystyle \mathbf {y} =\mathbf {w} ^{T}\mathbf {z} }

. If the weight vector w has unit length, then the variance of y is also 1, that is 



E
⁡
[
(


w


T



z


)

2


]
=
1


{\displaystyle \operatorname {E} [(\mathbf {w} ^{T}\mathbf {z} )^{2}]=1}

. The kurtosis can thus be written as:
 The updating process for 




w



{\displaystyle \mathbf {w} }

 is:
 where 



η


{\displaystyle \eta }

 is a small constant to guarantee that 




w



{\displaystyle \mathbf {w} }

 converges to the optimal solution. After each update, we normalize 





w


n
e
w


=




w


n
e
w




|



w


n
e
w



|






{\displaystyle \mathbf {w} _{new}={\frac {\mathbf {w} _{new}}{|\mathbf {w} _{new}|}}}

, and set 





w


o
l
d


=


w


n
e
w




{\displaystyle \mathbf {w} _{old}=\mathbf {w} _{new}}

, and repeat the updating process until convergence. We can also use another algorithm to update the weight vector 




w



{\displaystyle \mathbf {w} }

.
 Another approach is using negentropy[8][15] instead of kurtosis. Using negentropy is a more robust method than kurtosis, as kurtosis is very sensitive to outliers. The negentropy methods are based on an important property of Gaussian distribution: a Gaussian variable has the largest entropy among all continuous random variables of equal variance. This is also the reason why we want to find the most nongaussian variables. A simple proof can be found in Differential entropy.
 y is a Gaussian random variable of the same covariance matrix as x
 An approximation for negentropy is
 A proof can be found in the original papers of Comon;[16][8] it has been reproduced in the book Independent Component Analysis by Aapo Hyvärinen, Juha Karhunen, and Erkki Oja[17] This approximation also suffers from the same problem as kurtosis (sensitivity to outliers). Other approaches have been developed.[18]
 A choice of 




G

1




{\displaystyle G_{1}}

 and 




G

2




{\displaystyle G_{2}}

 are 
 Infomax ICA[19] is essentially a multivariate, parallel version of projection pursuit. Whereas projection pursuit extracts a series of signals one at a time from a set of M signal mixtures, ICA extracts M signals in parallel. This tends to make ICA more robust than projection pursuit.[20]
 The projection pursuit method uses Gram-Schmidt orthogonalization to ensure the independence of the extracted signal, while ICA use infomax and maximum likelihood estimate to ensure the independence of the extracted signal. The Non-Normality of the extracted signal is achieved by assigning an appropriate model, or prior, for the signal.
 The process of ICA based on infomax in short is: given a set of signal mixtures 




x



{\displaystyle \mathbf {x} }

 and a set of identical independent model cumulative distribution functions(cdfs) 



g


{\displaystyle g}

, we seek the unmixing matrix 




W



{\displaystyle \mathbf {W} }

 which maximizes the joint entropy of the signals 




Y

=
g
(

y

)


{\displaystyle \mathbf {Y} =g(\mathbf {y} )}

, where 




y

=

W
x



{\displaystyle \mathbf {y} =\mathbf {Wx} }

 are the signals extracted by 




W



{\displaystyle \mathbf {W} }

. Given the optimal 




W



{\displaystyle \mathbf {W} }

, the signals 




Y



{\displaystyle \mathbf {Y} }

 have maximum entropy and are therefore independent, which ensures that the extracted signals 




y

=

g

−
1


(

Y

)


{\displaystyle \mathbf {y} =g^{-1}(\mathbf {Y} )}

 are also independent. 



g


{\displaystyle g}

 is an invertible function, and is the signal model. Note that if the source signal model probability density function 




p

s




{\displaystyle p_{s}}

 matches the probability density function of the extracted signal 




p


y





{\displaystyle p_{\mathbf {y} }}

, then maximizing the joint entropy of 



Y


{\displaystyle Y}

 also maximizes the amount of mutual information between 




x



{\displaystyle \mathbf {x} }

 and 




Y



{\displaystyle \mathbf {Y} }

. For this reason, using entropy to extract independent signals is known as infomax.
 Consider the entropy of the vector variable 




Y

=
g
(

y

)


{\displaystyle \mathbf {Y} =g(\mathbf {y} )}

, where 




y

=

W
x



{\displaystyle \mathbf {y} =\mathbf {Wx} }

 is the set of signals extracted by the unmixing matrix 




W



{\displaystyle \mathbf {W} }

. For a finite set of values sampled from a distribution with pdf 




p


y





{\displaystyle p_{\mathbf {y} }}

, the entropy of 




Y



{\displaystyle \mathbf {Y} }

 can be estimated as:
 The joint pdf 




p


Y





{\displaystyle p_{\mathbf {Y} }}

 can be shown to be related to the joint pdf 




p


y





{\displaystyle p_{\mathbf {y} }}

 of the extracted signals by the multivariate form:
 where 




J

=



∂

Y



∂

y






{\displaystyle \mathbf {J} ={\frac {\partial \mathbf {Y} }{\partial \mathbf {y} }}}

 is the Jacobian matrix. We have 




|


J


|

=

g
′

(

y

)


{\displaystyle |\mathbf {J} |=g'(\mathbf {y} )}

, and 




g
′



{\displaystyle g'}

 is the pdf assumed for source signals 




g
′

=

p

s




{\displaystyle g'=p_{s}}

, therefore,
 therefore,
 We know that when 




p


y



=

p

s




{\displaystyle p_{\mathbf {y} }=p_{s}}

, 




p


Y





{\displaystyle p_{\mathbf {Y} }}

 is of uniform distribution, and 



H
(


Y


)


{\displaystyle H({\mathbf {Y} })}

 is maximized. Since
 where 




|


W


|



{\displaystyle |\mathbf {W} |}

 is the absolute value of the determinant of the unmixing matrix 




W



{\displaystyle \mathbf {W} }

. Therefore,
 so,
 since 



H
(

x

)
=
−


1
N



∑

t
=
1


N


ln
⁡

p


x



(


x


t


)


{\displaystyle H(\mathbf {x} )=-{\frac {1}{N}}\sum _{t=1}^{N}\ln p_{\mathbf {x} }(\mathbf {x} ^{t})}

, and maximizing 




W



{\displaystyle \mathbf {W} }

 does not affect 




H


x





{\displaystyle H_{\mathbf {x} }}

, so we can maximize the function
 to achieve the independence of the extracted signal.
 If there are M marginal pdfs of the model joint pdf 




p


s





{\displaystyle p_{\mathbf {s} }}

 are independent and use the commonly super-gaussian model pdf for the source signals 




p


s



=
(
1
−
tanh
⁡
(

s


)

2


)


{\displaystyle p_{\mathbf {s} }=(1-\tanh(\mathbf {s} )^{2})}

, then we have
 In the sum, given an observed signal mixture  




x



{\displaystyle \mathbf {x} }

, the corresponding set of extracted signals  




y



{\displaystyle \mathbf {y} }

  and source signal model 




p


s



=

g
′



{\displaystyle p_{\mathbf {s} }=g'}

,  we can find the optimal unmixing matrix 




W



{\displaystyle \mathbf {W} }

, and make the extracted signals independent and non-gaussian. Like the projection pursuit situation, we can use gradient descent method to find the optimal solution of the unmixing matrix.
 Maximum likelihood estimation (MLE) is a standard statistical tool for finding parameter values (e.g. the unmixing matrix 




W



{\displaystyle \mathbf {W} }

) that provide the best fit of some data (e.g., the extracted signals 



y


{\displaystyle y}

) to a given a model (e.g., the assumed joint probability density function (pdf) 




p

s




{\displaystyle p_{s}}

 of source signals).[20]
 The ML ""model"" includes a specification of a pdf, which in this case is the pdf 




p

s




{\displaystyle p_{s}}

 of the unknown source signals 



s


{\displaystyle s}

. Using ML ICA, the objective is to find an unmixing matrix that yields extracted signals 



y
=

W

x


{\displaystyle y=\mathbf {W} x}

 with a joint pdf as similar as possible to the joint pdf 




p

s




{\displaystyle p_{s}}

 of the unknown source signals 



s


{\displaystyle s}

.
 MLE is thus based on the assumption that if the model pdf 




p

s




{\displaystyle p_{s}}

 and the model parameters 




A



{\displaystyle \mathbf {A} }

 are correct then a high probability should be obtained for the data 



x


{\displaystyle x}

 that were actually observed. Conversely, if 




A



{\displaystyle \mathbf {A} }

 is far from the correct parameter values then a low probability of the observed data would be expected.
 Using MLE, we call the probability of the observed data for a given set of model parameter values (e.g., a pdf 




p

s




{\displaystyle p_{s}}

 and a matrix 




A



{\displaystyle \mathbf {A} }

) the likelihood of the model parameter values given the observed data.
 We define a likelihood function 




L
(
W
)



{\displaystyle \mathbf {L(W)} }

 of 




W



{\displaystyle \mathbf {W} }

:
 




L
(
W
)

=

p

s


(

W

x
)

|

det

W


|

.


{\displaystyle \mathbf {L(W)} =p_{s}(\mathbf {W} x)|\det \mathbf {W} |.}


 This equals to the probability density at 



x


{\displaystyle x}

, since 



s
=

W

x


{\displaystyle s=\mathbf {W} x}

.
 Thus, if we wish to find a 




W



{\displaystyle \mathbf {W} }

 that is most likely to have generated the observed mixtures 



x


{\displaystyle x}

 from the unknown source signals 



s


{\displaystyle s}

 with pdf 




p

s




{\displaystyle p_{s}}

 then we need only find that 




W



{\displaystyle \mathbf {W} }

 which maximizes the likelihood 




L
(
W
)



{\displaystyle \mathbf {L(W)} }

. The unmixing matrix that maximizes equation is known as the MLE of the optimal unmixing matrix.
 It is common practice to use the log likelihood, because this is easier to evaluate. As the logarithm is a monotonic function, the 




W



{\displaystyle \mathbf {W} }

 that maximizes the function 




L
(
W
)



{\displaystyle \mathbf {L(W)} }

 also maximizes its logarithm 



ln
⁡

L
(
W
)



{\displaystyle \ln \mathbf {L(W)} }

. This allows us to take the logarithm of equation above, which yields the log likelihood function
 



ln
⁡

L
(
W
)

=

∑

i



∑

t


ln
⁡

p

s


(

w

i


T



x

t


)
+
N
ln
⁡

|

det

W


|



{\displaystyle \ln \mathbf {L(W)} =\sum _{i}\sum _{t}\ln p_{s}(w_{i}^{T}x_{t})+N\ln |\det \mathbf {W} |}


 If we substitute a commonly used high-Kurtosis model pdf for the source signals 




p

s


=
(
1
−
tanh
⁡
(
s

)

2


)


{\displaystyle p_{s}=(1-\tanh(s)^{2})}

 then we have
 



ln
⁡

L
(
W
)

=


1
N



∑

i


M



∑

t


N


ln
⁡
(
1
−
tanh
⁡
(

w

i


T



x

t



)

2


)
+
ln
⁡

|

det

W


|



{\displaystyle \ln \mathbf {L(W)} ={1 \over N}\sum _{i}^{M}\sum _{t}^{N}\ln(1-\tanh(w_{i}^{T}x_{t})^{2})+\ln |\det \mathbf {W} |}


 This matrix 




W



{\displaystyle \mathbf {W} }

 that maximizes this function is the maximum likelihood estimation.
 The early general framework for independent component analysis was introduced by Jeanny Hérault and Bernard Ans from 1984,[21] further developed by Christian Jutten in 1985 and 1986,[2][22][23] and refined by Pierre Comon in 1991,[16] and popularized in his paper of 1994.[8] In 1995, Tony Bell and Terry Sejnowski introduced a fast and efficient ICA algorithm based on infomax, a principle introduced by Ralph Linsker in 1987. An interesting link between ML and Infomax approaches can be found in.[24] A quite comprehensive tutorial on ML approach has been published by J-F.Cardoso in 1998.[25]
 There are many algorithms available in the literature which do ICA. A largely used one, including in industrial applications, is the FastICA algorithm, developed by Hyvärinen and Oja,[26] which uses the negentropy as cost function, already proposed 7 years before by Pierre Comon in this context.[8] Other examples are rather related to blind source separation where a more general approach is used. For example, one can drop the independence assumption and separate mutually correlated signals, thus, statistically ""dependent"" signals. Sepp Hochreiter and Jürgen Schmidhuber showed how to obtain non-linear ICA or source separation as a by-product of regularization (1999).[27] Their method does not require a priori knowledge about the number of independent sources.
 ICA can be extended to analyze non-physical signals. For instance, ICA has been applied to discover discussion topics on a bag of news list archives.
 Some ICA applications are listed below:[6]
 ICA can be applied through the following software:
",signal process independ compon analysi ica comput method separ multivari signal addit subcompon done assum one subcompon gaussian subcompon statist independ ica invent jeanni hérault christian jutten ica special case blind sourc separ common exampl applic ica cocktail parti problem listen one person speech noisi room independ compon analysi attempt decompos multivari signal independ signal exampl sound usual signal compos numer addit time signal sever sourc question whether possibl separ contribut sourc observ total signal statist independ assumpt correct blind ica separ mix signal give good result also use signal suppos gener mix analysi purpos simpl applic ica cocktail parti problem underli speech signal separ sampl data consist peopl talk simultan room usual problem simplifi assum time delay echo note filter delay signal copi depend compon thu statist independ assumpt violat mix weight construct observ signal n n compon place n n matrix import thing consid n n sourc present least n n observ microphon observ signal audio need recov origin signal equal number observ sourc signal mix matrix squar n case underdetermin n n overdetermin n n investig success ica separ mix signal reli two assumpt three effect mix sourc signal two assumpt three effect mix sourc signal principl contribut basic establish ica signal extract set mixtur independ distribut low complex must sourc signal ica find independ compon also call factor latent variabl sourc maxim statist independ estim compon may choos one mani way defin proxi independ choic govern form ica algorithm two broadest definit independ ica inform mmi famili ica algorithm use measur like diverg maximum entropi famili ica algorithm motiv central limit theorem use kurtosi negentropi typic algorithm ica use center subtract mean creat zero mean signal whiten usual eigenvalu decomposit dimension reduct preprocess step order simplifi reduc complex problem actual iter algorithm whiten dimens reduct achiev princip compon analysi singular valu decomposit whiten ensur dimens treat equal priori algorithm run algorithm ica includ infomax fastica jade compon analysi among other gener ica identifi actual number sourc signal uniqu correct order sourc signal proper scale includ sign sourc signal ica import blind signal separ mani practic applic close relat even special case search factori code data new represent data vector get uniqu encod result code vector code code compon statist independ linear independ compon analysi divid noiseless noisi case noiseless ica special case noisi ica nonlinear ica consid separ case data repres observ random vector x x x x hidden compon random vector n n task transform observ data x x use linear static transform w w w x w x vector maxim independ compon measur function f n f n independ compon x observ random vector x x x x gener sum independ compon k k k n n x k k n n k k n n weight mix weight k k gener model written vector form x k n k k x n k k observ random vector x x repres basi vector k k k k k k basi vector k k form column mix matrix n n gener formula written x x n n given model realiz sampl x x n x x n random vector x x task estim mix matrix sourc done adapt calcul w w vector set cost function either maxim calcul k w x k w x minim mutual inform case priori knowledg probabl distribut sourc use cost function origin sourc recov multipli observ signal x x invers mix matrix w w also known unmix matrix assum mix matrix squar n number basi vector greater dimension observ vector n n task overcomplet still solvabl pseudo invers ad assumpt uncorrel gaussian nois n n diag σ n diag ica model take form x n x mix sourc need linear use nonlinear mix function f θ f paramet θ nonlinear ica model x f θ n independ compon identifi permut scale sourc identifi requir special variant ica binari ica signal sourc monitor binari form observ monitor disjunct mixtur binari independ sourc problem shown applic mani domain includ medic diagnosi assign network tomographi internet resourc manag let x x x set binari variabl monitor n n set binari variabl n n sourc connect repres unknown mix matrix g g g j ij indic signal sourc observ monitor system work follow time sourc activ connect monitor j j g j ij monitor j j observ activ x j j formal boolean boolean nois explicitli model rather treat independ sourc problem heurist solv assum variabl continu run fastica binari observ data get mix matrix g g real valu appli round number techniqu g g obtain binari valu approach shown produc highli inaccur result citat need anoth method use dynam program recurs break observ matrix x x run infer algorithm key observ lead algorithm x x x x x j j ij j correspond unbias observ matrix hidden compon connect monitor experiment result show approach accur moder nois level gener binari ica framework introduc broader problem formul necessit knowledg gener model word method attempt decompos sourc independ compon much possibl without lose inform prior assumpt way gener although problem appear quit complex accur solv branch bound search tree algorithm tightli upper bound singl multipl matrix vector signal mixtur tend gaussian probabl densiti function sourc signal tend probabl densiti function sourc signal extract set signal mixtur take inner product weight vector signal mixtur inner product provid orthogon project signal mixtur remain challeng find weight vector one type method project pursuit project pursuit seek one project time extract signal possibl contrast ica typic extract signal simultan signal mixtur requir estim unmix matrix one practic advantag project pursuit ica fewer signal extract requir sourc signal extract signal mixtur use weight vector use kurtosi recov multipl sourc signal find correct weight vector use project pursuit kurtosi probabl densiti function signal finit sampl comput sampl mean extract signal constant ensur gaussian signal zero kurtosi signal posit kurtosi signal neg kurtosi denomin varianc ensur measur kurtosi take account signal varianc goal project pursuit maxim kurtosi make extract signal possibl use kurtosi measur examin kurtosi signal w x w x extract set mixtur x x x x x vari weight vector w w rotat around origin given assumpt sourc signal would expect multipl sourc mixtur signal use kurtosi orthogon gso recov signal given signal mixtur space gso project data point onto space use weight vector guarante independ extract signal use gso order find correct valu w w use gradient descent method first whiten data transform x x new mixtur z z unit varianc z z z z z process achiev appli singular valu decomposit x x rescal vector u u e u e let z u z u signal extract weight vector w w w z w z weight vector w unit length varianc also e w z e w z kurtosi thu written updat process w w η small constant guarante w w converg optim solut updat normal w n e w w n e w w n e w w new w new w new set w l w n e w w old w new repeat updat process converg also use anoth algorithm updat weight vector w w anoth approach use negentropi instead kurtosi use negentropi robust method kurtosi kurtosi sensit outlier negentropi method base import properti gaussian distribut gaussian variabl largest entropi among continu random variabl equal varianc also reason want find nongaussian variabl simpl proof found differenti entropi gaussian random variabl covari matrix x approxim negentropi proof found origin paper comon reproduc book independ compon analysi aapo hyvärinen juha karhunen erkki oja approxim also suffer problem kurtosi sensit outlier approach develop choic g g infomax ica essenti multivari parallel version project pursuit wherea project pursuit extract seri signal one time set signal mixtur ica extract signal parallel tend make ica robust project pursuit project pursuit method use orthogon ensur independ extract signal ica use infomax maximum likelihood estim ensur independ extract signal extract signal achiev assign appropri model prior signal process ica base infomax short given set signal mixtur x x set ident independ model cumul distribut function cdf g g seek unmix matrix w w maxim joint entropi signal g w x wx signal extract w w given optim w w signal maximum entropi therefor independ ensur extract signal g also independ g g invert function signal model note sourc signal model probabl densiti function p match probabl densiti function extract signal p maxim joint entropi also maxim amount mutual inform x x reason use entropi extract independ signal known infomax consid entropi vector variabl g w x wx set signal extract unmix matrix w w finit set valu sampl distribut pdf p entropi estim joint pdf p shown relat joint pdf p extract signal multivari form j j jacobian matrix j g j g g pdf assum sourc signal g p therefor therefor know p p p uniform distribut h h maxim sinc w w absolut valu determin unmix matrix w w therefor sinc h x n n ln p x x h x n n x x maxim w w affect h x x maxim function achiev independ extract signal margin pdf model joint pdf p independ use commonli model pdf sourc signal p tanh sum given observ signal mixtur x x correspond set extract signal sourc signal model p g find optim unmix matrix w w make extract signal independ like project pursuit situat use gradient descent method find optim solut unmix matrix maximum likelihood estim mle standard statist tool find paramet valu unmix matrix w w provid best fit data extract signal given model assum joint probabl densiti function pdf p sourc signal ml model includ specif pdf case pdf p unknown sourc signal use ml ica object find unmix matrix yield extract signal w x w x joint pdf similar possibl joint pdf p unknown sourc signal mle thu base assumpt model pdf p model paramet correct high probabl obtain data x x actual observ convers far correct paramet valu low probabl observ data would expect use mle call probabl observ data given set model paramet valu pdf p matrix likelihood model paramet valu given observ data defin likelihood function l w l w w w l w p w x det w l w w x w equal probabl densiti x x sinc w x w x thu wish find w w like gener observ mixtur x x unknown sourc signal pdf p need find w w maxim likelihood l w l w unmix matrix maxim equat known mle optim unmix matrix common practic use log likelihood easier evalu logarithm monoton function w w maxim function l w l w also maxim logarithm ln l w l w allow us take logarithm equat yield log likelihood function ln l w ln p w x n ln det w l w w substitut commonli use model pdf sourc signal p tanh ln l w n n ln tanh w x ln det w l w n n w matrix w w maxim function maximum likelihood estim earli gener framework independ compon analysi introduc jeanni hérault bernard an develop christian jutten refin pierr comon popular paper toni bell terri sejnowski introduc fast effici ica algorithm base infomax principl introduc ralph linsker interest link ml infomax approach found quit comprehens tutori ml approach publish mani algorithm avail literatur ica larg use one includ industri applic fastica algorithm develop hyvärinen oja use negentropi cost function alreadi propos year pierr comon context exampl rather relat blind sourc separ gener approach use exampl one drop independ assumpt separ mutual correl signal thu statist depend signal sepp hochreit jürgen schmidhub show obtain ica sourc separ regular method requir priori knowledg number independ sourc ica extend analyz signal instanc ica appli discov discuss topic bag news list archiv ica applic list ica appli follow softwar
Linear discriminant analysis,https://en.wikipedia.org/wiki/Linear_discriminant_analysis,"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.
 LDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements.[2][3] However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e. the class label).[4] Logistic regression and probit regression are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method.
 LDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data.[5] LDA explicitly attempts to model the difference between the classes of data. PCA, in contrast, does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made.
 LDA works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis.[6][7]
 Discriminant analysis is used when groups are known a priori (unlike in cluster analysis). Each case must have a score on one or more quantitative predictor measures, and a score on a group measure.[8] In simple terms, discriminant function analysis is classification - the act of distributing things into groups, classes or categories of the same type.
 The original dichotomous discriminant analysis was developed by Sir Ronald Fisher in 1936.[9] It is different from an ANOVA or MANOVA, which is used to predict one (ANOVA) or multiple (MANOVA) continuous dependent variables by one or more independent categorical variables. Discriminant function analysis is useful in determining whether a set of variables is effective in predicting category membership.[10]
 Consider a set of observations 






x
→





{\displaystyle {\vec {x}}}

 (also called features, attributes, variables or measurements) for each sample of an object or event with known class 



y


{\displaystyle y}

. This set of samples is called the training set in a supervised learning context. The classification problem is then to find a good predictor for the class 



y


{\displaystyle y}

 of any sample of the same distribution (not necessarily from the training set) given only an observation 






x
→





{\displaystyle {\vec {x}}}

.[11]: 338 
 LDA approaches the problem by assuming that the conditional probability density functions 



p
(



x
→




|

y
=
0
)


{\displaystyle p({\vec {x}}|y=0)}

 and 



p
(



x
→




|

y
=
1
)


{\displaystyle p({\vec {x}}|y=1)}

 are both the normal distribution with mean and covariance parameters 




(





μ
→




0


,

Σ

0



)



{\displaystyle \left({\vec {\mu }}_{0},\Sigma _{0}\right)}

 and 




(





μ
→




1


,

Σ

1



)



{\displaystyle \left({\vec {\mu }}_{1},\Sigma _{1}\right)}

, respectively. Under this assumption, the Bayes-optimal solution is to predict points as being from the second class if the log of the likelihood ratios is bigger than some threshold T, so that:
 Without any further assumptions, the resulting classifier is referred to as quadratic discriminant analysis (QDA).
 LDA instead makes the additional simplifying homoscedasticity assumption (i.e. that the class covariances are identical, so 




Σ

0


=

Σ

1


=
Σ


{\displaystyle \Sigma _{0}=\Sigma _{1}=\Sigma }

) and that the covariances have full rank.
In this case, several terms cancel:
 and the above decision criterion
becomes a threshold on the dot product
 for some threshold constant c, where
 This means that the criterion of an input 






x
→





{\displaystyle {\vec {x}}}

 being in a class 



y


{\displaystyle y}

 is purely a function of this linear combination of the known observations.
 It is often useful to see this conclusion in geometrical terms: the criterion of an input 






x
→





{\displaystyle {\vec {x}}}

 being in a class 



y


{\displaystyle y}

 is purely a function of projection of multidimensional-space point 






x
→





{\displaystyle {\vec {x}}}

 onto vector 






w
→





{\displaystyle {\vec {w}}}

 (thus, we only consider its direction). In other words, the observation belongs to 



y


{\displaystyle y}

 if corresponding 






x
→





{\displaystyle {\vec {x}}}

 is located on a certain side of a hyperplane perpendicular to 






w
→





{\displaystyle {\vec {w}}}

. The location of the plane is defined by the threshold 



c


{\displaystyle c}

.
 The assumptions of discriminant analysis are the same as those for MANOVA. The analysis is quite sensitive to outliers and the size of the smallest group must be larger than the number of predictor variables.[8]
 It has been suggested that discriminant analysis is relatively robust to slight violations of these assumptions,[12] and it has also been shown that discriminant analysis may still be reliable when using dichotomous variables (where multivariate normality is often violated).[13]
 Discriminant analysis works by creating one or more linear combinations of predictors, creating a new latent variable for each function. These functions are called discriminant functions. The number of functions possible is either 




N

g


−
1


{\displaystyle N_{g}-1}

 where 




N

g




{\displaystyle N_{g}}

 = number of groups, or 



p


{\displaystyle p}

 (the number of predictors), whichever is smaller. The first function created maximizes the differences between groups on that function. The second function maximizes differences on that function, but also must not be correlated with the previous function. This continues with subsequent functions with the requirement that the new function not be correlated with any of the previous functions.
 Given group 



j


{\displaystyle j}

, with  





R


j




{\displaystyle \mathbb {R} _{j}}

  sets of sample space, there is a discriminant rule such that if 



x
∈


R


j




{\displaystyle x\in \mathbb {R} _{j}}

, then 



x
∈
j


{\displaystyle x\in j}

. Discriminant analysis then, finds “good” regions of  





R


j




{\displaystyle \mathbb {R} _{j}}

 to minimize classification error, therefore leading to a high percent correct classified in the classification table.[14]
 Each function is given a discriminant score[clarification needed] to determine how well it predicts group placement. 
 An eigenvalue in discriminant analysis is the characteristic root of each function.[clarification needed] It is an indication of how well that function differentiates the groups, where the larger the eigenvalue, the better the function differentiates.[8] This however, should be interpreted with caution, as eigenvalues have no upper limit.[10][8]
	The eigenvalue can be viewed as a ratio of SSbetween and SSwithin as in ANOVA when the dependent variable is the discriminant function, and the groups are the levels of the IV[clarification needed].[10] This means that the largest eigenvalue is associated with the first function, the second largest with the second, etc..
 Some suggest the use of eigenvalues as effect size measures, however, this is generally not supported.[10] Instead, the canonical correlation is the preferred measure of effect size. It is similar to the eigenvalue, but is the square root of the ratio of SSbetween and SStotal. It is the correlation between groups and the function.[10] 
	Another popular measure of effect size is the percent of variance[clarification needed] for each function.  This is calculated by: (λx/Σλi) X 100 where λx is the eigenvalue for the function and Σλi is the sum of all eigenvalues. This tells us how strong the prediction is for that particular function compared to the others.[10] 
	Percent correctly classified can also be analyzed as an effect size. The kappa value can describe this while correcting for chance agreement.[10]Kappa normalizes across all categorizes rather than biased by a significantly good or poorly performing classes.[clarification needed][17]
 Canonical discriminant analysis (CDA) finds axes (k − 1 canonical coordinates, k being the number of classes) that best separate the categories. These linear functions are uncorrelated and define, in effect, an optimal k − 1 space through the n-dimensional cloud of data that best separates (the projections in that space of) the k groups. See “Multiclass LDA” for details below.
 The terms Fisher's linear discriminant and LDA are often used interchangeably, although Fisher's original article[2] actually describes a slightly different discriminant, which does not make some of the assumptions of LDA such as normally distributed classes or equal class covariances.
 Suppose two classes of observations have means 







μ
→




0


,




μ
→




1




{\displaystyle {\vec {\mu }}_{0},{\vec {\mu }}_{1}}

 and covariances 




Σ

0


,

Σ

1




{\displaystyle \Sigma _{0},\Sigma _{1}}

. Then the linear combination of features 







w
→





T






x
→





{\displaystyle {\vec {w}}^{\mathrm {T} }{\vec {x}}}

 will have means 







w
→





T







μ
→




i




{\displaystyle {\vec {w}}^{\mathrm {T} }{\vec {\mu }}_{i}}

 and variances 







w
→





T




Σ

i





w
→





{\displaystyle {\vec {w}}^{\mathrm {T} }\Sigma _{i}{\vec {w}}}

 for 



i
=
0
,
1


{\displaystyle i=0,1}

. Fisher defined the separation between these two distributions to be the ratio of the variance between the classes to the variance within the classes:
 This measure is, in some sense, a measure of the signal-to-noise ratio for the class labelling. It can be shown that the maximum separation occurs when
 When the assumptions of LDA are satisfied, the above equation is equivalent to LDA.
 Be sure to note that the vector 






w
→





{\displaystyle {\vec {w}}}

 is the normal to the discriminant hyperplane. As an example, in a two dimensional problem, the line that best divides the two groups is perpendicular to 






w
→





{\displaystyle {\vec {w}}}

.
 Generally, the data points to be discriminated are projected onto 






w
→





{\displaystyle {\vec {w}}}

; then the threshold that best separates the data is chosen from analysis of the one-dimensional distribution. There is no general rule for the threshold. However, if projections of points from both classes exhibit approximately the same distributions, a good choice would be the hyperplane between projections of the two means, 






w
→



⋅




μ
→




0




{\displaystyle {\vec {w}}\cdot {\vec {\mu }}_{0}}

 and 






w
→



⋅




μ
→




1




{\displaystyle {\vec {w}}\cdot {\vec {\mu }}_{1}}

. In this case the parameter c in threshold condition 






w
→



⋅



x
→



>
c


{\displaystyle {\vec {w}}\cdot {\vec {x}}>c}

 can be found explicitly:
 Otsu's method is related to Fisher's linear discriminant, and was created to binarize the histogram of pixels in a grayscale image by optimally picking the black/white threshold that minimizes intra-class variance and maximizes inter-class variance within/between grayscales assigned to black and white pixel classes.
 In the case where there are more than two classes, the analysis used in the derivation of the Fisher discriminant can be extended to find a subspace which appears to contain all of the class variability.[18] This generalization is due to C. R. Rao.[19] Suppose that each of C classes has a mean 




μ

i




{\displaystyle \mu _{i}}

 and the same covariance 



Σ


{\displaystyle \Sigma }

. Then the scatter between class variability may be defined by the sample covariance of the class means
 where 



μ


{\displaystyle \mu }

 is the mean of the class means. The class separation in a direction 






w
→





{\displaystyle {\vec {w}}}

 in this case will be given by
 This means that when 






w
→





{\displaystyle {\vec {w}}}

 is an eigenvector of 




Σ

−
1



Σ

b




{\displaystyle \Sigma ^{-1}\Sigma _{b}}

 the separation will be equal to the corresponding eigenvalue.
 If 




Σ

−
1



Σ

b




{\displaystyle \Sigma ^{-1}\Sigma _{b}}

 is diagonalizable, the variability between features will be contained in the subspace spanned by the eigenvectors corresponding to the C − 1 largest eigenvalues (since 




Σ

b




{\displaystyle \Sigma _{b}}

 is of rank C − 1 at most). These eigenvectors are primarily used in feature reduction, as in PCA. The eigenvectors corresponding to the smaller eigenvalues will tend to be very sensitive to the exact choice of training data, and it is often necessary to use regularisation as described in the next section.
 If classification is required, instead of dimension reduction, there are a number of alternative techniques available. For instance, the classes may be partitioned, and a standard Fisher discriminant or LDA used to classify each partition. A common example of this is ""one against the rest"" where the points from one class are put in one group, and everything else in the other, and then LDA applied. This will result in C classifiers, whose results are combined. Another common
method is pairwise classification, where a new classifier is created for each pair of classes (giving C(C − 1)/2 classifiers in total), with the individual classifiers combined to produce a final classification.
 The typical implementation of the LDA technique requires that all the samples are available in advance. However,  there are situations where the entire data set is not available and the input data are observed as a stream. In this case, it is desirable for the LDA feature extraction to have the ability to update the computed LDA features by observing the new samples without running the algorithm on the whole data set. For example, in many real-time applications such as mobile robotics or on-line face recognition, it is important to update the extracted LDA features as soon as new observations are available. An LDA feature extraction technique that can update the LDA features by simply observing new samples is an incremental LDA algorithm, and this idea has been extensively studied over the last two decades.[20] Chatterjee and Roychowdhury proposed an incremental self-organized LDA algorithm for updating the LDA features.[21] In other work, Demir and Ozmehmet proposed online local learning algorithms for updating LDA features incrementally using error-correcting and the Hebbian learning rules.[22] Later, Aliyari et al. derived fast incremental algorithms to update the LDA features by observing the new samples.[20]
 In practice, the class means and covariances are not known. They can, however, be estimated from the training set. Either the maximum likelihood estimate or the maximum a posteriori estimate may be used in place of the exact value in the above equations. Although the estimates of the covariance may be considered optimal in some sense, this does not mean that the resulting discriminant obtained by substituting these values is optimal in any sense, even if the assumption of normally distributed classes is correct.
 Another complication in applying LDA and Fisher's discriminant to real data occurs when the number of measurements of each sample (i.e., the dimensionality of each data vector) exceeds the number of samples in each class.[5] In this case, the covariance estimates do not have full rank, and so cannot be inverted. There are a number of ways to deal with this. One is to use a pseudo inverse instead of the usual matrix inverse in the above formulae. However, better numeric stability may be achieved by first projecting the problem onto the subspace spanned by 




Σ

b




{\displaystyle \Sigma _{b}}

.[23]
Another strategy to deal with small sample size is to use a shrinkage estimator of the covariance matrix, which
can be expressed mathematically as
 where 



I


{\displaystyle I}

 is the identity matrix, and 



λ


{\displaystyle \lambda }

 is the shrinkage intensity or regularisation parameter.
This leads to the framework of regularized discriminant analysis[24] or shrinkage discriminant analysis.[25]
 Also, in many practical cases linear discriminants are not suitable. LDA and Fisher's discriminant can be extended for use in non-linear classification via the kernel trick. Here, the original observations are effectively mapped into a higher dimensional non-linear space. Linear classification in this non-linear space is then equivalent to non-linear classification in the original space. The most commonly used example of this is the kernel Fisher discriminant.
 LDA can be generalized to multiple discriminant analysis, where c becomes a categorical variable with N possible states, instead of only two. Analogously, if the class-conditional densities 



p
(



x
→



∣
c
=
i
)


{\displaystyle p({\vec {x}}\mid c=i)}

 are normal with shared covariances, the sufficient statistic for 



P
(
c
∣



x
→



)


{\displaystyle P(c\mid {\vec {x}})}

 are the values of N projections, which are the subspace spanned by the N means, affine projected by the inverse covariance matrix. These projections can be found by solving a generalized eigenvalue problem, where the numerator is the covariance matrix formed by treating the means as the samples, and the denominator is the shared covariance matrix. See “Multiclass LDA” above for details.
 In addition to the examples given below, LDA is applied in positioning and product management.
 In bankruptcy prediction based on accounting ratios and other financial variables, linear discriminant analysis was the first statistical method applied to systematically explain which firms entered bankruptcy vs. survived. Despite limitations including known nonconformance of accounting ratios to the normal distribution assumptions of LDA, Edward Altman's 1968 model[26] is still a leading model in practical applications.[27][28][29]
 In computerised face recognition, each face is represented by a large number of pixel values. Linear discriminant analysis is primarily used here to reduce the number of features to a more manageable number before classification. Each of the new dimensions is a linear combination of pixel values, which form a template. The linear combinations obtained using Fisher's linear discriminant are called Fisher faces, while those obtained using the related principal component analysis are called eigenfaces.
 In marketing, discriminant analysis was once often used to determine the factors which distinguish different types of customers and/or products on the basis of surveys or other forms of collected data. Logistic regression or other methods are now more commonly used. The use of discriminant analysis in marketing can be described by the following steps:
 The main application of discriminant analysis in medicine is the assessment of severity state of a patient and prognosis of disease outcome. For example, during retrospective analysis, patients are divided into groups according to severity of disease – mild, moderate, and severe form. Then results of clinical and laboratory analyses are studied to reveal statistically different variables in these groups. Using these variables, discriminant functions are built to classify disease severity in future patients. Additionally, Linear Discriminant Analysis (LDA) can help select more discriminative samples for data augmentation, improving classification performance.[30]
 In biology, similar principles are used in order to classify and define groups of different biological objects, for example, to define phage types of Salmonella enteritidis based on Fourier transform infrared spectra,[31] to detect animal source of Escherichia coli studying its virulence factors[32] etc.
 This method can be used to separate the alteration zones[clarification needed]. For example, when different data from various zones are available, discriminant analysis can find the pattern within the data and classify it effectively.[33]
 Discriminant function analysis is very similar to logistic regression, and both can be used to answer the same research questions.[10] Logistic regression does not have as many assumptions and restrictions as discriminant analysis. However, when discriminant analysis’ assumptions are met, it is more powerful than logistic regression.[34] Unlike logistic regression, discriminant analysis can be used with small sample sizes. It has been shown that when sample sizes are equal, and homogeneity of variance/covariance holds, discriminant analysis is more accurate.[8] Despite all these advantages, logistic regression has none-the-less become the common choice, since the assumptions of discriminant analysis are rarely met.[9][8]
 Geometric anomalies in higher dimensions lead to the well-known curse of dimensionality. Nevertheless, proper utilization of concentration of measure phenomena can make computation easier.[35] An important case of these  blessing of dimensionality phenomena was highlighted by Donoho and Tanner: if a sample is essentially high-dimensional then each point can be separated from the rest of the sample by linear inequality, with high probability, even for exponentially large samples.[36] These linear inequalities can be selected in the standard (Fisher's) form of the linear discriminant for a rich family of probability distribution.[37] In particular, such theorems are proven for log-concave distributions including multidimensional normal distribution (the proof is based on the concentration inequalities for log-concave measures[38]) and for product measures on a multidimensional cube (this is proven using Talagrand's concentration inequality for product probability spaces). Data separability by classical linear discriminants simplifies the problem of error correction for artificial intelligence systems in high dimension.[39]
",linear discrimin analysi lda normal discrimin analysi nda discrimin function analysi gener fisher linear discrimin method use statist field find linear combin featur character separ two class object event result combin may use linear classifi commonli dimension reduct later classif lda close relat analysi varianc anova regress analysi also attempt express one depend variabl linear combin featur measur howev anova use categor independ variabl continu depend variabl wherea discrimin analysi continu independ variabl categor depend variabl class label logist regress probit regress similar lda anova also explain categor variabl valu continu independ variabl method prefer applic reason assum independ variabl normal distribut fundament assumpt lda method lda also close relat princip compon analysi pca factor analysi look linear combin variabl best explain data lda explicitli attempt model differ class data pca contrast take account differ class factor analysi build featur combin base differ rather similar discrimin analysi also differ factor analysi interdepend techniqu distinct independ variabl depend variabl also call criterion variabl must made lda work measur made independ variabl observ continu quantiti deal categor independ variabl equival techniqu discrimin correspond analysi discrimin analysi use group known priori unlik cluster analysi case must score one quantit predictor measur score group measur simpl term discrimin function analysi classif act distribut thing group class categori type origin dichotom discrimin analysi develop sir ronald fisher differ anova manova use predict one anova multipl manova continu depend variabl one independ categor variabl discrimin function analysi use determin whether set variabl effect predict categori membership consid set observ x x also call featur attribut variabl measur sampl object event known class set sampl call train set supervis learn context classif problem find good predictor class sampl distribut necessarili train set given observ x x lda approach problem assum condit probabl densiti function p x p x p x p x normal distribut mean covari paramet μ σ μ σ respect assumpt solut predict point second class log likelihood ratio bigger threshold without assumpt result classifi refer quadrat discrimin analysi qda lda instead make addit simplifi homoscedast assumpt class covari ident σ σ σ covari full rank case sever term cancel decis criterion becom threshold dot product threshold constant c mean criterion input x x class pure function linear combin known observ often use see conclus geometr term criterion input x x class pure function project point x x onto vector w w thu consid direct word observ belong correspond x x locat certain side hyperplan perpendicular w w locat plane defin threshold c c assumpt discrimin analysi manova analysi quit sensit outlier size smallest group must larger number predictor variabl suggest discrimin analysi rel robust slight violat assumpt also shown discrimin analysi may still reliabl use dichotom variabl multivari normal often violat discrimin analysi work creat one linear combin predictor creat new latent variabl function function call discrimin function number function possibl either n g g n g g number group p p number predictor whichev smaller first function creat maxim differ group function second function maxim differ function also must correl previou function continu subsequ function requir new function correl previou function given group j j r j r j set sampl space discrimin rule x r j r j x j j discrimin analysi find good region r j r j minim classif error therefor lead high percent correct classifi classif tabl function given discrimin score clarif need determin well predict group placement eigenvalu discrimin analysi characterist root function clarif need indic well function differenti group larger eigenvalu better function differenti howev interpret caution eigenvalu upper limit eigenvalu view ratio ssbetween sswithin anova depend variabl discrimin function group level iv clarif need mean largest eigenvalu associ first function second largest second etc suggest use eigenvalu effect size measur howev gener support instead canon correl prefer measur effect size similar eigenvalu squar root ratio ssbetween sstotal correl group function anoth popular measur effect size percent varianc clarif need function calcul x λx eigenvalu function σλi sum eigenvalu tell us strong predict particular function compar other percent correctli classifi also analyz effect size kappa valu describ correct chanc agreement kappa normal across categor rather bias significantli good poorli perform class clarif need canon discrimin analysi cda find axe k canon coordin k number class best separ categori linear function uncorrel defin effect optim k space cloud data best separ project space k group see multiclass lda detail term fisher linear discrimin lda often use interchang although fisher origin articl actual describ slightli differ discrimin make assumpt lda normal distribut class equal class covari suppos two class observ mean μ μ covari σ σ linear combin featur w x w x mean w μ w varianc w σ w w w fisher defin separ two distribut ratio varianc class varianc within class measur sens measur ratio class label shown maximum separ occur assumpt lda satisfi equat equival lda sure note vector w w normal discrimin hyperplan exampl two dimension problem line best divid two group perpendicular w w gener data point discrimin project onto w w threshold best separ data chosen analysi distribut gener rule threshold howev project point class exhibit approxim distribut good choic would hyperplan project two mean w μ w w μ w case paramet c threshold condit w x c w x c found explicitli otsu method relat fisher linear discrimin creat binar histogram pixel grayscal imag optim pick threshold minim varianc maxim varianc grayscal assign black white pixel class case two class analysi use deriv fisher discrimin extend find subspac appear contain class variabl gener due r rao suppos c class mean μ covari σ scatter class variabl may defin sampl covari class mean μ mean class mean class separ direct w w case given mean w w eigenvector σ σ b b separ equal correspond eigenvalu σ σ b b diagonaliz variabl featur contain subspac span eigenvector correspond c largest eigenvalu sinc σ b b rank c eigenvector primarili use featur reduct pca eigenvector correspond smaller eigenvalu tend sensit exact choic train data often necessari use regularis describ next section classif requir instead dimens reduct number altern techniqu avail instanc class may partit standard fisher discrimin lda use classifi partit common exampl one rest point one class put one group everyth els lda appli result c classifi whose result combin anoth common method pairwis classif new classifi creat pair class give c c classifi total individu classifi combin produc final classif typic implement lda techniqu requir sampl avail advanc howev situat entir data set avail input data observ stream case desir lda featur extract abil updat comput lda featur observ new sampl without run algorithm whole data set exampl mani applic mobil robot face recognit import updat extract lda featur soon new observ avail lda featur extract techniqu updat lda featur simpli observ new sampl increment lda algorithm idea extens studi last two decad chatterje roychowdhuri propos increment lda algorithm updat lda featur work demir ozmehmet propos onlin local learn algorithm updat lda featur increment use hebbian learn rule later aliyari et al deriv fast increment algorithm updat lda featur observ new sampl practic class mean covari known howev estim train set either maximum likelihood estim maximum posteriori estim may use place exact valu equat although estim covari may consid optim sens mean result discrimin obtain substitut valu optim sens even assumpt normal distribut class correct anoth complic appli lda fisher discrimin real data occur number measur sampl dimension data vector exce number sampl class case covari estim full rank invert number way deal one use pseudo invers instead usual matrix invers formula howev better numer stabil may achiev first project problem onto subspac span σ b b anoth strategi deal small sampl size use shrinkag estim covari matrix express mathemat ident matrix λ shrinkag intens regularis paramet lead framework regular discrimin analysi shrinkag discrimin analysi also mani practic case linear discrimin suitabl lda fisher discrimin extend use classif via kernel trick origin observ effect map higher dimension space linear classif space equival classif origin space commonli use exampl kernel fisher discrimin lda gener multipl discrimin analysi c becom categor variabl n possibl state instead two analog densiti p x c p x normal share covari suffici statist p c x p x valu n project subspac span n mean affin project invers covari matrix project found solv gener eigenvalu problem numer covari matrix form treat mean sampl denomin share covari matrix see multiclass lda detail addit exampl given lda appli posit product manag bankruptci predict base account ratio financi variabl linear discrimin analysi first statist method appli systemat explain firm enter bankruptci surviv despit limit includ known nonconform account ratio normal distribut assumpt lda edward altman model still lead model practic applic computeris face recognit face repres larg number pixel valu linear discrimin analysi primarili use reduc number featur manag number classif new dimens linear combin pixel valu form templat linear combin obtain use fisher linear discrimin call fisher face obtain use relat princip compon analysi call eigenfac market discrimin analysi often use determin factor distinguish differ type custom product basi survey form collect data logist regress method commonli use use discrimin analysi market describ follow step main applic discrimin analysi medicin assess sever state patient prognosi diseas outcom exampl retrospect analysi patient divid group accord sever diseas mild moder sever form result clinic laboratori analys studi reveal statist differ variabl group use variabl discrimin function built classifi diseas sever futur patient addit linear discrimin analysi lda help select discrimin sampl data augment improv classif perform biolog similar principl use order classifi defin group differ biolog object exampl defin phage type salmonella enteritidi base fourier transform infrar spectra detect anim sourc escherichia coli studi virul factor etc method use separ alter zone clarif need exampl differ data variou zone avail discrimin analysi find pattern within data classifi effect discrimin function analysi similar logist regress use answer research question logist regress mani assumpt restrict discrimin analysi howev discrimin analysi assumpt met power logist regress unlik logist regress discrimin analysi use small sampl size shown sampl size equal homogen hold discrimin analysi accur despit advantag logist regress becom common choic sinc assumpt discrimin analysi rare met geometr anomali higher dimens lead curs dimension nevertheless proper util concentr measur phenomena make comput easier import case bless dimension phenomena highlight donoho tanner sampl essenti point separ rest sampl linear inequ high probabl even exponenti larg sampl linear inequ select standard fisher form linear discrimin rich famili probabl distribut particular theorem proven distribut includ multidimension normal distribut proof base concentr inequ measur product measur multidimension cube proven use talagrand concentr inequ product probabl space data separ classic linear discrimin simplifi problem error correct artifici intellig system high dimens
Non-negative matrix factorization,https://en.wikipedia.org/wiki/Non-negative_matrix_factorization,"Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.
 NMF finds applications in such fields as astronomy,[3][4] computer vision, document clustering,[1] missing data imputation,[5] chemometrics, audio signal processing, recommender systems,[6][7] and bioinformatics.[8]
 In chemometrics non-negative matrix factorization has a long history under the name ""self modeling curve resolution"".[9]
In this framework the vectors in the right matrix are continuous curves rather than discrete vectors.
Also early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the 1990s under the name positive matrix factorization.[10][11][12]
It became more widely known as non-negative matrix factorization after Lee and Seung investigated the properties of the algorithm and published some simple and useful
algorithms for two types of factorizations.[13][14]
 Let matrix V be the product of the matrices W and H,
 Matrix multiplication can be implemented as computing the column vectors of V as linear combinations of the column vectors in W using coefficients supplied by columns of H.  That is, each column of V can be computed as follows:
 where vi is the i-th column vector of the product matrix V and hi is the i-th column vector of the matrix H.
 When multiplying matrices, the dimensions of the factor matrices may be significantly lower than those of the product matrix and it is this property that forms the basis of NMF. NMF generates factors with significantly reduced dimensions compared to the original matrix. For example, if V is an m × n matrix, W is an m × p matrix, and H is a p × n matrix then p can be significantly less than both m and n.
 Here is an example based on a text-mining application: 
 This last point is the basis of NMF because we can consider each original document in our example as being built from a small set of hidden features. NMF generates these features.
 It is useful to think of each feature (column vector) in the features matrix W as a document archetype comprising a set of words where each word's cell value defines the word's rank in the feature: The higher a word's cell value the higher the word's rank in the feature. A column in the coefficients matrix H represents an original document with a cell value defining the document's rank for a feature. We can now reconstruct a document (column vector) from our input matrix by a linear combination of our features (column vectors in W) where each feature is weighted by the feature's cell value from the document's column in H.
 NMF has an inherent clustering property,[15] i.e., it automatically clusters the columns of input data 




V

=
(

v

1


,
…
,

v

n


)


{\displaystyle \mathbf {V} =(v_{1},\dots ,v_{n})}

.
 More specifically, the approximation of 




V



{\displaystyle \mathbf {V} }

 by 




V

≃

W


H



{\displaystyle \mathbf {V} \simeq \mathbf {W} \mathbf {H} }

 is achieved by finding 



W


{\displaystyle W}

 and 



H


{\displaystyle H}

 that minimize the error function (using the Frobenius norm)
 





‖

V
−
W
H

‖


F


,


{\displaystyle \left\|V-WH\right\|_{F},}

 subject to 



W
≥
0
,
H
≥
0.


{\displaystyle W\geq 0,H\geq 0.}

, 
 If we furthermore impose an orthogonality constraint on 




H



{\displaystyle \mathbf {H} }

, 
i.e. 




H



H


T


=
I


{\displaystyle \mathbf {H} \mathbf {H} ^{T}=I}

, then the above minimization is mathematically equivalent to the minimization of K-means clustering.[15]
 Furthermore, the computed 



H


{\displaystyle H}

 gives the cluster membership, i.e., if 





H


k
j


>


H


i
j




{\displaystyle \mathbf {H} _{kj}>\mathbf {H} _{ij}}

 for all i ≠ k, this suggests that the input data 




v

j




{\displaystyle v_{j}}

 belongs to 



k


{\displaystyle k}

-th cluster. The computed 



W


{\displaystyle W}

 gives the cluster centroids, i.e., the 



k


{\displaystyle k}

-th column gives the cluster centroid of 



k


{\displaystyle k}

-th cluster. This centroid's representation can be significantly enhanced by convex NMF.
 When the orthogonality constraint 




H



H


T


=
I


{\displaystyle \mathbf {H} \mathbf {H} ^{T}=I}

 is not explicitly imposed, the orthogonality holds to a large extent, and the clustering property holds too. 
 When the error function to be used is Kullback–Leibler divergence, NMF is identical to the probabilistic latent semantic analysis (PLSA), a popular document clustering method.[16]
 Usually the number of columns of W and the number of rows of H in NMF are selected so the product WH will become an approximation to V.  The full decomposition of V then amounts to the two non-negative matrices W and H as well as a residual U, such that: V = WH + U. The elements of the residual matrix can either be negative or positive.
 When W and H are smaller than V they become easier to store and manipulate. Another reason for factorizing V into smaller matrices W and H, is that if one's goal is to approximately represent the elements of V by significantly less data, then one has to infer some latent structure in the data.
 In standard NMF, matrix factor W ∈ R+m × k， i.e., W can be anything in that space.  Convex NMF[17] restricts the columns of W to convex combinations of the input data vectors 



(

v

1


,
…
,

v

n


)


{\displaystyle (v_{1},\dots ,v_{n})}

. This greatly improves the quality of data representation of W. Furthermore, the resulting matrix factor H becomes more sparse and orthogonal.
 In case the nonnegative rank of V is equal to its actual rank, V = WH is called a nonnegative rank factorization (NRF).[18][19][20] The problem of finding the NRF of V, if it exists, is known to be NP-hard.[21]
 There are different types of non-negative matrix factorizations.
The different types arise from using different cost functions for measuring the divergence between V and WH and possibly by regularization of the W and/or H matrices.[1]
 Two simple divergence functions studied by Lee and Seung are the squared error (or Frobenius norm) and an extension of the Kullback–Leibler divergence to positive matrices (the original Kullback–Leibler divergence is defined on probability distributions).
Each divergence leads to a different NMF algorithm, usually minimizing the divergence using iterative update rules.
 The factorization problem in the squared error version of NMF may be stated as:
Given a matrix 




V



{\displaystyle \mathbf {V} }

 find nonnegative matrices W and H that minimize the function
 Another type of NMF for images is based on the total variation norm.[22]
 When L1 regularization (akin to Lasso) is added to NMF with the mean squared error cost function, the resulting problem may be called non-negative sparse coding due to the similarity to the sparse coding problem,[23][24]
although it may also still be referred to as NMF.[25]
 Many standard NMF algorithms analyze all the data together; i.e., the whole matrix is available from the start. This may be unsatisfactory in applications where there are too many data to fit into memory or where the data are provided in streaming fashion. One such use is for collaborative filtering in recommendation systems, where there may be many users and many items to recommend, and it would be inefficient to recalculate everything when one user or one item is added to the system. The cost function for optimization in these cases may or may not be the same as for standard NMF, but the algorithms need to be rather different.[26][27]
 If the columns of V represent data sampled over spatial or temporal dimensions, e.g. time signals, images, or video, features that are equivariant w.r.t. shifts along these dimensions can be learned by Convolutional NMF. In this case, W is sparse with columns having local non-zero weight windows that are shared across shifts along the spatio-temporal dimensions of V, representing convolution kernels. By spatio-temporal pooling of H and repeatedly using the resulting representation as input to convolutional NMF, deep feature hierarchies can be learned.[28]
 There are several ways in which the W and H may be found: Lee and Seung's multiplicative update rule[14] has been a popular method due to the simplicity of implementation.  This algorithm is:
 Note that the updates are done on an element by element basis not matrix multiplication.
 We note that the multiplicative factors for W and H, i.e. the 








W



T




V





W



T




W


H






{\textstyle {\frac {\mathbf {W} ^{\mathsf {T}}\mathbf {V} }{\mathbf {W} ^{\mathsf {T}}\mathbf {W} \mathbf {H} }}}

 and 









V



H



T






W


H



H



T










{\textstyle {\textstyle {\frac {\mathbf {V} \mathbf {H} ^{\mathsf {T}}}{\mathbf {W} \mathbf {H} \mathbf {H} ^{\mathsf {T}}}}}}

 terms, are matrices of ones when 




V

=

W


H



{\displaystyle \mathbf {V} =\mathbf {W} \mathbf {H} }

.
 More recently other algorithms have been developed.
Some approaches are based on alternating non-negative least squares: in each step of such an algorithm, first H is fixed and W found by a non-negative least squares solver, then W is fixed and H is found analogously. The procedures used to solve for W and H may be the same[29] or different, as some NMF variants regularize one of W and H.[23] Specific approaches include the projected gradient descent methods,[29][30] the active set method,[6][31] the optimal gradient method,[32] and the block principal pivoting method[33] among several others.[34]
 Current algorithms are sub-optimal in that they only guarantee finding a local minimum, rather than a global minimum of the cost function. A provably optimal algorithm is unlikely in the near future as the problem has been shown to generalize the k-means clustering problem which is known to be NP-complete.[35] However, as in many other data mining applications, a local minimum may still prove to be useful.
 In addition to the optimization step, initialization has a significant effect on NMF. The initial values chosen for W and H may affect not only the rate of convergence, but also the overall error at convergence. Some options for initialization include complete randomization, SVD, k-means clustering, and more advanced strategies based on these and other paradigms.[36]
 The sequential construction of NMF components (W and H) was firstly used to relate NMF with Principal Component Analysis (PCA) in astronomy.[37] The contribution from the PCA components are ranked by the magnitude of their corresponding eigenvalues; for NMF, its components can be ranked empirically when they are constructed one by one (sequentially), i.e., learn the 



(
n
+
1
)


{\displaystyle (n+1)}

-th component with the first 



n


{\displaystyle n}

 components constructed.
 The contribution of the sequential NMF components can be compared with the Karhunen–Loève theorem, an application of PCA, using the plot of eigenvalues. A typical choice of the number of components with PCA is based on the ""elbow"" point, then the existence of the flat plateau is indicating that PCA is not capturing the data efficiently, and at last there exists a sudden drop reflecting the capture of random noise and falls into the regime of overfitting.[38][39] For sequential NMF, the plot of eigenvalues is approximated by the plot of the fractional residual variance curves, where the curves decreases continuously, and converge to a higher level than PCA,[4] which is the indication of less over-fitting of sequential NMF.
 Exact solutions for the variants of NMF can be expected (in polynomial time) when additional constraints hold for matrix V. A polynomial time algorithm for solving nonnegative rank factorization if V contains a monomial sub matrix of rank equal to its rank was given by Campbell and Poole in 1981.[40] Kalofolias and Gallopoulos (2012)[41] solved the symmetric counterpart of this problem, where V is symmetric and contains a diagonal principal sub matrix of rank r. Their algorithm runs in O(rm2) time in the dense case. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) give a polynomial time algorithm for exact NMF that works for the case where one of the factors W satisfies a separability condition.[42]
 In Learning the parts of objects by non-negative matrix factorization Lee and Seung[43] proposed NMF mainly for parts-based decomposition of images.  It compares NMF to vector quantization and principal component analysis, and shows that although the three techniques may be written as factorizations, they implement different constraints and therefore produce different results.
 It was later shown that some types of NMF are an instance of a more general probabilistic model called ""multinomial PCA"".[44]
When NMF is obtained by minimizing the Kullback–Leibler divergence, it is in fact equivalent to another instance of multinomial PCA, probabilistic latent semantic analysis,[45]
trained by maximum likelihood estimation.
That method is commonly used for analyzing and clustering textual data and is also related to the latent class model.
 NMF with the least-squares objective is equivalent to a relaxed form of K-means clustering: the matrix factor W contains cluster centroids and H contains cluster membership indicators.[15][46]  This provides a theoretical foundation for using NMF for data clustering. However, k-means does not enforce non-negativity on its centroids, so the closest analogy is in fact with ""semi-NMF"".[17]
 NMF can be seen as a two-layer directed graphical model with one layer of observed random variables and one layer of hidden random variables.[47]
 NMF extends beyond matrices to tensors of arbitrary order.[48][49][50] This extension may be viewed as a non-negative counterpart to, e.g., the PARAFAC model.
 Other extensions of NMF include joint factorization of several data matrices and tensors where some factors are shared. Such models are useful for sensor fusion and relational learning.[51]
 NMF is an instance of nonnegative quadratic programming, just like the support vector machine (SVM). However, SVM and NMF are related at a more intimate level than that of NQP, which allows direct application of the solution algorithms developed for either of the two methods to problems in both domains.[52]
 The factorization is not unique: A matrix and its inverse can be used to transform the two factorization matrices by, e.g.,[53]
 If the two new matrices 







W
~



=
W
B



{\displaystyle \mathbf {{\tilde {W}}=WB} }

 and 






H
~



=


B


−
1



H



{\displaystyle \mathbf {\tilde {H}} =\mathbf {B} ^{-1}\mathbf {H} }

 are non-negative they form another parametrization of the factorization.
 The non-negativity of 






W
~





{\displaystyle \mathbf {\tilde {W}} }

 and 






H
~





{\displaystyle \mathbf {\tilde {H}} }

 applies at least if B is a non-negative monomial matrix.
In this simple case it will just correspond to a scaling and a permutation.
 More control over the non-uniqueness of NMF is obtained with sparsity constraints.[54]
 In astronomy, NMF is a promising method for dimension reduction in the sense that astrophysical signals are non-negative. NMF has been applied to the spectroscopic observations[55][3] and the direct imaging observations[4] as a method to study the common properties of astronomical objects and post-process the astronomical observations. The advances in the spectroscopic observations by Blanton & Roweis (2007)[3] takes into account of the uncertainties of astronomical observations, which is later improved by Zhu (2016)[37] where missing data are also considered and parallel computing is enabled. Their method is then adopted by Ren et al. (2018)[4] to the direct imaging field as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar disks.
 Ren et al. (2018)[4] are able to prove the stability of NMF components when they are constructed sequentially (i.e., one by one), which enables the linearity of the NMF modeling process; the linearity property is used to separate the stellar light and the light scattered from the exoplanets and circumstellar disks.
 In direct imaging, to reveal the faint exoplanets and circumstellar disks from bright the surrounding stellar lights, which has a typical contrast from 10⁵ to 10¹⁰, various statistical methods have been adopted,[56][57][38] however the light from the exoplanets or circumstellar disks are usually over-fitted, where forward modeling have to be adopted to recover the true flux.[58][39] Forward modeling is currently optimized for point sources,[39] however not for extended sources, especially for irregularly shaped structures such as circumstellar disks. In this situation, NMF has been an excellent method, being less over-fitting in the sense of the non-negativity and sparsity of the NMF modeling coefficients, therefore forward modeling can be performed with a few scaling factors,[4] rather than a computationally intensive data re-reduction on generated models.
 To impute missing data in statistics, NMF can take missing data while minimizing its cost function, rather than treating these missing data as zeros.[5] This makes it a mathematically proven method for data imputation in statistics.[5] By first proving that the missing data are ignored in the cost function, then proving that the impact from missing data can be as small as a second order effect, Ren et al. (2020)[5] studied and applied such an approach for the field of astronomy. Their work focuses on two-dimensional matrices, specifically, it includes mathematical derivation, simulated data imputation, and application to on-sky data.
 The data imputation procedure with NMF can be composed of two steps. First, when the NMF components are known, Ren et al. (2020) proved that impact from missing data during data imputation (""target modeling"" in their study) is a second order effect. Second, when the NMF components are unknown, the authors proved that the impact from missing data during component construction is a first-to-second order effect.
 Depending on the way that the NMF components are obtained, the former step above can be either independent or dependent from the latter. In addition, the imputation quality can be increased when the more NMF components are used, see Figure 4 of Ren et al. (2020) for their illustration.[5]
 NMF can be used for text mining applications.
In this process, a document-term matrix is constructed with the weights of various terms (typically weighted word frequency information) from a set of documents.
This matrix is factored into a term-feature and a feature-document matrix.
The features are derived from the contents of the documents, and the feature-document matrix describes data clusters of related documents.
 One specific application used hierarchical NMF on a small subset of scientific abstracts from PubMed.[59]
Another research group clustered parts of the Enron email dataset[60]
with 65,033 messages and 91,133 terms into 50 clusters.[61]
NMF has also been applied to citations data, with one example clustering English Wikipedia articles and scientific journals based on the outbound scientific citations in English Wikipedia.[62]
 Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) have given polynomial-time algorithms to learn topic models using NMF. The algorithm assumes that the topic matrix satisfies a separability condition that is often found to hold in these settings.[42]
 Hassani, Iranmanesh and Mansouri (2019) proposed a feature agglomeration method for term-document matrices which operates using NMF. The algorithm reduces the term-document matrix into a smaller matrix more suitable for text clustering.[63]
 NMF is also used to analyze spectral data; one such use is in the classification of space objects and debris.[64]
 NMF is applied in scalable Internet distance (round-trip time) prediction. For a network with 



N


{\displaystyle N}

 hosts, with the help of NMF, the distances of all the 




N

2




{\displaystyle N^{2}}

 end-to-end links can be predicted after conducting only 



O
(
N
)


{\displaystyle O(N)}

 measurements. This kind of method was firstly introduced in Internet
Distance Estimation Service (IDES).[65] Afterwards, as a fully decentralized approach, Phoenix network coordinate system[66]
is proposed. It achieves better overall prediction accuracy by introducing the concept of weight.
 Speech denoising has been a long lasting problem in audio signal processing. There are many algorithms for denoising if the noise is stationary. For example, the Wiener filter is suitable for additive Gaussian noise. However, if the noise is non-stationary, the classical denoising algorithms usually have poor performance because the statistical information of the non-stationary noise is difficult to estimate. Schmidt et al.[67] use NMF to do speech denoising under non-stationary noise, which is completely different from classical statistical approaches. The key idea is that clean speech signal can be sparsely represented by a speech dictionary, but non-stationary noise cannot. Similarly, non-stationary noise can also be sparsely represented by a noise dictionary, but speech cannot.
 The algorithm for NMF denoising goes as follows. Two dictionaries, one for speech and one for noise, need to be trained offline. Once a noisy speech is given, we first calculate the magnitude of the Short-Time-Fourier-Transform. Second, separate it into two parts via NMF, one can be sparsely represented by the speech dictionary, and the other part can be sparsely represented by the noise dictionary. Third, the part that is represented by the speech dictionary will be the estimated clean speech.
 Sparse NMF is used in Population genetics for estimating individual admixture coefficients, detecting genetic clusters of individuals in a population sample or evaluating genetic admixture in sampled genomes. In human genetic clustering, NMF algorithms provide estimates similar to those of the computer program STRUCTURE, but the algorithms are more efficient computationally and allow analysis of large population genomic data sets.[68]
 NMF has been successfully applied in bioinformatics for clustering gene expression and DNA methylation data and finding the genes most representative of the clusters.[24][69][70][71] In the analysis of cancer mutations it has been used to identify common patterns of mutations that occur in many cancers and that probably have distinct causes.[72] NMF techniques can identify sources of variation such as cell types, disease subtypes, population stratification, tissue composition, and tumor clonality.[73]
 A particular variant of NMF, namely Non-Negative Matrix Tri-Factorization (NMTF),[74] has been use for drug repurposing tasks in order to predict novel protein targets and therapeutic indications for approved drugs[75] and to infer pair of synergic anticancer drugs.[76]
 NMF, also referred in this field as factor analysis, has been used since the 1980s[77] to analyze sequences of images in SPECT and PET dynamic medical imaging. Non-uniqueness of NMF was addressed using sparsity constraints.[78]
[79]
[80]
 Current research (since 2010) in nonnegative matrix factorization includes, but is not limited to,
",matrix factor nmf nnmf also matrix approxim group algorithm multivari analysi linear algebra matrix v factor usual two matric w h properti three matric neg element make result matric easier inspect also applic process audio spectrogram muscular activ inher data consid sinc problem exactli solvabl gener commonli approxim numer nmf find applic field astronomi comput vision document cluster miss data imput chemometr audio signal process recommend system bioinformat chemometr matrix factor long histori name self model curv resolut framework vector right matrix continu curv rather discret vector also earli work matrix factor perform finnish group research name posit matrix factor becam wide known matrix factor lee seung investig properti algorithm publish simpl use algorithm two type factor let matrix v product matric w h matrix multipl implement comput column vector v linear combin column vector w use coeffici suppli column column v comput follow vi column vector product matrix v hi column vector matrix multipli matric dimens factor matric may significantli lower product matrix properti form basi nmf nmf gener factor significantli reduc dimens compar origin matrix exampl v n matrix w p matrix h p n matrix p significantli less exampl base applic last point basi nmf consid origin document exampl built small set hidden featur nmf gener featur use think featur column vector featur matrix w document archetyp compris set word word cell valu defin word rank featur higher word cell valu higher word rank featur column coeffici matrix h repres origin document cell valu defin document rank featur reconstruct document column vector input matrix linear combin featur column vector w featur weight featur cell valu document column nmf inher cluster properti automat cluster column input data v v v n v n specif approxim v v v w h v w h achiev find w w h h minim error function use frobeniu norm v w h f f subject w h furthermor impos orthogon constraint h h h h h h minim mathemat equival minim cluster furthermor comput h h give cluster membership h k j h j h kj h ij k suggest input data v j j belong k k cluster comput w w give cluster centroid k k column give cluster centroid k k cluster centroid represent significantli enhanc convex nmf orthogon constraint h h h h explicitli impos orthogon hold larg extent cluster properti hold error function use diverg nmf ident probabilist latent semant analysi plsa popular document cluster method usual number column w number row h nmf select product wh becom approxim full decomposit v amount two matric w h well residu u v wh element residu matrix either neg posit w h smaller v becom easier store manipul anoth reason factor v smaller matric w h one goal approxim repres element v significantli less data one infer latent structur data standard nmf matrix factor w w anyth space convex nmf restrict column w convex combin input data vector v v n n greatli improv qualiti data represent furthermor result matrix factor h becom spars orthogon case nonneg rank v equal actual rank v wh call nonneg rank factor nrf problem find nrf v exist known differ type matrix factor differ type aris use differ cost function measur diverg v wh possibl regular w h matric two simpl diverg function studi lee seung squar error frobeniu norm extens diverg posit matric origin diverg defin probabl distribut diverg lead differ nmf algorithm usual minim diverg use iter updat rule factor problem squar error version nmf may state given matrix v v find nonneg matric w h minim function anoth type nmf imag base total variat norm regular akin lasso ad nmf mean squar error cost function result problem may call spars code due similar spars code problem although may also still refer nmf mani standard nmf algorithm analyz data togeth whole matrix avail start may unsatisfactori applic mani data fit memori data provid stream fashion one use collabor filter recommend system may mani user mani item recommend would ineffici recalcul everyth one user one item ad system cost function optim case may may standard nmf algorithm need rather differ column v repres data sampl spatial tempor dimens time signal imag video featur equivari shift along dimens learn convolut nmf case w spars column local weight window share across shift along dimens v repres convolut kernel pool h repeatedli use result represent input convolut nmf deep featur hierarchi learn sever way w h may found lee seung multipl updat rule popular method due simplic implement algorithm note updat done element element basi matrix multipl note multipl factor w h w v w w h w v w w h v h w h h v h w h h term matric one v w h v w h recent algorithm develop approach base altern least squar step algorithm first h fix w found least squar solver w fix h found analog procedur use solv w h may differ nmf variant regular one w specif approach includ project gradient descent method activ set method optim gradient method block princip pivot method among sever other current algorithm guarante find local minimum rather global minimum cost function provabl optim algorithm unlik near futur problem shown gener cluster problem known howev mani data mine applic local minimum may still prove use addit optim step initi signific effect nmf initi valu chosen w h may affect rate converg also overal error converg option initi includ complet random svd cluster advanc strategi base paradigm sequenti construct nmf compon w h firstli use relat nmf princip compon analysi pca astronomi contribut pca compon rank magnitud correspond eigenvalu nmf compon rank empir construct one one sequenti learn n compon first n n compon construct contribut sequenti nmf compon compar theorem applic pca use plot eigenvalu typic choic number compon pca base elbow point exist flat plateau indic pca captur data effici last exist sudden drop reflect captur random nois fall regim overfit sequenti nmf plot eigenvalu approxim plot fraction residu varianc curv curv decreas continu converg higher level pca indic less sequenti nmf exact solut variant nmf expect polynomi time addit constraint hold matrix polynomi time algorithm solv nonneg rank factor v contain monomi sub matrix rank equal rank given campbel pool kalofolia gallopoulo solv symmetr counterpart problem v symmetr contain diagon princip sub matrix rank algorithm run time dens case arora ge halpern mimno moitra sontag wu zhu give polynomi time algorithm exact nmf work case one factor w satisfi separ condit learn part object matrix factor lee seung propos nmf mainli decomposit imag compar nmf vector quantiz princip compon analysi show although three techniqu may written factor implement differ constraint therefor produc differ result later shown type nmf instanc gener probabilist model call multinomi pca nmf obtain minim diverg fact equival anoth instanc multinomi pca probabilist latent semant analysi train maximum likelihood estim method commonli use analyz cluster textual data also relat latent class model nmf object equival relax form cluster matrix factor w contain cluster centroid h contain cluster membership indic provid theoret foundat use nmf data cluster howev enforc centroid closest analog fact nmf seen direct graphic model one layer observ random variabl one layer hidden random variabl nmf extend beyond matric tensor arbitrari order extens may view counterpart parafac model extens nmf includ joint factor sever data matric tensor factor share model use sensor fusion relat learn nmf instanc nonneg quadrat program like support vector machin svm howev svm nmf relat intim level nqp allow direct applic solut algorithm develop either two method problem domain factor uniqu matrix invers use transform two factor matric two new matric w w b w h b h h b h form anoth parametr factor w w h h appli least b monomi matrix simpl case correspond scale permut control nmf obtain sparsiti constraint astronomi nmf promis method dimens reduct sens astrophys signal nmf appli spectroscop observ direct imag observ method studi common properti astronom object astronom observ advanc spectroscop observ blanton rowei take account uncertainti astronom observ later improv zhu miss data also consid parallel comput enabl method adopt ren et al direct imag field one method detect exoplanet especi direct imag circumstellar disk ren et al abl prove stabil nmf compon construct sequenti one one enabl linear nmf model process linear properti use separ stellar light light scatter exoplanet circumstellar disk direct imag reveal faint exoplanet circumstellar disk bright surround stellar light typic contrast variou statist method adopt howev light exoplanet circumstellar disk usual forward model adopt recov true flux forward model current optim point sourc howev extend sourc especi irregularli shape structur circumstellar disk situat nmf excel method less sens sparsiti nmf model coeffici therefor forward model perform scale factor rather comput intens data gener model imput miss data statist nmf take miss data minim cost function rather treat miss data zero make mathemat proven method data imput statist first prove miss data ignor cost function prove impact miss data small second order effect ren et al studi appli approach field astronomi work focus matric specif includ mathemat deriv simul data imput applic data data imput procedur nmf compos two step first nmf compon known ren et al prove impact miss data data imput target model studi second order effect second nmf compon unknown author prove impact miss data compon construct order effect depend way nmf compon obtain former step either independ depend latter addit imput qualiti increas nmf compon use see figur ren et al illustr nmf use text mine applic process matrix construct weight variou term typic weight word frequenc inform set document matrix factor matrix featur deriv content document matrix describ data cluster relat document one specif applic use hierarch nmf small subset scientif abstract pubm anoth research group cluster part enron email dataset messag term cluster nmf also appli citat data one exampl cluster english wikipedia articl scientif journal base outbound scientif citat english wikipedia arora ge halpern mimno moitra sontag wu zhu given algorithm learn topic model use nmf algorithm assum topic matrix satisfi separ condit often found hold set hassani iranmanesh mansouri propos featur agglomer method matric oper use nmf algorithm reduc matrix smaller matrix suitabl text cluster nmf also use analyz spectral data one use classif space object debri nmf appli scalabl internet distanc time predict network n n host help nmf distanc n link predict conduct n n measur kind method firstli introduc internet distanc estim servic ide afterward fulli decentr approach phoenix network coordin system propos achiev better overal predict accuraci introduc concept weight speech denois long last problem audio signal process mani algorithm denois nois stationari exampl wiener filter suitabl addit gaussian nois howev nois classic denois algorithm usual poor perform statist inform nois difficult estim schmidt et al use nmf speech denois nois complet differ classic statist approach key idea clean speech signal spars repres speech dictionari nois similarli nois also spars repres nois dictionari speech algorithm nmf denois goe follow two dictionari one speech one nois need train offlin noisi speech given first calcul magnitud second separ two part via nmf one spars repres speech dictionari part spars repres nois dictionari third part repres speech dictionari estim clean speech spars nmf use popul genet estim individu admixtur coeffici detect genet cluster individu popul sampl evalu genet admixtur sampl genom human genet cluster nmf algorithm provid estim similar comput program structur algorithm effici comput allow analysi larg popul genom data set nmf success appli bioinformat cluster gene express dna methyl data find gene repres cluster analysi cancer mutat use identifi common pattern mutat occur mani cancer probabl distinct caus nmf techniqu identifi sourc variat cell type diseas subtyp popul stratif tissu composit tumor clonal particular variant nmf name matrix nmtf use drug repurpos task order predict novel protein target therapeut indic approv drug infer pair synerg anticanc drug nmf also refer field factor analysi use sinc analyz sequenc imag spect pet dynam medic imag nmf address use sparsiti constraint current research sinc nonneg matrix factor includ limit
Principal component analysis,https://en.wikipedia.org/wiki/Principal_component_analysis,"Principal component analysis (PCA) is a linear dimensionality reduction technique with applications in exploratory data analysis, visualization and data preprocessing.
 The data is linearly transformed onto a new coordinate system such that the directions (principal components) capturing the largest variation in the data can be easily identified.
 The principal components of a collection of points in a real coordinate space are a sequence of 



p


{\displaystyle p}

 unit vectors, where the 



i


{\displaystyle i}

-th vector is the direction of a line that best fits the data while being orthogonal to the first 



i
−
1


{\displaystyle i-1}

 vectors. Here, a best-fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line. These directions (i.e., principal components) constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. Many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points.[1]
 Principal component analysis has applications in many fields such as population genetics, microbiome studies, and atmospheric science.
 When performing PCA, the first principal component of a set of 



p


{\displaystyle p}

 variables is the derived variable formed as a linear combination of the original variables that explains the most variance. The second principal component explains the most variance in what is left once the effect of the first component is removed, and we may proceed through 



p


{\displaystyle p}

 iterations until all the variance is explained. PCA is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set.
The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. The 



i


{\displaystyle i}

-th principal component can be taken as a direction orthogonal to the first 



i
−
1


{\displaystyle i-1}

 principal components that maximizes the variance of the projected data.
 For either objective, it can be shown that the principal components are eigenvectors of the data's covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix. PCA is the simplest of the true eigenvector-based multivariate analyses and is closely related to factor analysis. Factor analysis typically incorporates more domain-specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix. PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset.[2][3][4][5] Robust and L1-norm-based variants of standard PCA have also been proposed.[6][7][8][5]
 PCA was invented in 1901 by Karl Pearson,[9] as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s.[10] Depending on the field of application, it is also named the discrete Karhunen–Loève transform (KLT) in signal processing, the Hotelling transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, singular value decomposition (SVD) of X (invented in the last quarter of the 19th century[11]), eigenvalue decomposition (EVD) of XTX in linear algebra, factor analysis (for a discussion of the differences between PCA and factor analysis see Ch. 7 of Jolliffe's Principal Component Analysis),[12] Eckart–Young theorem (Harman, 1960), or empirical orthogonal functions (EOF) in meteorological science (Lorenz, 1956), empirical eigenfunction decomposition (Sirovich, 1987), quasiharmonic modes (Brooks et al., 1988), spectral decomposition in noise and vibration, and empirical modal analysis in structural dynamics.
 PCA can be thought of as fitting a p-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small.
 To find the axes of the ellipsoid, we must first center the values of each variable in the dataset on 0 by subtracting the mean of the variable's observed values from each of those values. These transformed values are used instead of the original observed values for each of the variables. Then, we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Then we must normalize each of the orthogonal eigenvectors to turn them into unit vectors. Once this is done, each of the mutually-orthogonal unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. This choice of basis will transform the covariance matrix into a diagonalized form, in which the diagonal elements represent the variance of each axis. The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues.
 Biplots and scree plots (degree of explained variance) are used to interpret findings of the PCA. 
 PCA is defined as an orthogonal linear transformation on a real inner product space that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.[12]
 Consider an 



n
×
p


{\displaystyle n\times p}

 data matrix, X, with column-wise zero empirical mean (the sample mean of each column has been shifted to zero), where each of the n rows represents a different repetition of the experiment, and each of the p columns gives a particular kind of feature (say, the results from a particular sensor).
 Mathematically, the transformation is defined by a set of size 



l


{\displaystyle l}

 of p-dimensional vectors of weights or coefficients 





w


(
k
)


=
(

w

1


,
…
,

w

p



)

(
k
)




{\displaystyle \mathbf {w} _{(k)}=(w_{1},\dots ,w_{p})_{(k)}}

 that map each row vector 





x


(
i
)


=
(

x

1


,
…
,

x

p



)

(
i
)




{\displaystyle \mathbf {x} _{(i)}=(x_{1},\dots ,x_{p})_{(i)}}

 of X to a new vector of principal component scores 





t


(
i
)


=
(

t

1


,
…
,

t

l



)

(
i
)




{\displaystyle \mathbf {t} _{(i)}=(t_{1},\dots ,t_{l})_{(i)}}

, given by
 in such a way that the individual variables 




t

1


,
…
,

t

l




{\displaystyle t_{1},\dots ,t_{l}}

 of t considered over the data set successively inherit the maximum possible variance from X, with each coefficient vector w constrained to be a unit vector (where 



l


{\displaystyle l}

 is usually selected to be strictly less than 



p


{\displaystyle p}

 to reduce dimensionality).
 The above may equivalently be written in matrix form as 
 where







T



i
k


=



t

k




(
i
)




{\displaystyle {\mathbf {T} }_{ik}={t_{k}}_{(i)}}

,







X



i
j


=



x

j




(
i
)




{\displaystyle {\mathbf {X} }_{ij}={x_{j}}_{(i)}}

, and







W



j
k


=



w

j




(
k
)




{\displaystyle {\mathbf {W} }_{jk}={w_{j}}_{(k)}}

.
 In order to maximize variance, the first weight vector w(1) thus has to satisfy
 Equivalently, writing this in matrix form gives
 Since w(1) has been defined to be a unit vector, it equivalently also satisfies
 The quantity to be maximised can be recognised as a Rayleigh quotient. A standard result for a positive semidefinite matrix such as XTX is that the quotient's maximum possible value is the largest eigenvalue of the matrix, which occurs when w is the corresponding eigenvector.
 With w(1) found, the first principal component of a data vector x(i) can then be given as a score t1(i) = x(i) ⋅ w(1) in the transformed co-ordinates, or as the corresponding vector in the original variables, {x(i) ⋅ w(1)} w(1).
 The k-th component can be found by subtracting the first k − 1 principal components from X:
 and then finding the weight vector which extracts the maximum variance from this new data matrix
 It turns out that this gives the remaining eigenvectors of XTX, with the maximum values for the quantity in brackets given by their corresponding eigenvalues. Thus the weight vectors are eigenvectors of XTX.
 The k-th principal component of a data vector x(i) can therefore be given as a score tk(i) = x(i) ⋅ w(k) in the transformed coordinates, or as the corresponding vector in the space of the original variables, {x(i) ⋅ w(k)} w(k), where w(k) is the kth eigenvector of XTX.
 The full principal components decomposition of X can therefore be given as
 where W is a p-by-p matrix of weights whose columns are the eigenvectors of XTX. The transpose of W is sometimes called the whitening or sphering transformation. Columns of W multiplied by the square root of corresponding eigenvalues, that is, eigenvectors scaled up by the variances, are called loadings in PCA or in Factor analysis.
 XTX itself can be recognized as proportional to the empirical sample covariance matrix of the dataset XT.[12]: 30–31 
 The sample covariance Q between two of the different principal components over the dataset is given by:
 where the eigenvalue property of w(k) has been used to move from line 2 to line 3. However eigenvectors w(j) and w(k) corresponding to eigenvalues of a symmetric matrix are orthogonal (if the eigenvalues are different), or can be orthogonalised (if the vectors happen to share an equal repeated value). The product in the final line is therefore zero; there is no sample covariance between different principal components over the dataset.
 Another way to characterise the principal components transformation is therefore as the transformation to coordinates which diagonalise the empirical sample covariance matrix.
 In matrix form, the empirical covariance matrix for the original variables can be written
 The empirical covariance matrix between the principal components becomes
 where Λ is the diagonal matrix of eigenvalues λ(k) of XTX. λ(k) is equal to the sum of the squares over the dataset associated with each component k, that is, λ(k) = Σi tk2(i) = Σi (x(i) ⋅ w(k))2.
 The transformation T = X W maps a data vector x(i) from an original space of p variables to a new space of p variables which are uncorrelated over the dataset. However, not all the principal components need to be kept. Keeping only the first L principal components, produced by using only the first L eigenvectors, gives the truncated transformation
 where the matrix TL now has n rows but only L columns. In other words, PCA learns a linear transformation 



t
=

W

L



T



x
,
x
∈


R


p


,
t
∈


R


L


,


{\displaystyle t=W_{L}^{\mathsf {T}}x,x\in \mathbb {R} ^{p},t\in \mathbb {R} ^{L},}

 where the columns of p × L matrix 




W

L




{\displaystyle W_{L}}

 form an orthogonal basis for the L features (the components of representation t) that are decorrelated.[13] By construction, of all the transformed data matrices with only L columns, this score matrix maximises the variance in the original data that has been preserved, while minimising the total squared reconstruction error 



‖

T



W


T


−


T


L




W


L


T



‖

2


2




{\displaystyle \|\mathbf {T} \mathbf {W} ^{T}-\mathbf {T} _{L}\mathbf {W} _{L}^{T}\|_{2}^{2}}

 or 



‖

X

−


X


L



‖

2


2




{\displaystyle \|\mathbf {X} -\mathbf {X} _{L}\|_{2}^{2}}

.
 Such dimensionality reduction can be a very useful step for visualising and processing high-dimensional datasets, while still retaining as much of the variance in the dataset as possible. For example, selecting L = 2 and keeping only the first two principal components finds the two-dimensional plane through the high-dimensional dataset in which the data is most spread out, so if the data contains clusters these too may be most spread out, and therefore most visible to be plotted out in a two-dimensional diagram; whereas if two directions through the data (or two of the original variables) are chosen at random, the clusters may be much less spread apart from each other, and may in fact be much more likely to substantially overlay each other, making them indistinguishable.
 Similarly, in regression analysis, the larger the number of explanatory variables allowed, the greater is the chance of overfitting the model, producing conclusions that fail to generalise to other datasets. One approach, especially when there are strong correlations between different possible explanatory variables, is to reduce them to a few principal components and then run the regression against them, a method called principal component regression.
 Dimensionality reduction may also be appropriate when the variables in a dataset are noisy. If each column of the dataset contains independent identically distributed Gaussian noise, then the columns of T will also contain similarly identically distributed Gaussian noise (such a distribution is invariant under the effects of the matrix W, which can be thought of as a high-dimensional rotation of the co-ordinate axes). However, with more of the total variance concentrated in the first few principal components compared to the same noise variance, the proportionate effect of the noise is less—the first few components achieve a higher signal-to-noise ratio. PCA thus can have the effect of concentrating much of the signal into the first few principal components, which can usefully be captured by dimensionality reduction; while the later principal components may be dominated by noise, and so disposed of without great loss. If the dataset is not too large, the significance of the principal components can be tested using parametric bootstrap, as an aid in determining how many principal components to retain.[14]
 The principal components transformation can also be associated with another matrix factorization, the singular value decomposition (SVD) of X,
 Here Σ is an n-by-p rectangular diagonal matrix of positive numbers σ(k), called the singular values of X; U is an n-by-n matrix, the columns of which are orthogonal unit vectors of length n called the left singular vectors of X; and W is a p-by-p matrix whose columns are orthogonal unit vectors of length p and called the right singular vectors of X.
 In terms of this factorization, the matrix XTX can be written
 where  






Σ
^





{\displaystyle \mathbf {\hat {\Sigma }} }

 is the square diagonal matrix with the singular values of X and the excess zeros chopped off that satisfies 








Σ
^




2



=


Σ



T




Σ



{\displaystyle \mathbf {{\hat {\Sigma }}^{2}} =\mathbf {\Sigma } ^{\mathsf {T}}\mathbf {\Sigma } }

. Comparison with the eigenvector factorization of XTX establishes that the right singular vectors W of X are equivalent to the eigenvectors of XTX, while the singular values σ(k) of  




X



{\displaystyle \mathbf {X} }

 are equal to the square-root of the eigenvalues λ(k) of XTX.
 Using the singular value decomposition the score matrix T can be written
 so each column of T is given by one of the left singular vectors of X multiplied by the corresponding singular value. This form is also the polar decomposition of T.
 Efficient algorithms exist to calculate the SVD of X without having to form the matrix XTX, so computing the SVD is now the standard way to calculate a principal components analysis from a data matrix,[15] unless only a handful of components are required.
 As with the eigen-decomposition, a truncated n × L score matrix TL can be obtained by considering only the first L largest singular values and their singular vectors:
 The truncation of a matrix M or T using a truncated singular value decomposition in this way produces a truncated matrix that is the nearest possible matrix of rank L to the original matrix, in the sense of the difference between the two having the smallest possible Frobenius norm, a result known as the Eckart–Young theorem [1936].
 The singular values (in Σ) are the square roots of the eigenvalues of the matrix XTX. Each eigenvalue is proportional to the portion of the ""variance"" (more correctly of the sum of the squared distances of the points from their multidimensional mean) that is associated with each eigenvector. The sum of all the eigenvalues is equal to the sum of the squared distances of the points from their multidimensional mean. PCA essentially rotates the set of points around their mean in order to align with the principal components. This moves as much of the variance as possible (using an orthogonal transformation) into the first few dimensions. The values in the remaining dimensions, therefore, tend to be small and may be dropped with minimal loss of information (see below). PCA is often used in this manner for dimensionality reduction. PCA has the distinction of being the optimal orthogonal transformation for keeping the subspace that has largest ""variance"" (as defined above). This advantage, however, comes at the price of greater computational requirements if compared, for example, and when applicable, to the discrete cosine transform, and in particular to the DCT-II which is simply known as the ""DCT"". Nonlinear dimensionality reduction techniques tend to be more computationally demanding than PCA.
 PCA is sensitive to the scaling of the variables. If we have just two variables and they have the same sample variance and are completely correlated, then the PCA will entail a rotation by 45° and the ""weights"" (they are the cosines of rotation) for the two variables with respect to the principal component will be equal. But if we multiply all values of the first variable by 100, then the first principal component will be almost the same as that variable, with a small contribution from the other variable, whereas the second component will be almost aligned with the second original variable. This means that whenever the different variables have different units (like temperature and mass), PCA is a somewhat arbitrary method of analysis. (Different results would be obtained if one used Fahrenheit rather than Celsius for example.) Pearson's original paper was entitled ""On Lines and Planes of Closest Fit to Systems of Points in Space"" – ""in space"" implies physical Euclidean space where such concerns do not arise. One way of making the PCA less arbitrary is to use variables scaled so as to have unit variance, by standardizing the data and hence use the autocorrelation matrix instead of the autocovariance matrix as a basis for PCA. However, this compresses (or expands) the fluctuations in all dimensions of the signal space to unit variance.
 Mean subtraction (a.k.a. ""mean centering"") is necessary for performing classical PCA to ensure that the first principal component describes the direction of maximum variance. If mean subtraction is not performed, the first principal component might instead correspond more or less to the mean of the data. A mean of zero is needed for finding a basis that minimizes the mean square error of the approximation of the data.[16]
 Mean-centering is unnecessary if performing a principal components analysis on a correlation matrix, as the data are already centered after calculating correlations. Correlations are derived from the cross-product of two standard scores (Z-scores) or statistical moments (hence the name: Pearson Product-Moment Correlation). Also see the article by Kromrey & Foster-Johnson (1998) on ""Mean-centering in Moderated Regression: Much Ado About Nothing"". Since covariances are correlations of normalized variables (Z- or standard-scores) a PCA based on the correlation matrix of X is equal to a PCA based on the covariance matrix of Z, the standardized version of X.
 PCA is a popular primary technique in pattern recognition. It is not, however, optimized for class separability.[17] However, it has been used to quantify the distance between two or more classes by calculating center of mass for each class in principal component space and reporting Euclidean distance between center of mass of two or more classes.[18] The linear discriminant analysis is an alternative which is optimized for class separability.
 
 Some properties of PCA include:[12][page needed]
 The statistical implication of this property is that the last few PCs are not simply unstructured left-overs after removing the important PCs. Because these last PCs have variances as small as possible they are useful in their own right. They can help to detect unsuspected near-constant linear relationships between the elements of x, and they may also be useful in regression, in selecting a subset of variables from x, and in outlier detection.
 Before we look at its usage, we first look at diagonal elements,
 Then, perhaps the main statistical implication of the result is that not only can we decompose the combined variances of all the elements of x into decreasing contributions due to each PC, but we can also decompose the whole covariance matrix into contributions 




λ

k



α

k



α

k

′



{\displaystyle \lambda _{k}\alpha _{k}\alpha _{k}'}

 from each PC. Although not strictly decreasing, the elements of 




λ

k



α

k



α

k

′



{\displaystyle \lambda _{k}\alpha _{k}\alpha _{k}'}

 will tend to become smaller as 



k


{\displaystyle k}

 increases, as 




λ

k



α

k



α

k

′



{\displaystyle \lambda _{k}\alpha _{k}\alpha _{k}'}

 is nonincreasing for increasing 



k


{\displaystyle k}

, whereas the elements of 




α

k




{\displaystyle \alpha _{k}}

 tend to stay about the same size because of the normalization constraints: 




α

k

′


α

k


=
1
,
k
=
1
,
…
,
p


{\displaystyle \alpha _{k}'\alpha _{k}=1,k=1,\dots ,p}

.
 As noted above, the results of PCA depend on the scaling of the variables. This can be cured by scaling each feature by its standard deviation, so that one ends up with dimensionless features with unital variance.[19]
 The applicability of PCA as described above is limited by certain (tacit) assumptions[20] made in its derivation. In particular, PCA can capture linear correlations between the features but fails when this assumption is violated (see Figure 6a in the reference). In some cases, coordinate transformations can restore the linearity assumption and PCA can then be applied (see kernel PCA).
 Another limitation is the mean-removal process before constructing the covariance matrix for PCA. In fields such as astronomy, all the signals are non-negative, and the mean-removal process will force the mean of some astrophysical exposures to be zero, which consequently creates unphysical negative fluxes,[21] and forward modeling has to be performed to recover the true magnitude of the signals.[22] As an alternative method, non-negative matrix factorization focusing only on the non-negative elements in the matrices, which is well-suited for astrophysical observations.[23][24][25] See more at Relation between PCA and Non-negative Matrix Factorization.
 PCA is at a disadvantage if the data has not been standardized before applying the algorithm to it. PCA transforms original data into data that is relevant to the principal components of that data, which means that the new data variables cannot be interpreted in the same ways that the originals were. They are linear interpretations of the original variables. Also, if PCA is not performed properly, there is a high likelihood of information loss.[26]
 PCA relies on a linear model. If a dataset has a pattern hidden inside it that is nonlinear, then PCA can actually steer the analysis in the complete opposite direction of progress.[27][page needed] Researchers at Kansas State University discovered that the sampling error in their experiments impacted the bias of PCA results. ""If the number of subjects or blocks is smaller than 30, and/or the researcher is interested in PC's beyond the first, it may be better to first correct for the serial correlation, before PCA is conducted"".[28] The researchers at Kansas State also found that PCA could be ""seriously biased if the autocorrelation structure of the data is not correctly handled"".[28]
 Dimensionality reduction results in a loss of information, in general. PCA-based dimensionality reduction tends to minimize that information loss, under certain signal and noise models.
 Under the assumption that
 that is, that the data vector 




x



{\displaystyle \mathbf {x} }

 is the sum of the desired information-bearing signal 




s



{\displaystyle \mathbf {s} }

 and a noise signal 




n



{\displaystyle \mathbf {n} }

 one can show that PCA can be optimal for dimensionality reduction, from an information-theoretic point-of-view.
 In particular, Linsker showed that if 




s



{\displaystyle \mathbf {s} }

 is Gaussian and 




n



{\displaystyle \mathbf {n} }

 is Gaussian noise with a covariance matrix proportional to the identity matrix, the PCA maximizes the mutual information 



I
(

y

;

s

)


{\displaystyle I(\mathbf {y} ;\mathbf {s} )}

 between the desired information 




s



{\displaystyle \mathbf {s} }

 and the dimensionality-reduced output 




y

=


W


L


T



x



{\displaystyle \mathbf {y} =\mathbf {W} _{L}^{T}\mathbf {x} }

.[29]
 If the noise is still Gaussian and has a covariance matrix proportional to the identity matrix (that is, the components of the vector 




n



{\displaystyle \mathbf {n} }

 are iid), but the information-bearing signal 




s



{\displaystyle \mathbf {s} }

 is non-Gaussian (which is a common scenario), PCA at least minimizes an upper bound on the information loss, which is defined as[30][31]
 The optimality of PCA is also preserved if the noise 




n



{\displaystyle \mathbf {n} }

 is iid and at least more Gaussian (in terms of the Kullback–Leibler divergence) than the information-bearing signal 




s



{\displaystyle \mathbf {s} }

.[32] In general, even if the above signal model holds, PCA loses its information-theoretic optimality as soon as the noise 




n



{\displaystyle \mathbf {n} }

 becomes dependent.
 The following is a detailed description of PCA using the covariance method[33] as opposed to the correlation method.[34]
 The goal is to transform a given data set X of dimension p to an alternative data set Y of smaller dimension L. Equivalently, we are seeking to find the matrix Y, where Y is the Karhunen–Loève transform (KLT) of matrix X:
 




Y

=

K
L
T

{

X

}


{\displaystyle \mathbf {Y} =\mathbb {KLT} \{\mathbf {X} \}}


 Suppose you have data comprising a set of observations of p variables, and you want to reduce the data so that each observation can be described with only L variables, L < p. Suppose further, that the data are arranged as a set of n data vectors 





x


1


…


x


n




{\displaystyle \mathbf {x} _{1}\ldots \mathbf {x} _{n}}

 with each 





x


i




{\displaystyle \mathbf {x} _{i}}

 representing a single grouped observation of the p variables.
 Mean subtraction is an integral part of the solution towards finding a principal component basis that minimizes the mean square error of approximating the data.[35] Hence we proceed by centering the data as follows:
 In some applications, each variable (column of B) may also be scaled to have a variance equal to 1 (see Z-score).[36] This step affects the calculated principal components, but makes them independent of the units used to measure the different variables.
 Let X be a d-dimensional random vector expressed as column vector. Without loss of generality, assume X has zero mean.
 We want to find 



(
∗
)


{\displaystyle (\ast )}

 a d × d orthonormal transformation matrix P so that PX has a diagonal covariance matrix (that is, PX is a random vector with all its distinct components pairwise uncorrelated).
 A quick computation assuming 



P


{\displaystyle P}

 were unitary yields:
 Hence 



(
∗
)


{\displaystyle (\ast )}

 holds if and only if 



cov
⁡
(
X
)


{\displaystyle \operatorname {cov} (X)}

 were diagonalisable by 



P


{\displaystyle P}

.
 This is very constructive, as cov(X) is guaranteed to be a non-negative definite matrix and thus is guaranteed to be diagonalisable by some unitary matrix.
 In practical implementations, especially with high dimensional data (large p), the naive covariance method is rarely used because it is not efficient due to high computational and memory costs of explicitly determining the covariance matrix. The covariance-free approach avoids the np2 operations of explicitly calculating and storing the covariance matrix XTX, instead utilizing one of matrix-free methods, for example, based on the function evaluating the product XT(X r) at the cost of 2np operations.
 One way to compute the first principal component efficiently[41] is shown in the following pseudo-code, for a data matrix X with zero mean, without ever computing its covariance matrix.
 This power iteration algorithm simply calculates the vector XT(X r), normalizes, and places the result back in r. The eigenvalue is approximated by rT (XTX) r, which is the Rayleigh quotient on the unit vector r for the covariance matrix XTX . If the largest singular value is well separated from the next largest one, the vector r gets close to the first principal component of X within the number of iterations c, which is small relative to p, at the total cost 2cnp. The power iteration convergence can be accelerated without noticeably sacrificing the small cost per iteration using more advanced matrix-free methods, such as the Lanczos algorithm or the Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) method.
 Subsequent principal components can be computed one-by-one via deflation or simultaneously as a block. In the former approach, imprecisions in already computed approximate principal components additively affect the accuracy of the subsequently computed principal components, thus increasing the error with every new computation. The latter approach in the block power method replaces single-vectors r and s with block-vectors, matrices R and S. Every column of R approximates one of the leading principal components, while all columns are iterated simultaneously. The main calculation is evaluation of the product XT(X R). Implemented, for example, in LOBPCG, efficient blocking eliminates the accumulation of the errors, allows using high-level BLAS matrix-matrix product functions, and typically leads to faster convergence, compared to the single-vector one-by-one technique.
 Non-linear iterative partial least squares (NIPALS) is a variant the classical power iteration with matrix deflation by subtraction implemented for computing the first few components in a principal component or partial least squares analysis. For very-high-dimensional datasets, such as those generated in the *omics sciences (for example, genomics, metabolomics) it is usually only necessary to compute the first few PCs. The non-linear iterative partial least squares (NIPALS) algorithm updates iterative approximations to the leading scores and loadings t1 and r1T by the power iteration multiplying on every iteration by X on the left and on the right, that is, calculation of the covariance matrix is avoided, just as in the matrix-free implementation of the power iterations to XTX, based on the function evaluating the product XT(X r) = ((X r)TX)T.
 The matrix deflation by subtraction is performed by subtracting the outer product, t1r1T from X leaving the deflated residual matrix used to calculate the subsequent leading PCs.[42]
For large data matrices, or matrices that have a high degree of column collinearity, NIPALS suffers from loss of orthogonality of PCs due to machine precision round-off errors accumulated in each iteration and matrix deflation by subtraction.[43] A Gram–Schmidt re-orthogonalization algorithm is applied to both the scores and the loadings at each iteration step to eliminate this loss of orthogonality.[44] NIPALS reliance on single-vector multiplications cannot take advantage of high-level BLAS and results in slow convergence for clustered leading singular values—both these deficiencies are resolved in more sophisticated matrix-free block solvers, such as the Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) method.
 In an ""online"" or ""streaming"" situation with data arriving piece by piece rather than being stored in a single batch, it is useful to make an estimate of the PCA projection that can be updated sequentially. This can be done efficiently, but requires different algorithms.[45]
 In PCA, it is common that we want to introduce qualitative variables as supplementary elements. For example, many quantitative variables have been measured on plants. For these plants, some qualitative variables are available as, for example, the species to which the plant belongs. These data were subjected to PCA for quantitative variables. When analyzing the results, it is natural to connect the principal components to the qualitative variable species.
For this, the following results are produced.
 These results are what is called introducing a qualitative variable as supplementary element. This procedure is detailed in and Husson, Lê, & Pagès (2009) and Pagès (2013).
Few software offer this option in an ""automatic"" way. This is the case of SPAD that historically, following the work of Ludovic Lebart, was the first to propose this option, and the R package FactoMineR.
 The earliest application of factor analysis was in locating and measuring components of human intelligence. It was believed that intelligence had various uncorrelated components such as spatial intelligence, verbal intelligence, induction, deduction etc and that scores on these could be adduced by factor analysis from results on various tests, to give a single index known as the Intelligence Quotient (IQ). The pioneering statistical psychologist Spearman actually developed factor analysis in 1904 for his two-factor theory of intelligence, adding a formal technique to the science of psychometrics. In 1924 Thurstone looked for 56 factors of intelligence, developing the notion of Mental Age. Standard IQ tests today are based on this early work.[46]
 In 1949, Shevky and Williams introduced the theory of factorial ecology, which dominated studies of residential differentiation from the 1950s to the 1970s.[47] Neighbourhoods in a city were recognizable or could be distinguished from one another by various characteristics which could be reduced to three by factor analysis. These were known as 'social rank' (an index of occupational status), 'familism' or family size, and 'ethnicity'; Cluster analysis could then be applied to divide the city into clusters or precincts according to values of the three key factor variables. An extensive literature developed around factorial ecology in urban geography, but the approach went out of fashion after 1980 as being methodologically primitive and having little place in postmodern geographical paradigms.
 One of the problems with factor analysis has always been finding convincing names for the various artificial factors. In 2000, Flood revived the factorial ecology approach to show that principal components analysis actually gave meaningful answers directly, without resorting to factor rotation. The principal components were actually dual variables or shadow prices of 'forces' pushing people together or apart in cities. The first component was 'accessibility', the classic trade-off between demand for travel and demand for space, around which classical urban economics is based. The next two components were 'disadvantage', which keeps people of similar status in separate neighbourhoods (mediated by planning), and ethnicity, where people of similar ethnic backgrounds try to co-locate.[48]
 About the same time, the Australian Bureau of Statistics defined distinct indexes of advantage and disadvantage taking the first principal component of sets of key variables that were thought to be important. These SEIFA indexes are regularly published for various jurisdictions, and are used frequently in spatial analysis.[49]
 PCA can be used as a formal method for the development of indexes. As an alternative confirmatory composite analysis has been proposed to develop and assess indexes.[50]
 The City Development Index was developed by PCA from about 200 indicators of city outcomes in a 1996 survey of 254 global cities. The first principal component was subject to iterative regression, adding the original variables singly until about 90% of its variation was accounted for. The index ultimately used about 15 indicators but was a good predictor of many more variables. Its comparative value agreed very well with a subjective assessment of the condition of each city. The coefficients on items of infrastructure were roughly proportional to the average costs of providing the underlying services, suggesting the Index was actually a measure of effective physical and social investment in the city.
 The country-level Human Development Index (HDI) from UNDP, which has been published since 1990 and is very extensively used in development studies,[51] has very similar coefficients on similar indicators, strongly suggesting it was originally constructed using PCA.
 In 1978 Cavalli-Sforza and others pioneered the use of principal components analysis (PCA) to summarise data on variation in human gene frequencies across regions. The components showed distinctive patterns, including gradients and sinusoidal waves. They interpreted these patterns as resulting from specific ancient migration events.
 Since then, PCA has been ubiquitous in population genetics, with thousands of papers using PCA as a display mechanism. Genetics varies largely according to proximity, so the first two principal components actually show spatial distribution and may be used to map the relative geographical location of different population groups, thereby showing individuals who have wandered from their original locations.[52]
 PCA in genetics has been technically controversial, in that the technique has been performed on discrete non-normal variables and often on binary allele markers. The lack of any measures of standard error in PCA are also an impediment to more consistent usage. In August 2022, the molecular biologist Eran Elhaik published a theoretical paper in Scientific Reports analyzing 12 PCA applications. He concluded that it was easy to manipulate the method, which, in his view, generated results that were 'erroneous, contradictory, and absurd.' Specifically, he argued, the results achieved in population genetics were characterized by cherry-picking and circular reasoning.[53]
 Market research has been an extensive user of PCA. It is used to develop customer satisfaction or customer loyalty scores for products, and with clustering, to develop market segments that may be targeted with advertising campaigns, in much the same way as factorial ecology will locate geographical areas with similar characteristics.[54]
 PCA rapidly transforms large amounts of data into smaller, easier-to-digest variables that can be more rapidly and readily analyzed. In any consumer questionnaire, there are series of questions designed to elicit consumer attitudes, and principal components seek out latent variables underlying these attitudes. For example, the Oxford Internet Survey in 2013 asked 2000 people about their attitudes and beliefs, and from these analysts extracted four principal component dimensions, which they identified as 'escape', 'social networking', 'efficiency', and 'problem creating'.[55]
 Another example from Joe Flood in 2008 extracted an attitudinal index toward housing from 28 attitude questions in a national survey of 2697 households in Australia. The first principal component represented a general attitude toward property and home ownership. The index, or the attitude questions it embodied, could be fed into a General Linear Model of tenure choice. The strongest determinant of private renting by far was the attitude index, rather than income, marital status or household type.[56]
 In quantitative finance, PCA is used[57]
in financial risk management, and has been applied to other problems such as portfolio optimization.
 PCA is commonly used in problems involving fixed income securities and portfolios, and interest rate derivatives. 
Valuations here depend on the entire yield curve, comprising numerous highly correlated instruments, and PCA is used to define a set of components or factors that explain rate movements,[58]
thereby facilitating the modelling.
One common risk management application is to calculating value at risk, VaR, applying PCA to the Monte Carlo simulation.
[59]
Here, for each simulation-sample, the components are stressed, and rates, and in turn option values, are then reconstructed; 
with VaR calculated, finally, over the entire run. 
PCA is also used in hedging exposure to interest rate risk, given partial durations and other sensitivities.
[58]
Under both, the first three, typically, principal components of the system are of interest (representing ""shift"", ""twist"", and ""curvature"").
These principal components are derived from an eigen-decomposition of the covariance matrix of yield at predefined maturities; 
[60]
and where the variance of each component is its eigenvalue (and as the components are orthogonal, no correlation need be incorporated in subsequent modelling).
 For equity, an optimal portfolio is one where the expected return is maximized for a given level of risk, or alternatively, where risk is minimized for a given return; see Markowitz model for discussion.
Thus, one approach is to reduce portfolio risk, where allocation strategies are applied to the ""principal portfolios"" instead of the underlying stocks.
A second approach is to enhance portfolio return, using the principal components to select companies' stocks with upside potential.
[61]
[62]
PCA has also been used to understand relationships [57] between international equity markets, and within markets between groups of companies in industries or sectors.
 PCA may also be applied to stress testing,[63] essentially an analysis of a bank's ability to endure a hypothetical adverse economic scenario. Its utility is in ""distilling the information contained in [several] macroeconomic variables into a more manageable data set, which can then [be used] for analysis.""[63] Here, the resulting factors are linked to e.g. interest rates – based on the largest elements of the factor's eigenvector – and it is then observed how a ""shock"" to each of the factors affects the implied assets of each of the banks.
 A variant of principal components analysis is used in neuroscience to identify the specific properties of a stimulus that increases a neuron's probability of generating an action potential.[64][65] This technique is known as spike-triggered covariance analysis. In a typical application an experimenter presents a white noise process as a stimulus (usually either as a sensory input to a test subject, or as a current injected directly into the neuron) and records a train of action potentials, or spikes, produced by the neuron as a result. Presumably, certain features of the stimulus make the neuron more likely to spike. In order to extract these features, the experimenter calculates the covariance matrix of the spike-triggered ensemble, the set of all stimuli (defined and discretized over a finite time window, typically on the order of 100 ms) that immediately preceded a spike. The eigenvectors of the difference between the spike-triggered covariance matrix and the covariance matrix of the prior stimulus ensemble (the set of all stimuli, defined over the same length time window) then indicate the directions in the space of stimuli along which the variance of the spike-triggered ensemble differed the most from that of the prior stimulus ensemble. Specifically, the eigenvectors with the largest positive eigenvalues correspond to the directions along which the variance of the spike-triggered ensemble showed the largest positive change compared to the variance of the prior. Since these were the directions in which varying the stimulus led to a spike, they are often good approximations of the sought after relevant stimulus features.
 In neuroscience, PCA is also used to discern the identity of a neuron from the shape of its action potential. Spike sorting is an important procedure because extracellular recording techniques often pick up signals from more than one neuron. In spike sorting, one first uses PCA to reduce the dimensionality of the space of action potential waveforms, and then performs clustering analysis to associate specific action potentials with individual neurons.
 PCA as a dimension reduction technique is particularly suited to detect coordinated activities of large neuronal ensembles. It has been used in determining collective variables, that is, order parameters, during phase transitions in the brain.[66]
 Correspondence analysis (CA)
was developed by Jean-Paul Benzécri[67]
and is conceptually similar to PCA, but scales the data (which should be non-negative) so that rows and columns are treated equivalently. It is traditionally applied to contingency tables.
CA decomposes the chi-squared statistic associated to this table into orthogonal factors.[68]
Because CA is a descriptive technique, it can be applied to tables for which the chi-squared statistic is appropriate or not.
Several variants of CA are available including detrended correspondence analysis and canonical correspondence analysis. One special extension is multiple correspondence analysis, which may be seen as the counterpart of principal component analysis for categorical data.[69]
 Principal component analysis creates variables that are linear combinations of the original variables. The new variables have the property that the variables are all orthogonal. The PCA transformation can be helpful as a pre-processing step before clustering. PCA is a variance-focused approach seeking to reproduce the total variable variance, in which components reflect both common and unique variance of the variable. PCA is generally preferred for purposes of data reduction (that is, translating variable space into optimal factor space) but not when the goal is to detect the latent construct or factors.
 Factor analysis is similar to principal component analysis, in that factor analysis also involves linear combinations of variables. Different from PCA, factor analysis is a correlation-focused approach seeking to reproduce the inter-correlations among variables, in which the factors ""represent the common variance of variables, excluding unique variance"".[70] In terms of the correlation matrix, this corresponds with focusing on explaining the off-diagonal terms (that is, shared co-variance), while PCA focuses on explaining the terms that sit on the diagonal. However, as a side result, when trying to reproduce the on-diagonal terms, PCA also tends to fit relatively well the off-diagonal correlations.[12]: 158  Results given by PCA and factor analysis are very similar in most situations, but this is not always the case, and there are some problems where the results are significantly different. Factor analysis is generally used when the research purpose is detecting data structure (that is, latent constructs or factors) or causal modeling. If the factor model is incorrectly formulated or the assumptions are not met, then factor analysis will give erroneous results.[71]
 It has been asserted that the relaxed solution of k-means clustering, specified by the cluster indicators, is given by the principal components, and the PCA subspace spanned by the principal directions is identical to the cluster centroid subspace.[72][73] However, that PCA is a useful relaxation of k-means clustering was not a new result,[74] and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.[75]
 Non-negative matrix factorization (NMF) is a dimension reduction method where only non-negative elements in the matrices are used, which is therefore a promising method in astronomy,[23][24][25] in the sense that astrophysical signals are non-negative. The PCA components are orthogonal to each other, while the NMF components are all non-negative and therefore constructs a non-orthogonal basis.
 In PCA, the contribution of each component is ranked based on the magnitude of its corresponding eigenvalue, which is equivalent to the fractional residual variance (FRV) in analyzing empirical data.[21] For NMF, its components are ranked based only on the empirical FRV curves.[25] The residual fractional eigenvalue plots, that is, 



1
−

∑

i
=
1


k



λ

i




/



∑

j
=
1


n



λ

j




{\displaystyle 1-\sum _{i=1}^{k}\lambda _{i}{\Big /}\sum _{j=1}^{n}\lambda _{j}}

 as a function of component number 



k


{\displaystyle k}

 given a total of 



n


{\displaystyle n}

 components, for PCA have a flat plateau, where no data is captured to remove the quasi-static noise, then the curves drop quickly as an indication of over-fitting (random noise).[21] The FRV curves for NMF is decreasing continuously[25] when the NMF components are constructed sequentially,[24] indicating the continuous capturing of quasi-static noise; then converge to higher levels than PCA,[25] indicating the less over-fitting property of NMF.
 It is often difficult to interpret the principal components when the data include many variables of various origins, or when some variables are qualitative. This leads the PCA user to a delicate elimination of several variables. If observations or variables have an excessive impact on the direction of the axes, they should be removed and then projected as supplementary elements. In addition, it is necessary to avoid interpreting the proximities between the points close to the center of the factorial plane.
 The iconography of correlations, on the contrary, which is not a projection on a system of axes, does not have these drawbacks. We can therefore keep all the variables.
 The principle of the diagram is to underline the ""remarkable"" correlations of the correlation matrix, by a solid line (positive correlation) or dotted line (negative correlation).
 A strong correlation is not ""remarkable"" if it is not direct, but caused by the effect of a third variable. Conversely, weak correlations can be ""remarkable"". For example, if a variable Y depends on several independent variables, the correlations of Y with each of them are weak and yet ""remarkable"".
 A particular disadvantage of PCA is that the principal components are usually linear combinations of all input variables. Sparse PCA overcomes this disadvantage by finding linear combinations that contain just a few input variables. It extends the classic method of principal component analysis (PCA) for the reduction of dimensionality of data by adding sparsity constraint on the input variables.
Several approaches have been proposed, including
 The methodological and theoretical developments of Sparse PCA as well as its applications in scientific studies were recently reviewed in a survey paper.[82]
 Most of the modern methods for nonlinear dimensionality reduction find their theoretical and algorithmic roots in PCA or K-means. Pearson's original idea was to take a straight line (or plane) which will be ""the best fit"" to a set of data points. Trevor Hastie expanded on this concept by proposing Principal curves[86] as the natural extension for the geometric interpretation of PCA, which explicitly constructs a manifold for data approximation followed by projecting the points onto it. See also the elastic map algorithm and principal geodesic analysis.[87] Another popular generalization is kernel PCA, which corresponds to PCA performed in a reproducing kernel Hilbert space associated with a positive definite kernel.
 In multilinear subspace learning,[88][89][90] PCA is generalized to multilinear PCA (MPCA) that extracts features directly from tensor representations. MPCA is solved by performing PCA in each mode of the tensor iteratively. MPCA has been applied to face recognition, gait recognition, etc. MPCA is further extended to uncorrelated MPCA, non-negative MPCA and robust MPCA.
 N-way principal component analysis may be performed with models such as Tucker decomposition, PARAFAC, multiple factor analysis, co-inertia analysis, STATIS, and DISTATIS.
 While PCA finds the mathematically optimal method (as in minimizing the squared error), it is still sensitive to outliers in the data that produce large errors, something that the method tries to avoid in the first place. It is therefore common practice to remove outliers before computing PCA. However, in some contexts, outliers can be difficult to identify.[91]
For example, in data mining algorithms like correlation clustering, the assignment of points to clusters and outliers is not known beforehand.
A recently proposed generalization of PCA[92] based on a weighted PCA increases robustness by assigning different weights to data objects based on their estimated relevancy.
 Outlier-resistant variants of PCA have also been proposed, based on L1-norm formulations (L1-PCA).[6][4]
 Robust principal component analysis (RPCA) via decomposition in low-rank and sparse matrices is a modification of PCA that works well with respect to grossly corrupted observations.[93][94][95]
 Independent component analysis (ICA) is directed to similar problems as principal component analysis, but finds additively separable components rather than successive approximations.
 Given a matrix 



E


{\displaystyle E}

, it tries to decompose it into two matrices such that 



E
=
A
P


{\displaystyle E=AP}

. A key difference from techniques such as PCA and ICA is that some of the entries of 



A


{\displaystyle A}

 are constrained to be 0. Here 



P


{\displaystyle P}

 is termed the regulatory layer. While in general such a decomposition can have multiple solutions, they prove that if the following conditions are satisfied :
 then the decomposition is unique up to multiplication by a scalar.[96]
 Discriminant analysis of principal components (DAPC) is a multivariate method used to identify and describe clusters of genetically related individuals. Genetic variation is partitioned into two components: variation between groups and within groups, and it maximizes the former. Linear discriminants are linear combinations of alleles which best separate the clusters. Alleles that most contribute to this discrimination are therefore those that are the most markedly different across groups. The contributions of alleles to the groupings identified by DAPC can allow identifying regions of the genome driving the genetic divergence among groups[97]
In DAPC, data is first transformed using a principal components analysis (PCA) and subsequently clusters are identified using discriminant analysis (DA).
 A DAPC can be realized on R using the package Adegenet. (more info: adegenet on the web)
 Directional component analysis (DCA) is a method used in the atmospheric sciences for analysing multivariate datasets.[98]
Like PCA, it allows for dimension reduction, improved visualization and improved interpretability of large data-sets.
Also like PCA, it is based on a covariance matrix derived from the input dataset.
The difference between PCA and DCA is that DCA additionally requires the input of a vector direction, referred to as the impact. 
Whereas PCA maximises explained variance, DCA maximises probability density given impact.
The motivation for DCA is to find components of a multivariate dataset that are both likely (measured using probability density) and important (measured using the impact).
DCA has been used to find the most likely and most serious heat-wave patterns in weather prediction ensembles
,[99] and the most likely and most impactful changes in rainfall due to climate change
.[100]
",princip compon analysi pca linear dimension reduct techniqu applic exploratori data analysi visual data preprocess data linearli transform onto new coordin system direct princip compon captur largest variat data easili identifi princip compon collect point real coordin space sequenc p p unit vector vector direct line best fit data orthogon first vector line defin one minim averag squar perpendicular distanc point line direct princip compon constitut orthonorm basi differ individu dimens data linearli uncorrel mani studi use first two princip compon order plot data two dimens visual identifi cluster close relat data point princip compon analysi applic mani field popul genet microbiom studi atmospher scienc perform pca first princip compon set p p variabl deriv variabl form linear combin origin variabl explain varianc second princip compon explain varianc left effect first compon remov may proceed p p iter varianc explain pca commonli use mani variabl highli correl desir reduc number independ set first princip compon equival defin direct maxim varianc project data princip compon taken direct orthogon first princip compon maxim varianc project data either object shown princip compon eigenvector data covari matrix thu princip compon often comput eigendecomposit data covari matrix singular valu decomposit data matrix pca simplest true multivari analys close relat factor analysi factor analysi typic incorpor assumpt underli structur solv eigenvector slightli differ matrix pca also relat canon correl analysi cca cca defin coordin system optim describ two dataset pca defin new orthogon coordin system optim describ varianc singl dataset robust variant standard pca also propos pca invent karl pearson analogu princip axi theorem mechan later independ develop name harold hotel depend field applic also name discret transform klt signal process hotel transform multivari qualiti control proper orthogon decomposit pod mechan engin singular valu decomposit svd x invent last quarter centuri eigenvalu decomposit evd xtx linear algebra factor analysi discuss differ pca factor analysi see ch jolliff princip compon analysi theorem harman empir orthogon function eof meteorolog scienc lorenz empir eigenfunct decomposit sirovich quasiharmon mode brook et spectral decomposit nois vibrat empir modal analysi structur dynam pca thought fit ellipsoid data axi ellipsoid repres princip compon axi ellipsoid small varianc along axi also small find axe ellipsoid must first center valu variabl dataset subtract mean variabl observ valu valu transform valu use instead origin observ valu variabl comput covari matrix data calcul eigenvalu correspond eigenvector covari matrix must normal orthogon eigenvector turn unit vector done unit eigenvector interpret axi ellipsoid fit data choic basi transform covari matrix diagon form diagon element repres varianc axi proport varianc eigenvector repres calcul divid eigenvalu correspond eigenvector sum eigenvalu biplot scree plot degre explain varianc use interpret find pca pca defin orthogon linear transform real inner product space transform data new coordin system greatest varianc scalar project data come lie first coordin call first princip compon second greatest varianc second coordin consid n p p data matrix x zero empir mean sampl mean column shift zero n row repres differ repetit experi p column give particular kind featur say result particular sensor mathemat transform defin set size l l vector weight coeffici w k w w p k w k p k map row vector x x x p x p x new vector princip compon score l l given way individu variabl l l consid data set success inherit maximum possibl varianc x coeffici vector w constrain unit vector l l usual select strictli less p p reduc dimension may equival written matrix form k k ik k x j x j x ij j w j k w j k w jk j k order maxim varianc first weight vector w thu satisfi equival write matrix form give sinc w defin unit vector equival also satisfi quantiti maximis recognis rayleigh quotient standard result posit semidefinit matrix xtx quotient maximum possibl valu largest eigenvalu matrix occur w correspond eigenvector w found first princip compon data vector x given score x w transform correspond vector origin variabl x w w compon found subtract first k princip compon x find weight vector extract maximum varianc new data matrix turn give remain eigenvector xtx maximum valu quantiti bracket given correspond eigenvalu thu weight vector eigenvector xtx princip compon data vector x therefor given score tk x w k transform coordin correspond vector space origin variabl x w k w k w k kth eigenvector xtx full princip compon decomposit x therefor given w matrix weight whose column eigenvector xtx transpos w sometim call whiten sphere transform column w multipli squar root correspond eigenvalu eigenvector scale varianc call load pca factor analysi xtx recogn proport empir sampl covari matrix dataset xt sampl covari q two differ princip compon dataset given eigenvalu properti w k use move line line howev eigenvector w j w k correspond eigenvalu symmetr matrix orthogon eigenvalu differ orthogonalis vector happen share equal repeat valu product final line therefor zero sampl covari differ princip compon dataset anoth way characteris princip compon transform therefor transform coordin diagonalis empir sampl covari matrix matrix form empir covari matrix origin variabl written empir covari matrix princip compon becom λ diagon matrix eigenvalu λ k xtx λ k equal sum squar dataset associ compon k λ k σi σi x w k transform x w map data vector x origin space p variabl new space p variabl uncorrel dataset howev princip compon need kept keep first l princip compon produc use first l eigenvector give truncat transform matrix tl n row l column word pca learn linear transform w l x x r p r l l x r p r l column p l matrix w l l form orthogon basi l featur compon represent decorrel construct transform data matric l column score matrix maximis varianc origin data preserv minimis total squar reconstruct error w l w l w l w l x x l x x l dimension reduct use step visualis process dataset still retain much varianc dataset possibl exampl select l keep first two princip compon find plane dataset data spread data contain cluster may spread therefor visibl plot diagram wherea two direct data two origin variabl chosen random cluster may much less spread apart may fact much like substanti overlay make indistinguish similarli regress analysi larger number explanatori variabl allow greater chanc overfit model produc conclus fail generalis dataset one approach especi strong correl differ possibl explanatori variabl reduc princip compon run regress method call princip compon regress dimension reduct may also appropri variabl dataset noisi column dataset contain independ ident distribut gaussian nois column also contain similarli ident distribut gaussian nois distribut invari effect matrix w thought rotat axe howev total varianc concentr first princip compon compar nois varianc proportion effect nois first compon achiev higher ratio pca thu effect concentr much signal first princip compon use captur dimension reduct later princip compon may domin nois dispos without great loss dataset larg signific princip compon test use parametr bootstrap aid determin mani princip compon retain princip compon transform also associ anoth matrix factor singular valu decomposit svd x σ rectangular diagon matrix posit number σ k call singular valu x u matrix column orthogon unit vector length n call left singular vector x w matrix whose column orthogon unit vector length p call right singular vector term factor matrix xtx written σ squar diagon matrix singular valu x excess zero chop satisfi σ σ σ comparison eigenvector factor xtx establish right singular vector w x equival eigenvector xtx singular valu σ k x x equal eigenvalu λ k xtx use singular valu decomposit score matrix written column given one left singular vector x multipli correspond singular valu form also polar decomposit effici algorithm exist calcul svd x without form matrix xtx comput svd standard way calcul princip compon analysi data matrix unless hand compon requir truncat n l score matrix tl obtain consid first l largest singular valu singular vector truncat matrix use truncat singular valu decomposit way produc truncat matrix nearest possibl matrix rank l origin matrix sens differ two smallest possibl frobeniu norm result known theorem singular valu σ squar root eigenvalu matrix xtx eigenvalu proport portion varianc correctli sum squar distanc point multidimension mean associ eigenvector sum eigenvalu equal sum squar distanc point multidimension mean pca essenti rotat set point around mean order align princip compon move much varianc possibl use orthogon transform first dimens valu remain dimens therefor tend small may drop minim loss inform see pca often use manner dimension reduct pca distinct optim orthogon transform keep subspac largest varianc defin advantag howev come price greater comput requir compar exampl applic discret cosin transform particular simpli known dct nonlinear dimension reduct techniqu tend comput demand pca pca sensit scale variabl two variabl sampl varianc complet correl pca entail rotat weight cosin rotat two variabl respect princip compon equal multipli valu first variabl first princip compon almost variabl small contribut variabl wherea second compon almost align second origin variabl mean whenev differ variabl differ unit like temperatur mass pca somewhat arbitrari method analysi differ result would obtain one use fahrenheit rather celsiu exampl pearson origin paper entitl line plane closest fit system point space space impli physic euclidean space concern aris one way make pca less arbitrari use variabl scale unit varianc standard data henc use autocorrel matrix instead autocovari matrix basi pca howev compress expand fluctuat dimens signal space unit varianc mean subtract mean center necessari perform classic pca ensur first princip compon describ direct maximum varianc mean subtract perform first princip compon might instead correspond less mean data mean zero need find basi minim mean squar error approxim data unnecessari perform princip compon analysi correl matrix data alreadi center calcul correl correl deriv two standard score statist moment henc name pearson correl also see articl kromrey moder regress much ado noth sinc covari correl normal variabl pca base correl matrix x equal pca base covari matrix z standard version pca popular primari techniqu pattern recognit howev optim class separ howev use quantifi distanc two class calcul center mass class princip compon space report euclidean distanc center mass two class linear discrimin analysi altern optim class separ properti pca includ page need statist implic properti last pc simpli unstructur remov import pc last pc varianc small possibl use right help detect unsuspect linear relationship element x may also use regress select subset variabl x outlier detect look usag first look diagon element perhap main statist implic result decompos combin varianc element x decreas contribut due pc also decompos whole covari matrix contribut λ k α k α k k k k pc although strictli decreas element λ k α k α k k k k tend becom smaller k k increas λ k α k α k k k k nonincreas increas k k wherea element α k k tend stay size normal constraint α k α k k p k k p note result pca depend scale variabl cure scale featur standard deviat one end dimensionless featur unit varianc applic pca describ limit certain tacit assumpt made deriv particular pca captur linear correl featur fail assumpt violat see figur refer case coordin transform restor linear assumpt pca appli see kernel pca anoth limit process construct covari matrix pca field astronomi signal process forc mean astrophys exposur zero consequ creat unphys neg flux forward model perform recov true magnitud signal altern method matrix factor focus element matric astrophys observ see relat pca matrix factor pca disadvantag data standard appli algorithm pca transform origin data data relev princip compon data mean new data variabl interpret way origin linear interpret origin variabl also pca perform properli high likelihood inform loss pca reli linear model dataset pattern hidden insid nonlinear pca actual steer analysi complet opposit direct progress page need research kansa state univers discov sampl error experi impact bia pca result number subject block smaller research interest pc beyond first may better first correct serial correl pca conduct research kansa state also found pca could serious bias autocorrel structur data correctli handl dimension reduct result loss inform gener dimension reduct tend minim inform loss certain signal nois model assumpt data vector x x sum desir signal nois signal n n one show pca optim dimension reduct particular linsker show gaussian n n gaussian nois covari matrix proport ident matrix pca maxim mutual inform desir inform output w l x w l x nois still gaussian covari matrix proport ident matrix compon vector n n iid signal common scenario pca least minim upper bound inform loss defin optim pca also preserv nois n n iid least gaussian term diverg signal gener even signal model hold pca lose optim soon nois n n becom depend follow detail descript pca use covari method oppos correl method goal transform given data set x dimens p altern data set smaller dimens equival seek find matrix transform klt matrix x k l x klt x suppos data compris set observ p variabl want reduc data observ describ l variabl l suppos data arrang set n data vector x x n x x n x x repres singl group observ p variabl mean subtract integr part solut toward find princip compon basi minim mean squar error approxim data henc proceed center data follow applic variabl column b may also scale varianc equal see step affect calcul princip compon make independ unit use measur differ variabl let x random vector express column vector without loss gener assum x zero mean want find orthonorm transform matrix p px diagon covari matrix px random vector distinct compon pairwis uncorrel quick comput assum p p unitari yield henc hold cov x cov x diagonalis p p construct cov x guarante definit matrix thu guarante diagonalis unitari matrix practic implement especi high dimension data larg p naiv covari method rare use effici due high comput memori cost explicitli determin covari matrix approach avoid oper explicitli calcul store covari matrix xtx instead util one method exampl base function evalu product xt x r cost oper one way comput first princip compon effici shown follow data matrix x zero mean without ever comput covari matrix power iter algorithm simpli calcul vector xt x r normal place result back eigenvalu approxim rt xtx r rayleigh quotient unit vector r covari matrix xtx largest singular valu well separ next largest one vector r get close first princip compon x within number iter c small rel p total cost power iter converg acceler without notic sacrif small cost per iter use advanc method lanczo algorithm local optim block precondit conjug gradient lobpcg method subsequ princip compon comput via deflat simultan block former approach imprecis alreadi comput approxim princip compon addit affect accuraci subsequ comput princip compon thu increas error everi new comput latter approach block power method replac r matric r everi column r approxim one lead princip compon column iter simultan main calcul evalu product xt x r implement exampl lobpcg effici block elimin accumul error allow use bla product function typic lead faster converg compar techniqu iter partial least squar nipal variant classic power iter matrix deflat subtract implement comput first compon princip compon partial least squar analysi dataset gener omic scienc exampl genom metabolom usual necessari comput first pc iter partial least squar nipal algorithm updat iter approxim lead score load power iter multipli everi iter x left right calcul covari matrix avoid implement power iter xtx base function evalu product xt x r x r tx matrix deflat subtract perform subtract outer product x leav deflat residu matrix use calcul subsequ lead pc larg data matric matric high degre column collinear nipal suffer loss orthogon pc due machin precis error accumul iter matrix deflat subtract algorithm appli score load iter step elimin loss orthogon nipal relianc multipl take advantag bla result slow converg cluster lead singular defici resolv sophist block solver local optim block precondit conjug gradient lobpcg method onlin stream situat data arriv piec piec rather store singl batch use make estim pca project updat sequenti done effici requir differ algorithm pca common want introduc qualit variabl supplementari element exampl mani quantit variabl measur plant plant qualit variabl avail exampl speci plant belong data subject pca quantit variabl analyz result natur connect princip compon qualit variabl speci follow result produc result call introduc qualit variabl supplementari element procedur detail husson lê pagè pagè softwar offer option automat way case spad histor follow work ludov lebart first propos option r packag factomin earliest applic factor analysi locat measur compon human intellig believ intellig variou uncorrel compon spatial intellig verbal intellig induct deduct etc score could adduc factor analysi result variou test give singl index known intellig quotient iq pioneer statist psychologist spearman actual develop factor analysi theori intellig ad formal techniqu scienc psychometr thurston look factor intellig develop notion mental age standard iq test today base earli work shevki william introduc theori factori ecolog domin studi residenti differenti neighbourhood citi recogniz could distinguish one anoth variou characterist could reduc three factor analysi known rank index occup statu famili size cluster analysi could appli divid citi cluster precinct accord valu three key factor variabl extens literatur develop around factori ecolog urban geographi approach went fashion methodolog primit littl place postmodern geograph paradigm one problem factor analysi alway find convinc name variou artifici factor flood reviv factori ecolog approach show princip compon analysi actual gave meaning answer directli without resort factor rotat princip compon actual dual variabl shadow price push peopl togeth apart citi first compon classic demand travel demand space around classic urban econom base next two compon keep peopl similar statu separ neighbourhood mediat plan ethnic peopl similar ethnic background tri time australian bureau statist defin distinct index advantag disadvantag take first princip compon set key variabl thought import seifa index regularli publish variou jurisdict use frequent spatial analysi pca use formal method develop index altern confirmatori composit analysi propos develop assess index citi develop index develop pca indic citi outcom survey global citi first princip compon subject iter regress ad origin variabl singli variat account index ultim use indic good predictor mani variabl compar valu agre well subject assess condit citi coeffici item infrastructur roughli proport averag cost provid underli servic suggest index actual measur effect physic social invest citi human develop index hdi undp publish sinc extens use develop studi similar coeffici similar indic strongli suggest origin construct use pca other pioneer use princip compon analysi pca summaris data variat human gene frequenc across region compon show distinct pattern includ gradient sinusoid wave interpret pattern result specif ancient migrat event sinc pca ubiquit popul genet thousand paper use pca display mechan genet vari larg accord proxim first two princip compon actual show spatial distribut may use map rel geograph locat differ popul group therebi show individu wander origin locat pca genet technic controversi techniqu perform discret variabl often binari allel marker lack measur standard error pca also impedi consist usag august molecular biologist eran elhaik publish theoret paper scientif report analyz pca applic conclud easi manipul method view gener result contradictori absurd specif argu result achiev popul genet character circular reason market research extens user pca use develop custom satisfact custom loyalti score product cluster develop market segment may target advertis campaign much way factori ecolog locat geograph area similar characterist pca rapidli transform larg amount data smaller variabl rapidli readili analyz consum questionnair seri question design elicit consum attitud princip compon seek latent variabl underli attitud exampl oxford internet survey ask peopl attitud belief analyst extract four princip compon dimens identifi network creat anoth exampl joe flood extract attitudin index toward hous attitud question nation survey household australia first princip compon repres gener attitud toward properti home ownership index attitud question embodi could fed gener linear model tenur choic strongest determin privat rent far attitud index rather incom marit statu household type quantit financ pca use financi risk manag appli problem portfolio optim pca commonli use problem involv fix incom secur portfolio interest rate deriv valuat depend entir yield curv compris numer highli correl instrument pca use defin set compon factor explain rate movement therebi facilit model one common risk manag applic calcul valu risk var appli pca mont carlo simul compon stress rate turn option valu reconstruct var calcul final entir run pca also use hedg exposur interest rate risk given partial durat sensit first three typic princip compon system interest repres shift twist curvatur princip compon deriv covari matrix yield predefin matur varianc compon eigenvalu compon orthogon correl need incorpor subsequ model equiti optim portfolio one expect return maxim given level risk altern risk minim given return see markowitz model discuss thu one approach reduc portfolio risk alloc strategi appli princip portfolio instead underli stock second approach enhanc portfolio return use princip compon select compani stock upsid potenti pca also use understand relationship intern equiti market within market group compani industri sector pca may also appli stress test essenti analysi bank abil endur hypothet advers econom scenario util distil inform contain sever macroeconom variabl manag data set use analysi result factor link interest rate base largest element factor eigenvector observ shock factor affect impli asset bank variant princip compon analysi use neurosci identifi specif properti stimulu increas neuron probabl gener action potenti techniqu known covari analysi typic applic experiment present white nois process stimulu usual either sensori input test subject current inject directli neuron record train action potenti spike produc neuron result presum certain featur stimulu make neuron like spike order extract featur experiment calcul covari matrix ensembl set stimuli defin discret finit time window typic order ms immedi preced spike eigenvector differ covari matrix covari matrix prior stimulu ensembl set stimuli defin length time window indic direct space stimuli along varianc ensembl differ prior stimulu ensembl specif eigenvector largest posit eigenvalu correspond direct along varianc ensembl show largest posit chang compar varianc prior sinc direct vari stimulu led spike often good approxim sought relev stimulu featur neurosci pca also use discern ident neuron shape action potenti spike sort import procedur extracellular record techniqu often pick signal one neuron spike sort one first use pca reduc dimension space action potenti waveform perform cluster analysi associ specif action potenti individu neuron pca dimens reduct techniqu particularli suit detect coordin activ larg neuron ensembl use determin collect variabl order paramet phase transit brain correspond analysi ca develop benzécri conceptu similar pca scale data row column treat equival tradit appli conting tabl ca decompos statist associ tabl orthogon factor ca descript techniqu appli tabl statist appropri sever variant ca avail includ detrend correspond analysi canon correspond analysi one special extens multipl correspond analysi may seen counterpart princip compon analysi categor data princip compon analysi creat variabl linear combin origin variabl new variabl properti variabl orthogon pca transform help step cluster pca approach seek reproduc total variabl varianc compon reflect common uniqu varianc variabl pca gener prefer purpos data reduct translat variabl space optim factor space goal detect latent construct factor factor analysi similar princip compon analysi factor analysi also involv linear combin variabl differ pca factor analysi approach seek reproduc among variabl factor repres common varianc variabl exclud uniqu varianc term correl matrix correspond focus explain term share pca focus explain term sit diagon howev side result tri reproduc term pca also tend fit rel well correl result given pca factor analysi similar situat alway case problem result significantli differ factor analysi gener use research purpos detect data structur latent construct factor causal model factor model incorrectli formul assumpt met factor analysi give erron result assert relax solut cluster specifi cluster indic given princip compon pca subspac span princip direct ident cluster centroid subspac howev pca use relax cluster new result straightforward uncov counterexampl statement cluster centroid subspac span princip direct matrix factor nmf dimens reduct method element matric use therefor promis method astronomi sens astrophys signal pca compon orthogon nmf compon therefor construct basi pca contribut compon rank base magnitud correspond eigenvalu equival fraction residu varianc frv analyz empir data nmf compon rank base empir frv curv residu fraction eigenvalu plot k λ j n λ j k n j function compon number k k given total n n compon pca flat plateau data captur remov nois curv drop quickli indic random nois frv curv nmf decreas continu nmf compon construct sequenti indic continu captur nois converg higher level pca indic less properti nmf often difficult interpret princip compon data includ mani variabl variou origin variabl qualit lead pca user delic elimin sever variabl observ variabl excess impact direct axe remov project supplementari element addit necessari avoid interpret proxim point close center factori plane iconographi correl contrari project system axe drawback therefor keep variabl principl diagram underlin remark correl correl matrix solid line posit correl dot line neg correl strong correl remark direct caus effect third variabl convers weak correl remark exampl variabl depend sever independ variabl correl weak yet remark particular disadvantag pca princip compon usual linear combin input variabl spars pca overcom disadvantag find linear combin contain input variabl extend classic method princip compon analysi pca reduct dimension data ad sparsiti constraint input variabl sever approach propos includ methodolog theoret develop spars pca well applic scientif studi recent review survey paper modern method nonlinear dimension reduct find theoret algorithm root pca pearson origin idea take straight line plane best fit set data point trevor hasti expand concept propos princip curv natur extens geometr interpret pca explicitli construct manifold data approxim follow project point onto see also elast map algorithm princip geodes analysi anoth popular gener kernel pca correspond pca perform reproduc kernel hilbert space associ posit definit kernel multilinear subspac learn pca gener multilinear pca mpca extract featur directli tensor represent mpca solv perform pca mode tensor iter mpca appli face recognit gait recognit etc mpca extend uncorrel mpca mpca robust mpca princip compon analysi may perform model tucker decomposit parafac multipl factor analysi analysi stati distati pca find mathemat optim method minim squar error still sensit outlier data produc larg error someth method tri avoid first place therefor common practic remov outlier comput pca howev context outlier difficult identifi exampl data mine algorithm like correl cluster assign point cluster outlier known beforehand recent propos gener pca base weight pca increas robust assign differ weight data object base estim relev variant pca also propos base formul robust princip compon analysi rpca via decomposit spars matric modif pca work well respect grossli corrupt observ independ compon analysi ica direct similar problem princip compon analysi find addit separ compon rather success approxim given matrix e e tri decompos two matric e p key differ techniqu pca ica entri constrain p p term regulatori layer gener decomposit multipl solut prove follow condit satisfi decomposit uniqu multipl scalar discrimin analysi princip compon dapc multivari method use identifi describ cluster genet relat individu genet variat partit two compon variat group within group maxim former linear discrimin linear combin allel best separ cluster allel contribut discrimin therefor markedli differ across group contribut allel group identifi dapc allow identifi region genom drive genet diverg among group dapc data first transform use princip compon analysi pca subsequ cluster identifi use discrimin analysi da dapc realiz r use packag adegenet info adegenet web direct compon analysi dca method use atmospher scienc analys multivari dataset like pca allow dimens reduct improv visual improv interpret larg also like pca base covari matrix deriv input dataset differ pca dca dca addit requir input vector direct refer impact wherea pca maximis explain varianc dca maximis probabl densiti given impact motiv dca find compon multivari dataset like measur use probabl densiti import measur use impact dca use find like seriou pattern weather predict ensembl like impact chang rainfal due climat chang
Proper generalized decomposition,https://en.wikipedia.org/wiki/Proper_generalized_decomposition,"The proper generalized decomposition (PGD) is an iterative numerical method for solving boundary value problems (BVPs), that is, partial differential equations constrained by a set of boundary conditions, such as the Poisson's equation or the Laplace's equation.
 The PGD algorithm computes an approximation of the solution of the BVP by successive enrichment. This means that, in each iteration, a new component (or mode) is computed and added to the approximation. In principle, the more modes obtained, the closer the approximation is to its theoretical solution. Unlike POD principal components, PGD modes are not necessarily orthogonal to each other.
 By selecting only the most relevant PGD modes, a reduced order model of the solution is obtained. Because of this, PGD is considered a dimensionality reduction algorithm.
 The proper generalized decomposition is a method characterized by 
 In the Proper Generalized Decomposition method, the variational formulation involves translating the problem into a format where the solution can be approximated by minimizing (or sometimes maximizing) a functional. A functional is a scalar quantity that depends on a function, which in this case, represents our problem.
 The most commonly implemented variational formulation in PGD is the Bubnov-Galerkin method.[3][4] This method is chosen for its ability to provide an approximate solution to complex problems, such as those described by partial differential equations (PDEs). In the Bubnov-Galerkin approach, the idea is to project the problem onto a space spanned by a finite number of basis functions. These basis functions are chosen to approximate the solution space of the problem.
 In the Bubnov-Galerkin method, we seek an approximate solution that satisfies the integral form of the PDEs over the domain of the problem. This is different from directly solving the differential equations. By doing so, the method transforms the problem into finding the coefficients that best fit this integral equation in the chosen function space.
 While the Bubnov-Galerkin method is prevalent, other variational formulations are also used in PGD,[5][3] depending on the specific requirements and characteristics of the problem, such as:
 The discretization of the domain is a well defined set of procedures that cover (a) the creation of finite element meshes, (b) the definition of basis function on reference elements (also called shape functions) and (c) the mapping of reference elements onto the elements of the mesh.
 PGD assumes that the solution u of a (multidimensional) problem can be approximated as a separate representation of the form





u

≈


u


N


(

x

1


,

x

2


,
…
,

x

d


)
=

∑

i
=
1


N





X

1




i


(

x

1


)
⋅



X

2




i


(

x

2


)
⋯



X

d




i


(

x

d


)
,


{\displaystyle \mathbf {u} \approx \mathbf {u} ^{N}(x_{1},x_{2},\ldots ,x_{d})=\sum _{i=1}^{N}\mathbf {X_{1}} _{i}(x_{1})\cdot \mathbf {X_{2}} _{i}(x_{2})\cdots \mathbf {X_{d}} _{i}(x_{d}),}


where the number of addends N and the functional products X1(x1), X2(x2), ..., Xd(xd), each depending on a variable (or variables), are unknown beforehand.
 The solution is sought by applying a greedy algorithm, usually the fixed point algorithm, to the weak formulation of the problem. For each iteration i of the algorithm, a mode of the solution is computed. Each mode consists of a set of numerical values of the functional products X1(x1), ..., Xd(xd), which enrich the approximation of the solution. Due to the greedy nature of the algorithm, the term 'enrich' is used rather than 'improve', since some modes may actually worsen the approach. The number of computed modes required to obtain an approximation of the solution below a certain error threshold depends on the stopping criterion of the iterative algorithm.
 PGD is suitable for solving high-dimensional problems, since it overcomes the limitations of classical approaches. In particular, PGD avoids the curse of dimensionality, as solving decoupled problems is computationally much less expensive than solving multidimensional problems.
 Therefore, PGD enables to re-adapt parametric problems into a multidimensional framework by setting the parameters of the problem as extra coordinates:





u

≈


u


N


(

x

1


,
…
,

x

d


;

k

1


,
…
,

k

p


)
=

∑

i
=
1


N





X

1




i


(

x

1


)
⋯



X

d




i


(

x

d


)
⋅



K

1




i


(

k

1


)
⋯



K

p




i


(

k

p


)
,


{\displaystyle \mathbf {u} \approx \mathbf {u} ^{N}(x_{1},\ldots ,x_{d};k_{1},\ldots ,k_{p})=\sum _{i=1}^{N}\mathbf {X_{1}} _{i}(x_{1})\cdots \mathbf {X_{d}} _{i}(x_{d})\cdot \mathbf {K_{1}} _{i}(k_{1})\cdots \mathbf {K_{p}} _{i}(k_{p}),}


where a series of functional products K1(k1), K2(k2), ..., Kp(kp), each depending on a parameter (or parameters), has been incorporated to the equation.
 In this case, the obtained approximation of the solution is called computational vademecum: a general meta-model containing all the particular solutions for every possible value of the involved parameters.[7]
 The Sparse Subspace Learning (SSL) method leverages the use of hierarchical collocation to approximate the numerical solution of parametric models. With respect to traditional projection-based reduced order modeling, the use of a collocation enables non-intrusive approach based on sparse adaptive sampling of the parametric space. This allows to recover the lowdimensional structure of the parametric solution subspace while also learning the functional dependency from the parameters in explicit form. A sparse low-rank approximate tensor representation of the parametric solution can be built through an incremental strategy that only needs to have access to the output of a deterministic solver. Non-intrusiveness makes this approach straightforwardly applicable to challenging problems characterized by nonlinearity or non affine weak forms.[8]
",proper gener decomposit pgd iter numer method solv boundari valu problem bvp partial differenti equat constrain set boundari condit poisson equat laplac equat pgd algorithm comput approxim solut bvp success enrich mean iter new compon mode comput ad approxim principl mode obtain closer approxim theoret solut unlik pod princip compon pgd mode necessarili orthogon select relev pgd mode reduc order model solut obtain pgd consid dimension reduct algorithm proper gener decomposit method character proper gener decomposit method variat formul involv translat problem format solut approxim minim sometim maxim function function scalar quantiti depend function case repres problem commonli implement variat formul pgd method method chosen abil provid approxim solut complex problem describ partial differenti equat pde approach idea project problem onto space span finit number basi function basi function chosen approxim solut space problem method seek approxim solut satisfi integr form pde domain problem differ directli solv differenti equat method transform problem find coeffici best fit integr equat chosen function space method preval variat formul also use pgd depend specif requir characterist problem discret domain well defin set procedur cover creation finit element mesh b definit basi function refer element also call shape function c map refer element onto element mesh pgd assum solut u multidimension problem approxim separ represent form u u n x x x n x x x x x x u u n n number addend n function product xd xd depend variabl variabl unknown beforehand solut sought appli greedi algorithm usual fix point algorithm weak formul problem iter algorithm mode solut comput mode consist set numer valu function product xd xd enrich approxim solut due greedi natur algorithm term use rather sinc mode may actual worsen approach number comput mode requir obtain approxim solut certain error threshold depend stop criterion iter algorithm pgd suitabl solv problem sinc overcom limit classic approach particular pgd avoid curs dimension solv decoupl problem comput much less expens solv multidimension problem therefor pgd enabl parametr problem multidimension framework set paramet problem extra coordin u u n x x k k p n x x x x k k k p k p u u n p n p p seri function product kp kp depend paramet paramet incorpor equat case obtain approxim solut call comput vademecum gener contain particular solut everi possibl valu involv paramet spars subspac learn ssl method leverag use hierarch colloc approxim numer solut parametr model respect tradit reduc order model use colloc enabl approach base spars adapt sampl parametr space allow recov lowdimension structur parametr solut subspac also learn function depend paramet explicit form spars approxim tensor represent parametr solut built increment strategi need access output determinist solver make approach straightforwardli applic challeng problem character nonlinear non affin weak form
t-distributed stochastic neighbor embedding,https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding,"t-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map. It is based on Stochastic Neighbor Embedding originally developed by Geoffrey Hinton and Sam Roweis,[1] where Laurens van der Maaten and Hinton proposed the t-distributed variant.[2] It is a nonlinear dimensionality reduction technique for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.
 The t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a lower probability. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback–Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map. While the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this can be changed as appropriate. A Riemannian variant is UMAP.
 t-SNE has been used for visualization in a wide range of applications, including genomics, computer security research,[3] natural language processing, music analysis,[4] cancer research,[5] bioinformatics,[6] geological domain interpretation,[7][8][9] and biomedical signal processing.[10]
 For a data set with n elements, t-SNE runs in O(n2) time and requires O(n2) space.[11]
 Given a set of 



N


{\displaystyle N}

 high-dimensional objects 





x


1


,
…
,


x


N




{\displaystyle \mathbf {x} _{1},\dots ,\mathbf {x} _{N}}

, t-SNE first computes probabilities 




p

i
j




{\displaystyle p_{ij}}

 that are proportional to the similarity of objects 





x


i




{\displaystyle \mathbf {x} _{i}}

 and 





x


j




{\displaystyle \mathbf {x} _{j}}

, as follows.
 For 



i
≠
j


{\displaystyle i\neq j}

, define 
 and set 




p

i
∣
i


=
0


{\displaystyle p_{i\mid i}=0}

. 
Note the above denominator ensures 




∑

j



p

j
∣
i


=
1


{\displaystyle \sum _{j}p_{j\mid i}=1}

 for all 



i


{\displaystyle i}

.
 As van der Maaten and Hinton explained:  ""The similarity of datapoint 




x

j




{\displaystyle x_{j}}

 to datapoint 




x

i




{\displaystyle x_{i}}

 is the conditional probability, 




p

j

|

i




{\displaystyle p_{j|i}}

, that 




x

i




{\displaystyle x_{i}}

 would pick 




x

j




{\displaystyle x_{j}}

 as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered at 




x

i




{\displaystyle x_{i}}

.""[2]
 Now define 
 This is motivated because  




p

i




{\displaystyle p_{i}}

 and  




p

j




{\displaystyle p_{j}}

  from the N samples are estimated as 1/N, so the conditional probability  can be written as   




p

i
∣
j


=
N

p

i
j




{\displaystyle p_{i\mid j}=Np_{ij}}

  and     




p

j
∣
i


=
N

p

j
i




{\displaystyle p_{j\mid i}=Np_{ji}}

 .  Since 




p

i
j


=

p

j
i




{\displaystyle p_{ij}=p_{ji}}

, you can obtain previous formula.
 Also note that  




p

i
i


=
0


{\displaystyle p_{ii}=0}

 and 




∑

i
,
j



p

i
j


=
1


{\displaystyle \sum _{i,j}p_{ij}=1}

.
 The bandwidth of the Gaussian kernels 




σ

i




{\displaystyle \sigma _{i}}

 is set in such a way that the entropy of the conditional distribution equals a predefined entropy using the bisection method. As a result, the bandwidth is adapted to the density of the data: smaller values of 




σ

i




{\displaystyle \sigma _{i}}

 are used in denser parts of the data space. The entropy increases with the perplexity of this distribution 




P

i




{\displaystyle P_{i}}

; this relation is seen as
 where 



H
(

P

i


)


{\displaystyle H(P_{i})}

 is the shannon entropy 



H
(

P

i


)
=
−

∑

j



p

j

|

i



log

2


⁡

p

j

|

i


.


{\displaystyle H(P_{i})=-\sum _{j}p_{j|i}\log _{2}p_{j|i}.}


 The perplexity is a hand-chosen parameter of t-SNE, and as the authors state, ""perplexity can be interpreted as a smooth measure of the effective number of neighbors. The performance of SNE is fairly robust to changes in the perplexity, and typical values are between 5 and 50."".[2]
 Since the Gaussian kernel uses the Euclidean distance 



‖

x

i


−

x

j


‖


{\displaystyle \lVert x_{i}-x_{j}\rVert }

, it is affected by the curse of dimensionality, and in high dimensional data when distances lose the ability to discriminate, the 




p

i
j




{\displaystyle p_{ij}}

 become too similar (asymptotically, they would converge to a constant). It has been proposed to adjust the distances with a power transform, based on the intrinsic dimension of each point, to alleviate this.[12]
 t-SNE aims to learn a 



d


{\displaystyle d}

-dimensional map 





y


1


,
…
,


y


N




{\displaystyle \mathbf {y} _{1},\dots ,\mathbf {y} _{N}}

 (with 





y


i


∈


R


d




{\displaystyle \mathbf {y} _{i}\in \mathbb {R} ^{d}}

 and 



d


{\displaystyle d}

 typically chosen as 2 or 3) that reflects the similarities  




p

i
j




{\displaystyle p_{ij}}

 as well as possible. To this end, it measures similarities 




q

i
j




{\displaystyle q_{ij}}

 between two points in the map 





y


i




{\displaystyle \mathbf {y} _{i}}

 and 





y


j




{\displaystyle \mathbf {y} _{j}}

, using a very similar approach. 
Specifically, for 



i
≠
j


{\displaystyle i\neq j}

, define 




q

i
j




{\displaystyle q_{ij}}

 as
 and set 




q

i
i


=
0


{\displaystyle q_{ii}=0}

. 
Herein a heavy-tailed Student t-distribution (with one-degree of freedom, which is the same as a Cauchy distribution) is used to measure similarities between low-dimensional points in order to allow dissimilar objects to be modeled far apart in the map.
 The locations of the points 





y


i




{\displaystyle \mathbf {y} _{i}}

 in the map are determined by minimizing the (non-symmetric) Kullback–Leibler divergence of the distribution 



P


{\displaystyle P}

 from the distribution 



Q


{\displaystyle Q}

, that is:
 The minimization of the Kullback–Leibler divergence with respect to the points 





y


i




{\displaystyle \mathbf {y} _{i}}

 is performed using gradient descent. 
The result of this optimization is a map that reflects the similarities between the high-dimensional inputs.
 While t-SNE plots often seem to display clusters, the visual clusters can be strongly influenced by the chosen parameterization (especially the perplexity) and so a good understanding of the parameters for t-SNE is needed. Such ""clusters"" can be shown to even appear in structured data with no clear clustering,[13] and so may be false findings. Similarly, the size of clusters produced by t-SNE is not informative, and neither is the distance between clusters.[14] Thus, interactive exploration may be needed to choose parameters and validate results.[15][16] It has been shown that t-SNE can often recover well-separated clusters, and with special parameter choices, approximates a simple form of spectral clustering.[17]
",stochast neighbor embed statist method visual data give datapoint locat two map base stochast neighbor embed origin develop geoffrey hinton sam rowei lauren van der maaten hinton propos variant nonlinear dimension reduct techniqu embed data visual space two three dimens specif model object point way similar object model nearbi point dissimilar object model distant point high probabl algorithm compris two main stage first construct probabl distribut pair object way similar object assign higher probabl dissimilar point assign lower probabl second defin similar probabl distribut point map minim diverg kl diverg two distribut respect locat point map origin algorithm use euclidean distanc object base similar metric chang appropri riemannian variant umap use visual wide rang applic includ genom comput secur research natur languag process music analysi cancer research bioinformat geolog domain interpret biomed signal process data set n element run time requir space given set n n object x x n x x n first comput probabl p j ij proport similar object x x x j x j follow j j defin set p note denomin ensur j p j j van der maaten hinton explain similar datapoint x j j datapoint x condit probabl p j x would pick x j j neighbor neighbor pick proport probabl densiti gaussian center x defin motiv p p j j n sampl estim condit probabl written p j n p j j ij p j n p j ji sinc p j p j ij ji obtain previou formula also note p ii j p j j ij bandwidth gaussian kernel σ set way entropi condit distribut equal predefin entropi use bisect method result bandwidth adapt densiti data smaller valu σ use denser part data space entropi increas perplex distribut p relat seen h p h shannon entropi h p j p j log p j h j perplex paramet author state perplex interpret smooth measur effect number neighbor perform sne fairli robust chang perplex typic valu sinc gaussian kernel use euclidean distanc x x j j affect curs dimension high dimension data distanc lose abil discrimin p j ij becom similar asymptot would converg constant propos adjust distanc power transform base intrins dimens point allevi aim learn map n n r r typic chosen reflect similar p j ij well possibl end measur similar q j ij two point map j j use similar approach specif j j defin q j ij set q ii herein student freedom cauchi distribut use measur similar point order allow dissimilar object model far apart map locat point map determin minim diverg distribut p p distribut q q minim diverg respect point perform use gradient descent result optim map reflect similar input plot often seem display cluster visual cluster strongli influenc chosen parameter especi perplex good understand paramet need cluster shown even appear structur data clear cluster may fals find similarli size cluster produc inform neither distanc cluster thu interact explor may need choos paramet valid result shown often recov cluster special paramet choic approxim simpl form spectral cluster
Sparse dictionary learning,https://en.wikipedia.org/wiki/Sparse_dictionary_learning,"Sparse dictionary learning (also known as sparse coding or SDL) is a representation learning method which aims to find a sparse representation of the input data in the form of a linear combination of basic elements as well as those basic elements themselves. These elements are called atoms, and they compose a dictionary. Atoms in the dictionary are not required to be orthogonal, and they may be an over-complete spanning set. This problem setup also allows the dimensionality of the signals being represented to be higher than any one of the signals being observed. These two properties lead to having seemingly redundant atoms that allow multiple representations of the same signal, but also provide an improvement in sparsity and flexibility of the representation.
 One of the most important applications of sparse dictionary learning is in the field of compressed sensing or signal recovery. In compressed sensing, a high-dimensional signal can be recovered with only a few linear measurements, provided that the signal is sparse or near-sparse. Since not all signals satisfy this condition, it is crucial to find a sparse representation of that signal such as the wavelet transform or the directional gradient of a rasterized matrix. Once a matrix or a high-dimensional vector is transferred to a sparse space, different recovery algorithms like basis pursuit, CoSaMP,[1] or fast non-iterative algorithms[2] can be used to recover the signal.
 One of the key principles of dictionary learning is that the dictionary has to be inferred from the input data. The emergence of sparse dictionary learning methods was stimulated by the fact that in signal processing, one typically wants to represent the input data using a minimal amount of components. Before this approach, the general practice was to use predefined dictionaries such as Fourier or wavelet transforms. However, in certain cases, a dictionary that is trained to fit the input data can significantly improve the sparsity, which has applications in data decomposition, compression, and analysis, and has been used in the fields of image denoising and classification, and video and audio processing. Sparsity and overcomplete dictionaries have immense applications in image compression, image fusion, and inpainting.  
 Given the input dataset 



X
=
[

x

1


,
.
.
.
,

x

K


]
,

x

i


∈


R


d




{\displaystyle X=[x_{1},...,x_{K}],x_{i}\in \mathbb {R} ^{d}}

 we wish to find a dictionary 




D

∈


R


d
×
n


:
D
=
[

d

1


,
.
.
.
,

d

n


]


{\displaystyle \mathbf {D} \in \mathbb {R} ^{d\times n}:D=[d_{1},...,d_{n}]}

 and a representation 



R
=
[

r

1


,
.
.
.
,

r

K


]
,

r

i


∈


R


n




{\displaystyle R=[r_{1},...,r_{K}],r_{i}\in \mathbb {R} ^{n}}

 such that both 



‖
X
−

D

R

‖

F


2




{\displaystyle \|X-\mathbf {D} R\|_{F}^{2}}

 is minimized and the representations 




r

i




{\displaystyle r_{i}}

 are sparse enough. This can be formulated as the following optimization problem:
 





argmin


D

∈


C


,

r

i


∈


R


n






∑

i
=
1


K


‖

x

i


−

D


r

i



‖

2


2


+
λ
‖

r

i



‖

0




{\displaystyle {\underset {\mathbf {D} \in {\mathcal {C}},r_{i}\in \mathbb {R} ^{n}}{\text{argmin}}}\sum _{i=1}^{K}\|x_{i}-\mathbf {D} r_{i}\|_{2}^{2}+\lambda \|r_{i}\|_{0}}

, where 





C


≡
{

D

∈


R


d
×
n


:
‖

d

i



‖

2


≤
1


∀
i
=
1
,
.
.
.
,
n
}


{\displaystyle {\mathcal {C}}\equiv \{\mathbf {D} \in \mathbb {R} ^{d\times n}:\|d_{i}\|_{2}\leq 1\,\,\forall i=1,...,n\}}

, 



λ
>
0


{\displaystyle \lambda >0}


 





C




{\displaystyle {\mathcal {C}}}

 is required to constrain 




D



{\displaystyle \mathbf {D} }

 so that its atoms would not reach arbitrarily high values allowing for arbitrarily low (but non-zero) values of 




r

i




{\displaystyle r_{i}}

. 



λ


{\displaystyle \lambda }

 controls the trade off between the sparsity and the minimization error.
 The minimization problem above is not convex because of the ℓ0-""norm"" and solving this problem is NP-hard.[3] In some cases L1-norm is known to ensure sparsity[4] and so the above becomes a convex optimization problem with respect to each of the variables 




D



{\displaystyle \mathbf {D} }

 and 




R



{\displaystyle \mathbf {R} }

 when the other one is fixed, but it is not jointly convex in 



(

D

,

R

)


{\displaystyle (\mathbf {D} ,\mathbf {R} )}

.
 The dictionary 




D



{\displaystyle \mathbf {D} }

 defined above can be ""undercomplete"" if 



n
<
d


{\displaystyle n<d}

 or ""overcomplete"" in case 



n
>
d


{\displaystyle n>d}

 with the latter being a typical assumption for a sparse dictionary learning problem. The case of a complete dictionary does not provide any improvement from a representational point of view and thus isn't considered.
 Undercomplete dictionaries represent the setup in which the actual input data lies in a lower-dimensional space. This case is strongly related to dimensionality reduction and techniques like principal component analysis which require atoms 




d

1


,
.
.
.
,

d

n




{\displaystyle d_{1},...,d_{n}}

 to be orthogonal.   The choice of these subspaces is crucial for efficient dimensionality reduction, but it is not trivial.  And dimensionality reduction based on dictionary representation can be extended to address specific tasks such as data analysis or classification. However, their main downside is limiting the choice of atoms.
 Overcomplete dictionaries, however, do not require the atoms to be orthogonal (they will never have a basis anyway) thus allowing for more flexible dictionaries and richer data representations.
 An overcomplete dictionary which allows for sparse representation of signal can be a famous transform matrix (wavelets transform, fourier transform) or it can be formulated so that its elements are changed in such a way that it sparsely represents the given signal in a best way. Learned dictionaries are capable of giving sparser solutions as compared to predefined transform matrices.
 As the optimization problem described above can be solved as a convex problem with respect to either dictionary or sparse coding while the other one of the two is fixed, most of the algorithms are based on the idea of iteratively updating one and then the other.
 The problem of finding an optimal sparse coding 



R


{\displaystyle R}

 with a given dictionary 




D



{\displaystyle \mathbf {D} }

 is known as sparse approximation (or sometimes just sparse coding problem). A number of algorithms have been developed to solve it (such as matching pursuit and LASSO) and are incorporated in the algorithms described below.
 The method of optimal directions (or MOD) was one of the first methods introduced to tackle the sparse dictionary learning problem.[5] The core idea of it is to solve the minimization problem subject to the limited number of non-zero components of the representation vector:
 




min


D

,
R


{
‖
X
−

D

R

‖

F


2


}



s.t.



∀
i


‖

r

i



‖

0


≤
T


{\displaystyle \min _{\mathbf {D} ,R}\{\|X-\mathbf {D} R\|_{F}^{2}\}\,\,{\text{s.t.}}\,\,\forall i\,\,\|r_{i}\|_{0}\leq T}


 Here, 



F


{\displaystyle F}

 denotes the Frobenius norm. MOD alternates between getting the sparse coding using a method such as matching pursuit and updating the dictionary by computing the analytical solution of the problem given by 




D

=
X

R

+




{\displaystyle \mathbf {D} =XR^{+}}

 where 




R

+




{\displaystyle R^{+}}

 is a Moore-Penrose pseudoinverse. After this update 




D



{\displaystyle \mathbf {D} }

 is renormalized to fit the constraints and the new sparse coding is obtained again. The process is repeated until convergence (or until a sufficiently small residue).
 MOD has proved to be a very efficient method for low-dimensional input data 



X


{\displaystyle X}

 requiring just a few iterations to converge. However, due to the high complexity of the matrix-inversion operation, computing the pseudoinverse in high-dimensional cases is in many cases intractable. This shortcoming has inspired the development of other dictionary learning methods.
 K-SVD is an algorithm that performs SVD at its core to update the atoms of the dictionary one by one and basically is a generalization of K-means. It enforces that each element of the input data 




x

i




{\displaystyle x_{i}}

 is encoded by a linear combination of not more than 




T

0




{\displaystyle T_{0}}

 elements in a way identical to the MOD approach:
 




min


D

,
R


{
‖
X
−

D

R

‖

F


2


}



s.t.



∀
i


‖

r

i



‖

0


≤

T

0




{\displaystyle \min _{\mathbf {D} ,R}\{\|X-\mathbf {D} R\|_{F}^{2}\}\,\,{\text{s.t.}}\,\,\forall i\,\,\|r_{i}\|_{0}\leq T_{0}}


 This algorithm's essence is to first fix the dictionary, find the best possible 



R


{\displaystyle R}

 under the above constraint (using Orthogonal Matching Pursuit) and then iteratively update the atoms of dictionary 




D



{\displaystyle \mathbf {D} }

 in the following manner:
 



‖
X
−

D

R

‖

F


2


=


|

X
−

∑

i
=
1


K



d

i



x

T


i



|


F


2


=
‖

E

k


−

d

k



x

T


k



‖

F


2




{\displaystyle \|X-\mathbf {D} R\|_{F}^{2}=\left|X-\sum _{i=1}^{K}d_{i}x_{T}^{i}\right|_{F}^{2}=\|E_{k}-d_{k}x_{T}^{k}\|_{F}^{2}}


 The next steps of the algorithm include rank-1 approximation of the residual matrix 




E

k




{\displaystyle E_{k}}

, updating 




d

k




{\displaystyle d_{k}}

 and enforcing the sparsity of 




x

k




{\displaystyle x_{k}}

 after the update. This algorithm is considered to be standard for dictionary learning and is used in a variety of applications. However, it shares weaknesses with MOD being efficient only for signals with relatively low dimensionality and having the possibility for being stuck at local minima.
 One can also apply a widespread stochastic gradient descent method with iterative projection to solve this problem.[6] The idea of this method is to update the dictionary using the first order stochastic gradient and project it on the constraint set 





C




{\displaystyle {\mathcal {C}}}

. The step that occurs at i-th iteration is described by this expression:
 





D


i


=


proj



C




{



D


i
−
1


−

δ

i



∇


D




∑

i
∈
S


‖

x

i


−

D


r

i



‖

2


2


+
λ
‖

r

i



‖

1



}



{\displaystyle \mathbf {D} _{i}={\text{proj}}_{\mathcal {C}}\left\{\mathbf {D} _{i-1}-\delta _{i}\nabla _{\mathbf {D} }\sum _{i\in S}\|x_{i}-\mathbf {D} r_{i}\|_{2}^{2}+\lambda \|r_{i}\|_{1}\right\}}

, where 



S


{\displaystyle S}

 is a random subset of 



{
1...
K
}


{\displaystyle \{1...K\}}

 and 




δ

i




{\displaystyle \delta _{i}}

 is a gradient step.
 An algorithm based on solving a dual Lagrangian problem provides an efficient way to solve for the dictionary having no complications induced by the sparsity function.[7] Consider the following Lagrangian:
 





L


(

D

,
Λ
)
=

tr


(

(
X
−

D

R

)

T


(
X
−

D

R
)

)

+

∑

j
=
1


n



λ

j



(


∑

i
=
1


d




D


i
j


2


−
c

)



{\displaystyle {\mathcal {L}}(\mathbf {D} ,\Lambda )={\text{tr}}\left((X-\mathbf {D} R)^{T}(X-\mathbf {D} R)\right)+\sum _{j=1}^{n}\lambda _{j}\left({\sum _{i=1}^{d}\mathbf {D} _{ij}^{2}-c}\right)}

, where 



c


{\displaystyle c}

 is a constraint on the norm of the atoms and 




λ

i




{\displaystyle \lambda _{i}}

 are the so-called dual variables forming the diagonal matrix 



Λ


{\displaystyle \Lambda }

.
 We can then provide an analytical expression for the Lagrange dual after minimization over 




D



{\displaystyle \mathbf {D} }

:
 





D


(
Λ
)
=

min


D





L


(

D

,
Λ
)
=

tr

(

X

T


X
−
X

R

T


(
R

R

T


+
Λ

)

−
1


(
X

R

T



)

T


−
c
Λ
)


{\displaystyle {\mathcal {D}}(\Lambda )=\min _{\mathbf {D} }{\mathcal {L}}(\mathbf {D} ,\Lambda )={\text{tr}}(X^{T}X-XR^{T}(RR^{T}+\Lambda )^{-1}(XR^{T})^{T}-c\Lambda )}

.
 After applying one of the optimization methods to the value of the dual (such as Newton's method or conjugate gradient) we get the value of 




D



{\displaystyle \mathbf {D} }

:
 





D


T


=
(
R

R

T


+
Λ

)

−
1


(
X

R

T



)

T




{\displaystyle \mathbf {D} ^{T}=(RR^{T}+\Lambda )^{-1}(XR^{T})^{T}}


 Solving this problem is less computational hard because the amount of dual variables 



n


{\displaystyle n}

 is a lot of times much less than the amount of variables in the primal problem.
 In this approach, the optimization problem is formulated as:
 




min

r
∈


R


n




{


‖
r

‖

1


}



subject to



‖
X
−

D

R

‖

F


2


<
ϵ


{\displaystyle \min _{r\in \mathbb {R} ^{n}}\{\,\,\|r\|_{1}\}\,\,{\text{subject to}}\,\,\|X-\mathbf {D} R\|_{F}^{2}<\epsilon }

, where 



ϵ


{\displaystyle \epsilon }

 is the permitted error in the reconstruction LASSO.
 It finds an estimate of 




r

i




{\displaystyle r_{i}}

 by minimizing the least square error subject to a L1-norm constraint in the solution vector, formulated as:
 




min

r
∈


R


n









1
2





‖
X
−

D

r

‖

F


2


+
λ


‖
r

‖

1




{\displaystyle \min _{r\in \mathbb {R} ^{n}}\,\,{\dfrac {1}{2}}\,\,\|X-\mathbf {D} r\|_{F}^{2}+\lambda \,\,\|r\|_{1}}

, where 



λ
>
0


{\displaystyle \lambda >0}

 controls the trade-off between sparsity and the reconstruction error. This gives the global optimal solution.[8] See also Online dictionary learning for Sparse coding
 Parametric training methods are aimed to incorporate the best of both worlds — the realm of analytically constructed dictionaries and the learned ones.[9] This allows to construct more powerful generalized dictionaries that can potentially be applied to the cases of arbitrary-sized signals. Notable approaches include: 
 Many common approaches to sparse dictionary learning rely on the fact that the whole input data 



X


{\displaystyle X}

 (or at least a large enough training dataset) is available for the algorithm. However, this might not be the case in the real-world scenario as the size of the input data might be too big to fit it into memory. The other case where this assumption can not be made is when the input data comes in a form of a stream. Such cases lie in the field of study of online learning which essentially suggests iteratively updating the model upon the new data points 



x


{\displaystyle x}

 becoming available.
 A dictionary can be learned in an online manner the following way:[13]
 This method allows us to gradually update the dictionary as new data becomes available for sparse representation learning and helps drastically reduce the amount of memory needed to store the dataset (which often has a huge size).
 The dictionary learning framework, namely the linear decomposition of an input signal using a few basis elements learned from data itself, has led to state-of-art[citation needed] results in various image and video processing tasks. This technique can be applied to classification problems in a way that if we have built specific dictionaries for each class, the input signal can be classified by finding the dictionary corresponding to the sparsest representation.
It also has properties that are useful for signal denoising since usually one can learn a dictionary to represent the meaningful part of the input signal in a sparse way but the noise in the input will have a much less sparse representation.[14]
 Sparse dictionary learning has been successfully applied to various image, video and audio processing tasks as well as to texture synthesis[15] and unsupervised clustering.[16] In evaluations with the Bag-of-Words model,[17][18] sparse coding was found empirically to outperform other coding approaches on the object category recognition tasks.
 Dictionary learning is used to analyse medical signals in detail. Such medical signals include those from electroencephalography (EEG), electrocardiography (ECG), magnetic resonance imaging (MRI), functional MRI (fMRI), continuous glucose monitors [19] and ultrasound computer tomography (USCT), where different assumptions are used to analyze each signal.
",spars dictionari learn also known spars code sdl represent learn method aim find spars represent input data form linear combin basic element well basic element element call atom compos dictionari atom dictionari requir orthogon may span set problem setup also allow dimension signal repres higher one signal observ two properti lead seemingli redund atom allow multipl represent signal also provid improv sparsiti flexibl represent one import applic spars dictionari learn field compress sens signal recoveri compress sens signal recov linear measur provid signal spars sinc signal satisfi condit crucial find spars represent signal wavelet transform direct gradient raster matrix matrix vector transfer spars space differ recoveri algorithm like basi pursuit cosamp fast algorithm use recov signal one key principl dictionari learn dictionari infer input data emerg spars dictionari learn method stimul fact signal process one typic want repres input data use minim amount compon approach gener practic use predefin dictionari fourier wavelet transform howev certain case dictionari train fit input data significantli improv sparsiti applic data decomposit compress analysi use field imag denois classif video audio process sparsiti overcomplet dictionari immens applic imag compress imag fusion inpaint given input dataset x x x k x r k r wish find dictionari r n n r n n represent r r r k r r n k r n x r f f minim represent r spars enough formul follow optim problem argmin c r r n k x r λ r c r n argmin k c r n n c r n λ c c requir constrain atom would reach arbitrarili high valu allow arbitrarili low valu r λ control trade sparsiti minim error minim problem convex norm solv problem case known ensur sparsiti becom convex optim problem respect variabl r r one fix jointli convex r r dictionari defin undercomplet n n overcomplet case n n latter typic assumpt spars dictionari learn problem case complet dictionari provid improv represent point view thu consid undercomplet dictionari repres setup actual input data lie space case strongli relat dimension reduct techniqu like princip compon analysi requir atom n n orthogon choic subspac crucial effici dimension reduct trivial dimension reduct base dictionari represent extend address specif task data analysi classif howev main downsid limit choic atom overcomplet dictionari howev requir atom orthogon never basi anyway thu allow flexibl dictionari richer data represent overcomplet dictionari allow spars represent signal famou transform matrix wavelet transform fourier transform formul element chang way spars repres given signal best way learn dictionari capabl give sparser solut compar predefin transform matric optim problem describ solv convex problem respect either dictionari spars code one two fix algorithm base idea iter updat one problem find optim spars code r r given dictionari known spars approxim sometim spars code problem number algorithm develop solv match pursuit lasso incorpor algorithm describ method optim direct mod one first method introduc tackl spars dictionari learn problem core idea solv minim problem subject limit number compon represent vector min r x r f r r f f f denot frobeniu norm mod altern get spars code use method match pursuit updat dictionari comput analyt solut problem given x r r pseudoinvers updat renorm fit constraint new spars code obtain process repeat converg suffici small residu mod prove effici method input data x x requir iter converg howev due high complex oper comput pseudoinvers case mani case intract shortcom inspir develop dictionari learn method algorithm perform svd core updat atom dictionari one one basic gener enforc element input data x encod linear combin element way ident mod approach min r x r f r r f algorithm essenc first fix dictionari find best possibl r r constraint use orthogon match pursuit iter updat atom dictionari follow manner x r f x k x f e k k x k f f k f k k k f next step algorithm includ approxim residu matrix e k k updat k k enforc sparsiti x k k updat algorithm consid standard dictionari learn use varieti applic howev share weak mod effici signal rel low dimension possibl stuck local minima one also appli widespread stochast gradient descent method iter project solv problem idea method updat dictionari use first order stochast gradient project constraint set c c step occur iter describ express proj c δ x r λ r proj c random subset k δ gradient step algorithm base solv dual lagrangian problem provid effici way solv dictionari complic induc sparsiti function consid follow lagrangian l λ tr x r x r j n λ j j c l tr r r n j ij c c constraint norm atom λ dual variabl form diagon matrix λ provid analyt express lagrang dual minim λ min l λ tr x x x r r r λ x r c λ l tr appli one optim method valu dual newton method conjug gradient get valu r r λ x r solv problem less comput hard amount dual variabl n n lot time much less amount variabl primal problem approach optim problem formul min r r n r subject x r f ϵ r n subject f ϵ permit error reconstruct lasso find estim r minim least squar error subject constraint solut vector formul min r r n x r f λ r r n f λ control sparsiti reconstruct error give global optim solut see also onlin dictionari learn spars code parametr train method aim incorpor best world realm analyt construct dictionari learn one allow construct power gener dictionari potenti appli case signal notabl approach includ mani common approach spars dictionari learn reli fact whole input data x x least larg enough train dataset avail algorithm howev might case scenario size input data might big fit memori case assumpt made input data come form stream case lie field studi onlin learn essenti suggest iter updat model upon new data point x x becom avail dictionari learn onlin manner follow way method allow us gradual updat dictionari new data becom avail spars represent learn help drastic reduc amount memori need store dataset often huge size dictionari learn framework name linear decomposit input signal use basi element learn data led citat need result variou imag video process task techniqu appli classif problem way built specif dictionari class input signal classifi find dictionari correspond sparsest represent also properti use signal denois sinc usual one learn dictionari repres meaning part input signal spars way nois input much less spars represent spars dictionari learn success appli variou imag video audio process task well textur synthesi unsupervis cluster evalu model spars code found empir outperform code approach object categori recognit task dictionari learn use analys medic signal detail medic signal includ electroencephalographi eeg electrocardiographi ecg magnet reson imag mri function mri fmri continu glucos monitor ultrasound comput tomographi usct differ assumpt use analyz signal
Graphical model,https://en.wikipedia.org/wiki/Graphical_model,"A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. They are commonly used in probability theory, statistics—particularly Bayesian statistics—and machine learning.
 Generally, probabilistic graphical models use a graph-based representation as the foundation for encoding a  distribution over a multi-dimensional space and a graph that is a compact or factorized representation of a set of independences that hold in the specific distribution. Two branches of graphical representations of distributions are commonly used, namely, Bayesian networks and Markov random fields. Both families encompass the properties of factorization and independences, but they differ in the set of independences they can encode and the factorization of the distribution that they induce.[1]
 The undirected graph shown may have one of several interpretations; the common feature is that the presence of an edge implies some sort of dependence between the corresponding random variables. From this graph, we might deduce that B, C, and D are all conditionally independent given A. This means that if the value of A is known, then the values of B, C, and D provide no further information about each other. Equivalently (in this case), the joint probability distribution can be factorized as:
 for some non-negative functions 




f

A
B


,

f

A
C


,

f

A
D




{\displaystyle f_{AB},f_{AC},f_{AD}}

.
 
If the network structure of the model is a directed acyclic graph, the model represents a factorization of the joint probability of all random variables.  More precisely, if the events are 




X

1


,
…
,

X

n




{\displaystyle X_{1},\ldots ,X_{n}}

 then the joint probability satisfies
 where 




pa

(

X

i


)


{\displaystyle {\text{pa}}(X_{i})}

 is the set of parents of node 




X

i




{\displaystyle X_{i}}

 (nodes with edges directed towards 




X

i




{\displaystyle X_{i}}

).  In other words, the joint distribution factors into a product of conditional distributions. For example, in the directed acyclic graph shown in the Figure this factorization would be
 Any two nodes are conditionally independent given the values of their parents.  In general, any two sets of nodes are conditionally independent given a third set if a criterion called d-separation holds in the graph.  Local independences and global independences are equivalent in Bayesian networks.
 This type of graphical model is known as a directed graphical model, Bayesian network, or belief network. Classic machine learning models like hidden Markov models, neural networks and newer models such as variable-order Markov models can be considered special cases of Bayesian networks.
 One of the simplest Bayesian Networks is the Naive Bayes classifier.
 The next figure depicts a graphical model with a cycle.  This may be interpreted in terms of each variable 'depending' on the values of its parents in some manner.  
The particular graph shown suggests a joint probability density that factors as
 but other interpretations are possible.
[2]
 The framework of the models, which provides algorithms for discovering and analyzing structure in complex distributions  to describe them succinctly and extract the unstructured information, allows them to be constructed and utilized effectively.[1] Applications of graphical models include causal inference, information extraction, speech recognition, computer vision, decoding of low-density parity-check codes, modeling of gene regulatory networks, gene finding and diagnosis of diseases, and graphical models for protein structure.
",graphic model probabilist graphic model pgm structur probabilist model probabilist model graph express condit depend structur random variabl commonli use probabl theori bayesian machin learn gener probabilist graphic model use represent foundat encod distribut space graph compact factor represent set independ hold specif distribut two branch graphic represent distribut commonli use name bayesian network markov random field famili encompass properti factor independ differ set independ encod factor distribut induc undirect graph shown may one sever interpret common featur presenc edg impli sort depend correspond random variabl graph might deduc b c condit independ given mean valu known valu b c provid inform equival case joint probabl distribut factor function f b f c f ab ac ad network structur model direct acycl graph model repres factor joint probabl random variabl precis event x x n n joint probabl satisfi pa x pa set parent node x node edg direct toward x word joint distribut factor product condit distribut exampl direct acycl graph shown figur factor would two node condit independ given valu parent gener two set node condit independ given third set criterion call hold graph local independ global independ equival bayesian network type graphic model known direct graphic model bayesian network belief network classic machin learn model like hidden markov model neural network newer model markov model consid special case bayesian network one simplest bayesian network naiv bay classifi next figur depict graphic model cycl may interpret term variabl valu parent manner particular graph shown suggest joint probabl densiti factor interpret possibl framework model provid algorithm discov analyz structur complex distribut describ succinctli extract unstructur inform allow construct util effect applic graphic model includ causal infer inform extract speech recognit comput vision decod code model gene regulatori network gene find diagnosi diseas graphic model protein structur
Bayesian network,https://en.wikipedia.org/wiki/Bayesian_network,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG).[1] While it is one of several forms of causal notation, causal networks are special cases of Bayesian networks. Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.
 Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.
 Formally, Bayesian networks are directed acyclic graphs (DAGs) whose nodes represent variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Each edge represents a direct conditional dependency. Any pair of nodes that are not connected (i.e. no path connects one node to the other) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if 



m


{\displaystyle m}

 parent nodes represent 



m


{\displaystyle m}

 Boolean variables, then the probability function could be represented by a table of 




2

m




{\displaystyle 2^{m}}

 entries, one entry for each of the 




2

m




{\displaystyle 2^{m}}

 possible parent combinations. Similar ideas may be applied to undirected, and possibly cyclic, graphs such as Markov networks.
 Let us use an illustration to enforce the concepts of a Bayesian network. Suppose we want to model the dependencies between three variables: the sprinkler (or more appropriately, its state - whether it is on or not), the presence or absence of rain and whether the grass is wet or not. Observe that two events can cause the grass to become wet: an active sprinkler or rain. Rain has a direct effect on the use of the sprinkler (namely that when it rains, the sprinkler usually is not active). This situation can be modeled with a Bayesian network (shown to the right). Each variable has two possible values, T (for true) and F (for false).
 The joint probability function is, by the chain rule of probability,
 where G = ""Grass wet (true/false)"", S = ""Sprinkler turned on (true/false)"", and R = ""Raining (true/false)"".
 The model can answer questions about the presence of a cause given the presence of an effect (so-called inverse probability) like ""What is the probability that it is raining, given the grass is wet?"" by using the conditional probability formula and summing over all nuisance variables:
 Using the expansion for the joint probability function 



Pr
(
G
,
S
,
R
)


{\displaystyle \Pr(G,S,R)}

 and the conditional probabilities from the conditional probability tables (CPTs) stated in the diagram, one can evaluate each term in the sums in the numerator and denominator. For example,
 Then the numerical results (subscripted by the associated variable values) are
 To answer an interventional question, such as ""What is the probability that it would rain, given that we wet the grass?"" the answer is governed by the post-intervention joint distribution function
 obtained by removing the factor 



Pr
(
G
∣
S
,
R
)


{\displaystyle \Pr(G\mid S,R)}

 from the pre-intervention distribution. The do operator forces the value of G to be true. The probability of rain is unaffected by the action:
 To predict the impact of turning the sprinkler on:
 with the term 



Pr
(
S
=
T
∣
R
)


{\displaystyle \Pr(S=T\mid R)}

 removed, showing that the action affects the grass but not the rain.
 These predictions may not be feasible given unobserved variables, as in most policy evaluation problems. The effect of the action 




do

(
x
)


{\displaystyle {\text{do}}(x)}

 can still be predicted, however, whenever the back-door criterion is satisfied.[2][3] It states that, if a set Z of nodes can be observed that d-separates[4] (or blocks) all back-door paths from X to Y then
 A back-door path is one that ends with an arrow into X. Sets that satisfy the back-door criterion are called ""sufficient"" or ""admissible."" For example, the set Z = R is admissible for predicting the effect of S = T on G, because R d-separates the (only) back-door path S ← R → G. However, if S is not observed, no other set d-separates this path and the effect of turning the sprinkler on (S = T) on the grass (G) cannot be predicted from passive observations. In that case P(G | do(S = T)) is not ""identified"". This reflects the fact that, lacking interventional data, the observed dependence between S and G is due to a causal connection or is spurious
(apparent dependence arising from a common cause, R). (see Simpson's paradox)
 To determine whether a causal relation is identified from an arbitrary Bayesian network with unobserved variables, one can use the three rules of ""do-calculus""[2][5] and test whether all do terms can be removed from the expression of that relation, thus confirming that the desired quantity is estimable from frequency data.[6]
 Using a Bayesian network can save considerable amounts of memory over exhaustive probability tables, if the dependencies in the joint distribution are sparse. For example, a naive way of storing the conditional probabilities of 10 two-valued variables as a table requires storage space for 




2

10


=
1024


{\displaystyle 2^{10}=1024}

 values. If no variable's local distribution depends on more than three parent variables, the Bayesian network representation stores at most 



10
⋅

2

3


=
80


{\displaystyle 10\cdot 2^{3}=80}

 values.
 One advantage of Bayesian networks is that it is intuitively easier for a human to understand (a sparse set of) direct dependencies and local distributions than complete joint distributions.
 Bayesian networks perform three main inference tasks:
 Because a Bayesian network is a complete model for its variables and their relationships, it can be used to answer probabilistic queries about them. For example, the network can be used to update knowledge of the state of a subset of variables when other variables (the evidence variables) are observed. This process of computing the posterior distribution of variables given evidence is called probabilistic inference. The posterior gives a universal sufficient statistic for detection applications, when choosing values for the variable subset that minimize some expected loss function, for instance the probability of decision error. A Bayesian network can thus be considered a mechanism for automatically applying Bayes' theorem to complex problems.
 The most common exact inference methods are: variable elimination, which eliminates (by integration or summation) the non-observed non-query variables one by one by distributing the sum over the product; clique tree propagation, which caches the computation so that many variables can be queried at one time and new evidence can be propagated quickly; and recursive conditioning and AND/OR search, which allow for a space–time tradeoff and match the efficiency of variable elimination when enough space is used. All of these methods have complexity that is exponential in the network's treewidth. The most common approximate inference algorithms are importance sampling, stochastic MCMC simulation, mini-bucket elimination, loopy belief propagation, generalized belief propagation and variational methods.
 In order to fully specify the Bayesian network and thus fully represent the joint probability distribution, it is necessary to specify for each node X the probability distribution for X conditional upon X's parents. The distribution of X conditional upon its parents may have any form. It is common to work with discrete or Gaussian distributions since that simplifies calculations. Sometimes only constraints on distribution are known; one can then use the principle of maximum entropy to determine a single distribution, the one with the greatest entropy given the constraints. (Analogously, in the specific context of a dynamic Bayesian network, the conditional distribution for the hidden state's temporal evolution is commonly specified to maximize the entropy rate of the implied stochastic process.)
 Often these conditional distributions include parameters that are unknown and must be estimated from data, e.g., via the maximum likelihood approach. Direct maximization of the likelihood (or of the posterior probability) is often complex given unobserved variables. A classical approach to this problem is the expectation-maximization algorithm, which alternates computing expected values of the unobserved variables conditional on observed data, with maximizing the complete likelihood (or posterior) assuming that previously computed expected values are correct. Under mild regularity conditions, this process converges on maximum likelihood (or maximum posterior) values for parameters.
 A more fully Bayesian approach to parameters is to treat them as additional unobserved variables and to compute a full posterior distribution over all nodes conditional upon observed data, then to integrate out the parameters. This approach can be expensive and lead to large dimension models, making classical parameter-setting approaches more tractable.
 In the simplest case, a Bayesian network is specified by an expert and is then used to perform inference. In other applications, the task of defining the network is too complex for humans. In this case, the network structure and the parameters of the local distributions must be learned from data.
 Automatically learning the graph structure of a Bayesian network (BN) is a challenge pursued within machine learning. The basic idea goes back to a recovery algorithm developed by Rebane and Pearl[7] and rests on the distinction between the three possible patterns allowed in a 3-node DAG:
 The first 2 represent the same dependencies (



X


{\displaystyle X}

 and 



Z


{\displaystyle Z}

 are independent given 



Y


{\displaystyle Y}

) and are, therefore, indistinguishable. The collider, however, can be uniquely identified, since 



X


{\displaystyle X}

 and 



Z


{\displaystyle Z}

 are marginally independent and all other pairs are dependent. Thus, while the skeletons (the graphs stripped of arrows) of these three triplets are identical, the directionality of the arrows is partially identifiable. The same distinction applies when 



X


{\displaystyle X}

 and 



Z


{\displaystyle Z}

 have common parents, except that one must first condition on those parents. Algorithms have been developed to systematically determine the skeleton of the underlying graph and, then, orient all arrows whose directionality is dictated by the conditional independences observed.[2][8][9][10]
 An alternative method of structural learning uses optimization-based search. It requires a scoring function and a search strategy. A common scoring function is posterior probability of the structure given the training data, like the BIC or the BDeu. The time requirement of an exhaustive search returning a structure that maximizes the score is superexponential in the number of variables. A local search strategy makes incremental changes aimed at improving the score of the structure. A global search algorithm like Markov chain Monte Carlo can avoid getting trapped in local minima. Friedman et al.[11][12] discuss using mutual information between variables and finding a structure that maximizes this. They do this by restricting the parent candidate set to k nodes and exhaustively searching therein.
 A particularly fast method for exact BN learning is to cast the problem as an optimization problem, and solve it using integer programming. Acyclicity constraints are added to the integer program (IP) during solving in the form of cutting planes.[13] Such method can handle problems with up to 100 variables.
 In order to deal with problems with thousands of variables, a different approach is necessary. One is to first sample one ordering, and then find the optimal BN structure with respect to that ordering. This implies working on the search space of the possible orderings, which is convenient as it is smaller than the space of network structures. Multiple orderings are then sampled and evaluated. This method has been proven to be the best available in literature when the number of variables is huge.[14]
 Another method consists of focusing on the sub-class of decomposable models, for which the MLE have a closed form. It is then possible to discover a consistent structure for hundreds of variables.[15]
 Learning Bayesian networks with bounded treewidth is necessary to allow exact, tractable inference, since the worst-case inference complexity is exponential in the treewidth k (under the exponential time hypothesis). Yet, as a global property of the graph, it considerably increases the difficulty of the learning process. In this context it is possible to use K-tree for effective learning.[16]
 Given data 



x




{\displaystyle x\,\!}

 and parameter 



θ


{\displaystyle \theta }

, a simple Bayesian analysis starts with a prior probability (prior) 



p
(
θ
)


{\displaystyle p(\theta )}

 and likelihood 



p
(
x
∣
θ
)


{\displaystyle p(x\mid \theta )}

 to compute a posterior probability 



p
(
θ
∣
x
)
∝
p
(
x
∣
θ
)
p
(
θ
)


{\displaystyle p(\theta \mid x)\propto p(x\mid \theta )p(\theta )}

.
 Often the prior on 



θ


{\displaystyle \theta }

 depends in turn on other parameters 



φ


{\displaystyle \varphi }

 that are not mentioned in the likelihood. So, the prior 



p
(
θ
)


{\displaystyle p(\theta )}

 must be replaced by a likelihood 



p
(
θ
∣
φ
)


{\displaystyle p(\theta \mid \varphi )}

, and a prior 



p
(
φ
)


{\displaystyle p(\varphi )}

 on the newly introduced parameters 



φ


{\displaystyle \varphi }

 is required, resulting in a posterior probability
 This is the simplest example of a hierarchical Bayes model.
 The process may be repeated; for example, the parameters 



φ


{\displaystyle \varphi }

 may depend in turn on additional parameters 



ψ




{\displaystyle \psi \,\!}

, which require their own prior. Eventually the process must terminate, with priors that do not depend on unmentioned parameters.
 Given the measured quantities 




x

1


,
…
,

x

n






{\displaystyle x_{1},\dots ,x_{n}\,\!}

each with normally distributed errors of known standard deviation 



σ




{\displaystyle \sigma \,\!}

,
 Suppose we are interested in estimating the 




θ

i




{\displaystyle \theta _{i}}

. An approach would be to estimate the 




θ

i




{\displaystyle \theta _{i}}

 using a maximum likelihood approach; since the observations are independent, the likelihood factorizes and the maximum likelihood estimate is simply
 However, if the quantities are related, so that for example the individual 




θ

i




{\displaystyle \theta _{i}}

have themselves been drawn from an underlying distribution, then this relationship destroys the independence and suggests a more complex model, e.g.,
 with improper priors 



φ
∼

flat



{\displaystyle \varphi \sim {\text{flat}}}

, 



τ
∼

flat

∈
(
0
,
∞
)


{\displaystyle \tau \sim {\text{flat}}\in (0,\infty )}

. When 



n
≥
3


{\displaystyle n\geq 3}

, this is an identified model (i.e. there exists a unique solution for the model's parameters), and the posterior distributions of the individual 




θ

i




{\displaystyle \theta _{i}}

 will tend to move, or shrink away from the maximum likelihood estimates towards their common mean. This shrinkage is a typical behavior in hierarchical Bayes models.
 Some care is needed when choosing priors in a hierarchical model, particularly on scale variables at higher levels of the hierarchy such as the variable 



τ




{\displaystyle \tau \,\!}

 in the example. The usual priors such as the Jeffreys prior often do not work, because the posterior distribution will not be normalizable and estimates made by minimizing the expected loss will be inadmissible.
 Several equivalent definitions of a Bayesian network have been offered. For the following, let G = (V,E) be a directed acyclic graph (DAG) and let X = (Xv), v ∈ V be a set of random variables indexed by V.
 X is a Bayesian network with respect to G if its joint probability density function (with respect to a product measure) can be written as a product of the individual density functions, conditional on their parent variables:[17]
 where pa(v) is the set of parents of v (i.e. those vertices pointing directly to v via a single edge).
 For any set of random variables, the probability of any member of a joint distribution can be calculated from conditional probabilities using the chain rule (given a topological ordering of X) as follows:[17]
 Using the definition above, this can be written as:
 The difference between the two expressions is the conditional independence of the variables from any of their non-descendants, given the values of their parent variables.
 X is a Bayesian network with respect to G if it satisfies the local Markov property: each variable is conditionally independent of its non-descendants given its parent variables:[18]
 where de(v) is the set of descendants and V \ de(v) is the set of non-descendants of v.
 This can be expressed in terms similar to the first definition, as
 The set of parents is a subset of the set of non-descendants because the graph is acyclic.
 In general, learning a Bayesian network from data is known to be NP-hard.[19] This is due in part to the combinatorial explosion of enumerating DAGs as the number of variables increases. Nevertheless, insights about an underlying Bayesian network can be learned from data in polynomial time by focusing on its marginal independence structure:[20] while the conditional independence statements of a distribution modeled by a Bayesian network are encoded by a DAG (according to the factorization and Markov properties above), its marginal independence statements—the conditional independence statements in which the conditioning set is empty—are encoded by a simple undirected graph with special properties such as equal intersection and independence numbers.
 Developing a Bayesian network often begins with creating a DAG G such that X satisfies the local Markov property with respect to G. Sometimes this is a causal DAG. The conditional probability distributions of each variable given its parents in G are assessed. In many cases, in particular in the case where the variables are discrete, if the joint distribution of X is the product of these conditional distributions, then X is a Bayesian network with respect to G.[21]
 The Markov blanket of a node is the set of nodes consisting of its parents, its children, and any other parents of its children. The Markov blanket renders the node independent of the rest of the network; the joint distribution of the variables in the Markov blanket of a node is sufficient knowledge for calculating the distribution of the node. X is a Bayesian network with respect to G if every node is conditionally independent of all other nodes in the network, given its Markov blanket.[18]
 This definition can be made more general by defining the ""d""-separation of two nodes, where d stands for directional.[2] We first define the ""d""-separation of a trail and then we will define the ""d""-separation of two nodes in terms of that.
 Let P be a trail from node u to v. A trail is a loop-free, undirected (i.e. all edge directions are ignored) path between two nodes. Then P is said to be d-separated by a set of nodes Z if any of the following conditions holds:
 The nodes u and v are d-separated by Z if all trails between them are d-separated. If u and v are not d-separated, they are d-connected.
 X is a Bayesian network with respect to G if, for any two nodes u, v:
 where Z is a set which d-separates u and v. (The Markov blanket is the minimal set of nodes which d-separates node v from all other nodes.)
 Although Bayesian networks are often used to represent causal relationships, this need not be the case: a directed edge from u to v does not require that Xv be causally dependent on Xu. This is demonstrated by the fact that Bayesian networks on the graphs:
 are equivalent: that is they impose exactly the same conditional independence requirements.
 A causal network is a Bayesian network with the requirement that the relationships be causal. The additional semantics of causal networks specify that if a node X is actively caused to be in a given state x (an action written as do(X = x)), then the probability density function changes to that of the network obtained by cutting the links from the parents of X to X, and setting X to the caused value x.[2] Using these semantics, the impact of external interventions from data obtained prior to intervention can be predicted.
 In 1990, while working at Stanford University on large bioinformatic applications, Cooper proved that exact inference in Bayesian networks is NP-hard.[22] This result prompted research on approximation algorithms with the aim of developing a tractable approximation to probabilistic inference. In 1993, Paul Dagum and Michael Luby proved two surprising results on the complexity of approximation of probabilistic inference in Bayesian networks.[23] First, they proved that no tractable deterministic algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2. Second, they proved that no tractable randomized algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2 with confidence probability greater than 1/2.
 At about the same time, Roth proved that exact inference in Bayesian networks is in fact #P-complete (and thus as hard as counting the number of satisfying assignments of a conjunctive normal form formula (CNF)) and that approximate inference within a factor 2n1−ɛ for every ɛ > 0, even for Bayesian networks with restricted architecture, is NP-hard.[24][25]
 In practical terms, these complexity results suggested that while Bayesian networks were rich representations for AI and machine learning applications, their use in large real-world applications would need to be tempered by either topological structural constraints, such as naïve Bayes networks, or by restrictions on the conditional probabilities. The bounded variance algorithm[26] developed by Dagum and Luby was the first provable fast approximation algorithm to efficiently approximate probabilistic inference in Bayesian networks with guarantees on the error approximation. This powerful algorithm required the minor restriction on the conditional probabilities of the Bayesian network to be bounded away from zero and one by 



1

/

p
(
n
)


{\displaystyle 1/p(n)}

 where 



p
(
n
)


{\displaystyle p(n)}

 was any polynomial of the number of nodes in the network, 



n


{\displaystyle n}

.
 Notable software for Bayesian networks include:
 The term Bayesian network was coined by Judea Pearl in 1985 to emphasize:[28]
 In the late 1980s Pearl's Probabilistic Reasoning in Intelligent Systems[30] and Neapolitan's Probabilistic Reasoning in Expert Systems[31] summarized their properties and established them as a field of study.
",bayesian network also known bay network bay net belief network decis network probabilist graphic model repres set variabl condit depend via direct acycl graph dag one sever form causal notat causal network special case bayesian network bayesian network ideal take event occur predict likelihood one sever possibl known caus contribut factor exampl bayesian network could repres probabilist relationship diseas symptom given symptom network use comput probabl presenc variou diseas effici algorithm perform infer learn bayesian network bayesian network model sequenc variabl speech signal protein sequenc call dynam bayesian network gener bayesian network repres solv decis problem uncertainti call influenc diagram formal bayesian network direct acycl graph dag whose node repres variabl bayesian sens may observ quantiti latent variabl unknown paramet hypothes edg repres direct condit depend pair node connect path connect one node repres variabl condit independ node associ probabl function take input particular set valu node parent variabl give output probabl probabl distribut applic variabl repres node exampl parent node repres boolean variabl probabl function could repres tabl entri one entri possibl parent combin similar idea may appli undirect possibl cyclic graph markov network let us use illustr enforc concept bayesian network suppos want model depend three variabl sprinkler appropri state whether presenc absenc rain whether grass wet observ two event caus grass becom wet activ sprinkler rain rain direct effect use sprinkler name rain sprinkler usual activ situat model bayesian network shown right variabl two possibl valu true f fals joint probabl function chain rule probabl g grass wet sprinkler turn r rain model answer question presenc caus given presenc effect invers probabl like probabl rain given grass wet use condit probabl formula sum nuisanc variabl use expans joint probabl function pr g r g r condit probabl condit probabl tabl cpt state diagram one evalu term sum numer denomin exampl numer result subscript associ variabl valu answer intervent question probabl would rain given wet grass answer govern joint distribut function obtain remov factor pr g r r distribut oper forc valu g true probabl rain unaffect action predict impact turn sprinkler term pr r r remov show action affect grass rain predict may feasibl given unobserv variabl polici evalu problem effect action x x still predict howev whenev criterion satisfi state set z node observ block path x path one end arrow set satisfi criterion call suffici admiss exampl set z r admiss predict effect g r path r howev observ set path effect turn sprinkler grass g predict passiv observ case p g identifi reflect fact lack intervent data observ depend g due causal connect spuriou appar depend aris common caus r see simpson paradox determin whether causal relat identifi arbitrari bayesian network unobserv variabl one use three rule test whether term remov express relat thu confirm desir quantiti estim frequenc data use bayesian network save consider amount memori exhaust probabl tabl depend joint distribut spars exampl naiv way store condit probabl variabl tabl requir storag space valu variabl local distribut depend three parent variabl bayesian network represent store valu one advantag bayesian network intuit easier human understand spars set direct depend local distribut complet joint distribut bayesian network perform three main infer task bayesian network complet model variabl relationship use answer probabilist queri exampl network use updat knowledg state subset variabl variabl evid variabl observ process comput posterior distribut variabl given evid call probabilist infer posterior give univers suffici statist detect applic choos valu variabl subset minim expect loss function instanc probabl decis error bayesian network thu consid mechan automat appli bay theorem complex problem common exact infer method variabl elimin elimin integr summat variabl one one distribut sum product cliqu tree propag cach comput mani variabl queri one time new evid propag quickli recurs condit search allow tradeoff match effici variabl elimin enough space use method complex exponenti network treewidth common approxim infer algorithm import sampl stochast mcmc simul elimin loopi belief propag gener belief propag variat method order fulli specifi bayesian network thu fulli repres joint probabl distribut necessari specifi node x probabl distribut x condit upon x parent distribut x condit upon parent may form common work discret gaussian distribut sinc simplifi calcul sometim constraint distribut known one use principl maximum entropi determin singl distribut one greatest entropi given constraint analog specif context dynam bayesian network condit distribut hidden state tempor evolut commonli specifi maxim entropi rate impli stochast process often condit distribut includ paramet unknown must estim data via maximum likelihood approach direct maxim likelihood posterior probabl often complex given unobserv variabl classic approach problem algorithm altern comput expect valu unobserv variabl condit observ data maxim complet likelihood posterior assum previous comput expect valu correct mild regular condit process converg maximum likelihood maximum posterior valu paramet fulli bayesian approach paramet treat addit unobserv variabl comput full posterior distribut node condit upon observ data integr paramet approach expens lead larg dimens model make classic approach tractabl simplest case bayesian network specifi expert use perform infer applic task defin network complex human case network structur paramet local distribut must learn data automat learn graph structur bayesian network bn challeng pursu within machin learn basic idea goe back recoveri algorithm develop reban pearl rest distinct three possibl pattern allow dag first repres depend x x z z independ given therefor indistinguish collid howev uniqu identifi sinc x x z z margin independ pair depend thu skeleton graph strip arrow three triplet ident direction arrow partial identifi distinct appli x x z z common parent except one must first condit parent algorithm develop systemat determin skeleton underli graph orient arrow whose direction dictat condit independ observ altern method structur learn use search requir score function search strategi common score function posterior probabl structur given train data like bic bdeu time requir exhaust search return structur maxim score superexponenti number variabl local search strategi make increment chang aim improv score structur global search algorithm like markov chain mont carlo avoid get trap local minima friedman et al discuss use mutual inform variabl find structur maxim restrict parent candid set k node exhaust search therein particularli fast method exact bn learn cast problem optim problem solv use integ program acycl constraint ad integ program ip solv form cut plane method handl problem variabl order deal problem thousand variabl differ approach necessari one first sampl one order find optim bn structur respect order impli work search space possibl order conveni smaller space network structur multipl order sampl evalu method proven best avail literatur number variabl huge anoth method consist focus decompos model mle close form possibl discov consist structur hundr variabl learn bayesian network bound treewidth necessari allow exact tractabl infer sinc infer complex exponenti treewidth k exponenti time hypothesi yet global properti graph consider increas difficulti learn process context possibl use effect learn given data x paramet θ simpl bayesian analysi start prior probabl prior p θ p likelihood p x θ p comput posterior probabl p θ x p x θ p θ p x p p often prior θ depend turn paramet φ mention likelihood prior p θ p must replac likelihood p θ φ p prior p φ p newli introduc paramet φ requir result posterior probabl simplest exampl hierarch bay model process may repeat exampl paramet φ may depend turn addit paramet ψ requir prior eventu process must termin prior depend unment paramet given measur quantiti x x n n normal distribut error known standard deviat σ suppos interest estim θ approach would estim θ use maximum likelihood approach sinc observ independ likelihood factor maximum likelihood estim simpli howev quantiti relat exampl individu θ drawn underli distribut relationship destroy independ suggest complex model improp prior φ flat flat τ flat flat n identifi model exist uniqu solut model paramet posterior distribut individu θ tend move shrink away maximum likelihood estim toward common mean shrinkag typic behavior hierarch bay model care need choos prior hierarch model particularli scale variabl higher level hierarchi variabl τ exampl usual prior jeffrey prior often work posterior distribut normaliz estim made minim expect loss inadmiss sever equival definit bayesian network offer follow let g v e direct acycl graph dag let x xv v v set random variabl index x bayesian network respect g joint probabl densiti function respect product measur written product individu densiti function condit parent variabl pa v set parent v vertic point directli v via singl edg set random variabl probabl member joint distribut calcul condit probabl use chain rule given topolog order x follow use definit written differ two express condit independ variabl given valu parent variabl x bayesian network respect g satisfi local markov properti variabl condit independ given parent variabl de v set descend v de v set express term similar first definit set parent subset set graph acycl gener learn bayesian network data known due part combinatori explos enumer dag number variabl increas nevertheless insight underli bayesian network learn data polynomi time focus margin independ structur condit independ statement distribut model bayesian network encod dag accord factor markov properti margin independ condit independ statement condit set encod simpl undirect graph special properti equal intersect independ number develop bayesian network often begin creat dag g x satisfi local markov properti respect sometim causal dag condit probabl distribut variabl given parent g assess mani case particular case variabl discret joint distribut x product condit distribut x bayesian network respect markov blanket node set node consist parent children parent children markov blanket render node independ rest network joint distribut variabl markov blanket node suffici knowledg calcul distribut node x bayesian network respect g everi node condit independ node network given markov blanket definit made gener defin two node stand direct first defin trail defin two node term let p trail node u trail undirect edg direct ignor path two node p said set node z follow condit hold node u v z trail u v x bayesian network respect g two node u v z set u markov blanket minim set node node v node although bayesian network often use repres causal relationship need case direct edg u v requir xv causal depend xu demonstr fact bayesian network graph equival impos exactli condit independ requir causal network bayesian network requir relationship causal addit semant causal network specifi node x activ caus given state x action written x x probabl densiti function chang network obtain cut link parent x x set x caus valu x use semant impact extern intervent data obtain prior intervent predict work stanford univers larg bioinformat applic cooper prove exact infer bayesian network result prompt research approxim algorithm aim develop tractabl approxim probabilist infer paul dagum michael lubi prove two surpris result complex approxim probabilist infer bayesian network first prove tractabl determinist algorithm approxim probabilist infer within absolut error ɛ second prove tractabl random algorithm approxim probabilist infer within absolut error ɛ confid probabl greater time roth prove exact infer bayesian network fact thu hard count number satisfi assign conjunct normal form formula cnf approxim infer within factor everi ɛ even bayesian network restrict architectur practic term complex result suggest bayesian network rich represent ai machin learn applic use larg applic would need temper either topolog structur constraint naïv bay network restrict condit probabl bound varianc algorithm develop dagum lubi first provabl fast approxim algorithm effici approxim probabilist infer bayesian network guarante error approxim power algorithm requir minor restrict condit probabl bayesian network bound away zero one p n n p n p n polynomi number node network n n notabl softwar bayesian network includ term bayesian network coin judea pearl emphas late pearl probabilist reason intellig system neapolitan probabilist reason expert system summar properti establish field studi
Conditional random field,https://en.wikipedia.org/wiki/Conditional_random_field,"Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering ""neighbouring"" samples, a CRF can take context into account. To do so, the predictions are modelled as a graphical model, which represents the presence of dependencies between the predictions. The kind of graph used depends on the application. For example, in natural language processing, ""linear chain"" CRFs are popular, for which each prediction is dependent only on its immediate neighbours. In image processing, the graph typically connects locations to nearby and/or similar locations to enforce that they receive similar predictions.
 Other examples where CRFs are used are: labeling or parsing of sequential data for natural language processing or biological sequences,[1] part-of-speech tagging, shallow parsing,[2] named entity recognition,[3] gene finding, peptide critical functional region finding,[4] and object recognition[5] and image segmentation in computer vision.[6]
 CRFs are a type of discriminative undirected probabilistic graphical model.
 Lafferty, McCallum and Pereira[1] define a CRF on observations 




X



{\displaystyle {\boldsymbol {X}}}

 and random variables 




Y



{\displaystyle {\boldsymbol {Y}}}

 as follows:
 Let 



G
=
(
V
,
E
)


{\displaystyle G=(V,E)}

 be a graph such that 




Y

=
(


Y


v



)

v
∈
V




{\displaystyle {\boldsymbol {Y}}=({\boldsymbol {Y}}_{v})_{v\in V}}

, so that 




Y



{\displaystyle {\boldsymbol {Y}}}

 is indexed by the vertices of 



G


{\displaystyle G}

.
 Then 



(

X

,

Y

)


{\displaystyle ({\boldsymbol {X}},{\boldsymbol {Y}})}

 is a conditional random field when each random variable 





Y


v




{\displaystyle {\boldsymbol {Y}}_{v}}

, conditioned on 




X



{\displaystyle {\boldsymbol {X}}}

, obeys the Markov property with respect to the graph; that is, its probability is dependent only on its neighbours in G:
 



P
(


Y


v



|


X

,
{


Y


w


:
w
≠
v
}
)
=
P
(


Y


v



|


X

,
{


Y


w


:
w
∼
v
}
)


{\displaystyle P({\boldsymbol {Y}}_{v}|{\boldsymbol {X}},\{{\boldsymbol {Y}}_{w}:w\neq v\})=P({\boldsymbol {Y}}_{v}|{\boldsymbol {X}},\{{\boldsymbol {Y}}_{w}:w\sim v\})}

, where 





w


∼
v


{\displaystyle {\mathit {w}}\sim v}

 means
that 



w


{\displaystyle w}

 and 



v


{\displaystyle v}

 are neighbors in 



G


{\displaystyle G}

.
 What this means is that a CRF is an undirected graphical model whose nodes can be divided into exactly two disjoint sets 




X



{\displaystyle {\boldsymbol {X}}}

 and 




Y



{\displaystyle {\boldsymbol {Y}}}

, the observed and output variables, respectively; the conditional distribution 



p
(

Y


|


X

)


{\displaystyle p({\boldsymbol {Y}}|{\boldsymbol {X}})}

 is then modeled.
 For general graphs, the problem of exact inference in CRFs is intractable. The inference problem for a CRF is basically the same as for an MRF and the same arguments hold.[7]
However, there exist special cases for which exact inference is feasible:
 If exact inference is impossible, several algorithms can be used to obtain approximate solutions. These include:
 Learning the parameters 



θ


{\displaystyle \theta }

 is usually done by maximum likelihood learning for 



p
(

Y

i



|


X

i


;
θ
)


{\displaystyle p(Y_{i}|X_{i};\theta )}

. If all nodes have exponential family distributions and all nodes are observed during training, this optimization is convex.[7] It can be solved for example using gradient descent algorithms, or Quasi-Newton methods such as the L-BFGS algorithm. On the other hand, if some variables are unobserved, the inference problem has to be solved for these variables. Exact inference is intractable in general graphs, so approximations have to be used.
 In sequence modeling, the graph of interest is usually a chain graph. An input sequence of observed variables 



X


{\displaystyle X}

 represents a sequence of observations and 



Y


{\displaystyle Y}

 represents a hidden (or unknown) state variable that needs to be inferred given the observations. The 




Y

i




{\displaystyle Y_{i}}

 are structured to form a chain, with an edge between each 




Y

i
−
1




{\displaystyle Y_{i-1}}

 and 




Y

i




{\displaystyle Y_{i}}

. As well as having a simple interpretation of the 




Y

i




{\displaystyle Y_{i}}

 as ""labels"" for each element in the input sequence, this layout admits efficient algorithms for:
 The conditional dependency of each 




Y

i




{\displaystyle Y_{i}}

 on 



X


{\displaystyle X}

 is defined through a fixed set of feature functions of the form 



f
(
i
,

Y

i
−
1


,

Y

i


,
X
)


{\displaystyle f(i,Y_{i-1},Y_{i},X)}

, which can be thought of as measurements on the input sequence that partially determine the likelihood of each possible value for 




Y

i




{\displaystyle Y_{i}}

. The model assigns each feature a numerical weight and combines them to determine the probability of a certain value for 




Y

i




{\displaystyle Y_{i}}

.
 Linear-chain CRFs have many of the same applications as conceptually simpler hidden Markov models (HMMs), but relax certain assumptions about the input and output sequence distributions. An HMM can loosely be understood as a CRF with very specific feature functions that use constant probabilities to model state transitions and emissions. Conversely, a CRF can loosely be understood as a generalization of an HMM that makes the constant transition probabilities into arbitrary functions that vary across the positions in the sequence of hidden states, depending on the input sequence.
 Notably, in contrast to HMMs, CRFs can contain any number of feature functions, the feature functions can inspect the entire input sequence 



X


{\displaystyle X}

 at any point during inference, and the range of the feature functions need not have a probabilistic interpretation.
 CRFs can be extended into higher order models by making each 




Y

i




{\displaystyle Y_{i}}

 dependent on a fixed number 



k


{\displaystyle k}

 of previous variables 




Y

i
−
k


,
.
.
.
,

Y

i
−
1




{\displaystyle Y_{i-k},...,Y_{i-1}}

. In conventional formulations of higher order CRFs, training and inference are only practical for small values of 



k


{\displaystyle k}

 (such as k ≤ 5),[8] since their computational cost increases exponentially with 



k


{\displaystyle k}

.
 However, another recent advance has managed to ameliorate these issues by leveraging concepts and tools from the field of Bayesian nonparametrics. Specifically, the CRF-infinity approach[9] constitutes a CRF-type model that is capable of learning infinitely-long temporal dynamics in a scalable fashion. This is effected by introducing a novel potential function for CRFs that is based on the Sequence Memoizer (SM), a nonparametric Bayesian model for learning infinitely-long dynamics in sequential observations.[10] To render such a model computationally tractable, CRF-infinity employs a mean-field approximation[11] of the postulated novel potential functions (which are driven by an SM). This allows for devising efficient approximate training and inference algorithms for the model, without undermining its capability to capture and model temporal dependencies of arbitrary length.
 There exists another generalization of CRFs, the semi-Markov conditional random field (semi-CRF), which models variable-length segmentations of the label sequence 



Y


{\displaystyle Y}

.[12] This provides much of the power of higher-order CRFs to model long-range dependencies of the 




Y

i




{\displaystyle Y_{i}}

, at a reasonable computational cost.
 Finally, large-margin models for structured prediction, such as the structured Support Vector Machine can be seen as an alternative training procedure to CRFs.
 Latent-dynamic conditional random fields (LDCRF) or discriminative probabilistic latent variable models (DPLVM) are a type of CRFs for sequence tagging tasks. They are latent variable models that are trained discriminatively.
 In an LDCRF, like in any sequence tagging task, given a sequence of observations x = 




x

1


,
…
,

x

n




{\displaystyle x_{1},\dots ,x_{n}}

, the main problem the model must solve is how to assign a sequence of labels y = 




y

1


,
…
,

y

n




{\displaystyle y_{1},\dots ,y_{n}}

 from one finite set of labels Y. Instead of directly modeling P(y|x) as an ordinary linear-chain CRF would do, a set of latent variables h is ""inserted"" between x and y using the chain rule of probability:[13]
 This allows capturing latent structure between the observations and labels.[14] While LDCRFs can be trained using quasi-Newton methods, a specialized version of the perceptron algorithm called the latent-variable perceptron has been developed for them as well, based on Collins' structured perceptron algorithm.[13] These models find applications in computer vision, specifically gesture recognition from video streams[14] and shallow parsing.[13]
",condit random field crf class statist model method often appli pattern recognit machin learn use structur predict wherea classifi predict label singl sampl without consid neighbour sampl crf take context account predict model graphic model repres presenc depend predict kind graph use depend applic exampl natur languag process linear chain crf popular predict depend immedi neighbour imag process graph typic connect locat nearbi similar locat enforc receiv similar predict exampl crf use label pars sequenti data natur languag process biolog sequenc tag shallow pars name entiti recognit gene find peptid critic function region find object recognit imag segment comput vision crf type discrimin undirect probabilist graphic model lafferti mccallum pereira defin crf observ x x random variabl follow let g v e v e graph v v v v v index vertic g g x x condit random field random variabl v v condit x x obey markov properti respect graph probabl depend neighbour g p v x w w v p v x w w v p v x w v x w w v w v mean w w v v neighbor g g mean crf undirect graphic model whose node divid exactli two disjoint set x x observ output variabl respect condit distribut p x p x model gener graph problem exact infer crf intract infer problem crf basic mrf argument hold howev exist special case exact infer feasibl exact infer imposs sever algorithm use obtain approxim solut includ learn paramet θ usual done maximum likelihood learn p x θ p node exponenti famili distribut node observ train optim convex solv exampl use gradient descent algorithm method algorithm hand variabl unobserv infer problem solv variabl exact infer intract gener graph approxim use sequenc model graph interest usual chain graph input sequenc observ variabl x x repres sequenc observ repres hidden unknown state variabl need infer given observ structur form chain edg well simpl interpret label element input sequenc layout admit effici algorithm condit depend x x defin fix set featur function form f x f x thought measur input sequenc partial determin likelihood possibl valu model assign featur numer weight combin determin probabl certain valu crf mani applic conceptu simpler hidden markov model hmm relax certain assumpt input output sequenc distribut hmm loos understood crf specif featur function use constant probabl model state transit emiss convers crf loos understood gener hmm make constant transit probabl arbitrari function vari across posit sequenc hidden state depend input sequenc notabl contrast hmm crf contain number featur function featur function inspect entir input sequenc x x point infer rang featur function need probabilist interpret crf extend higher order model make depend fix number k k previou variabl k convent formul higher order crf train infer practic small valu k k k sinc comput cost increas exponenti k k howev anoth recent advanc manag amelior issu leverag concept tool field bayesian nonparametr specif approach constitut model capabl learn tempor dynam scalabl fashion effect introduc novel potenti function crf base sequenc memoiz sm nonparametr bayesian model learn dynam sequenti observ render model comput tractabl employ approxim postul novel potenti function driven sm allow devis effici approxim train infer algorithm model without undermin capabl captur model tempor depend arbitrari length exist anoth gener crf condit random field model segment label sequenc provid much power crf model depend reason comput cost final model structur predict structur support vector machin seen altern train procedur crf condit random field ldcrf discrimin probabilist latent variabl model dplvm type crf sequenc tag task latent variabl model train discrimin ldcrf like sequenc tag task given sequenc observ x x x n n main problem model must solv assign sequenc label n n one finit set label instead directli model p ordinari crf would set latent variabl h insert x use chain rule probabl allow captur latent structur observ label ldcrf train use method special version perceptron algorithm call perceptron develop well base collin structur perceptron algorithm model find applic comput vision specif gestur recognit video stream shallow pars
Hidden Markov model,https://en.wikipedia.org/wiki/Hidden_Markov_model,"
 A hidden Markov model (HMM) is a Markov model in which the observations are dependent on a latent (or hidden) Markov process (referred to as 



X


{\displaystyle X}

). An HMM requires that there be an observable process 



Y


{\displaystyle Y}

 whose outcomes depend on the outcomes of 



X


{\displaystyle X}

 in a known way. Since 



X


{\displaystyle X}

 cannot be observed directly, the goal is to learn about state of 



X


{\displaystyle X}

 by observing 



Y


{\displaystyle Y}

. By definition of being a Markov model, an HMM has an additional requirement that the outcome of 



Y


{\displaystyle Y}

 at time 



t
=

t

0




{\displaystyle t=t_{0}}

 must be ""influenced"" exclusively by the outcome of 



X


{\displaystyle X}

 at 



t
=

t

0




{\displaystyle t=t_{0}}

 and that the outcomes of 



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

 at 



t
<

t

0




{\displaystyle t<t_{0}}

 must be conditionally independent of 



Y


{\displaystyle Y}

 at 



t
=

t

0




{\displaystyle t=t_{0}}

 given 



X


{\displaystyle X}

 at time 



t
=

t

0




{\displaystyle t=t_{0}}

. Estimation of the parameters in an HMM can be performed using maximum likelihood estimation. For linear chain HMMs, the Baum–Welch algorithm can be used to estimate parameters.
 Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition—such as speech,[1] handwriting, gesture recognition,[2] part-of-speech tagging, musical score following,[3] partial discharges[4] and bioinformatics.[5][6]
 Let 




X

n




{\displaystyle X_{n}}

 and 




Y

n




{\displaystyle Y_{n}}

 be discrete-time stochastic processes and 



n
≥
1


{\displaystyle n\geq 1}

. The pair 



(

X

n


,

Y

n


)


{\displaystyle (X_{n},Y_{n})}

 is a hidden Markov model if
 Let 




X

t




{\displaystyle X_{t}}

 and 




Y

t




{\displaystyle Y_{t}}

 be continuous-time stochastic processes. The pair 



(

X

t


,

Y

t


)


{\displaystyle (X_{t},Y_{t})}

 is a hidden Markov model if
 The states of the process 




X

n




{\displaystyle X_{n}}

 (resp. 




X

t


)


{\displaystyle X_{t})}

 are called hidden states, and 





P


⁡


(



Y

n


∈
A
∣

X

n


=

x

n




)




{\displaystyle \operatorname {\mathbf {P} } {\bigl (}Y_{n}\in A\mid X_{n}=x_{n}{\bigr )}}

 (resp. 





P


⁡


(



Y

t


∈
A
∣

X

t


∈

B

t




)


)


{\displaystyle \operatorname {\mathbf {P} } {\bigl (}Y_{t}\in A\mid X_{t}\in B_{t}{\bigr )})}

 is called emission probability or output probability.
 In its discrete form, a hidden Markov process can be visualized as a generalization of the urn problem with replacement (where each item from the urn is returned to the original urn before the next step).[7] Consider this example: in a room that is not visible to an observer there is a genie. The room contains urns X1, X2, X3, ... each of which contains a known mix of balls, with each ball having a unique label y1, y2, y3, ... . The genie chooses an urn in that room and randomly draws a ball from that urn. It then puts the ball onto a conveyor belt, where the observer can observe the sequence of the balls but not the sequence of urns from which they were drawn. The genie has some procedure to choose urns; the choice of the urn for the n-th ball depends only upon a random number and the choice of the urn for the (n − 1)-th ball. The choice of urn does not directly depend on the urns chosen before this single previous urn; therefore, this is called a Markov process. It can be described by the upper part of Figure 1.
 The Markov process cannot be observed, only the sequence of labeled balls, thus this arrangement is called a hidden Markov process. This is illustrated by the lower part of the diagram shown in Figure 1, where one can see that balls y1, y2, y3, y4 can be drawn at each state. Even if the observer knows the composition of the urns and has just observed a sequence of three balls, e.g. y1, y2 and y3 on the conveyor belt, the observer still cannot be sure which urn (i.e., at which state) the genie has drawn the third ball from. However, the observer can work out other information, such as the likelihood that the third ball came from each of the urns.
 Consider two friends, Alice and Bob, who live far apart from each other and who talk together daily over the telephone about what they did that day. Bob is only interested in three activities: walking in the park, shopping, and cleaning his apartment. The choice of what to do is determined exclusively by the weather on a given day. Alice has no definite information about the weather, but she knows general trends. Based on what Bob tells her he did each day, Alice tries to guess what the weather must have been like.
 Alice believes that the weather operates as a discrete Markov chain. There are two states, ""Rainy"" and ""Sunny"", but she cannot observe them directly, that is, they are hidden from her. On each day, there is a certain chance that Bob will perform one of the following activities, depending on the weather: ""walk"", ""shop"", or ""clean"". Since Bob tells Alice about his activities, those are the observations. The entire system is that of a hidden Markov model (HMM).
 Alice knows the general weather trends in the area, and what Bob likes to do on average. In other words, the parameters of the HMM are known. They can be represented as follows in Python:
 In this piece of code, start_probability represents Alice's belief about which state the HMM is in when Bob first calls her (all she knows is that it tends to be rainy on average). The particular probability distribution used here is not the equilibrium one, which is (given the transition probabilities) approximately {'Rainy': 0.57, 'Sunny': 0.43}. The transition_probability represents the change of the weather in the underlying Markov chain. In this example, there is only a 30% chance that tomorrow will be sunny if today is rainy. The emission_probability represents how likely Bob is to perform a certain activity on each day. If it is rainy, there is a 50% chance that he is cleaning his apartment; if it is sunny, there is a 60% chance that he is outside for a walk.
 A similar example is further elaborated in the Viterbi algorithm page.
 The diagram below shows the general architecture of an instantiated HMM. Each oval shape represents a random variable that can adopt any of a number of values. The random variable x(t) is the hidden state at time t (with the model from the above diagram, x(t) ∈ { x1, x2, x3 }). The random variable y(t) is the observation at time t (with y(t) ∈ { y1, y2, y3, y4 }). The arrows in the diagram (often called a trellis diagram) denote conditional dependencies.
 From the diagram, it is clear that the conditional probability distribution of the hidden variable x(t) at time t, given the values of the hidden variable x at all times, depends only on the value of the hidden variable x(t − 1); the values at time t − 2 and before have no influence. This is called the Markov property. Similarly, the value of the observed variable y(t) depends on only the value of the hidden variable x(t) (both at time t).
 In the standard type of hidden Markov model considered here, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a categorical distribution) or continuous (typically from a Gaussian distribution). The parameters of a hidden Markov model are of two types, transition probabilities and emission probabilities (also known as output probabilities). The transition probabilities control the way the hidden state at time t is chosen given the hidden state at time 



t
−
1


{\displaystyle t-1}

.
 The hidden state space is assumed to consist of one of N possible values, modelled as a categorical distribution. (See the section below on extensions for other possibilities.) This means that for each of the N possible states that a hidden variable at time t can be in, there is a transition probability from this state to each of the N possible states of the hidden variable at time 



t
+
1


{\displaystyle t+1}

, for a total of 




N

2




{\displaystyle N^{2}}

 transition probabilities. The set of transition probabilities for transitions from any given state must sum to 1. Thus, the 



N
×
N


{\displaystyle N\times N}

 matrix of transition probabilities is a Markov matrix. Because any transition probability can be determined once the others are known, there are a total of 



N
(
N
−
1
)


{\displaystyle N(N-1)}

 transition parameters.
 In addition, for each of the N possible states, there is a set of emission probabilities governing the distribution of the observed variable at a particular time given the state of the hidden variable at that time. The size of this set depends on the nature of the observed variable. For example, if the observed variable is discrete with M possible values, governed by a categorical distribution, there will be 



M
−
1


{\displaystyle M-1}

 separate parameters, for a total of 



N
(
M
−
1
)


{\displaystyle N(M-1)}

 emission parameters over all hidden states. On the other hand, if the observed variable is an M-dimensional vector distributed according to an arbitrary multivariate Gaussian distribution, there will be M parameters controlling the means and 






M
(
M
+
1
)

2




{\displaystyle {\frac {M(M+1)}{2}}}

 parameters controlling the covariance matrix, for a total of 



N

(

M
+



M
(
M
+
1
)

2



)

=



N
M
(
M
+
3
)

2


=
O
(
N

M

2


)


{\displaystyle N\left(M+{\frac {M(M+1)}{2}}\right)={\frac {NM(M+3)}{2}}=O(NM^{2})}

 emission parameters. (In such a case, unless the value of M is small, it may be more practical to restrict the nature of the covariances between individual elements of the observation vector, e.g. by assuming that the elements are independent of each other, or less restrictively, are independent of all but a fixed number of adjacent elements.)
 Several inference problems are associated with hidden Markov models, as outlined below.
 The task is to compute in a best way, given the parameters of the model, the probability of a particular output sequence. This requires summation over all possible state sequences:
 The probability of observing a sequence
 of length L is given by
 where the sum runs over all possible hidden-node sequences
 Applying the principle of dynamic programming, this problem, too, can be handled efficiently using the forward algorithm.
 A number of related tasks ask about the probability of one or more of the latent variables, given the model's parameters and a sequence of observations 



y
(
1
)
,
…
,
y
(
t
)


{\displaystyle y(1),\dots ,y(t)}

.
 The task is to compute, given the model's parameters and a sequence of observations, the distribution over hidden states of the last latent variable at the end of the sequence, i.e. to compute 



P
(
x
(
t
)
∣
y
(
1
)
,
…
,
y
(
t
)
)


{\displaystyle P(x(t)\mid y(1),\dots ,y(t))}

. This task is used when the sequence of latent variables is thought of as the underlying states that a process moves through at a sequence of points in time, with corresponding observations at each point. Then, it is natural to ask about the state of the process at the end.
 This problem can be handled efficiently using the forward algorithm. An example is when the algorithm is applied to a Hidden Markov Network to determine 




P



(



h

t


∣

v

1
:
t




)




{\displaystyle \mathrm {P} {\big (}h_{t}\mid v_{1:t}{\big )}}

.
 This is similar to filtering but asks about the distribution of a latent variable somewhere in the middle of a sequence, i.e. to compute 



P
(
x
(
k
)
∣
y
(
1
)
,
…
,
y
(
t
)
)


{\displaystyle P(x(k)\mid y(1),\dots ,y(t))}

 for some 



k
<
t


{\displaystyle k<t}

. From the perspective described above, this can be thought of as the probability distribution over hidden states for a point in time k in the past, relative to time t.
 The forward-backward algorithm is a good method for computing the smoothed values for all hidden state variables.
 The task, unlike the previous two, asks about the joint probability of the entire sequence of hidden states that generated a particular sequence of observations (see illustration on the right). This task is generally applicable when HMM's are applied to different sorts of problems from those for which the tasks of filtering and smoothing are applicable. An example is part-of-speech tagging, where the hidden states represent the underlying parts of speech corresponding to an observed sequence of words. In this case, what is of interest is the entire sequence of parts of speech, rather than simply the part of speech for a single word, as filtering or smoothing would compute.
 This task requires finding a maximum over all possible state sequences, and can be solved efficiently by the Viterbi algorithm.
 For some of the above problems, it may also be interesting to ask about statistical significance. What is the probability that a sequence drawn from some null distribution will have an HMM probability (in the case of the forward algorithm) or a maximum state sequence probability (in the case of the Viterbi algorithm) at least as large as that of a particular output sequence?[8]  When an HMM is used to evaluate the relevance of a hypothesis for a particular output sequence, the statistical significance indicates the false positive rate associated with failing to reject the hypothesis for the output sequence.
 The parameter learning task in HMMs is to find, given an output sequence or a set of such sequences, the best set of state transition and emission probabilities. The task is usually to derive the maximum likelihood estimate of the parameters of the HMM given the set of output sequences. No tractable algorithm is known for solving this problem exactly, but a local maximum likelihood can be derived efficiently using the Baum–Welch algorithm or the Baldi–Chauvin algorithm. The Baum–Welch algorithm is a special case of the expectation-maximization algorithm. 
 If the HMMs are used for time series prediction, more sophisticated Bayesian inference methods, like Markov chain Monte Carlo (MCMC) sampling are proven to be favorable over finding a single maximum likelihood model both in terms of accuracy and stability.[9] Since MCMC imposes significant computational burden, in cases where computational scalability is also of interest, one may alternatively resort to variational approximations to Bayesian inference, e.g.[10] Indeed, approximate variational inference offers computational efficiency comparable to expectation-maximization, while yielding an accuracy profile only slightly inferior to exact MCMC-type Bayesian inference.
 HMMs can be applied in many fields where the goal is to recover a data sequence that is not immediately observable (but other data that depend on the sequence are). Applications include:
 Hidden Markov models were described in a series of statistical papers by Leonard E. Baum and other authors in the second half of the 1960s.[29][30][31][32][33] One of the first applications of HMMs was speech recognition, starting in the mid-1970s.[34][35][36][37] From the linguistics point of view, hidden Markov models are equivalent to stochastic regular grammar.[38]
 In the second half of the 1980s, HMMs began to be applied to the analysis of biological sequences,[39] in particular DNA. Since then, they have become ubiquitous in the field of bioinformatics.[40]
 In the hidden Markov models considered above, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a categorical distribution) or continuous (typically from a Gaussian distribution). Hidden Markov models can also be generalized to allow continuous state spaces. Examples of such models are those where the Markov process over hidden variables is a linear dynamical system, with a linear relationship among related variables and where all hidden and observed variables follow a Gaussian distribution. In simple cases, such as the linear dynamical system just mentioned, exact inference is tractable (in this case, using the Kalman filter); however, in general, exact inference in HMMs with continuous latent variables is infeasible, and approximate methods must be used, such as the extended Kalman filter or the particle filter.
 Nowadays, inference in hidden Markov models is performed in nonparametric settings, where the dependency structure enables  identifiability of the model[41] and the learnability limits are still under exploration.[42]
 Hidden Markov models are generative models, in which the joint distribution of observations and hidden states, or equivalently both the prior distribution of hidden states (the transition probabilities) and conditional distribution of observations given states (the emission probabilities), is modeled. The above algorithms implicitly assume a uniform prior distribution over the transition probabilities. However, it is also possible to create hidden Markov models with other types of prior distributions. An obvious candidate, given the categorical distribution of the transition probabilities, is the Dirichlet distribution, which is the conjugate prior distribution of the categorical distribution. Typically, a symmetric Dirichlet distribution is chosen, reflecting ignorance about which states are inherently more likely than others. The single parameter of this distribution (termed the concentration parameter) controls the relative density or sparseness of the resulting transition matrix. A choice of 1 yields a uniform distribution. Values greater than 1 produce a dense matrix, in which the transition probabilities between pairs of states are likely to be nearly equal. Values less than 1 result in a sparse matrix in which, for each given source state, only a small number of destination states have non-negligible transition probabilities. It is also possible to use a two-level prior Dirichlet distribution, in which one Dirichlet distribution (the upper distribution) governs the parameters of another Dirichlet distribution (the lower distribution), which in turn governs the transition probabilities. The upper distribution governs the overall distribution of states, determining how likely each state is to occur; its concentration parameter determines the density or sparseness of states. Such a two-level prior distribution, where both concentration parameters are set to produce sparse distributions, might be useful for example in unsupervised part-of-speech tagging, where some parts of speech occur much more commonly than others; learning algorithms that assume a uniform prior distribution generally perform poorly on this task. The parameters of models of this sort, with non-uniform prior distributions, can be learned using Gibbs sampling or extended versions of the expectation-maximization algorithm.
 An extension of the previously described hidden Markov models with Dirichlet priors uses a Dirichlet process in place of a Dirichlet distribution. This type of model allows for an unknown and potentially infinite number of states. It is common to use a two-level Dirichlet process, similar to the previously described model with two levels of Dirichlet distributions. Such a model is called a hierarchical Dirichlet process hidden Markov model, or HDP-HMM for short. It was originally described under the name ""Infinite Hidden Markov Model""[43] and was further formalized in ""Hierarchical Dirichlet Processes"".[44]
 A different type of extension uses a discriminative model in place of the generative model of standard HMMs. This type of model directly models the conditional distribution of the hidden states given the observations, rather than modeling the joint distribution. An example of this model is the so-called maximum entropy Markov model (MEMM), which models the conditional distribution of the states using logistic regression (also known as a ""maximum entropy model""). The advantage of this type of model is that arbitrary features (i.e. functions) of the observations can be modeled, allowing domain-specific knowledge of the problem at hand to be injected into the model. Models of this sort are not limited to modeling direct dependencies between a hidden state and its associated observation; rather, features of nearby observations, of combinations of the associated observation and nearby observations, or in fact of arbitrary observations at any distance from a given hidden state can be included in the process used to determine the value of a hidden state. Furthermore, there is no need for these features to be statistically independent of each other, as would be the case if such features were used in a generative model. Finally, arbitrary features over pairs of adjacent hidden states can be used rather than simple transition probabilities. The disadvantages of such models are: (1) The types of prior distributions that can be placed on hidden states are severely limited; (2) It is not possible to predict the probability of seeing an arbitrary observation. This second limitation is often not an issue in practice, since many common usages of HMM's do not require such predictive probabilities.
 A variant of the previously described discriminative model is the linear-chain conditional random field. This uses an undirected graphical model (aka Markov random field) rather than the directed graphical models of MEMM's and similar models. The advantage of this type of model is that it does not suffer from the so-called label bias problem of MEMM's, and thus may make more accurate predictions. The disadvantage is that training can be slower than for MEMM's.
 Yet another variant is the factorial hidden Markov model, which allows for a single observation to be conditioned on the corresponding hidden variables of a set of 



K


{\displaystyle K}

 independent Markov chains, rather than a single Markov chain. It is equivalent to a single HMM, with 




N

K




{\displaystyle N^{K}}

 states (assuming there are 



N


{\displaystyle N}

 states for each chain), and therefore, learning in such a model is difficult: for a sequence of length 



T


{\displaystyle T}

, a straightforward Viterbi algorithm has complexity 



O
(

N

2
K



T
)


{\displaystyle O(N^{2K}\,T)}

. To find an exact solution, a junction tree algorithm could be used, but it results in an 



O
(

N

K
+
1



K

T
)


{\displaystyle O(N^{K+1}\,K\,T)}

 complexity. In practice, approximate techniques, such as variational approaches, could be used.[45]
 All of the above models can be extended to allow for more distant dependencies among hidden states, e.g. allowing for a given state to be dependent on the previous two or three states rather than a single previous state; i.e. the transition probabilities are extended to encompass sets of three or four adjacent states (or in general 



K


{\displaystyle K}

 adjacent states). The disadvantage of such models is that dynamic-programming algorithms for training them have an 



O
(

N

K



T
)


{\displaystyle O(N^{K}\,T)}

 running time, for 



K


{\displaystyle K}

 adjacent states and 



T


{\displaystyle T}

 total observations (i.e. a length-



T


{\displaystyle T}

 Markov chain). This extension has been widely used in bioinformatics, in the modeling of DNA sequences.
 Another recent extension is the triplet Markov model,[46] in which an auxiliary underlying process is added to model some data specificities. Many variants of this model have been proposed. One should also mention the interesting link that has been established between the theory of evidence and the triplet Markov models[47] and which allows to fuse data in Markovian context[48] and to model nonstationary data.[49][50] Alternative multi-stream data fusion strategies have also been proposed in recent literature, e.g.,[51]
 Finally, a different rationale towards addressing the problem of modeling nonstationary data by means of hidden Markov models was suggested in 2012.[52] It consists in employing a small recurrent neural network (RNN), specifically a reservoir network,[53] to capture the evolution of the temporal dynamics in the observed data. This information, encoded in the form of a high-dimensional vector, is used as a conditioning variable of the HMM state transition probabilities. Under such a setup, eventually is obtained a nonstationary HMM, the transition probabilities of which evolve over time in a manner that is inferred from the data, in contrast to some unrealistic ad-hoc model of temporal evolution.
 In 2023, two innovative algorithms were introduced for the Hidden Markov Model. These algorithms enable the computation of the posterior distribution of the HMM without the necessity of explicitly modeling the joint distribution, utilizing only the conditional distributions.[54][55] Unlike traditional methods such as the Forward-Backward and Viterbi algorithms, which require knowledge of the joint law of the HMM and can be computationally intensive to learn, the Discriminative Forward-Backward and Discriminative Viterbi algorithms circumvent the need for the observation's law.[56][57] This breakthrough allows the HMM to be applied as a discriminative model, offering a more efficient and versatile approach to leveraging Hidden Markov Models in various applications.
 The model suitable in the context of longitudinal data is named latent Markov model.[58] The basic version of this model has been extended to include individual covariates, random effects and to model more complex data structures such as multilevel data. A complete overview of the latent Markov models, with special attention to the model assumptions and  to their practical use is provided in[59]
 Given a Markov transition matrix and an invariant distribution on the states, a probability measure can be imposed on the set of subshifts. For example, consider the Markov chain given on the left on the states 



A
,

B

1


,

B

2




{\displaystyle A,B_{1},B_{2}}

, with invariant distribution 



π
=
(
2

/

7
,
4

/

7
,
1

/

7
)


{\displaystyle \pi =(2/7,4/7,1/7)}

. By ignoring the distinction between 




B

1


,

B

2




{\displaystyle B_{1},B_{2}}

, this space of subshifts is projected on 



A
,

B

1


,

B

2




{\displaystyle A,B_{1},B_{2}}

 into another space of subshifts on 



A
,
B


{\displaystyle A,B}

, and this projection also projects the probability measure down to a probability measure on the subshifts on 



A
,
B


{\displaystyle A,B}

.
 The curious thing is that the probability measure on the subshifts on 



A
,
B


{\displaystyle A,B}

 is not created by a Markov chain on 



A
,
B


{\displaystyle A,B}

, not even multiple orders. Intuitively, this is because if one observes a long sequence of 




B

n




{\displaystyle B^{n}}

, then one would become increasingly sure that the 



Pr
(
A
∣

B

n


)
→


2
3




{\displaystyle \Pr(A\mid B^{n})\to {\frac {2}{3}}}

, meaning that the observable part of the system can be affected by something infinitely in the past.[60][61]
 Conversely, there exists a space of subshifts on 6 symbols, projected to subshifts on 2 symbols, such that any Markov measure on the smaller subshift has a preimage measure that is not Markov of any order (example 2.6[61]).
",hidden markov model hmm markov model observ depend latent hidden markov process refer x x hmm requir observ process whose outcom depend outcom x x known way sinc x x observ directli goal learn state x x observ definit markov model hmm addit requir outcom time must influenc exclus outcom x x outcom x x must condit independ given x x time estim paramet hmm perform use maximum likelihood estim linear chain hmm algorithm use estim paramet hidden markov model known applic thermodynam statist mechan physic chemistri econom financ signal process inform theori pattern speech handwrit gestur recognit tag music score follow partial discharg bioinformat let x n n n n stochast process n pair x n n n n hidden markov model let x stochast process pair x hidden markov model state process x n n resp x call hidden state p n x n x n p n n n resp p x b p call emiss probabl output probabl discret form hidden markov process visual gener urn problem replac item urn return origin urn next step consid exampl room visibl observ geni room contain urn contain known mix ball ball uniqu label geni choos urn room randomli draw ball urn put ball onto conveyor belt observ observ sequenc ball sequenc urn drawn geni procedur choos urn choic urn ball depend upon random number choic urn n ball choic urn directli depend urn chosen singl previou urn therefor call markov process describ upper part figur markov process observ sequenc label ball thu arrang call hidden markov process illustr lower part diagram shown figur one see ball drawn state even observ know composit urn observ sequenc three ball conveyor belt observ still sure urn state geni drawn third ball howev observ work inform likelihood third ball came urn consid two friend alic bob live far apart talk togeth daili telephon day bob interest three activ walk park shop clean apart choic determin exclus weather given day alic definit inform weather know gener trend base bob tell day alic tri guess weather must like alic believ weather oper discret markov chain two state raini sunni observ directli hidden day certain chanc bob perform one follow activ depend weather walk shop clean sinc bob tell alic activ observ entir system hidden markov model hmm alic know gener weather trend area bob like averag word paramet hmm known repres follow python piec code repres alic belief state hmm bob first call know tend raini averag particular probabl distribut use equilibrium one given transit probabl approxim repres chang weather underli markov chain exampl chanc tomorrow sunni today raini repres like bob perform certain activ day raini chanc clean apart sunni chanc outsid walk similar exampl elabor viterbi algorithm page diagram show gener architectur instanti hmm oval shape repres random variabl adopt number valu random variabl x hidden state time model diagram x random variabl observ time arrow diagram often call trelli diagram denot condit depend diagram clear condit probabl distribut hidden variabl x time given valu hidden variabl x time depend valu hidden variabl x valu time influenc call markov properti similarli valu observ variabl depend valu hidden variabl x time standard type hidden markov model consid state space hidden variabl discret observ either discret typic gener categor distribut continu typic gaussian distribut paramet hidden markov model two type transit probabl emiss probabl also known output probabl transit probabl control way hidden state time chosen given hidden state time hidden state space assum consist one n possibl valu model categor distribut see section extens possibl mean n possibl state hidden variabl time transit probabl state n possibl state hidden variabl time total n transit probabl set transit probabl transit given state must sum thu n n n matrix transit probabl markov matrix transit probabl determin other known total n n n transit paramet addit n possibl state set emiss probabl govern distribut observ variabl particular time given state hidden variabl time size set depend natur observ variabl exampl observ variabl discret possibl valu govern categor distribut separ paramet total n n emiss paramet hidden state hand observ variabl vector distribut accord arbitrari multivari gaussian distribut paramet control mean paramet control covari matrix total n n n nm emiss paramet case unless valu small may practic restrict natur covari individu element observ vector assum element independ less restrict independ fix number adjac element sever infer problem associ hidden markov model outlin task comput best way given paramet model probabl particular output sequenc requir summat possibl state sequenc probabl observ sequenc length l given sum run possibl sequenc appli principl dynam program problem handl effici use forward algorithm number relat task ask probabl one latent variabl given model paramet sequenc observ task comput given model paramet sequenc observ distribut hidden state last latent variabl end sequenc comput p x p x task use sequenc latent variabl thought underli state process move sequenc point time correspond observ point natur ask state process end problem handl effici use forward algorithm exampl algorithm appli hidden markov network determin p h v p similar filter ask distribut latent variabl somewher middl sequenc comput p x k p x k k k perspect describ thought probabl distribut hidden state point time k past rel time algorithm good method comput smooth valu hidden state variabl task unlik previou two ask joint probabl entir sequenc hidden state gener particular sequenc observ see illustr right task gener applic hmm appli differ sort problem task filter smooth applic exampl tag hidden state repres underli part speech correspond observ sequenc word case interest entir sequenc part speech rather simpli part speech singl word filter smooth would comput task requir find maximum possibl state sequenc solv effici viterbi algorithm problem may also interest ask statist signific probabl sequenc drawn null distribut hmm probabl case forward algorithm maximum state sequenc probabl case viterbi algorithm least larg particular output sequenc hmm use evalu relev hypothesi particular output sequenc statist signific indic fals posit rate associ fail reject hypothesi output sequenc paramet learn task hmm find given output sequenc set sequenc best set state transit emiss probabl task usual deriv maximum likelihood estim paramet hmm given set output sequenc tractabl algorithm known solv problem exactli local maximum likelihood deriv effici use algorithm algorithm algorithm special case algorithm hmm use time seri predict sophist bayesian infer method like markov chain mont carlo mcmc sampl proven favor find singl maximum likelihood model term accuraci stabil sinc mcmc impos signific comput burden case comput scalabl also interest one may altern resort variat approxim bayesian infer inde approxim variat infer offer comput effici compar yield accuraci profil slightli inferior exact bayesian infer hmm appli mani field goal recov data sequenc immedi observ data depend sequenc applic includ hidden markov model describ seri statist paper leonard baum author second half one first applic hmm speech recognit start linguist point view hidden markov model equival stochast regular grammar second half hmm began appli analysi biolog sequenc particular dna sinc becom ubiquit field bioinformat hidden markov model consid state space hidden variabl discret observ either discret typic gener categor distribut continu typic gaussian distribut hidden markov model also gener allow continu state space exampl model markov process hidden variabl linear dynam system linear relationship among relat variabl hidden observ variabl follow gaussian distribut simpl case linear dynam system mention exact infer tractabl case use kalman filter howev gener exact infer hmm continu latent variabl infeas approxim method must use extend kalman filter particl filter nowaday infer hidden markov model perform nonparametr set depend structur enabl identifi model learnabl limit still explor hidden markov model gener model joint distribut observ hidden state equival prior distribut hidden state transit probabl condit distribut observ given state emiss probabl model algorithm implicitli assum uniform prior distribut transit probabl howev also possibl creat hidden markov model type prior distribut obviou candid given categor distribut transit probabl dirichlet distribut conjug prior distribut categor distribut typic symmetr dirichlet distribut chosen reflect ignor state inher like other singl paramet distribut term concentr paramet control rel densiti spars result transit matrix choic yield uniform distribut valu greater produc dens matrix transit probabl pair state like nearli equal valu less result spars matrix given sourc state small number destin state transit probabl also possibl use prior dirichlet distribut one dirichlet distribut upper distribut govern paramet anoth dirichlet distribut lower distribut turn govern transit probabl upper distribut govern overal distribut state determin like state occur concentr paramet determin densiti spars state prior distribut concentr paramet set produc spars distribut might use exampl unsupervis tag part speech occur much commonli other learn algorithm assum uniform prior distribut gener perform poorli task paramet model sort prior distribut learn use gibb sampl extend version algorithm extens previous describ hidden markov model dirichlet prior use dirichlet process place dirichlet distribut type model allow unknown potenti infinit number state common use dirichlet process similar previous describ model two level dirichlet distribut model call hierarch dirichlet process hidden markov model short origin describ name infinit hidden markov model formal hierarch dirichlet process differ type extens use discrimin model place gener model standard hmm type model directli model condit distribut hidden state given observ rather model joint distribut exampl model maximum entropi markov model memm model condit distribut state use logist regress also known maximum entropi model advantag type model arbitrari featur function observ model allow knowledg problem hand inject model model sort limit model direct depend hidden state associ observ rather featur nearbi observ combin associ observ nearbi observ fact arbitrari observ distanc given hidden state includ process use determin valu hidden state furthermor need featur statist independ would case featur use gener model final arbitrari featur pair adjac hidden state use rather simpl transit probabl disadvantag model type prior distribut place hidden state sever limit possibl predict probabl see arbitrari observ second limit often issu practic sinc mani common usag hmm requir predict probabl variant previous describ discrimin model condit random field use undirect graphic model aka markov random field rather direct graphic model memm similar model advantag type model suffer label bia problem memm thu may make accur predict disadvantag train slower memm yet anoth variant factori hidden markov model allow singl observ condit correspond hidden variabl set k k independ markov chain rather singl markov chain equival singl hmm n k k state assum n n state chain therefor learn model difficult sequenc length straightforward viterbi algorithm complex n k find exact solut junction tree algorithm could use result n k k complex practic approxim techniqu variat approach could use model extend allow distant depend among hidden state allow given state depend previou two three state rather singl previou state transit probabl extend encompass set three four adjac state gener k k adjac state disadvantag model algorithm train n k k run time k k adjac state total observ markov chain extens wide use bioinformat model dna sequenc anoth recent extens triplet markov model auxiliari underli process ad model data specif mani variant model propos one also mention interest link establish theori evid triplet markov model allow fuse data markovian context model nonstationari data altern data fusion strategi also propos recent literatur final differ rational toward address problem model nonstationari data mean hidden markov model suggest consist employ small recurr neural network rnn specif reservoir network captur evolut tempor dynam observ data inform encod form vector use condit variabl hmm state transit probabl setup eventu obtain nonstationari hmm transit probabl evolv time manner infer data contrast unrealist model tempor evolut two innov algorithm introduc hidden markov model algorithm enabl comput posterior distribut hmm without necess explicitli model joint distribut util condit distribut unlik tradit method viterbi algorithm requir knowledg joint law hmm comput intens learn discrimin discrimin viterbi algorithm circumv need observ law breakthrough allow hmm appli discrimin model offer effici versatil approach leverag hidden markov model variou applic model suitabl context longitudin data name latent markov model basic version model extend includ individu covari random effect model complex data structur multilevel data complet overview latent markov model special attent model assumpt practic use provid given markov transit matrix invari distribut state probabl measur impos set subshift exampl consid markov chain given left state b b invari distribut π ignor distinct b b space subshift project b b anoth space subshift b b project also project probabl measur probabl measur subshift b b curiou thing probabl measur subshift b b creat markov chain b b even multipl order intuit one observ long sequenc b n n one would becom increasingli sure pr b n n mean observ part system affect someth infinit past convers exist space subshift symbol project subshift symbol markov measur smaller subshift preimag measur markov order exampl
Random sample consensus,https://en.wikipedia.org/wiki/Random_sample_consensus,"Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence[clarify] on the values of the estimates. Therefore, it also can be interpreted as an outlier detection method.[1] It is a non-deterministic algorithm in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are allowed. The algorithm was first published by Fischler and Bolles at SRI International in 1981. They used RANSAC to solve the location determination problem (LDP), where the goal is to determine the points in the space that project onto an image into a set of landmarks with known locations.
 RANSAC uses repeated random sub-sampling.[2] A basic assumption is that the data consists of ""inliers"", i.e., data whose distribution can be explained by some set of model parameters, though may be subject to noise, and ""outliers"", which are data that do not fit the model. The outliers can come, for example, from extreme values of the noise or from erroneous measurements or incorrect hypotheses about the interpretation of data. RANSAC also assumes that, given a (usually small) set of inliers, there exists a procedure that can estimate the parameters of a model optimally explaining or fitting this data.
 A simple example is fitting a line in two dimensions to a set of observations. Assuming that this set contains both inliers, i.e., points which approximately can be fitted to a line, and outliers, points which cannot be fitted to this line, a simple least squares method for line fitting will generally produce a line with a bad fit to the data including inliers and outliers. The reason is that it is optimally fitted to all points, including the outliers. RANSAC, on the other hand, attempts to exclude the outliers and find a linear model that only uses the inliers in its calculation. This is done by fitting linear models to several random samplings of the data and returning the model that has the best fit to a subset of the data. Since the inliers tend to be more linearly related than a random mixture of inliers and outliers, a random subset that consists entirely of inliers will have the best model fit. In practice, there is no guarantee that a subset of inliers will be randomly sampled, and the probability of the algorithm succeeding depends on the proportion of inliers in the data as well as the choice of several algorithm parameters.
 The RANSAC algorithm is a learning technique to estimate parameters of a model by random sampling of observed data. Given a dataset whose data elements contain both inliers and outliers, RANSAC uses the voting scheme to find the optimal fitting result. Data elements in the dataset are used to vote for one or multiple models. The implementation of this voting scheme is based on two assumptions: that the noisy features will not vote consistently for any single model (few outliers) and there are enough features to agree on a good model (few missing data). The RANSAC algorithm is essentially composed of two steps that are iteratively repeated:
 The set of inliers obtained for the fitting model is called the consensus set. The RANSAC algorithm will iteratively repeat the above two steps until the obtained consensus set in certain iteration has enough inliers.
 The input to the RANSAC algorithm is a set of observed data values, a model to fit to the observations, and some confidence parameters defining outliers. In more details than the aforementioned RANSAC algorithm overview, RANSAC achieves its goal by repeating the following steps:
 To converge to a sufficiently good model parameter set, this procedure is repeated a fixed number of times, each time producing either the rejection of a model because too few points are a part of the consensus set, or a refined model with a consensus set size larger than the previous consensus set.
 The generic RANSAC algorithm works as the following pseudocode:
 A Python implementation mirroring the pseudocode. This also defines a LinearRegressor based on least squares, applies RANSAC to a 2D regression problem, and visualizes the outcome: 
 The threshold value to determine when a data point fits a model (t), and the number of inliers (data points fitted to the model within t) required to assert that the model fits well to data (d) are determined based on specific requirements of the application and the dataset, and possibly based on experimental evaluation. The number of iterations (k), however, can be roughly determined as a function of the desired probability of success (p) as shown below.
 Let p be the desired probability that the RANSAC algorithm provides at least one useful result after running. In extreme (for simplifying the derivation), RANSAC returns a successful result if in some iteration it selects only inliers from the input data set when it chooses n points from the data set from which the model parameters are estimated. (In other words, all the selected n data points are inliers of the model estimated by these points). Let 



w


{\displaystyle w}

 be the probability of choosing an inlier each time a single data point is selected, that is roughly,
 A common case is that 



w


{\displaystyle w}

 is not well known beforehand because of an unknown number of inliers in data before running the RANSAC algorithm, but some rough value can be given. With a given rough value of 



w


{\displaystyle w}

 and roughly assuming that the n points needed for estimating a model are selected independently (It is a rough assumption because each data point selection reduces the number of data point candidates to choose in the next selection in reality), 




w

n




{\displaystyle w^{n}}

 is the probability that all n points are inliers and 



1
−

w

n




{\displaystyle 1-w^{n}}

 is the probability that at least one of the n points is an outlier, a case which implies that a bad model will be estimated from this point set. That probability to the power of k (the number of iterations in running the algorithm) is the probability that the algorithm never selects a set of n points which all are inliers, and this is the same as 



1
−
p


{\displaystyle 1-p}

 (the probability that the algorithm does not result in a successful model estimation) in extreme. Consequently,
 which, after taking the logarithm of both sides, leads to
 This result assumes that the n data points are selected independently, that is, a point which has been selected once is replaced and can be selected again in the same iteration. This is often not a reasonable approach and the derived value for k should be taken as an upper limit in the case that the points are selected without replacement. For example, in the case of finding a line which fits the data set illustrated in the above figure, the RANSAC algorithm typically chooses two points in each iteration and computes maybe_model as the line between the points and it is then critical that the two points are distinct.
 To gain additional confidence, the standard deviation or multiples thereof can be added to k. The standard deviation of k is defined as
 An advantage of RANSAC is its ability to do robust estimation[3] of the model parameters, i.e., it can estimate the parameters with a high degree of accuracy even when a significant number of outliers are present in the data set.  A disadvantage of RANSAC is that there is no upper bound on the time it takes to compute these parameters (except exhaustion).  When the number of iterations computed is limited, the solution obtained may not be optimal, and it may not even be one that fits the data in a good way.  In this way RANSAC offers a trade-off; by computing a greater number of iterations, the probability of a reasonable model being produced is increased.  Moreover, RANSAC is not always able to find the optimal set even for moderately contaminated sets, and it usually performs badly when the number of inliers is less than 50%. Optimal RANSAC[4] was proposed to handle both these problems and is capable of finding the optimal set for heavily contaminated sets, even for an inlier ratio under 5%. Another disadvantage of RANSAC is that it requires the setting of problem-specific thresholds.
 RANSAC can only estimate one model for a particular data set.  As for any one-model approach when two (or more) model instances exist, RANSAC may fail to find either one. The Hough transform is one alternative robust estimation technique that may be useful when more than one model instance is present. Another approach for multi-model fitting is known as PEARL,[5] which combines model sampling from data points as in RANSAC with iterative re-estimation of inliers and the multi-model fitting being formulated as an optimization problem with a global energy function describing the quality of the overall solution.
 The RANSAC algorithm is often used in computer vision, e.g., to simultaneously solve the correspondence problem and estimate the fundamental matrix related to a pair of stereo cameras; see also: Structure from motion, scale-invariant feature transform, image stitching, rigid motion segmentation.
 Since 1981 RANSAC has become a fundamental tool in the computer vision and image processing community. In 2006, for the 25th anniversary of the algorithm, a workshop was organized at the International Conference on Computer Vision and Pattern Recognition (CVPR) to summarize the most recent contributions and variations to the original algorithm, mostly meant to improve the speed of the algorithm, the robustness and accuracy of the estimated solution and to decrease the dependency from user defined constants.
 RANSAC can be sensitive to the choice of the correct noise threshold that defines which data points fit a model instantiated with a certain set of parameters. If such threshold is too large, then all the hypotheses tend to be ranked equally (good). On the other hand, when the noise threshold is too small, the estimated parameters tend to be unstable ( i.e. by simply adding or removing a datum to the set of inliers, the estimate of the parameters may fluctuate). To partially compensate for this undesirable effect, Torr et al. proposed two modification of RANSAC called MSAC (M-estimator SAmple and Consensus) and MLESAC (Maximum Likelihood Estimation SAmple and Consensus).[6] The main idea is to evaluate the quality of the consensus set ( i.e. the data that fit a model and a certain set of parameters) calculating its likelihood (whereas in the original formulation by Fischler and Bolles the rank was the cardinality of such set). An extension to MLESAC which takes into account the prior probabilities associated to the input dataset is proposed by Tordoff.[7]  The resulting algorithm is dubbed Guided-MLESAC. Along similar lines, Chum proposed to guide the sampling procedure if some a priori information regarding the input data is known, i.e. whether a datum is likely to be an inlier or an outlier. The proposed approach is called PROSAC, PROgressive SAmple Consensus.[8]
 Chum et al. also proposed a randomized version of RANSAC called R-RANSAC [9] to reduce the computational burden to identify a good consensus set. The basic idea is to initially evaluate the goodness of the currently instantiated model using only a reduced set of points instead of the entire dataset. A sound strategy will tell with high confidence when it is the case to evaluate the fitting of the entire dataset or when the model can be readily discarded. It is reasonable to think that the impact of this approach is more relevant in cases where the percentage of inliers is large. The type of strategy proposed by Chum et al. is called preemption scheme. Nistér proposed a paradigm called Preemptive RANSAC[10] that allows real time robust estimation of the structure of a scene and of the motion of the camera. The core idea of the approach consists in generating a fixed number of hypotheses so that the comparison happens with respect to the quality of the generated hypothesis rather than against some absolute quality metric.
 Other researchers tried to cope with difficult situations where the noise scale is not known and/or multiple model instances are present. The first problem has been tackled in the work by Wang and Suter.[11] Toldo et al. represent each datum with the characteristic function of the set of random models that fit the point. Then multiple models are revealed as clusters which group the points supporting the same model. The clustering algorithm, called J-linkage, does not require prior specification of the number of models, nor does it necessitate manual parameters tuning.[12]
 RANSAC has also been tailored for recursive state estimation applications, where the input measurements are corrupted by outliers and Kalman filter approaches, which rely on a Gaussian distribution of the measurement error, are doomed to fail. Such an approach is dubbed KALMANSAC.[13]
",random sampl consensu ransac iter method estim paramet mathemat model set observ data contain outlier outlier accord influenc clarifi valu estim therefor also interpret outlier detect method algorithm sens produc reason result certain probabl probabl increas iter allow algorithm first publish fischler boll sri intern use ransac solv locat determin problem ldp goal determin point space project onto imag set landmark known locat ransac use repeat random basic assumpt data consist inlier data whose distribut explain set model paramet though may subject nois outlier data fit model outlier come exampl extrem valu nois erron measur incorrect hypothes interpret data ransac also assum given usual small set inlier exist procedur estim paramet model optim explain fit data simpl exampl fit line two dimens set observ assum set contain inlier point approxim fit line outlier point fit line simpl least squar method line fit gener produc line bad fit data includ inlier outlier reason optim fit point includ outlier ransac hand attempt exclud outlier find linear model use inlier calcul done fit linear model sever random sampl data return model best fit subset data sinc inlier tend linearli relat random mixtur inlier outlier random subset consist entir inlier best model fit practic guarante subset inlier randomli sampl probabl algorithm succeed depend proport inlier data well choic sever algorithm paramet ransac algorithm learn techniqu estim paramet model random sampl observ data given dataset whose data element contain inlier outlier ransac use vote scheme find optim fit result data element dataset use vote one multipl model implement vote scheme base two assumpt noisi featur vote consist singl model outlier enough featur agre good model miss data ransac algorithm essenti compos two step iter repeat set inlier obtain fit model call consensu set ransac algorithm iter repeat two step obtain consensu set certain iter enough inlier input ransac algorithm set observ data valu model fit observ confid paramet defin outlier detail aforement ransac algorithm overview ransac achiev goal repeat follow step converg suffici good model paramet set procedur repeat fix number time time produc either reject model point part consensu set refin model consensu set size larger previou consensu set gener ransac algorithm work follow pseudocod python implement mirror pseudocod also defin linearregressor base least squar appli ransac regress problem visual outcom threshold valu determin data point fit model number inlier data point fit model within requir assert model fit well data determin base specif requir applic dataset possibl base experiment evalu number iter k howev roughli determin function desir probabl success p shown let p desir probabl ransac algorithm provid least one use result run extrem simplifi deriv ransac return success result iter select inlier input data set choos n point data set model paramet estim word select n data point inlier model estim point let w w probabl choos inlier time singl data point select roughli common case w w well known beforehand unknown number inlier data run ransac algorithm rough valu given given rough valu w w roughli assum n point need estim model select independ rough assumpt data point select reduc number data point candid choos next select realiti w n n probabl n point inlier w n n probabl least one n point outlier case impli bad model estim point set probabl power k number iter run algorithm probabl algorithm never select set n point inlier p probabl algorithm result success model estim extrem consequ take logarithm side lead result assum n data point select independ point select replac select iter often reason approach deriv valu k taken upper limit case point select without replac exampl case find line fit data set illustr figur ransac algorithm typic choos two point iter comput line point critic two point distinct gain addit confid standard deviat multipl thereof ad standard deviat k defin advantag ransac abil robust estim model paramet estim paramet high degre accuraci even signific number outlier present data set disadvantag ransac upper bound time take comput paramet except exhaust number iter comput limit solut obtain may optim may even one fit data good way way ransac offer comput greater number iter probabl reason model produc increas moreov ransac alway abl find optim set even moder contamin set usual perform badli number inlier less optim ransac propos handl problem capabl find optim set heavili contamin set even inlier ratio anoth disadvantag ransac requir set threshold ransac estim one model particular data set approach two model instanc exist ransac may fail find either one hough transform one altern robust estim techniqu may use one model instanc present anoth approach fit known pearl combin model sampl data point ransac iter inlier fit formul optim problem global energi function describ qualiti overal solut ransac algorithm often use comput vision simultan solv correspond problem estim fundament matrix relat pair stereo camera see also structur motion featur transform imag stitch rigid motion segment sinc ransac becom fundament tool comput vision imag process commun anniversari algorithm workshop organ intern confer comput vision pattern recognit cvpr summar recent contribut variat origin algorithm mostli meant improv speed algorithm robust accuraci estim solut decreas depend user defin constant ransac sensit choic correct nois threshold defin data point fit model instanti certain set paramet threshold larg hypothes tend rank equal good hand nois threshold small estim paramet tend unstabl simpli ad remov datum set inlier estim paramet may fluctuat partial compens undesir effect torr et al propos two modif ransac call msac sampl consensu mlesac maximum likelihood estim sampl consensu main idea evalu qualiti consensu set data fit model certain set paramet calcul likelihood wherea origin formul fischler boll rank cardin set extens mlesac take account prior probabl associ input dataset propos tordoff result algorithm dub along similar line chum propos guid sampl procedur priori inform regard input data known whether datum like inlier outlier propos approach call prosac progress sampl consensu chum et al also propos random version ransac call reduc comput burden identifi good consensu set basic idea initi evalu good current instanti model use reduc set point instead entir dataset sound strategi tell high confid case evalu fit entir dataset model readili discard reason think impact approach relev case percentag inlier larg type strategi propos chum et al call preemption scheme nistér propos paradigm call preemptiv ransac allow real time robust estim structur scene motion camera core idea approach consist gener fix number hypothes comparison happen respect qualiti gener hypothesi rather absolut qualiti metric research tri cope difficult situat nois scale known multipl model instanc present first problem tackl work wang suter toldo et al repres datum characterist function set random model fit point multipl model reveal cluster group point support model cluster algorithm call requir prior specif number model necessit manual paramet tune ransac also tailor recurs state estim applic input measur corrupt outlier kalman filter approach reli gaussian distribut measur error doom fail approach dub kalmansac
Local outlier factor,https://en.wikipedia.org/wiki/Local_outlier_factor,"In anomaly detection, the local outlier factor (LOF) is an algorithm proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours.[1]
 LOF shares some concepts with DBSCAN and OPTICS such as the concepts of ""core distance"" and ""reachability distance"", which are used for local density estimation.[2]
 The local outlier factor is based on a concept of a local density, where locality is given by k nearest neighbors, whose distance is used to estimate the density. By comparing the local density of an object to the local densities of its neighbors, one can identify regions of similar density, and points that have a substantially lower density than their neighbors. These are considered to be outliers.
 The local density is estimated by the typical distance at which a point can be ""reached"" from its neighbors. The definition of ""reachability distance"" used in LOF is an additional measure to produce more stable results within clusters. The ""reachability distance"" used by LOF has some subtle details that are often found incorrect in secondary sources, e.g., in the textbook of Ethem Alpaydin.[3]
 Let k-distance(A) be the distance of the object A to the k-th nearest neighbor. Note that the set of the k nearest neighbors includes all objects at this distance, which can in the case of a ""tie"" be more than k objects. We denote the set of k nearest neighbors as Nk(A).
 This distance is used to define what is called reachability distance:
 reachability-distancek(A,B)=max{k-distance(B), d(A,B)} 
 In words, the reachability distance of an object A from B is the true distance of the two objects, but at least the k-distance of B. Objects that belong to the k nearest neighbors of B (the ""core"" of B, see DBSCAN cluster analysis) are considered to be equally distant. The reason for this is to reduce the statistical fluctuations between all points A close to B, where increasing the value for k increases the smoothing effect.[1] Note that this is not a distance in the mathematical definition, since it is not symmetric. (While it is a common mistake[4] to always use the k-distance(A), this yields a slightly different method, referred to as Simplified-LOF[4])
 The local reachability density of an object A is defined by
 lrdk(A):=1/(⁠ΣB ∈ Nk(A)reachability-distancek(A, B)/|Nk(A)|⁠)
 which is the inverse of the average reachability distance of the object A from its neighbors. Note that it is not the average reachability of the neighbors from A (which by definition would be the k-distance(A)), but the distance at which A can be ""reached"" from its neighbors. With duplicate points, this value can become infinite.
 The local reachability densities are then compared with those of the neighbors using
 
LOFk(A):=⁠ΣB ∈ Nk(A)⁠lrdk(B)/lrdk(A)⁠/|Nk(A)|⁠
= ⁠ΣB ∈ Nk(A)lrdk(B)/|Nk(A)| · lrdk(A)⁠

 which is the average local reachability density of the neighbors divided by the object's own local reachability density. A value of approximately 1 indicates that the object is comparable to its neighbors (and thus not an outlier). A value below 1 indicates a denser region (which would be an inlier), while values significantly larger than 1 indicate outliers.
 LOF(k) ~ 1 means Similar density as neighbors,
 LOF(k) < 1 means Higher density than neighbors (Inlier),
 LOF(k) > 1 means Lower density than neighbors (Outlier)
 Due to the local approach, LOF is able to identify outliers in a data set that would not be outliers in another area of the data set. For example, a point at a ""small"" distance to a very dense cluster is an outlier, while a point within a sparse cluster might exhibit similar distances to its neighbors.
 While the geometric intuition of LOF is only applicable to low-dimensional vector spaces, the algorithm can be applied in any context a dissimilarity function can be defined. It has experimentally been shown to work very well in numerous setups, often outperforming the competitors, for example in network intrusion detection[5] and on processed classification benchmark data.[6]
 The LOF family of methods can be easily generalized and then applied to various other problems, such as detecting outliers in geographic data, video streams or authorship networks.[4]
 The resulting values are quotient-values and hard to interpret. A value of 1 or even less indicates a clear inlier, but there is no clear rule for when a point is an outlier. In one data set, a value of 1.1 may already be an outlier, in another dataset and parameterization (with strong local fluctuations) a value of 2 could still be an inlier. These differences can also occur within a dataset due to the locality of the method. There exist extensions of LOF that try to improve over LOF in these aspects:
",anomali detect local outlier factor lof algorithm propos marku breunig kriegel raymond ng jörg sander find anomal data point measur local deviat given data point respect neighbour lof share concept dbscan optic concept core distanc reachabl distanc use local densiti estim local outlier factor base concept local densiti local given k nearest neighbor whose distanc use estim densiti compar local densiti object local densiti neighbor one identifi region similar densiti point substanti lower densiti neighbor consid outlier local densiti estim typic distanc point reach neighbor definit reachabl distanc use lof addit measur produc stabl result within cluster reachabl distanc use lof subtl detail often found incorrect secondari sourc textbook ethem alpaydin let distanc object nearest neighbor note set k nearest neighbor includ object distanc case tie k object denot set k nearest neighbor nk distanc use defin call reachabl distanc b b b word reachabl distanc object b true distanc two object least object belong k nearest neighbor b core b see dbscan cluster analysi consid equal distant reason reduc statist fluctuat point close b increas valu k increas smooth effect note distanc mathemat definit sinc symmetr common mistak alway use yield slightli differ method refer local reachabl densiti object defin lrdk nk b invers averag reachabl distanc object neighbor note averag reachabl neighbor definit would distanc reach neighbor duplic point valu becom infinit local reachabl densiti compar neighbor use lofk nk b nk lrdk b lrdk averag local reachabl densiti neighbor divid object local reachabl densiti valu approxim indic object compar neighbor thu outlier valu indic denser region would inlier valu significantli larger indic outlier lof k mean similar densiti neighbor lof k mean higher densiti neighbor inlier lof k mean lower densiti neighbor outlier due local approach lof abl identifi outlier data set would outlier anoth area data set exampl point small distanc dens cluster outlier point within spars cluster might exhibit similar distanc neighbor geometr intuit lof applic vector space algorithm appli context dissimilar function defin experiment shown work well numer setup often outperform competitor exampl network intrus detect process classif benchmark data lof famili method easili gener appli variou problem detect outlier geograph data video stream authorship network result valu hard interpret valu even less indic clear inlier clear rule point outlier one data set valu may alreadi outlier anoth dataset parameter strong local fluctuat valu could still inlier differ also occur within dataset due local method exist extens lof tri improv lof aspect
Isolation forest,https://en.wikipedia.org/wiki/Isolation_forest,"Isolation Forest is an algorithm for data anomaly detection using binary trees. It was developed by Fei Tony Liu in 2008.[1]  It has a linear time complexity and a low memory use, which works well for high-volume data.[2][3] It is based on the assumption that because anomalies are few and different from other data, they can be isolated using few partitions. Like decision tree algorithms, it does not perform density estimation. Unlike decision tree algorithms, it uses only path length to output an anomaly score, and does not use leaf node statistics of class distribution or target value.
 Isolation Forest is fast because it splits the data space, randomly selecting an attribute and split point.  The anomaly score is inversely associated with the path-length because anomalies need fewer splits to be isolated, because they are few and different. 
 The Isolation Forest (iForest) algorithm was initially proposed by Fei Tony Liu, Kai Ming Ting and Zhi-Hua Zhou in 2008.[2] In 2012 the same authors showed that iForest has linear time complexity, a small memory requirement, and is applicable to high-dimensional data.[3] In 2010, an extension of the algorithm, SCiforest, was published to address clustered and axis-paralleled anomalies.[4]
 The premise of the Isolation Forest algorithm is that anomalous data points are easier to separate from the rest of the sample. In order to isolate a data point, the algorithm recursively generates partitions on the sample by randomly selecting an attribute and then randomly selecting a split value between the minimum and maximum values allowed for that attribute.
 An example of random partitioning in a 2D dataset of normally distributed points is shown in the first figure for a non-anomalous point and in the second one for a point that is more likely to be an anomaly. It is apparent from the pictures how anomalies require fewer random partitions to be isolated, compared to normal points.
 Recursive partitioning can be represented by a tree structure named Isolation Tree, while the number of partitions required to isolate a point can be interpreted as the length of the path, within the tree, to reach a terminating node starting from the root. For example, the path length of point 




x

i




{\displaystyle x_{i}}

 in the first figure is greater than the path length of 




x

j




{\displaystyle x_{j}}

 in the second figure.
 Let 



X
=
{

x

1


,
…
,

x

n


}


{\displaystyle X=\{x_{1},\dots ,x_{n}\}}

 be a set of d-dimensional points and 




X
′

⊂
X


{\displaystyle X'\subset X}

. An Isolation Tree (iTree) is defined as a data structure with the following properties:
 In order to build an iTree, the algorithm recursively divides 




X
′



{\displaystyle X'}

 by randomly selecting an attribute 



q


{\displaystyle q}

 and a split value 



p


{\displaystyle p}

, until either
 When the iTree is fully grown, each point in 



X


{\displaystyle X}

 is isolated at one of the external nodes. Intuitively, the anomalous points are those (easier to isolate, hence) with the smaller path length in the tree, where the path length 



h
(

x

i


)


{\displaystyle h(x_{i})}

 of point 




x

i


∈
X


{\displaystyle x_{i}\in X}

 is defined as the number of edges 




x

i




{\displaystyle x_{i}}

 traverses from the root node to get to an external node.
 A probabilistic explanation of iTree is provided in the original iForest paper.[2]
 Anomaly detection with Isolation Forest is done as follows:[4]
 The algorithm for computing the anomaly score of a data point is based on the observation that the structure of iTrees is equivalent to that of Binary Search Trees (BST): a termination to an external node of the iTree corresponds to an unsuccessful search in the BST.[4] Therefore, the estimation of average 



h
(
x
)


{\displaystyle h(x)}

 for external node terminations is the same as that of the unsuccessful searches in BST, that is[5]
 



c
(
m
)
=


{



2
H
(
m
−
1
)
−



2
(
m
−
1
)

n





for 

m
>
2




1



for 

m
=
2




0



otherwise









{\displaystyle c(m)={\begin{cases}2H(m-1)-{\frac {2(m-1)}{n}}&{\text{for }}m>2\\1&{\text{for }}m=2\\0&{\text{otherwise}}\end{cases}}}


 where 



n


{\displaystyle n}

 is the test set size, 



m


{\displaystyle m}

 is the sample set size and 



H


{\displaystyle H}

 is the harmonic number, which can be estimated by 



H
(
i
)
=
l
n
(
i
)
+
γ


{\displaystyle H(i)=ln(i)+\gamma }

, where 



γ
=
0.5772156649


{\displaystyle \gamma =0.5772156649}

 is the Euler-Mascheroni constant.
 Above, 



c
(
m
)


{\displaystyle c(m)}

 is the average 



h
(
x
)


{\displaystyle h(x)}

 given 



m


{\displaystyle m}

, so we can use it to normalize 



h
(
x
)


{\displaystyle h(x)}

 to get an estimate of the anomaly score for a given instance x:
 



s
(
x
,
m
)
=

2



−
E
(
h
(
x
)
)


c
(
m
)






{\displaystyle s(x,m)=2^{\frac {-E(h(x))}{c(m)}}}


 where 



E
(
h
(
x
)
)


{\displaystyle E(h(x))}

 is the average value of 



h
(
x
)


{\displaystyle h(x)}

 from a collection of iTrees. For any data point 



x


{\displaystyle x}

:
 The Isolation Forest algorithm has shown its effectiveness in spotting anomalies in data sets like uncovering credit card fraud instances among transactions, by European cardholders with an unbalanced dataset where it can distinguish fraudulent activities from legitimate ones by identifying rare patterns that show notable differences.[6]
 In this research projects dataset, there are 284807 transactions recorded in total out of which only 492 are identified as fraudulent (0.172%). Due to this imbalance between authentic and fraudulent transactions detection of fraud becomes quite demanding; hence specialized metrics such as the Area Under the Precision Recall Curve (AUPRC) are essential for accurate evaluation rather, than relying solely on traditional accuracy measures.[6]
 The dataset consists of PCA transformed features (from V1, to V28) well as the Time (time elapsed since the initial transaction) and Amount (transaction value). We processed the dataset using the steps:
 Scaling : The Time and Amount features by utilizing StandardScaler to standardize their input range.[7]
 Imputation:  Missing data in the dataset were filled in by using the average of the corresponding columns, with SimpleImputer.[7]
 Feature Selection : To enhance the model's effectiveness and accuracy in predictions and analysis tasks was to choose features with the kurtosis value for further examination because these particular features tend to have the most significant outliers that could potentially signal irregularities or anomalies within the data set used for modeling purposes. For training purposes specifically, a selection of the 10 features were identified and prioritized as key components, in refining the model's capabilities and enhancing its overall performance.[8]
 The Isolation Forest model was specifically trained on transactions (Class=0) focusing on recognizing common behavioral patterns in data analysis tasks. The algorithm separates out instances by measuring the distance needed to isolate them within a collection of randomly divided trees.[6]
 Hyperparameter Tuning:
 A grid search was performed over the following hyperparameters
 Contamination: Expected percentage of anomalies in the dataset, tested at values 0.01, 0.02, and 0.05 [8]
 Max Features: Number of features to sample for each tree, tested at values 5, 8, and 10.[8]
 The best configuration was found with:
 The model was evaluated on a separate test set using accuracy, precision, recall, and the Area Under the Precision-Recall Curve (AUPRC). Below are the key results:
 While the accuracy seems impressive at glance it mainly showcases the prevalence of regular transactions within the dataset. Precision and recall emphasize the challenges in detecting fraud because of the significant imbalance present. In assessing both precision and recall the AUPRC offers an evaluation by considering the balance between precision and recall.[6]
 1. Scatter Plot of Detected Anomalies
 Observation: The plot shows that many fraudulent transactions (red points) are located on the edges or far from the central cluster of normal transactions (blue points). However, some red points overlap with the blue cluster, indicating potential false positives or challenging cases for the model.
 Key Details:
 Observation:
 The Isolation Forest algorithm provides a robust solution for anomaly detection, particularly in domains like fraud detection where anomalies are rare and challenging to identify. However, its reliance on hyperparameters and sensitivity to imbalanced data necessitate careful tuning and complementary techniques for optimal results.[6][8]
 The performance of the Isolation Forest algorithm is highly dependent on the selection of its parameters. Properly tuning these parameters can significantly enhance the algorithm's ability to accurately identify anomalies. Understanding the role and impact of each parameter is crucial for optimizing the model's performance.[10]
 The Isolation Forest algorithm involves several key parameters that influence its behavior and effectiveness. These parameters control various aspects of the tree construction process, the size of the sub-samples, and the thresholds for identifying anomalies.[10] Selecting appropriate parameters is the key to the performance of the Isolation Forest algorithm. Each of the parameters influences anomaly detection differently. Key parameters include:
 Number of Trees : This parameter determines the number of trees in the Isolation Forest. A higher number of trees improves anomaly detection accuracy but increases computational costs. The optimal number balances resource availability with performance needs. For example, a smaller dataset might require fewer trees to save on computation, while larger datasets benefit from additional trees to capture more complexity.[2]
 Subsample Size : The subsample size dictates the number of data points used to construct each tree. Smaller subsample sizes reduce computational complexity but may capture less variability in the data. For instance, a subsample size of 256 is commonly used, but the optimal value depends on dataset characteristics.[2]
 Contamination Factor : This parameter estimates the proportion of outliers in the dataset. Higher contamination values flag more data points as anomalies, which can lead to false positives. Tuning this parameter carefully based on domain knowledge or cross-validation is critical to avoid bias or misclassification.[3]
 Maximum Features: This parameter specifies the number of random features to consider for each split in the tree. Limiting the number of features increases randomness, making the model more robust. However, in high-dimensional datasets, selecting only the most informative features prevents overfitting and improves generalization.[2][3]
 Tree Depth : Tree depth determines the maximum number of splits for a tree. Deeper trees better capture data complexity but risk overfitting, especially in small datasets. Shallow trees, on the other hand, improve computational efficiency.[3]
 The table below summarizes parameter selection strategies based on dataset characteristics.
 Benefits of Proper Parameter Tuning:
Improved Accuracy: Fine-tuning parameters helps the algorithm better distinguish between normal data and anomalies, reducing false positives and negatives.[10]Computational Efficiency: Selecting appropriate values for parameters like the number of trees and sub-sample size makes the algorithm more efficient without sacrificing accuracy.[10]Generalization: Limiting tree depth and using bootstrap sampling helps the model generalize better to new data, reducing overfitting.[10]
 SCiForest (Isolation Forest with Split-selection Criterion) is an extension of the original Isolation Forest algorithm, specifically designed to target clustered anomalies. It introduces a split-selection criterion and uses random hyper-planes that are non-axis-parallel to the original attributes. SCiForest does not require an optimal hyper-plane at every node; instead, it generates multiple random hyper-planes, and through sufficient trials, a good-enough hyper-plane is selected. This approach makes the resulting model highly effective due to the aggregate power of the ensemble learner.[4]
 The implementation of SciForest involves four primary steps, each tailored to improve anomaly detection by isolating clustered anomalies more effectively than standard Isolation Forest methods.
 Using techniques like KMeans or hierarchical clustering, SciForest organizes features into clusters to identify meaningful subsets. By sampling random subspaces, SciForest emphasizes meaningful feature groups, reducing noise and improving focus. This reduces the impact of irrelevant or noisy dimensions.[4]
 Within each selected subspace, isolation trees are constructed. These trees isolate points through random recursive splitting:
 Anomalous points, being sparse or distinct, are isolated more quickly (shorter path lengths) compared to normal points.[2]
 For each data point, the isolation depth (



h
(
x
)


{\displaystyle h(x)}

) is calculated for all trees across all subspaces. The anomaly score 



S
(
x
)


{\displaystyle S(x)}

 for a data point 



x


{\displaystyle x}

 is defined as:
 



S
(
x
)
=


1
n



∑

i
=
1


n



h

i


(
x
)


{\displaystyle S(x)={\frac {1}{n}}\sum _{i=1}^{n}h_{i}(x)}


 Where:
 Points with lower average path lengths (



S
(
x
)


{\displaystyle S(x)}

) are more likely to be anomalies. [3]
 The final anomaly scores are compared against a predefined threshold 



θ


{\displaystyle \theta }

 to classify data points. If 



S
(
x
)
>
θ


{\displaystyle S(x)>\theta }

, the point is classified as anomalous; otherwise, it is normal. The anomaly score threshold, θ, can be tailored to specific applications to control the proportion of identified anomalies. [6]
 These steps collectively enable SciForest to adapt to varied data distributions while maintaining efficiency in anomaly detection.
 This flowchart visually represents the step-by-step process of SCiForest implementation, from inputting high-dimensional datasets to detecting anomalies. Each step is highlighted with its key functionality, providing a clear overview of the methodology.
 Extended Isolation Forest (Extended IF or EIF) is another extension of the original Isolation Forest algorithm. Extended IF uses rotated trees in different planes, similarly to SCiForest and random values are selected to split the data, such as a random slope or intercept.
 The standard Isolation Forest requires two pieces of information, those being 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data. The Extended IF also requires only two pieces of information, this time being 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data. This makes the Extended IF simpler than using rotation trees.[15]
 The figure depicts a score map of a regular Isolation Forest in comparison to an Extended Isolation Forest for a sinusoidal-shaped data-set. This image allows us to clearly observe the improvement made by the Extended Isolation Forest in evaluating the scores much more accurately when compared to the shape of the data. While the regular isolation forest fails in capturing the sinusoid shape of the data and properly evaluating the anomaly scores. The regular Isolation Forest shapes the anomaly scores into a rectangular shape and simply assumes that any region nearby the sinusoid data point is not to be anomalous. In comparison, the EIF is more accurate in evaluating anomaly scores with more detail and unlike its predecessor, the EIF is able to detect anomalies that are close to the sinusoid shape of the data but are still anomalous. The original EIF publication includes also this comparison with a single-blob-shaped data-set and a two-blob-shaped data-set, also comparing the EIF results to isolation forest using rotation trees.[15]
 The Extended Isolation Forest enhances the traditional Isolation Forest algorithm by addressing some of its limitations, particularly in handling high-dimensional data and improving anomaly detection accuracy. Key improvements in EIF include:
 Enhanced Splitting Mechanism: Unlike traditional Isolation Forest, which uses random axis-aligned splits, EIF uses hyperplanes for splitting data. This approach allows for more flexible and accurate partitioning of the data space, which is especially useful in high-dimensional datasets.
 Improved Anomaly Scoring: EIF refines the anomaly scoring process by considering the distance of data points from the hyperplane used in splitting. This provides a more granular and precise anomaly score, leading to better differentiation between normal and anomalous points.
 Handling of High-Dimensional Data: The use of hyperplanes also improves EIF's performance in high-dimensional spaces. Traditional Isolation Forest can suffer from the curse of dimensionality in such scenarios, but EIF mitigates this issue by creating more meaningful and informative partitions in the data space.[16]
 Original implementation by Fei Tony Liu is Isolation Forest in R.
 Other implementations (in alphabetical order):
 Other variations of Isolation Forest algorithm implementations:
 The isolation forest algorithm is commonly used by data scientists through the version made available in the scikit-learn library. The snippet below depicts a brief implementation of an isolation forest, with direct explanations with comments.
 In this snippet we can observe the simplicity of a standard implementation of the algorithm. The only requirement data that the user needs to adjust is the outlier fraction in which the user determines a percentage of the samples to be classifier as outliers. This can be commonly done by selection a group among the positive and negative samples according to a giving classification. Most of the other steps are pretty standard to any decision tree based technique made available  through scikit-learn, in which the user simply needs to split the target variable from the features and fit the model after it is defined with a giving number of estimators (or trees).
 This snippet is a shortened adapted version of an implementation explored by GeeksforGeeks, which can be accessed for further explorations.[19]
",isol forest algorithm data anomali detect use binari tree develop fei toni liu linear time complex low memori use work well data base assumpt anomali differ data isol use partit like decis tree algorithm perform densiti estim unlik decis tree algorithm use path length output anomali score use leaf node statist class distribut target valu isol forest fast split data space randomli select attribut split point anomali score invers associ anomali need fewer split isol differ isol forest iforest algorithm initi propos fei toni liu kai ming ting zhou author show iforest linear time complex small memori requir applic data extens algorithm sciforest publish address cluster anomali premis isol forest algorithm anomal data point easier separ rest sampl order isol data point algorithm recurs gener partit sampl randomli select attribut randomli select split valu minimum maximum valu allow attribut exampl random partit dataset normal distribut point shown first figur point second one point like anomali appar pictur anomali requir fewer random partit isol compar normal point recurs partit repres tree structur name isol tree number partit requir isol point interpret length path within tree reach termin node start root exampl path length point x first figur greater path length x j j second figur let x x x n n set point x x x isol tree itre defin data structur follow properti order build itre algorithm recurs divid x x randomli select attribut q q split valu p p either itre fulli grown point x x isol one extern node intuit anomal point easier isol henc smaller path length tree path length h x h point x x x defin number edg x travers root node get extern node probabilist explan itre provid origin iforest paper anomali detect isol forest done follow algorithm comput anomali score data point base observ structur itre equival binari search tree bst termin extern node itre correspond unsuccess search bst therefor estim averag h x h x extern node termin unsuccess search bst c h n otherwis c case n otherwis case n n test set size sampl set size h h harmon number estim h l n γ h γ constant c c averag h x h x given use normal h x h x get estim anomali score given instanc x x e h x c x h x c e h x e h x averag valu h x h x collect itre data point x x isol forest algorithm shown effect spot anomali data set like uncov credit card fraud instanc among transact european cardhold unbalanc dataset distinguish fraudul activ legitim one identifi rare pattern show notabl differ research project dataset transact record total identifi fraudul due imbal authent fraudul transact detect fraud becom quit demand henc special metric area precis recal curv auprc essenti accur evalu rather reli sole tradit accuraci measur dataset consist pca transform featur well time time elaps sinc initi transact amount transact valu process dataset use step scale time amount featur util standardscal standard input rang imput miss data dataset fill use averag correspond column simpleimput featur select enhanc model effect accuraci predict analysi task choos featur kurtosi valu examin particular featur tend signific outlier could potenti signal irregular anomali within data set use model purpos train purpos specif select featur identifi priorit key compon refin model capabl enhanc overal perform isol forest model specif train transact focus recogn common behavior pattern data analysi task algorithm separ instanc measur distanc need isol within collect randomli divid tree hyperparamet tune grid search perform follow hyperparamet contamin expect percentag anomali dataset test valu max featur number featur sampl tree test valu best configur found model evalu separ test set use accuraci precis recal area curv auprc key result accuraci seem impress glanc mainli showcas preval regular transact within dataset precis recal emphas challeng detect fraud signific imbal present assess precis recal auprc offer evalu consid balanc precis recal scatter plot detect anomali observ plot show mani fraudul transact red point locat edg far central cluster normal transact blue point howev red point overlap blue cluster indic potenti fals posit challeng case model key detail observ isol forest algorithm provid robust solut anomali detect particularli domain like fraud detect anomali rare challeng identifi howev relianc hyperparamet sensit imbalanc data necessit care tune complementari techniqu optim result perform isol forest algorithm highli depend select paramet properli tune paramet significantli enhanc algorithm abil accur identifi anomali understand role impact paramet crucial optim model perform isol forest algorithm involv sever key paramet influenc behavior effect paramet control variou aspect tree construct process size threshold identifi anomali select appropri paramet key perform isol forest algorithm paramet influenc anomali detect differ key paramet includ number tree paramet determin number tree isol forest higher number tree improv anomali detect accuraci increas comput cost optim number balanc resourc avail perform need exampl smaller dataset might requir fewer tree save comput larger dataset benefit addit tree captur complex subsampl size subsampl size dictat number data point use construct tree smaller subsampl size reduc comput complex may captur less variabl data instanc subsampl size commonli use optim valu depend dataset characterist contamin factor paramet estim proport outlier dataset higher contamin valu flag data point anomali lead fals posit tune paramet care base domain knowledg critic avoid bia misclassif maximum featur paramet specifi number random featur consid split tree limit number featur increas random make model robust howev dataset select inform featur prevent overfit improv gener tree depth tree depth determin maximum number split tree deeper tree better captur data complex risk overfit especi small dataset shallow tree hand improv comput effici tabl summar paramet select strategi base dataset characterist benefit proper paramet tune improv accuraci paramet help algorithm better distinguish normal data anomali reduc fals posit neg comput effici select appropri valu paramet like number tree size make algorithm effici without sacrif accuraci gener limit tree depth use bootstrap sampl help model gener better new data reduc overfit sciforest isol forest criterion extens origin isol forest algorithm specif design target cluster anomali introduc criterion use random origin attribut sciforest requir optim everi node instead gener multipl random suffici trial select approach make result model highli effect due aggreg power ensembl learner implement sciforest involv four primari step tailor improv anomali detect isol cluster anomali effect standard isol forest method use techniqu like kmean hierarch cluster sciforest organ featur cluster identifi meaning subset sampl random subspac sciforest emphas meaning featur group reduc nois improv focu reduc impact irrelev noisi dimens within select subspac isol tree construct tree isol point random recurs split anomal point spars distinct isol quickli shorter path length compar normal point data point isol depth h x h x calcul tree across subspac anomali score x x data point x x defin x n n h x x n n x point lower averag path length x x like anomali final anomali score compar predefin threshold θ classifi data point x θ x point classifi anomal otherwis normal anomali score threshold θ tailor specif applic control proport identifi anomali step collect enabl sciforest adapt vari data distribut maintain effici anomali detect flowchart visual repres process sciforest implement input dataset detect anomali step highlight key function provid clear overview methodolog extend isol forest extend eif anoth extens origin isol forest algorithm extend use rotat tree differ plane similarli sciforest random valu select split data random slope intercept standard isol forest requir two piec inform random featur coordin random valu featur rang avail valu data extend also requir two piec inform time random slope branch cut random intercept branch cut chosen rang avail valu train data make extend simpler use rotat tree figur depict score map regular isol forest comparison extend isol forest imag allow us clearli observ improv made extend isol forest evalu score much accur compar shape data regular isol forest fail captur sinusoid shape data properli evalu anomali score regular isol forest shape anomali score rectangular shape simpli assum region nearbi sinusoid data point anomal comparison eif accur evalu anomali score detail unlik predecessor eif abl detect anomali close sinusoid shape data still anomal origin eif public includ also comparison also compar eif result isol forest use rotat tree extend isol forest enhanc tradit isol forest algorithm address limit particularli handl data improv anomali detect accuraci key improv eif includ enhanc split mechan unlik tradit isol forest use random split eif use hyperplan split data approach allow flexibl accur partit data space especi use dataset improv anomali score eif refin anomali score process consid distanc data point hyperplan use split provid granular precis anomali score lead better differenti normal anomal point handl data use hyperplan also improv eif perform space tradit isol forest suffer curs dimension scenario eif mitig issu creat meaning inform partit data space origin implement fei toni liu isol forest implement alphabet order variat isol forest algorithm implement isol forest algorithm commonli use data scientist version made avail librari snippet depict brief implement isol forest direct explan comment snippet observ simplic standard implement algorithm requir data user need adjust outlier fraction user determin percentag sampl classifi outlier commonli done select group among posit neg sampl accord give classif step pretti standard decis tree base techniqu made avail user simpli need split target variabl featur fit model defin give number estim tree snippet shorten adapt version implement explor geeksforgeek access explor
Autoencoder,https://en.wikipedia.org/wiki/Autoencoder,"
 An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation. The autoencoder learns an efficient representation (encoding) for a set of data, typically for dimensionality reduction, to generate lower-dimensional embeddings for subsequent use by other machine learning algorithms.[1]
 Variants exist which aim to make the learned representations assume useful properties.[2] Examples are regularized autoencoders (sparse, denoising and contractive autoencoders), which are effective in learning representations for subsequent classification tasks,[3] and variational autoencoders, which can be used as generative models.[4] Autoencoders are applied to many problems, including facial recognition,[5] feature detection,[6] anomaly detection, and learning the meaning of words.[7][8] In terms of data synthesis, autoencoders can also be used to randomly generate new data that is similar to the input (training) data.[6]
 
An autoencoder is defined by the following components:  Two sets: the space of decoded messages 





X




{\displaystyle {\mathcal {X}}}

; the space of encoded messages 





Z




{\displaystyle {\mathcal {Z}}}

. Typically 





X




{\displaystyle {\mathcal {X}}}

 and 





Z




{\displaystyle {\mathcal {Z}}}

 are Euclidean spaces, that is, 





X


=


R


m


,


Z


=


R


n




{\displaystyle {\mathcal {X}}=\mathbb {R} ^{m},{\mathcal {Z}}=\mathbb {R} ^{n}}

 with 



m
>
n
.


{\displaystyle m>n.}

  Two parametrized families of functions: the encoder family 




E

ϕ


:


X


→


Z




{\displaystyle E_{\phi }:{\mathcal {X}}\rightarrow {\mathcal {Z}}}

, parametrized by 



ϕ


{\displaystyle \phi }

; the decoder family 




D

θ


:


Z


→


X




{\displaystyle D_{\theta }:{\mathcal {Z}}\rightarrow {\mathcal {X}}}

, parametrized by 



θ


{\displaystyle \theta }

. For any 



x
∈


X




{\displaystyle x\in {\mathcal {X}}}

, we usually write 



z
=

E

ϕ


(
x
)


{\displaystyle z=E_{\phi }(x)}

, and refer to it as the code, the latent variable, latent representation, latent vector, etc. Conversely, for any 



z
∈


Z




{\displaystyle z\in {\mathcal {Z}}}

, we usually write 




x
′

=

D

θ


(
z
)


{\displaystyle x'=D_{\theta }(z)}

, and refer to it as the (decoded) message.
 Usually, both the encoder and the decoder are defined as multilayer perceptrons (MLPs). For example, a one-layer-MLP encoder 




E

ϕ




{\displaystyle E_{\phi }}

 is:
 where 



σ


{\displaystyle \sigma }

 is an element-wise activation function, 



W


{\displaystyle W}

 is a ""weight"" matrix, and 



b


{\displaystyle b}

 is a ""bias"" vector.
 An autoencoder, by itself, is simply a tuple of two functions. To judge its quality, we need a task. A task is defined by a reference probability distribution 




μ

r
e
f




{\displaystyle \mu _{ref}}

 over 





X




{\displaystyle {\mathcal {X}}}

, and a ""reconstruction quality"" function 



d
:


X


×


X


→
[
0
,
∞
]


{\displaystyle d:{\mathcal {X}}\times {\mathcal {X}}\to [0,\infty ]}

, such that 



d
(
x
,

x
′

)


{\displaystyle d(x,x')}

 measures how much 




x
′



{\displaystyle x'}

 differs from 



x


{\displaystyle x}

.
 With those, we can define the loss function for the autoencoder as



L
(
θ
,
ϕ
)
:=



E



x
∼

μ

r
e
f




[
d
(
x
,

D

θ


(

E

ϕ


(
x
)
)
)
]


{\displaystyle L(\theta ,\phi ):=\mathbb {\mathbb {E} } _{x\sim \mu _{ref}}[d(x,D_{\theta }(E_{\phi }(x)))]}

The optimal autoencoder for the given task 



(

μ

r
e
f


,
d
)


{\displaystyle (\mu _{ref},d)}

 is then 



arg
⁡

min

θ
,
ϕ


L
(
θ
,
ϕ
)


{\displaystyle \arg \min _{\theta ,\phi }L(\theta ,\phi )}

. The search for the optimal autoencoder can be accomplished by any mathematical optimization technique, but usually by gradient descent. This search process is referred to as ""training the autoencoder"".
 In most situations, the reference distribution is just the empirical distribution given by a dataset 



{

x

1


,
.
.
.
,

x

N


}
⊂


X




{\displaystyle \{x_{1},...,x_{N}\}\subset {\mathcal {X}}}

, so that




μ

r
e
f


=


1
N



∑

i
=
1


N



δ


x

i






{\displaystyle \mu _{ref}={\frac {1}{N}}\sum _{i=1}^{N}\delta _{x_{i}}}


 where 




δ


x

i






{\displaystyle \delta _{x_{i}}}

 is the Dirac measure, the quality function is just L2 loss: 



d
(
x
,

x
′

)
=
‖
x
−

x
′


‖

2


2




{\displaystyle d(x,x')=\|x-x'\|_{2}^{2}}

, and 



‖
⋅

‖

2




{\displaystyle \|\cdot \|_{2}}

 is the Euclidean norm. Then the problem of searching for the optimal autoencoder is just a least-squares optimization:




min

θ
,
ϕ


L
(
θ
,
ϕ
)
,


where 

L
(
θ
,
ϕ
)
=


1
N



∑

i
=
1


N


‖

x

i


−

D

θ


(

E

ϕ


(

x

i


)
)

‖

2


2




{\displaystyle \min _{\theta ,\phi }L(\theta ,\phi ),\qquad {\text{where }}L(\theta ,\phi )={\frac {1}{N}}\sum _{i=1}^{N}\|x_{i}-D_{\theta }(E_{\phi }(x_{i}))\|_{2}^{2}}


 An autoencoder has two main parts: an encoder that maps the message to a code, and a decoder that reconstructs the message from the code. An optimal autoencoder would perform as close to perfect reconstruction as possible, with ""close to perfect"" defined by the reconstruction quality function 



d


{\displaystyle d}

.
 The simplest way to perform the copying task perfectly would be to duplicate the signal. To suppress this behavior, the code space 





Z




{\displaystyle {\mathcal {Z}}}

 usually has fewer dimensions than the message space 





X




{\displaystyle {\mathcal {X}}}

.
 Such an autoencoder is called undercomplete. It can be interpreted as compressing the message, or reducing its dimensionality.[9][10]
 At the limit of an ideal undercomplete autoencoder, every possible code 



z


{\displaystyle z}

 in the code space is used to encode a message 



x


{\displaystyle x}

 that really appears in the distribution 




μ

r
e
f




{\displaystyle \mu _{ref}}

, and the decoder is also perfect: 




D

θ


(

E

ϕ


(
x
)
)
=
x


{\displaystyle D_{\theta }(E_{\phi }(x))=x}

. This ideal autoencoder can then be used to generate messages indistinguishable from real messages, by feeding its decoder arbitrary code 



z


{\displaystyle z}

 and obtaining 




D

θ


(
z
)


{\displaystyle D_{\theta }(z)}

, which is a message that really appears in the distribution 




μ

r
e
f




{\displaystyle \mu _{ref}}

.
 If the code space 





Z




{\displaystyle {\mathcal {Z}}}

 has dimension larger than (overcomplete), or equal to, the message space 





X




{\displaystyle {\mathcal {X}}}

, or the hidden units are given enough capacity, an autoencoder can learn the identity function and become useless. However, experimental results found that overcomplete autoencoders might still learn useful features.[11]
 In the ideal setting, the code dimension and the model capacity could be set on the basis of the complexity of the data distribution to be modeled. A standard way to do so is to add modifications to the basic autoencoder, to be detailed below.[2]
 Variational autoencoders (VAEs) belong to the families of variational Bayesian methods. Despite the architectural similarities with basic autoencoders, VAEs are architected with different goals and have a different mathematical formulation. The latent space is, in this case, composed of a mixture of distributions instead of fixed vectors.
 Given an input dataset 



x


{\displaystyle x}

 characterized by an unknown probability function 



P
(
x
)


{\displaystyle P(x)}

 and a multivariate latent encoding vector 



z


{\displaystyle z}

, the objective is to model the data as a distribution 




p

θ


(
x
)


{\displaystyle p_{\theta }(x)}

, with 



θ


{\displaystyle \theta }

 defined as the set of the network parameters so that 




p

θ


(
x
)
=

∫

z



p

θ


(
x
,
z
)
d
z


{\displaystyle p_{\theta }(x)=\int _{z}p_{\theta }(x,z)dz}

.
 
Inspired by the sparse coding hypothesis in neuroscience, sparse autoencoders (SAE) are variants of autoencoders, such that the codes 




E

ϕ


(
x
)


{\displaystyle E_{\phi }(x)}

 for messages tend to be sparse codes, that is, 




E

ϕ


(
x
)


{\displaystyle E_{\phi }(x)}

 is close to zero in most entries. Sparse autoencoders may include more (rather than fewer) hidden units than inputs, but only a small number of the hidden units are allowed to be active at the same time.[12] Encouraging sparsity improves performance on classification tasks.[13]  There are two main ways to enforce sparsity. One way is to simply clamp all but the highest-k activations of the latent code to zero. This is the k-sparse autoencoder.[13]
 The k-sparse autoencoder inserts the following ""k-sparse function"" in the latent layer of a standard autoencoder:




f

k


(

x

1


,
.
.
.
,

x

n


)
=
(

x

1



b

1


,
.
.
.
,

x

n



b

n


)


{\displaystyle f_{k}(x_{1},...,x_{n})=(x_{1}b_{1},...,x_{n}b_{n})}

where 




b

i


=
1


{\displaystyle b_{i}=1}

 if 




|


x

i



|



{\displaystyle |x_{i}|}

 ranks in the top k, and 0 otherwise.
 Backpropagating through 




f

k




{\displaystyle f_{k}}

 is simple: set gradient to 0 for 




b

i


=
0


{\displaystyle b_{i}=0}

 entries, and keep gradient for 




b

i


=
1


{\displaystyle b_{i}=1}

 entries. This is essentially a generalized ReLU function.[13]
 The other way is a relaxed version of the k-sparse autoencoder. Instead of forcing sparsity, we add a sparsity regularization loss, then optimize for




min

θ
,
ϕ


L
(
θ
,
ϕ
)
+
λ

L

sparse


(
θ
,
ϕ
)


{\displaystyle \min _{\theta ,\phi }L(\theta ,\phi )+\lambda L_{\text{sparse}}(\theta ,\phi )}

where 



λ
>
0


{\displaystyle \lambda >0}

 measures how much sparsity we want to enforce.[14]
 Let the autoencoder architecture have 



K


{\displaystyle K}

 layers. To define a sparsity regularization loss, we need a ""desired"" sparsity 







ρ
^




k




{\displaystyle {\hat {\rho }}_{k}}

 for each layer, a weight 




w

k




{\displaystyle w_{k}}

 for how much to enforce each sparsity, and a function 



s
:
[
0
,
1
]
×
[
0
,
1
]
→
[
0
,
∞
]


{\displaystyle s:[0,1]\times [0,1]\to [0,\infty ]}

 to measure how much two sparsities differ.
 For each input 



x


{\displaystyle x}

, let the actual sparsity of activation in each layer 



k


{\displaystyle k}

 be




ρ

k


(
x
)
=


1
n



∑

i
=
1


n



a

k
,
i


(
x
)


{\displaystyle \rho _{k}(x)={\frac {1}{n}}\sum _{i=1}^{n}a_{k,i}(x)}

where 




a

k
,
i


(
x
)


{\displaystyle a_{k,i}(x)}

 is the activation in the 



i


{\displaystyle i}

 -th neuron of the 



k


{\displaystyle k}

 -th layer upon input 



x


{\displaystyle x}

.
 The sparsity loss upon input 



x


{\displaystyle x}

 for one layer is 



s
(




ρ
^




k


,

ρ

k


(
x
)
)


{\displaystyle s({\hat {\rho }}_{k},\rho _{k}(x))}

, and the sparsity regularization loss for the entire autoencoder is the expected weighted sum of sparsity losses:




L

sparse


(
θ
,
ϕ
)
=



E



x
∼

μ

X





[


∑

k
∈
1
:
K



w

k


s
(




ρ
^




k


,

ρ

k


(
x
)
)

]



{\displaystyle L_{\text{sparse}}(\theta ,\phi )=\mathbb {\mathbb {E} } _{x\sim \mu _{X}}\left[\sum _{k\in 1:K}w_{k}s({\hat {\rho }}_{k},\rho _{k}(x))\right]}

Typically, the function 



s


{\displaystyle s}

 is either the Kullback-Leibler (KL) divergence, as[13][14][15][16]
 or the L1 loss, as 



s
(
ρ
,



ρ
^



)
=

|

ρ
−



ρ
^




|



{\displaystyle s(\rho ,{\hat {\rho }})=|\rho -{\hat {\rho }}|}

, or the L2 loss, as 



s
(
ρ
,



ρ
^



)
=

|

ρ
−



ρ
^





|


2




{\displaystyle s(\rho ,{\hat {\rho }})=|\rho -{\hat {\rho }}|^{2}}

.
 Alternatively, the sparsity regularization loss may be defined without reference to any ""desired sparsity"", but simply force as much sparsity as possible. In this case, one can define the sparsity regularization loss as 




L

sparse


(
θ
,
ϕ
)
=



E



x
∼

μ

X





[


∑

k
∈
1
:
K



w

k


‖

h

k


‖

]



{\displaystyle L_{\text{sparse}}(\theta ,\phi )=\mathbb {\mathbb {E} } _{x\sim \mu _{X}}\left[\sum _{k\in 1:K}w_{k}\|h_{k}\|\right]}

where 




h

k




{\displaystyle h_{k}}

 is the activation vector in the 



k


{\displaystyle k}

-th layer of the autoencoder. The norm 



‖
⋅
‖


{\displaystyle \|\cdot \|}

 is usually the L1 norm (giving the L1 sparse autoencoder) or the L2 norm (giving the L2 sparse autoencoder).
 Denoising autoencoders (DAE) try to achieve a good representation by changing the reconstruction criterion.[2][3]
 A DAE, originally called a ""robust autoassociative network"" by Mark A. Kramer,[17] is trained by intentionally corrupting the inputs of a standard autoencoder during training. A noise process is defined by a probability distribution 




μ

T




{\displaystyle \mu _{T}}

 over functions 



T
:


X


→


X




{\displaystyle T:{\mathcal {X}}\to {\mathcal {X}}}

. That is, the function 



T


{\displaystyle T}

 takes a message 



x
∈


X




{\displaystyle x\in {\mathcal {X}}}

, and corrupts it to a noisy version 



T
(
x
)


{\displaystyle T(x)}

. The function 



T


{\displaystyle T}

 is selected randomly, with a probability distribution 




μ

T




{\displaystyle \mu _{T}}

.
 Given a task 



(

μ

ref


,
d
)


{\displaystyle (\mu _{\text{ref}},d)}

, the problem of training a DAE is the optimization problem:




min

θ
,
ϕ


L
(
θ
,
ϕ
)
=



E



x
∼

μ

X


,
T
∼

μ

T




[
d
(
x
,
(

D

θ


∘

E

ϕ


∘
T
)
(
x
)
)
]


{\displaystyle \min _{\theta ,\phi }L(\theta ,\phi )=\mathbb {\mathbb {E} } _{x\sim \mu _{X},T\sim \mu _{T}}[d(x,(D_{\theta }\circ E_{\phi }\circ T)(x))]}

That is, the optimal DAE should take any noisy message and attempt to recover the original message without noise, thus the name ""denoising"".
 Usually, the noise process 



T


{\displaystyle T}

 is applied only during training and testing, not during downstream use.
 The use of DAE depends on two assumptions:
 Example noise processes include:
 A contractive autoencoder (CAE) adds the contractive regularization loss to the standard autoencoder loss:




min

θ
,
ϕ


L
(
θ
,
ϕ
)
+
λ

L

cont


(
θ
,
ϕ
)


{\displaystyle \min _{\theta ,\phi }L(\theta ,\phi )+\lambda L_{\text{cont}}(\theta ,\phi )}

where 



λ
>
0


{\displaystyle \lambda >0}

 measures how much contractive-ness we want to enforce. The contractive regularization loss itself is defined as the expected Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input:




L

cont


(
θ
,
ϕ
)
=


E


x
∼

μ

r
e
f




‖

∇

x



E

ϕ


(
x
)

‖

F


2




{\displaystyle L_{\text{cont}}(\theta ,\phi )=\mathbb {E} _{x\sim \mu _{ref}}\|\nabla _{x}E_{\phi }(x)\|_{F}^{2}}

To understand what 




L

cont




{\displaystyle L_{\text{cont}}}

 measures, note the fact



‖

E

ϕ


(
x
+
δ
x
)
−

E

ϕ


(
x
)

‖

2


≤
‖

∇

x



E

ϕ


(
x
)

‖

F


‖
δ
x

‖

2




{\displaystyle \|E_{\phi }(x+\delta x)-E_{\phi }(x)\|_{2}\leq \|\nabla _{x}E_{\phi }(x)\|_{F}\|\delta x\|_{2}}

for any message 



x
∈


X




{\displaystyle x\in {\mathcal {X}}}

, and small variation 



δ
x


{\displaystyle \delta x}

 in it. Thus, if 



‖

∇

x



E

ϕ


(
x
)

‖

F


2




{\displaystyle \|\nabla _{x}E_{\phi }(x)\|_{F}^{2}}

 is small, it means that a small neighborhood of the message maps to a small neighborhood of its code. This is a desired property, as it means small variation in the message leads to small, perhaps even zero, variation in its code, like how two pictures may look the same even if they are not exactly the same.
 The DAE can be understood as an infinitesimal limit of CAE: in the limit of small Gaussian input noise, DAEs make the reconstruction function resist small but finite-sized input perturbations, while CAEs make the extracted features resist infinitesimal input perturbations.
 A minimum description length autoencoder (MDL-AE) is an advanced variation of the traditional autoencoder, which leverages principles from information theory, specifically the Minimum Description Length (MDL) principle. The MDL principle posits that the best model for a dataset is the one that provides the shortest combined encoding of the model and the data. In the context of autoencoders, this principle is applied to ensure that the learned representation is not only compact but also interpretable and efficient for reconstruction.
 The MDL-AE seeks to minimize the total description length of the data, which includes the size of the latent representation (code length) and the error in reconstructing the original data. The objective can be expressed as





L

code


+

L

error




{\displaystyle L_{\text{code}}+L_{\text{error}}}

, where 




L

code




{\displaystyle L_{\text{code}}}

 represents the length of the compressed latent representation and 




L

error




{\displaystyle L_{\text{error}}}

 denotes the reconstruction error.[18]
 The concrete autoencoder is designed for discrete feature selection.[19] A concrete autoencoder forces the latent space to consist only of a user-specified number of features. The concrete autoencoder uses a continuous relaxation of the categorical distribution to allow gradients to pass through the feature selector layer, which makes it possible to use standard backpropagation to learn an optimal subset of input features that minimize reconstruction loss.
 Autoencoders are often trained with a single-layer encoder and a single-layer decoder, but using many-layered (deep) encoders and decoders offers many advantages.[2]
 Geoffrey Hinton developed the deep belief network technique for training many-layered deep autoencoders. His method involves treating each neighboring set of two layers as a restricted Boltzmann machine so that pretraining approximates a good solution, then using backpropagation to fine-tune the results.[10]
 Researchers have debated whether joint training (i.e. training the whole architecture together with a single global reconstruction objective to optimize) would be better for deep auto-encoders.[20] A 2015 study showed that joint training learns better data models along with more representative features for classification as compared to the layerwise method.[20] However, their experiments showed that the success of joint training depends heavily on the regularization strategies adopted.[20][21]
 (Oja, 1982)[22] noted that PCA is equivalent to a neural network with one hidden layer with identity activation function. In the language of autoencoding, the input-to-hidden module is the encoder, and the hidden-to-output module is the decoder. Subsequently, in (Baldi and Hornik, 1989)[23] and (Kramer, 1991)[9] generalized PCA to autoencoders, which they termed as ""nonlinear PCA"".
 Immediately after the resurgence of neural networks in the 1980s, it was suggested in 1986[24] that a neural network be put in ""auto-association mode"". This was then implemented in (Harrison, 1987)[25] and (Elman, Zipser, 1988)[26] for speech and in (Cottrell, Munro, Zipser, 1987)[27] for images.[28] In (Hinton, Salakhutdinov, 2006),[29] deep belief networks were developed. These train a pair restricted Boltzmann machines as encoder-decoder pairs, then train another pair on the latent representation of the first pair, and so on.[30]
 The first applications of AE date to early 1990s.[2][31][18] Their most traditional application was dimensionality reduction or feature learning, but the concept became widely used for learning generative models of data.[32][33] Some of the most powerful AIs in the 2010s involved autoencoder modules as a component of larger AI systems, such as VAE in Stable Diffusion, discrete VAE in Transformer-based image generators like DALL-E 1, etc.
 During the early days, when the terminology was uncertain, the autoencoder has also been called identity mapping,[34][9] auto-associating,[35] self-supervised backpropagation,[9] or Diabolo network.[36][11]
 The two main applications of autoencoders are dimensionality reduction and information retrieval (or associative memory),[2] but modern variations have been applied to other tasks.
 Dimensionality reduction was one of the first deep learning applications.[2]
 For Hinton's 2006 study,[10] he pretrained a multi-layer autoencoder with a stack of RBMs and then used their weights to initialize a deep autoencoder with gradually smaller hidden layers until hitting a bottleneck of 30 neurons. The resulting 30 dimensions of the code yielded a smaller reconstruction error compared to the first 30 components of a principal component analysis (PCA), and learned a representation that was qualitatively easier to interpret, clearly separating data clusters.[2][10]
 Representing dimensions can improve performance on tasks such as classification.[2] Indeed, the hallmark of dimensionality reduction is to place semantically related examples near each other.[38]
 If linear activations are used, or only a single sigmoid hidden layer, then the optimal solution to an autoencoder is strongly related to principal component analysis (PCA).[28][39] The weights of an autoencoder with a single hidden layer of size 



p


{\displaystyle p}

 (where 



p


{\displaystyle p}

 is less than the size of the input) span the same vector subspace as the one spanned by the first 



p


{\displaystyle p}

 principal components, and the output of the autoencoder is an orthogonal projection onto this subspace. The autoencoder weights are not equal to the principal components, and are generally not orthogonal, yet the principal components may be recovered from them using the singular value decomposition.[40]
 However, the potential of autoencoders resides in their non-linearity, allowing the model to learn more powerful generalizations compared to PCA, and to reconstruct the input with significantly lower information loss.[10]
 Information retrieval benefits particularly from dimensionality reduction in that search can become more efficient in certain kinds of low dimensional spaces. Autoencoders were indeed applied to semantic hashing, proposed by Salakhutdinov and Hinton in 2007.[38] By training the algorithm to produce a low-dimensional binary code, all database entries could be stored in a hash table mapping binary code vectors to entries. This table would then support information retrieval by returning all entries with the same binary code as the query, or slightly less similar entries by flipping some bits from the query encoding.
 The encoder-decoder architecture, often used in natural language processing and neural networks, can be scientifically applied in the field of SEO (Search Engine Optimization) in various ways:
 In essence, the encoder-decoder architecture or autoencoders can be leveraged in SEO to optimize web page content, improve their indexing, and enhance their appeal to both search engines and users.
 Another application for autoencoders is anomaly detection.[17][41][42][43][44][45] By learning to replicate the most salient features in the training data under some of the constraints described previously, the model is encouraged to learn to precisely reproduce the most frequently observed characteristics. When facing anomalies, the model should worsen its reconstruction performance. In most cases, only data with normal instances are used to train the autoencoder; in others, the frequency of anomalies is small compared to the observation set so that its contribution to the learned representation could be ignored. After training, the autoencoder will accurately reconstruct ""normal"" data, while failing to do so with unfamiliar anomalous data.[43] Reconstruction error (the error between the original data and its low dimensional reconstruction) is used as an anomaly score to detect anomalies.[43]
 Recent literature has however shown that certain autoencoding models can, counterintuitively, be very good at reconstructing anomalous examples and consequently not able to reliably perform anomaly detection.[46][47]
 The characteristics of autoencoders are useful in image processing.
 One example can be found in lossy image compression, where autoencoders outperformed other approaches and proved competitive against JPEG 2000.[48][49]
 Another useful application of autoencoders in image preprocessing is image denoising.[50][51][52]
 Autoencoders found use in more demanding contexts such as medical imaging where they have been used for image denoising[53] as well as super-resolution.[54][55] In image-assisted diagnosis, experiments have applied autoencoders for breast cancer detection[56] and for modelling the relation between the cognitive decline of Alzheimer's disease and the latent features of an autoencoder trained with MRI.[57]
 In 2019 molecules generated with variational autoencoders were validated experimentally in mice.[58][59]
 Recently, a stacked autoencoder framework produced promising results in predicting popularity of social media posts,[60] which is helpful for online advertising strategies.
 Autoencoders have been applied to machine translation, which is usually referred to as neural machine translation (NMT).[61][62] Unlike traditional autoencoders, the output does not match the input - it is in another language. In NMT, texts are treated as sequences to be encoded into the learning procedure, while on the decoder side sequences in the target language(s) are generated. Language-specific autoencoders incorporate further linguistic features into the learning procedure, such as Chinese decomposition features.[63] Machine translation is rarely still done with autoencoders, due to the availability of more effective transformer networks.
 Autoencoders in communication systems are important because they help in encoding data into a more resilient representation for channel impairments, which is crucial for transmitting information while minimizing errors. In Addition, AE-based systems can optimize end-to-end communication performance. This approach can solve the several limitations of designing communication systems such as the inherent difficulty in accurately modeling the complex behavior of real-world channels [64]. 
",autoencod type artifici neural network use learn effici code unlabel data unsupervis learn autoencod learn two function encod function transform input data decod function recreat input data encod represent autoencod learn effici represent encod set data typic dimension reduct gener embed subsequ use machin learn algorithm variant exist aim make learn represent assum use properti exampl regular autoencod spars denois contract autoencod effect learn represent subsequ classif task variat autoencod use gener model autoencod appli mani problem includ facial recognit featur detect anomali detect learn mean word term data synthesi autoencod also use randomli gener new data similar input train data autoencod defin follow compon two set space decod messag x x space encod messag z z typic x x z z euclidean space x r z r n x r z r n n n two parametr famili function encod famili e ϕ x z x z parametr ϕ decod famili θ z x z x parametr θ x x x usual write z e ϕ x x refer code latent variabl latent represent latent vector etc convers z z z usual write x θ z z refer decod messag usual encod decod defin multilay perceptron mlp exampl encod e ϕ σ activ function w w weight matrix b b bia vector autoencod simpli tupl two function judg qualiti need task task defin refer probabl distribut μ r e f ref x x reconstruct qualiti function x x x x x x x x measur much x x differ x x defin loss function autoencod l θ ϕ e x μ r e f x θ e ϕ x l e ref x x optim autoencod given task μ r e f ref arg min θ ϕ l θ ϕ l search optim autoencod accomplish mathemat optim techniqu usual gradient descent search process refer train autoencod situat refer distribut empir distribut given dataset x x n x n x μ r e f n n δ x ref n n δ x dirac measur qualiti function loss x x x x x x euclidean norm problem search optim autoencod optim min θ ϕ l θ ϕ l θ ϕ n n x θ e ϕ x l l n n autoencod two main part encod map messag code decod reconstruct messag code optim autoencod would perform close perfect reconstruct possibl close perfect defin reconstruct qualiti function simplest way perform copi task perfectli would duplic signal suppress behavior code space z z usual fewer dimens messag space x x autoencod call undercomplet interpret compress messag reduc dimension limit ideal undercomplet autoencod everi possibl code z z code space use encod messag x x realli appear distribut μ r e f ref decod also perfect θ e ϕ x x x ideal autoencod use gener messag indistinguish real messag feed decod arbitrari code z z obtain θ z z messag realli appear distribut μ r e f ref code space z z dimens larger overcomplet equal messag space x x hidden unit given enough capac autoencod learn ident function becom useless howev experiment result found overcomplet autoencod might still learn use featur ideal set code dimens model capac could set basi complex data distribut model standard way add modif basic autoencod detail variat autoencod vae belong famili variat bayesian method despit architectur similar basic autoencod vae architect differ goal differ mathemat formul latent space case compos mixtur distribut instead fix vector given input dataset x x character unknown probabl function p x p x multivari latent encod vector z z object model data distribut p θ x x θ defin set network paramet p θ x z p θ x z z x z x z dz inspir spars code hypothesi neurosci spars autoencod sae variant autoencod code e ϕ x x messag tend spars code e ϕ x x close zero entri spars autoencod may includ rather fewer hidden unit input small number hidden unit allow activ time encourag sparsiti improv perform classif task two main way enforc sparsiti one way simpli clamp activ latent code zero autoencod autoencod insert follow function latent layer standard autoencod f k x x n x b x n b n k n n n b x rank top k otherwis backpropag f k k simpl set gradient b entri keep gradient b entri essenti gener relu function way relax version autoencod instead forc sparsiti add sparsiti regular loss optim min θ ϕ l θ ϕ λ l spars θ ϕ l spars λ measur much sparsiti want enforc let autoencod architectur k k layer defin sparsiti regular loss need desir sparsiti ρ k k layer weight w k k much enforc sparsiti function measur much two sparsiti differ input x x let actual sparsiti activ layer k k ρ k x n n k x k x n n k x k x k x activ neuron k k layer upon input x x sparsiti loss upon input x x one layer ρ k ρ k x k k x sparsiti regular loss entir autoencod expect weight sum sparsiti loss l spars θ ϕ e x μ x k k w k ρ k ρ k x spars e x k k k k x typic function either kl diverg loss ρ ρ ρ ρ loss ρ ρ ρ ρ altern sparsiti regular loss may defin without refer desir sparsiti simpli forc much sparsiti possibl case one defin sparsiti regular loss l spars θ ϕ e x μ x k k w k h k spars e x k k k h k k activ vector k k layer autoencod norm usual norm give spars autoencod norm give spars autoencod denois autoencod dae tri achiev good represent chang reconstruct criterion dae origin call robust autoassoci network mark kramer train intent corrupt input standard autoencod train nois process defin probabl distribut μ function x x x x function take messag x x x corrupt noisi version x x function select randomli probabl distribut μ given task μ ref ref problem train dae optim problem min θ ϕ l θ ϕ e x μ x μ x θ e ϕ x l e x x x optim dae take noisi messag attempt recov origin messag without nois thu name denois usual nois process appli train test downstream use use dae depend two assumpt exampl nois process includ contract autoencod cae add contract regular loss standard autoencod loss min θ ϕ l θ ϕ λ l cont θ ϕ l cont λ measur much want enforc contract regular loss defin expect frobeniu norm jacobian matrix encod activ respect input l cont θ ϕ e x μ r e f x e ϕ x f cont e ref x x f understand l cont cont measur note fact e ϕ x δ x e ϕ x x e ϕ x f δ x x x x x f messag x x x small variat δ x x thu x e ϕ x f x x f small mean small neighborhood messag map small neighborhood code desir properti mean small variat messag lead small perhap even zero variat code like two pictur may look even exactli dae understood infinitesim limit cae limit small gaussian input nois dae make reconstruct function resist small input perturb cae make extract featur resist infinitesim input perturb minimum descript length autoencod advanc variat tradit autoencod leverag principl inform theori specif minimum descript length mdl principl mdl principl posit best model dataset one provid shortest combin encod model data context autoencod principl appli ensur learn represent compact also interpret effici reconstruct seek minim total descript length data includ size latent represent code length error reconstruct origin data object express l code l error code error l code code repres length compress latent represent l error error denot reconstruct error concret autoencod design discret featur select concret autoencod forc latent space consist number featur concret autoencod use continu relax categor distribut allow gradient pass featur selector layer make possibl use standard backpropag learn optim subset input featur minim reconstruct loss autoencod often train encod decod use deep encod decod offer mani advantag geoffrey hinton develop deep belief network techniqu train deep autoencod method involv treat neighbor set two layer restrict boltzmann machin pretrain approxim good solut use backpropag result research debat whether joint train train whole architectur togeth singl global reconstruct object optim would better deep studi show joint train learn better data model along repres featur classif compar layerwis method howev experi show success joint train depend heavili regular strategi adopt oja note pca equival neural network one hidden layer ident activ function languag autoencod modul encod modul decod subsequ baldi hornik kramer gener pca autoencod term nonlinear pca immedi resurg neural network suggest neural network put mode implement harrison elman zipser speech cottrel munro zipser imag hinton salakhutdinov deep belief network develop train pair restrict boltzmann machin pair train anoth pair latent represent first pair first applic ae date earli tradit applic dimension reduct featur learn concept becam wide use learn gener model data power ai involv autoencod modul compon larger ai system vae stabl diffus discret vae imag gener like etc earli day terminolog uncertain autoencod also call ident map backpropag diabolo network two main applic autoencod dimension reduct inform retriev associ memori modern variat appli task dimension reduct one first deep learn applic hinton studi pretrain autoencod stack rbm use weight initi deep autoencod gradual smaller hidden layer hit bottleneck neuron result dimens code yield smaller reconstruct error compar first compon princip compon analysi pca learn represent qualit easier interpret clearli separ data cluster repres dimens improv perform task classif inde hallmark dimension reduct place semant relat exampl near linear activ use singl sigmoid hidden layer optim solut autoencod strongli relat princip compon analysi pca weight autoencod singl hidden layer size p p p p less size input span vector subspac one span first p p princip compon output autoencod orthogon project onto subspac autoencod weight equal princip compon gener orthogon yet princip compon may recov use singular valu decomposit howev potenti autoencod resid allow model learn power gener compar pca reconstruct input significantli lower inform loss inform retriev benefit particularli dimension reduct search becom effici certain kind low dimension space autoencod inde appli semant hash propos salakhutdinov hinton train algorithm produc binari code databas entri could store hash tabl map binari code vector entri tabl would support inform retriev return entri binari code queri slightli less similar entri flip bit queri encod architectur often use natur languag process neural network scientif appli field seo search engin optim variou way essenc architectur autoencod leverag seo optim web page content improv index enhanc appeal search engin user anoth applic autoencod anomali detect learn replic salient featur train data constraint describ previous model encourag learn precis reproduc frequent observ characterist face anomali model worsen reconstruct perform case data normal instanc use train autoencod other frequenc anomali small compar observ set contribut learn represent could ignor train autoencod accur reconstruct normal data fail unfamiliar anomal data reconstruct error error origin data low dimension reconstruct use anomali score detect anomali recent literatur howev shown certain autoencod model counterintuit good reconstruct anomal exampl consequ abl reliabl perform anomali detect characterist autoencod use imag process one exampl found lossi imag compress autoencod outperform approach prove competit jpeg anoth use applic autoencod imag preprocess imag denois autoencod found use demand context medic imag use imag denois well diagnosi experi appli autoencod breast cancer detect model relat cognit declin alzheim diseas latent featur autoencod train mri molecul gener variat autoencod valid experiment mice recent stack autoencod framework produc promis result predict popular social media post help onlin advertis strategi autoencod appli machin translat usual refer neural machin translat nmt unlik tradit autoencod output match input anoth languag nmt text treat sequenc encod learn procedur decod side sequenc target languag gener autoencod incorpor linguist featur learn procedur chines decomposit featur machin translat rare still done autoencod due avail effect transform network autoencod commun system import help encod data resili represent channel impair crucial transmit inform minim error addit system optim commun perform approach solv sever limit design commun system inher difficulti accur model complex behavior channel
Deep learning,https://en.wikipedia.org/wiki/Deep_learning,"Deep learning is a subset of machine learning that focuses on utilizing neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and ""training"" them to process data. The adjective ""deep"" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be either supervised, semi-supervised or unsupervised.[2]
 Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]
 Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.[6]
 Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.[7]
 Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a slightly more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.
 Importantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.[8][2]
 The word ""deep"" in ""deep learning"" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.[9] No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function.[10] Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.
 Deep learning architectures can be constructed with a greedy layer-by-layer method.[11] Deep learning helps to disentangle these abstractions and pick out which features improve performance.[8]
 Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.[8][12]
 The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986,[13] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[14][15] Although the history of its appearance is apparently more complicated.[16]
 Deep neural networks are generally interpreted in terms of the universal approximation theorem[17][18][19][20][21] or probabilistic inference.[22][23][8][9][24]
 The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.[17][18][19][20] In 1989, the first proof was published by George Cybenko for sigmoid activation functions[17] and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.[18] Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit.[25][26]
 The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.[21] proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.
 The probabilistic interpretation[24] derives from the field of machine learning. It features inference,[23][7][8][9][12][24] as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.[24] The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.[27]
 There are two types of artificial neural network (ANN): feedforward neural network (FNN) or multilayer perceptron (MLP) and recurrent neural networks (RNN). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model[28][29] which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive.[30][31] His learning RNN was republished by John Hopfield in 1982.[32] Other early recurrent neural networks were published by Kaoru Nakano in 1971.[33][34] Already in 1948, Alan Turing produced work on ""Intelligent Machinery""  that was not published in his lifetime,[35] containing ""ideas related to artificial evolution and learning RNNs"".[31]
 Frank Rosenblatt (1958)[36] proposed the perceptron, an MLP with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons ""with adaptive preterminal networks"" where the last two layers have learned weights (here he credits H. D. Block and B. W. Knight).[37]: section 16  The book cites an earlier network by R. D. Joseph (1960)[38] ""functionally equivalent to a variation of"" this four-layer system (the book mentions Joseph over 30 times). Should Joseph therefore be considered the originator of proper adaptive multilayer perceptrons with learning hidden units? Unfortunately, the learning algorithm was not a functional one, and fell into oblivion.
 The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in 1965. They regarded it as a form of polynomial regression,[39] or a generalization of Rosenblatt's perceptron.[40] A 1971 paper described a deep network with eight layers trained by this method,[41] which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or ""gates"".[31]
 The first deep learning multilayer perceptron trained by stochastic gradient descent[42] was published in 1967 by Shun'ichi Amari.[43] In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes.[31] Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.
 In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function.[25][31] The rectifier has become the most popular activation function for deep learning.[44]
 Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.[45][46]
 Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673[47] to networks of differentiable nodes. The terminology ""back-propagating errors"" was actually introduced in 1962 by Rosenblatt,[37] but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory.[48] The modern form of backpropagation was first published in Seppo Linnainmaa's master thesis (1970).[49][50][31] G.M. Ostrovski et al. republished it in 1971.[51][52] Paul Werbos applied backpropagation to neural networks in 1982[53] (his 1974 PhD thesis, reprinted in a 1994 book,[54] did not yet describe the algorithm[52]). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.[55][56]
 The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.[57][58]  In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.[59] 
In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days.[60] In 1990, Wei Zhang implemented a CNN on optical computing hardware.[61] In 1991, a CNN was applied to medical image object segmentation[62] and breast cancer detection in mammograms.[63] LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.[64]
 Recurrent neural networks (RNN)[28][30] were further developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were the Jordan network (1986)[65] and the Elman network (1990),[66] which applied RNN to study problems in cognitive psychology.
 In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, in 1991, Jürgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below.[67][68] This ""neural history compressor"" uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by  distilling a higher level chunker network into a lower level automatizer network.[67][68][31] In 1993, a neural history compressor solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time.[69] The ""P"" in ChatGPT refers to such pre-training.
 Sepp Hochreiter's diploma thesis (1991)[70] implemented the neural history compressor,[67] and identified and analyzed the vanishing gradient problem.[70][71]  Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem. This led to the long short-term memory (LSTM), published in 1995.[72] LSTM can learn ""very deep learning"" tasks[9] with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a ""forget gate"", introduced in 1999,[73] which became the standard RNN architecture.
 In 1991, Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss.[74][75] The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity"". In 2014, this principle was used in generative adversarial networks (GANs).[76]
 During 1985–1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine,[77] restricted Boltzmann machine,[78] Helmholtz machine,[79] and the wake-sleep algorithm.[80] These were designed for unsupervised learning of deep generative models. However, those were more computationally expensive compared to backpropagation. Boltzmann machine learning algorithm, published in 1985, was briefly popular before being eclipsed by the backpropagation algorithm in 1986. (p. 112 [81]). A 1988 network became state of the art in protein structure prediction, an early application of deep learning to bioinformatics.[82]
 Both shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years.[83][84][85] These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[86] Key difficulties have been analyzed, including gradient diminishing[70] and weak temporal correlation structure in neural predictive models.[87][88] Additional difficulties were the lack of training data and limited computing power.
 Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI researched in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 NIST Speaker Recognition benchmark.[89][90] It was deployed in the Nuance Verifier, representing the first major industrial application of deep learning.[91]
 The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s,[90] showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.[92]
 Neural networks entered a null, and simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) became the preferred choices in the 1990s and 2000s, because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks.[citation needed]
 In 2003, LSTM became competitive with traditional speech recognizers on certain tasks.[93] In 2006, Alex Graves, Santiago Fernández, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification (CTC)[94] in stacks of LSTMs.[95] In 2009, it became the first RNN to win a pattern recognition contest, in connected handwriting recognition.[96][9]
 In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh[97][98] deep belief networks were developed for generative modeling. They are trained by training one restricted Boltzmann machine, then freezing it and training another one on top of the first one, and so on, then optionally fine-tuned using supervised backpropagation.[99] They could model high-dimensional probability distributions, such as the distribution of MNIST images, but convergence was slow.[100][101][102]
 The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun.[103] Industrial applications of deep learning to large-scale speech recognition started around 2010.
 The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems.[104] The nature of the recognition errors produced by the two types of systems was characteristically different,[105] offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems.[23][106][107] Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition.[105]  That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.[104][105][108]
In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.[109][110][111][106]
 The deep learning revolution started around CNN- and GPU-based computer vision.
 Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years,[112] including CNNs,[113] faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning.[114]
 A key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004.[112][113] In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training.[115]
 In 2011, a CNN named DanNet[116][117] by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3.[9] It then won more contests.[118][119] They also showed how max-pooling CNNs on GPU improved performance significantly.[3]
 In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.[120]
 In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton[4] won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman[121] and Google's Inceptionv3.[122]
 The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.[123][124][125]
 In 2014, the state of the art was training “very deep neural network” with 20 to 30 layers.[126] Stacking too many layers led to a steep reduction in training accuracy,[127] known as the ""degradation"" problem.[128] In 2015, two techniques were developed to train very deep networks: the Highway Network was published in May 2015, and the residual neural network (ResNet)[129] in Dec 2015. ResNet behaves like an open-gated Highway Net.
 Around the same time, deep learning started impacting the field of art. Early examples included Google DeepDream (2015), and neural style transfer (2015),[130] both of which were based on pretrained image classification neural networks, such as VGG-19.
 Generative adversarial network (GAN) by (Ian Goodfellow et al., 2014)[131] (based on  Jürgen Schmidhuber's principle of artificial curiosity[74][76])
became state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved by Nvidia's StyleGAN (2018)[132] based on the Progressive GAN by Tero Karras et al.[133] Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.[134] Diffusion models (2015)[135] eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022).
 In 2015, Google's speech recognition improved by 49% by an LSTM-based model, which they made available through Google Voice Search on smartphone.[136][137]
 Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved.[104][138] Convolutional neural networks were superseded for ASR by LSTM.[137][139][140][141] but are more successful in computer vision.
 Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for ""conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing"".[142]
 Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.
 An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.
 Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.
 The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.
 Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
 As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing ""Go""[144]).
 A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers.[7][9] There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions.[145] These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm.[citation needed]
 For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, [146] and complex DNN have many layers, hence the name ""deep"" networks. 
 DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.[147] The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.[7] For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.[148]
 Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.[146]
 DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or ""weights"", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights.[149] That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.
 Recurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling.[150][151][152][153][154] Long short-term memory is particularly effective for this use.[155][156]
 Convolutional neural networks (CNNs) are used in computer vision.[157] CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).[158]
 As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.
 DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning[41] or weight decay (




ℓ

2




{\displaystyle \ell _{2}}

-regularization) or sparsity (




ℓ

1




{\displaystyle \ell _{1}}

-regularization) can be applied during training to combat overfitting.[159] Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies.[160] Another interesting recent development is research into models of just enough complexity through an estimation of the intrinsic complexity of the task being modelled. This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction.[161] Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.[162]
 DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples)[163] speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.[164][165]
 Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.[166][167]
 Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.[168] By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI .[169] OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.[170][171]
 Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones[172] and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform.[173] Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2).[174][175]
 Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage.
In 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).[176]
 In 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing.[177] The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds.[177] Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.[177]
 Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn ""Very Deep Learning"" tasks[9] that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates[156] is competitive with traditional speech recognizers on certain tasks.[93]
 The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences.[178] Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.
 The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:[23][108][106]
 All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.[23][183][184]
 A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.[185]
 Deep learning-based image recognition has become ""superhuman"", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.[186][187]
 Deep learning-trained vehicles now interpret 360° camera views.[188] Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.
 Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of
 Neural networks have been used for implementing language models since the early 2000s.[150] LSTM helped to improve machine translation and language modeling.[151][152][153]
 Other key techniques in this field are negative sampling[191] and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN.[192] Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing.[192] Deep neural architectures provide the best results for constituency parsing,[193] sentiment analysis,[194] information retrieval,[195][196] spoken language understanding,[197] machine translation,[151][198] contextual entity linking,[198] writing style recognition,[199] named-entity recognition (token classification),[200] text classification, and others.[201]
 Recent developments generalize word embedding to sentence embedding.
 Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network.[202][203][204][205] Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system ""learns from millions of examples"".[203] It translates ""whole sentences at a time, rather than pieces"". Google Translate supports over one hundred languages.[203] The network encodes the ""semantics of the sentence rather than simply memorizing phrase-to-phrase translations"".[203][206] GT uses English as an intermediate between most language pairs.[206]
 A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects.[207][208] Research has explored use of deep learning to predict the biomolecular targets,[209][210] off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.[211][212][213]
 AtomNet is a deep learning system for structure-based rational drug design.[214] AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus[215] and multiple sclerosis.[216][215]
 In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set.[217] In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.[218][219]
 Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.[220]
 Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations.[221][222] Multi-view deep learning has been applied for learning user preferences from multiple domains.[223] The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.
 An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.[224]
 In medical informatics, deep learning was used to predict sleep quality based on data from wearables[225] and predictions of health complications from electronic health record data.[226]
 Deep neural networks have shown unparalleled performance in predicting protein structure, according to the sequence of the amino acids that make it up. In 2020, AlphaFold, a deep-learning based system, achieved a level of accuracy significantly higher than all previous computational methods.[227][228]
 Deep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE).[229] Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically, the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y, given input X. For example, in image classification tasks, the NJEE maps a vector of pixels' color values to probabilities over possible image classes. In practice, the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions, such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in case of large alphabet sizes.[229]
 Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement.[230][231] Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.[232][233]
 Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server.[234] Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.
 Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization.[235] These applications include learning methods such as ""Shrinkage Fields for Effective Image Restoration""[236] which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.
 Deep learning is being successfully applied to financial fraud detection, tax evasion detection,[237] and anti-money laundering.[238]
 In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.[239][240][241]
 The United States Department of Defense applied deep learning to train robots in new tasks through observation.[242]
 Physics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner.[243] One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods rely on.[244][245]
 Deep backward stochastic differential equation method is a numerical method that combines deep learning with Backward stochastic differential equation (BSDE). This method is particularly useful for solving high-dimensional problems in financial mathematics. By leveraging the powerful function approximation capabilities of deep neural networks, deep BSDE addresses the computational challenges faced by traditional numerical methods in high-dimensional settings. Specifically, traditional methods like finite difference methods or Monte Carlo simulations often struggle with the curse of dimensionality, where computational cost increases exponentially with the number of dimensions. Deep BSDE methods, however, employ deep neural networks to approximate solutions of high-dimensional partial differential equations (PDEs), effectively reducing the computational burden.[246]
 In addition, the integration of Physics-informed neural networks (PINNs) into the deep BSDE framework enhances its capability by embedding the underlying physical laws directly into the neural network architecture. This ensures that the solutions not only fit the data but also adhere to the governing stochastic differential equations. PINNs leverage the power of deep learning while respecting the constraints imposed by the physical models, resulting in more accurate and reliable solutions for financial mathematics problems.
 Image reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging [247] and ultrasound imaging.[248]
 Traditional weather prediction systems solve a very complex system of partial differential equations. GraphCast is a deep learning based model, trained on a long history of weather data to predict how weather patterns change over time. It is able to  predict weather conditions for up to 10 days globally, at a very detailed level, and in under a minute, with precision similar to state of the art systems.[249][250]
 An epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples.[251] The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity. The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity.
 Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s.[252][253][254][255] These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, ""...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature"".[256]
 A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism.[257][258] Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality.[259][260] In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.[261]
 Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons[262] and neural populations.[263] Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system[264] both at the single-unit[265] and at the population[266] levels.
 Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.[267]
 Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player.[268][269][270] Google Translate uses a neural network to translate between more than 100 languages.
 In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.[271]
 As of 2008,[272] researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor.[242] First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation.[242] Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as ""good job"" and ""bad job"".[273]
 Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science.
 A main criticism concerns the lack of theory surrounding some methods.[274] Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear.[citation needed] (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.[275]
 Others point out that deep learning should be looked at as a step towards realizing strong AI[disambiguation needed], not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed to realize this goal entirely. Research psychologist Gary Marcus noted:
 Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning.[276]
 In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained[277] demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's[278] website.
 Some deep learning architectures display problematic behaviors,[279] such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014)[280] and misclassifying minuscule perturbations of correctly classified images (2013).[281] Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures.[279] These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar[282] decompositions of observed entities and events.[279] Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition[283] and artificial intelligence (AI).[284]
 As deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception.[285] By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an ""adversarial attack"".[286]
 In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points, and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system.[287] One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.[288]
 Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.[287]
 ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.[287]
 In 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could ""serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware)"".[287]
 In ""data poisoning"", false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.[287]
 The deep learning systems that are trained using supervised learning often rely on data that is created and/or annotated by humans.[289] It has been argued that not only low-paid clickwork (such as on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such.[290] The philosopher Rainer Mühlhoff distinguishes five types of ""machinic capture"" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) ""trapping and tracking"" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.[290]
",deep learn subset machin learn focus util neural network perform task classif regress represent learn field take inspir biolog neurosci center around stack artifici neuron layer train process data adject deep refer use multipl layer rang three sever hundr thousand network method use either supervis unsupervis common deep learn network architectur includ fulli connect network deep belief network recurr neural network convolut neural network gener adversari network transform neural radianc field architectur appli field includ comput vision speech recognit natur languag process machin translat bioinformat drug design medic imag analysi climat scienc materi inspect board game program produc result compar case surpass human expert perform earli form neural network inspir inform process distribut commun node biolog system particularli human brain howev current neural network intend model brain function organ gener seen model purpos modern deep learn model base neural network convolut neural network transform although also includ proposit formula latent variabl organ deep gener model node deep belief network deep boltzmann machin fundament deep learn refer class machin learn algorithm hierarchi layer use transform input data slightli abstract composit represent exampl imag recognit model raw input may imag repres tensor pixel first represent layer may attempt identifi basic shape line circl second layer may compos encod arrang edg third layer may encod nose eye fourth layer may recogn imag contain face importantli deep learn process learn featur optim place level prior deep learn machin learn techniqu often involv featur engin transform data suitabl represent classif algorithm oper deep learn approach featur model discov use featur represent data automat elimin need exampl vari number layer layer size provid differ degre abstract word deep deep learn refer number layer data transform precis deep learn system substanti credit assign path cap depth cap chain transform input output cap describ potenti causal connect input output feedforward neural network depth cap network number hidden layer plu one output layer also parameter recurr neural network signal may propag layer cap depth potenti unlimit univers threshold depth divid shallow learn deep learn research agre deep learn involv cap depth higher two cap depth two shown univers approxim sens emul function beyond layer add function approxim abil network deep model cap two abl extract better featur shallow model henc extra layer help learn featur effect deep learn architectur construct greedi method deep learn help disentangl abstract pick featur improv perform deep learn algorithm appli unsupervis learn task import benefit unlabel data abund label data exampl deep structur train unsupervis manner deep belief network term deep learn introduc machin learn commun rina dechter artifici neural network igor aizenberg colleagu context boolean threshold neuron although histori appear appar complic deep neural network gener interpret term univers approxim theorem probabilist infer classic univers approxim theorem concern capac feedforward neural network singl hidden layer finit size approxim continu function first proof publish georg cybenko sigmoid activ function generalis architectur kurt hornik recent work also show univers approxim also hold activ function kunihiko fukushima rectifi linear unit univers approxim theorem deep neural network concern capac network bound width depth allow grow lu et al prove width deep neural network relu activ strictli larger input dimens network approxim lebesgu integr function width smaller equal input dimens deep neural network univers approxim probabilist interpret deriv field machin learn featur infer well optim concept train test relat fit gener respect specif probabilist interpret consid activ nonlinear cumul distribut function probabilist interpret led introduct dropout regular neural network probabilist interpret introduc research includ hopfield widrow narendra popular survey one bishop two type artifici neural network ann feedforward neural network fnn multilay perceptron mlp recurr neural network rnn rnn cycl connect structur fnn wilhelm lenz ernst ise creat ise model essenti rnn architectur consist threshold element amari made architectur adapt learn rnn republish john hopfield earli recurr neural network publish kaoru nakano alreadi alan ture produc work intellig machineri publish lifetim contain idea relat artifici evolut learn rnn frank rosenblatt propos perceptron mlp layer input layer hidden layer random weight learn output layer later publish book also introduc variant comput experi includ version perceptron adapt pretermin network last two layer learn weight credit block w knight section book cite earlier network joseph function equival variat system book mention joseph time joseph therefor consid origin proper adapt multilay perceptron learn hidden unit unfortun learn algorithm function one fell oblivion first work deep learn algorithm group method data handl method train arbitrarili deep neural network publish alexey ivakhnenko lapa regard form polynomi regress gener rosenblatt perceptron paper describ deep network eight layer train method base layer layer train regress analysi superflu hidden unit prune use separ valid set sinc activ function node polynomi also first deep network multipl unit gate first deep learn multilay perceptron train stochast gradient descent publish amari comput experi conduct amari student saito five layer mlp two modifi layer learn intern represent classifi separ pattern class subsequ develop hardwar hyperparamet tune made stochast gradient descent current domin train techniqu kunihiko fukushima introduc relu rectifi linear unit activ function rectifi becom popular activ function deep learn deep learn architectur convolut neural network cnn convolut layer downsampl layer began neocognitron introduc kunihiko fukushima though train backpropag backpropag effici applic chain rule deriv gottfri wilhelm leibniz network differenti node terminolog error actual introduc rosenblatt know implement although henri kelley continu precursor backpropag context control theori modern form backpropag first publish seppo linnainmaa master thesi ostrovski et al republish paul werbo appli backpropag neural network phd thesi reprint book yet describ algorithm david rumelhart et al popularis backpropag cite origin work time delay neural network tdnn introduc alex waibel appli cnn phonem recognit use convolut weight share backpropag wei zhang appli cnn alphabet recognit yann lecun et al creat cnn call lenet recogn handwritten zip code mail train requir day wei zhang implement cnn optic comput hardwar cnn appli medic imag object segment breast cancer detect mammogram cnn yann lecun et classifi digit appli sever bank recogn number check digit pixel imag recurr neural network rnn develop recurr use sequenc process recurr network unrol mathemat resembl deep feedforward layer consequ similar properti issu develop mutual influenc rnn two earli influenti work jordan network elman network appli rnn studi problem cognit psycholog backpropag work well deep learn long credit assign path overcom problem jürgen schmidhub propos hierarchi rnn one level time learn rnn tri predict next input next unexpect input rnn neural histori compressor use predict code learn intern represent multipl time scale substanti facilit downstream deep learn rnn hierarchi collaps singl rnn distil higher level chunker network lower level automat network neural histori compressor solv deep learn task requir subsequ layer rnn unfold time p chatgpt refer sepp hochreit diploma thesi implement neural histori compressor identifi analyz vanish gradient problem hochreit propos recurr residu connect solv vanish gradient problem led long memori lstm publish lstm learn deep learn task long credit assign path requir memori event happen thousand discret time step lstm yet modern architectur requir forget gate introduc becam standard rnn architectur jürgen schmidhub also publish adversari neural network contest form game one network gain network loss first network gener model model probabl distribut output pattern second network learn gradient descent predict reaction environ pattern call artifici curios principl use gener adversari network gan inspir statist mechan sever architectur method develop terri sejnowski peter dayan geoffrey hinton includ boltzmann machin restrict boltzmann machin helmholtz machin algorithm design unsupervis learn deep gener model howev comput expens compar backpropag boltzmann machin learn algorithm publish briefli popular eclips backpropag algorithm network becam state art protein structur predict earli applic deep learn bioinformat shallow deep learn recurr net ann speech recognit explor mani year method never outperform gaussian mixtur markov model technolog base gener model speech train discrimin key difficulti analyz includ gradient diminish weak tempor correl structur neural predict model addit difficulti lack train data limit comput power speech recognit research move away neural net pursu gener model except sri intern late fund us govern nsa darpa sri research speech speaker recognit speaker recognit team led larri heck report signific success deep neural network speech process nist speaker recognit benchmark deploy nuanc verifi repres first major industri applic deep learn principl elev raw featur optim first explor success architectur deep autoencod raw spectrogram linear featur late show superior featur contain stage fix transform spectrogram raw featur speech waveform later produc excel result neural network enter null simpler model use handcraft featur gabor filter support vector machin svm becam prefer choic artifici neural network comput cost lack understand brain wire biolog network citat need lstm becam competit tradit speech recogn certain task alex grave santiago fernández faustino gomez schmidhub combin connectionist tempor classif ctc stack lstm becam first rnn win pattern recognit contest connect handwrit recognit public geoff hinton ruslan salakhutdinov osindero teh deep belief network develop gener model train train one restrict boltzmann machin freez train anoth one top first one option use supervis backpropag could model probabl distribut distribut mnist imag converg slow impact deep learn industri began earli cnn alreadi process estim check written us accord yann lecun industri applic deep learn speech recognit start around nip workshop deep learn speech recognit motiv limit deep gener model speech possibl given capabl hardwar data set deep neural net might becom practic believ dnn use gener model deep belief net dbn would overcom main difficulti neural net howev discov replac larg amount train data straightforward backpropag use dnn larg output layer produc error rate dramat lower gaussian mixtur model gmm markov model hmm also gener system natur recognit error produc two type system characterist differ offer technic insight integr deep learn exist highli effici speech decod system deploy major speech recognit system analysi around contrast gmm gener speech model dnn model stimul earli industri invest deep learn speech recognit analysi done compar perform less error rate discrimin dnn gener model research extend deep learn timit larg vocabulari speech recognit adopt larg output layer dnn base hmm state construct decis tree deep learn revolut start around comput vision although cnn train backpropag around decad gpu implement nn year includ cnn faster implement cnn gpu need progress comput vision later deep learn becom widespread special hardwar algorithm optim develop specif deep learn key advanc deep learn revolut hardwar advanc especi gpu earli work date back raina madhavan andrew ng report deep belief network train nvidia geforc gtx gpu earli demonstr deep learn report time faster train cnn name dannet dan ciresan ueli meier jonathan masci luca maria gambardella jürgen schmidhub achiev first time superhuman perform visual pattern recognit contest outperform tradit method factor contest also show cnn gpu improv perform significantli andrew ng jeff dean creat fnn learn recogn concept cat watch unlabel imag taken youtub video octob alexnet alex krizhevski ilya sutskev geoffrey hinton imagenet competit signific margin shallow machin learn method increment improv includ network karen simonyan andrew zisserman googl success imag classif extend challeng task gener descript caption imag often combin cnn lstm state art train deep neural network layer stack mani layer led steep reduct train accuraci known degrad problem two techniqu develop train deep network highway network publish may residu neural network resnet dec resnet behav like highway net around time deep learn start impact field art earli exampl includ googl deepdream neural style transfer base pretrain imag classif neural network gener adversari network gan ian goodfellow et base jürgen schmidhub principl artifici curios becam state art gener model period excel imag qualiti achiev nvidia stylegan base progress gan tero karra et al gan gener grown small larg scale pyramid fashion imag gener gan reach popular success provok discuss concern deepfak diffus model eclips gan gener model sinc system stabl diffus googl speech recognit improv model made avail googl voic search smartphon deep learn part system variou disciplin particularli comput vision automat speech recognit asr result commonli use evalu set timit asr mnist imag classif well rang speech recognit task steadili improv convolut neural network supersed asr lstm success comput vision yoshua bengio geoffrey hinton yann lecun award ture award conceptu engin breakthrough made deep neural network critic compon comput artifici neural network ann connectionist system comput system inspir biolog neural network constitut anim brain system learn progress improv abil task consid exampl gener without program exampl imag recognit might learn identifi imag contain cat analyz exampl imag manual label cat cat use analyt result identifi cat imag found use applic difficult express tradit comput algorithm use program ann base collect connect unit call artifici neuron analog biolog neuron biolog brain connect synaps neuron transmit signal anoth neuron receiv postsynapt neuron process signal signal downstream neuron connect neuron may state gener repres real number typic neuron synaps may also weight vari learn proce increas decreas strength signal send downstream typic neuron organ layer differ layer may perform differ kind transform input signal travel first input last output layer possibl travers layer multipl time origin goal neural network approach solv problem way human brain would time attent focus match specif mental abil lead deviat biolog backpropag pass inform revers direct adjust network reflect inform neural network use varieti task includ comput vision speech recognit machin translat social network filter play board video game medic diagnosi neural network typic thousand million unit million connect despit number sever order magnitud less number neuron human brain network perform mani task level beyond human recogn face play go deep neural network dnn artifici neural network multipl layer input output layer differ type neural network alway consist compon neuron synaps weight bias function compon whole function way mimic function human brain train like ml algorithm citat need exampl dnn train recogn dog breed go given imag calcul probabl dog imag certain breed user review result select probabl network display certain threshold etc return propos label mathemat manipul consid layer complex dnn mani layer henc name deep network dnn model complex relationship dnn architectur gener composit model object express layer composit primit extra layer enabl composit featur lower layer potenti model complex data fewer unit similarli perform shallow network instanc prove spars multivari polynomi exponenti easier approxim dnn shallow network deep architectur includ mani variant basic approach architectur found success specif domain alway possibl compar perform multipl architectur unless evalu data set dnn typic feedforward network data flow input layer output layer without loop back first dnn creat map virtual neuron assign random numer valu weight connect weight input multipli return output network accur recogn particular pattern algorithm would adjust weight way algorithm make certain paramet influenti determin correct mathemat manipul fulli process data recurr neural network data flow direct use applic languag model long memori particularli effect use convolut neural network cnn use comput vision cnn also appli acoust model automat speech recognit asr ann mani issu aris naiv train dnn two common issu overfit comput time dnn prone overfit ad layer abstract allow model rare depend train data regular method ivakhnenko unit prune weight decay ℓ sparsiti ℓ appli train combat overfit altern dropout regular randomli omit unit hidden layer train help exclud rare depend anoth interest recent develop research model enough complex estim intrins complex task model approach success appli multivari time seri predict task traffic predict final data augment via method crop rotat smaller train set increas size reduc chanc overfit dnn must consid mani train paramet size number layer number unit per layer learn rate initi weight sweep paramet space optim paramet may feasibl due cost time comput resourc variou trick batch comput gradient sever train exampl rather individu exampl speed comput larg process capabl architectur gpu intel xeon phi produc signific speedup train suitabl process architectur matrix vector comput altern engin may look type neural network straightforward converg train algorithm cmac cerebellar model articul control one kind neural network requir learn rate random initi weight train process guarante converg one step new batch data comput complex train algorithm linear respect number neuron involv sinc advanc machin learn algorithm comput hardwar led effici method train deep neural network contain mani layer hidden unit larg output layer graphic process unit gpu often enhanc displac cpu domin method train commerci cloud ai openai estim hardwar comput use largest deep learn project alexnet alphazero found increas amount comput requir trendlin month special electron circuit call deep learn processor design speed deep learn algorithm deep learn processor includ neural process unit npu huawei cellphon cloud comput server tensor process unit tpu googl cloud platform cerebra system also built dedic system handl larg deep learn model base largest processor industri wafer scale engin atom thin semiconductor consid promis deep learn hardwar basic devic structur use logic oper data storag marega et al publish experi activ channel materi develop devic circuit base transistor fgfet feldmann et al propos integr photon hardwar acceler parallel convolut process author identifi two key advantag integr photon electron counterpart massiv parallel data transfer wavelength divis multiplex conjunct frequenc comb extrem high data modul speed system execut trillion oper per second indic potenti integr photon ai applic automat speech recognit first convinc success case deep learn lstm rnn learn deep learn task involv interv contain speech event separ thousand discret time step one time step correspond lstm forget gate competit tradit speech recogn certain task initi success speech recognit base recognit task base timit data set contain speaker eight major dialect american english speaker read sentenc small size let mani configur tri importantli timit task concern recognit unlik recognit allow weak phone bigram languag model let strength acoust model aspect speech recognit easili analyz error rate list includ earli result measur percent phone error rate per summar sinc debut dnn speaker recognit late speech recognit around lstm around acceler progress eight major area major commerci speech recognit system microsoft cortana xbox skype translat amazon alexa googl appl siri baidu iflytek voic search rang nuanc speech product etc base deep learn common evalu set imag classif mnist databas data set mnist compos handwritten digit includ train exampl test exampl timit small size let user test multipl configur comprehens list result set avail deep imag recognit becom superhuman produc accur result human contest first occur recognit traffic sign recognit human face deep vehicl interpret camera view anoth exampl facial dysmorpholog novel analysi fdna use analyz case human malform connect larg databas genet syndrom close relat progress made imag recognit increas applic deep learn techniqu variou visual art task dnn proven capabl exampl neural network use implement languag model sinc earli lstm help improv machin translat languag model key techniqu field neg sampl word embed word embed thought represent layer deep learn architectur transform atom word posit represent word rel word dataset posit repres point vector space use word embed rnn input layer allow network pars sentenc phrase use effect composit vector grammar composit vector grammar thought probabilist context free grammar pcfg implement rnn recurs built atop word embed assess sentenc similar detect paraphras deep neural architectur provid best result constitu pars sentiment analysi inform retriev spoken languag understand machin translat contextu entiti link write style recognit recognit token classif text classif other recent develop gener word embed sentenc embed googl translat gt use larg long memori lstm network googl neural machin translat gnmt use machin translat method system learn million exampl translat whole sentenc time rather piec googl translat support one hundr languag network encod semant sentenc rather simpli memor translat gt use english intermedi languag pair larg percentag candid drug fail win regulatori approv failur caus insuffici efficaci effect undesir interact effect unanticip toxic effect research explor use deep learn predict biomolecular target toxic effect environment chemic nutrient household product drug atomnet deep learn system ration drug design atomnet use predict novel candid biomolecul diseas target ebola viru multipl sclerosi graph neural network use first time predict variou properti molecul larg toxicolog data set gener neural network use produc molecul valid experiment way mice deep reinforc learn use approxim valu possibl direct market action defin term rfm variabl estim valu function shown natur interpret custom lifetim valu recommend system use deep learn extract meaning featur latent factor model music journal recommend deep learn appli learn user prefer multipl domain model use hybrid collabor approach enhanc recommend multipl task autoencod ann use bioinformat predict gene ontolog annot relationship medic informat deep learn use predict sleep qualiti base data wearabl predict health complic electron health record data deep neural network shown unparallel perform predict protein structur accord sequenc amino acid make alphafold base system achiev level accuraci significantli higher previou comput method deep neural network use estim entropi stochast process call neural joint entropi estim njee estim provid insight effect input random variabl independ random variabl practic dnn train classifi map input vector matrix x output probabl distribut possibl class random variabl given input exampl imag classif task njee map vector pixel color valu probabl possibl imag class practic probabl distribut obtain softmax layer number node equal alphabet size njee use continu differenti activ function condit univers approxim theorem hold shown method provid strongli consist estim outperform method case larg alphabet size deep learn shown produc competit result medic applic cancer cell classif lesion detect organ segment imag enhanc modern deep learn tool demonstr high accuraci detect variou diseas help use specialist improv diagnosi effici find appropri mobil audienc mobil advertis alway challeng sinc mani data point must consid analyz target segment creat use ad serv ad server deep learn use interpret larg advertis dataset mani data point collect internet advertis cycl inform form basi machin learn improv ad select deep learn success appli invers problem denois inpaint film color applic includ learn method shrinkag field effect imag restor train imag dataset deep imag prior train imag need restor deep learn success appli financi fraud detect tax evas detect launder novemb research googl deepmind lawrenc berkeley nation laboratori announc develop ai system known gnome system contribut materi scienc discov million new materi within rel short timefram gnome employ deep learn techniqu effici explor potenti materi structur achiev signific increas identif stabl inorgan crystal structur system predict valid autonom robot experi demonstr noteworthi success rate data newli discov materi publicli avail materi project databas offer research opportun identifi materi desir properti variou applic develop implic futur scientif discoveri integr ai materi scienc research potenti expedit materi innov reduc cost product develop use ai deep learn suggest possibl minim elimin manual lab experi allow scientist focu design analysi uniqu compound unit state depart defens appli deep learn train robot new task observ physic inform neural network use solv partial differenti equat forward invers problem data driven manner one exampl reconstruct fluid flow govern equat use physic inform neural network requir often expens mesh gener convent cfd method reli deep backward stochast differenti equat method numer method combin deep learn backward stochast differenti equat bsde method particularli use solv problem financi mathemat leverag power function approxim capabl deep neural network deep bsde address comput challeng face tradit numer method set specif tradit method like finit differ method mont carlo simul often struggl curs dimension comput cost increas exponenti number dimens deep bsde method howev employ deep neural network approxim solut partial differenti equat pde effect reduc comput burden addit integr neural network pinn deep bsde framework enhanc capabl embed underli physic law directli neural network architectur ensur solut fit data also adher govern stochast differenti equat pinn leverag power deep learn respect constraint impos physic model result accur reliabl solut financi mathemat problem imag reconstruct reconstruct underli imag measur sever work show better superior perform deep learn method compar analyt method variou applic spectral imag ultrasound imag tradit weather predict system solv complex system partial differenti equat graphcast deep learn base model train long histori weather data predict weather pattern chang time abl predict weather condit day global detail level minut precis similar state art system epigenet clock biochem test use measur age galkin et al use deep neural network train epigenet age clock unpreced accuraci use blood sampl clock use inform cpg site predict peopl certain condit older healthi control ibd frontotempor dementia ovarian cancer obes age clock plan releas public use insilico medicin spinoff compani deep longev deep learn close relat class theori brain develop specif neocort develop propos cognit neuroscientist earli development theori instanti comput model make predecessor deep learn system development model share properti variou propos learn dynam brain wave nerv growth factor support somewhat analog neural network util deep learn model like neocortex neural network employ hierarchi layer filter layer consid inform prior layer oper environ pass output possibl origin input layer process yield stack transduc oper environ descript state infant brain seem organ influenc wave differ region brain becom connect sequenti one layer tissu matur anoth whole brain matur varieti approach use investig plausibl deep learn model neurobiolog perspect one hand sever variant backpropag algorithm propos order increas process realism research argu unsupervis form deep learn base hierarch gener model deep belief network may closer biolog realiti respect gener neural network model relat neurobiolog evid process cerebr cortex although systemat comparison human brain organ neuron encod deep network yet establish sever analog report exampl comput perform deep learn unit could similar actual neuron neural popul similarli represent develop deep learn model similar measur primat visual system popul level facebook ai lab perform task automat tag upload pictur name peopl googl deepmind technolog develop system capabl learn play atari video game use pixel data input demonstr alphago system learn game go well enough beat profession go player googl translat use neural network translat languag launch focus integr deep learn factori research univers texa austin ut develop machin learn framework call train agent manual via evalu reinforc tamer propos new method robot comput program learn perform task interact human instructor first develop tamer new algorithm call deep tamer later introduc collabor armi research laboratori arl ut research deep tamer use deep learn provid robot abil learn new task observ use deep tamer robot learn task human trainer watch video stream observ human perform task robot later practic task help coach trainer provid feedback good job bad job deep learn attract critic comment case outsid field comput scienc main critic concern lack theori surround method learn common deep architectur implement use gradient descent howev theori surround algorithm contrast diverg less clear citat need converg fast approxim deep learn method often look black box confirm done empir rather theoret other point deep learn look step toward realiz strong ai disambigu need solut despit power deep learn method still lack much function need realiz goal entir research psychologist gari marcu note realist deep learn part larger challeng build intellig machin techniqu lack way repres causal relationship obviou way perform logic infer also still long way integr abstract knowledg inform object typic use power system like watson use techniqu like deep learn one element complic ensembl techniqu rang statist techniqu bayesian infer deduct reason refer idea artist sensit might inher rel low level cognit hierarchi publish seri graphic represent intern state deep layer neural network attempt discern within essenti random data imag train demonstr visual appeal origin research notic receiv well comment subject time frequent access articl guardian websit deep learn architectur display problemat behavior confid classifi unrecogniz imag belong familiar categori ordinari imag misclassifi minuscul perturb correctli classifi imag goertzel hypothes behavior due limit intern represent limit would inhibit integr heterogen artifici gener intellig agi architectur issu may possibl address deep learn architectur intern form state homolog decomposit observ entiti event learn grammar visual linguist train data would equival restrict system commonsens reason oper concept term grammat product rule basic goal human languag acquisit artifici intellig ai deep learn move lab world research experi show artifici neural network vulner hack decept identifi pattern system use function attack modifi input ann way ann find match human observ would recogn exampl attack make subtl chang imag ann find match even though imag look human noth like search target manipul term adversari attack research use one ann doctor imag trial error fashion identifi anoth focal point therebi gener imag deceiv modifi imag look differ human eye anoth group show printout doctor imag photograph success trick imag classif system one defens revers imag search possibl fake imag submit site tiney find instanc refin search use part imag identifi imag piec may taken anoth group show certain psychedel spectacl could fool facial recognit system think ordinari peopl celebr potenti allow one person imperson anoth research ad sticker stop sign caus ann misclassifi ann howev train detect attempt decept potenti lead attack defend arm race similar kind alreadi defin malwar defens industri ann train defeat softwar repeatedli attack defens malwar continu alter genet algorithm trick retain abil damag target anoth group demonstr certain sound could make googl voic command system open particular web address hypothes could serv step stone attack open web page host malwar data poison fals data continu smuggl machin learn system train set prevent achiev masteri deep learn system train use supervis learn often reli data creat annot human argu clickwork amazon mechan turk regularli deploy purpos also implicit form human microwork often recogn philosoph rainer mühlhoff distinguish five type machin captur human microwork gener train data gamif embed annot comput task flow game trap track captcha imag recognit googl search result page exploit social motiv tag face facebook obtain label facial imag inform mine leverag devic activ tracker clickwork
Feedforward neural network,https://en.wikipedia.org/wiki/Feedforward_neural_network,"A feedforward neural network (FNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers.[2] Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops[2] (in contrast to recurrent neural networks,[3] which have a bi-directional flow). Modern feedforward networks are trained using backpropagation,[4][5][6][7][8] and are colloquially referred to as ""vanilla"" neural networks.[9]
 Note to avoid confusion: neural networks that include cycles or loops, including recurrent/temporal neural networks which go back in time, may technically still be considered feedforward because: 1) they multiply inputs through weights to get outputs and 2) backpropagation could not be used otherwise. Neural networks do not contain feedback like negative feedback or positive feedback where the outputs feedback to the VERY SAME inputs and modify them.
 The two historically common activation functions are both sigmoids, and are described by
 The first is a hyperbolic tangent that ranges from -1 to 1, while the other is the logistic function, which is similar in shape but ranges from 0 to 1. Here 




y

i




{\displaystyle y_{i}}

 is the output of the 



i


{\displaystyle i}

th node (neuron) and 




v

i




{\displaystyle v_{i}}

 is the weighted sum of the input connections. Alternative activation functions have been proposed, including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks, another class of supervised neural network models).
 In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids.
 Learning occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of supervised learning, and is carried out through backpropagation.
 We can represent the degree of error in an output node 



j


{\displaystyle j}

 in the 



n


{\displaystyle n}

th data point (training example) by 




e

j


(
n
)
=

d

j


(
n
)
−

y

j


(
n
)


{\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)}

, where 




d

j


(
n
)


{\displaystyle d_{j}(n)}

 is the desired target value for 



n


{\displaystyle n}

th data point at node 



j


{\displaystyle j}

, and 




y

j


(
n
)


{\displaystyle y_{j}(n)}

 is the value produced at node 



j


{\displaystyle j}

 when the 



n


{\displaystyle n}

th data point is given as an input.
 The node weights can then be adjusted based on corrections that minimize the error in the entire output for the 



n


{\displaystyle n}

th data point, given by
 Using gradient descent, the change in each weight 




w

i
j




{\displaystyle w_{ij}}

 is
 where 




y

i


(
n
)


{\displaystyle y_{i}(n)}

 is the output of the previous neuron 



i


{\displaystyle i}

, and 



η


{\displaystyle \eta }

 is the learning rate, which is selected to ensure that the weights quickly converge to a response, without oscillations. In the previous expression, 






∂


E


(
n
)


∂

v

j


(
n
)





{\displaystyle {\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}}

 denotes the partial derivate of the error 





E


(
n
)


{\displaystyle {\mathcal {E}}(n)}

 according to the weighted sum 




v

j


(
n
)


{\displaystyle v_{j}(n)}

 of the input connections of neuron 



i


{\displaystyle i}

.
 The derivative to be calculated depends on the induced local field 




v

j




{\displaystyle v_{j}}

, which itself varies. It is easy to prove that for an output node this derivative can be simplified to
 where 




ϕ

′




{\displaystyle \phi ^{\prime }}

 is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is
 This depends on the change in weights of the 



k


{\displaystyle k}

th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function.[10]
 If using a threshold, i.e. a linear activation function,  the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel non-linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1,1] despite the limited computational power of single unit with a linear threshold function.[31]
 Perceptrons can be trained by a simple learning algorithm that is usually called the delta rule. It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of gradient descent.
 A multilayer perceptron (MLP) is a misnomer for a modern feedforward artificial neural network, consisting of fully connected neurons (hence the synonym sometimes used of fully connected network (FCN)), often with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not linearly separable.[32]
 Examples of other feedforward networks include convolutional neural networks and radial basis function networks, which use a different activation function.
",feedforward neural network fnn one two broad type artifici neural network character direct flow inform layer flow mean inform model flow one input node hidden node output node without cycl loop contrast recurr neural network flow modern feedforward network train use backpropag colloqui refer vanilla neural network note avoid confus neural network includ cycl loop includ neural network go back time may technic still consid feedforward multipli input weight get output backpropag could use otherwis neural network contain feedback like neg feedback posit feedback output feedback input modifi two histor common activ function sigmoid describ first hyperbol tangent rang logist function similar shape rang output th node neuron v weight sum input connect altern activ function propos includ rectifi softplu function special activ function includ radial basi function use radial basi network anoth class supervis neural network model recent develop deep learn rectifi linear unit relu frequent use one possibl way overcom numer problem relat sigmoid learn occur chang connect weight piec data process base amount error output compar expect result exampl supervis learn carri backpropag repres degre error output node j j n n th data point train exampl e j n j n j n j n j n j n j n j n desir target valu n n th data point node j j j n j n valu produc node j j n n th data point given input node weight adjust base correct minim error entir output n n th data point given use gradient descent chang weight w j ij n n output previou neuron η learn rate select ensur weight quickli converg respons without oscil previou express e n v j n e n j n denot partial deriv error e n e n accord weight sum v j n j n input connect neuron deriv calcul depend induc local field v j j vari easi prove output node deriv simplifi ϕ deriv activ function describ vari analysi difficult chang weight hidden node shown relev deriv depend chang weight k k th node repres output layer chang hidden layer weight output layer weight chang accord deriv activ function algorithm repres backpropag activ function use threshold linear activ function result linear threshold unit call perceptron often term use denot one unit multipl parallel unit abl approxim continu function compact interv real number interv despit limit comput power singl unit linear threshold function perceptron train simpl learn algorithm usual call delta rule calcul error calcul output sampl output data use creat adjust weight thu implement form gradient descent multilay perceptron mlp misnom modern feedforward artifici neural network consist fulli connect neuron henc synonym sometim use fulli connect network fcn often nonlinear kind activ function organ least three layer notabl abl distinguish data linearli separ exampl feedforward network includ convolut neural network radial basi function network use differ activ function
Recurrent neural network,https://en.wikipedia.org/wiki/Recurrent_neural_network,"Recurrent neural networks (RNNs) are a class of artificial neural network commonly used for sequential data processing. Unlike feedforward neural networks, which process data in a single pass, RNNs process data across multiple time steps, making them well-adapted for modelling and processing text, speech, and time series.[1]
 The building block of RNNs is the recurrent unit. This unit maintains a hidden state, essentially a form of memory, which is updated at each time step based on the current input and the previous hidden state. This feedback loop allows the network to learn from past inputs, and incorporate that knowledge into its current processing.
 Early RNNs suffered from the vanishing gradient problem, limiting their ability to learn long-range dependencies. This was solved by the long short-term memory (LSTM) variant in 1997, thus making it the standard architecture for RNN.
 RNNs have been applied to tasks such as unsegmented, connected handwriting recognition,[2] speech recognition,[3][4] natural language processing, and neural machine translation.[5][6]
 
One origin of RNN was neuroscience. The word ""recurrent"" is used to describe loop-like structures in anatomy. In 1901, Cajal observed ""recurrent semicircles"" in the cerebellar cortex formed by parallel fiber, Purkinje cells, and granule cells.[7][8] In 1933, Lorente de Nó discovered ""recurrent, reciprocal connections"" by Golgi's method, and proposed that excitatory loops explain certain aspects of the vestibulo-ocular reflex.[9][10] During 1940s, multiple people proposed the existence of feedback in the brain, which was a contrast to the previous understanding of the neural system as a purely feedforward structure. Hebb considered ""reverberating circuit"" as an explanation for short-term memory.[11] The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles. The current activity of such networks can be affected by activity indefinitely far in the past.[12] They were both interested in closed loops as possible explanations for e.g. epilepsy and causalgia.[13][14] Recurrent inhibition was proposed in 1946 as a negative feedback mechanism in motor control. Neural feedback loops were a common topic of discussion at the Macy conferences.[15] See [16] for an extensive review of recurrent neural network models in neuroscience. Frank Rosenblatt in 1960 published ""close-loop cross-coupled perceptrons"", which are 3-layered perceptron networks whose middle layer contains recurrent connections that change by a Hebbian learning rule.[18]: 73–75  Later, in Principles of Neurodynamics (1961), he described ""closed-loop cross-coupled"" and ""back-coupled"" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks,[17]: Chapter 19, 21  and noted that a fully cross-coupled perceptron network is equivalent to an infinitely deep feedforward network.[17]: Section 19.11 
 Similar networks were published by Kaoru Nakano in 1971[19][20],Shun'ichi Amari in 1972,[21] and William A. Little [de] in 1974,[22] who was acknowledged by Hopfield in his 1982 paper.
 Another origin of RNN was statistical mechanics. The Ising model was developed by Wilhelm Lenz[23] and Ernst Ising[24] in the 1920s[25] as a simple statistical mechanical model of magnets at equilibrium. Glauber in 1963 studied the Ising model evolving in time, as a process towards equilibrium (Glauber dynamics), adding in the component of time.[26]
 The Sherrington–Kirkpatrick model of spin glass, published in 1975,[27] is the Hopfield network with random initialization. Sherrington and Kirkpatrick found that it is highly likely for the energy function of the SK model to have many local minima. In the 1982 paper, Hopfield applied this recently developed theory to study the Hopfield network with binary activation functions.[28] In a 1984 paper he extended this to continuous activation functions.[29] It became a standard model for the study of neural networks through statistical mechanics.[30][31]
 Modern RNN networks are mainly based on two architectures: LSTM and BRNN.[32]
 At the resurgence of neural networks in the 1980s, recurrent networks were studied again. They were sometimes called ""iterated nets"".[33] Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. In 1993, a neural history compressor system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time.[34]
 Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.[35][36] It became the default choice for RNN architecture.
 Bidirectional recurrent neural networks (BRNN) uses two RNN that processes the same input in opposite directions.[37] These two are often combined, giving the bidirectional LSTM architecture.
 Around 2006, bidirectional LSTM started to revolutionize speech recognition, outperforming traditional models in certain speech applications.[38][39] They also improved large-vocabulary speech recognition[3][4] and text-to-speech synthesis[40] and was used in Google voice search, and dictation on Android devices.[41] They broke records for improved machine translation,[42] language modeling[43] and Multilingual Language Processing.[44] Also, LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning.[45]
 The idea of encoder-decoder sequence transduction had been developed in the early 2010s. The papers most commonly cited as the originators that produced seq2seq are two papers from 2014.[46][47] A seq2seq architecture employs two RNN, typically LSTM, an ""encoder"" and a ""decoder"", for sequence transduction, such as machine translation. They became state of the art in machine translation, and was instrumental in the development of attention mechanism and Transformer.
 An RNN-based model can be factored into two parts: configuration and architecture. Multiple RNN can be combined in a data flow, and the data flow itself is the configuration. Each RNN itself may have any architecture, including LSTM, GRU, etc.
 RNNs come in many variants. Abstractly speaking, an RNN is a function 




f

θ




{\displaystyle f_{\theta }}

 of type 



(

x

t


,

h

t


)
↦
(

y

t


,

h

t
+
1


)


{\displaystyle (x_{t},h_{t})\mapsto (y_{t},h_{t+1})}

, where
 In words, it is a neural network that maps an input 




x

t




{\displaystyle x_{t}}

 into an output 




y

t




{\displaystyle y_{t}}

, with the hidden vector 




h

t




{\displaystyle h_{t}}

 playing the role of ""memory"", a partial record of all previous input-output pairs. At each step, it transforms input to an output, and modifies its ""memory"" to help it to better perform future processing.
 The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in ""layers"" and the drawing gives that appearance. However, what appears to be layers are, in fact, different steps in time, ""unfolded"" to produce the appearance of layers.
 A stacked RNN, or deep RNN, is composed of multiple RNNs stacked one above the other. Abstractly, it is structured as follows
 Each layer operates as a stand-alone RNN, and each layer's output sequence is used as the input sequence to the layer above. There is no conceptual limit to the depth of stacked RNN.
 A bidirectional RNN (biRNN) is composed of two RNNs, one processing the input sequence in one direction, and another in the opposite direction. Abstractly, it is structured as follows:
 The two output sequences are then concatenated to give the total output: 



(
(

y

0


,

y

0

′

)
,
(

y

1


,

y

1

′

)
,
…
,
(

y

N


,

y

N

′

)
)


{\displaystyle ((y_{0},y_{0}'),(y_{1},y_{1}'),\dots ,(y_{N},y_{N}'))}

.
 Bidirectional RNN allows the model to process a token both in the context of what came before it and what came after it. By stacking multiple bidirectional RNNs together, the model can process a token increasingly contextually. The ELMo model (2018)[48] is a stacked bidirectional LSTM which takes character-level as inputs and produces word-level embeddings.
 
Two RNNs can be run front-to-back in an encoder-decoder configuration. The encoder RNN processes an input sequence into a sequence of hidden vectors, and the decoder RNN processes the sequence of hidden vectors to an output sequence, with an optional attention mechanism. This was used to construct state of the art neural machine translators during the 2014–2017 period. This was an instrumental step towards the development of Transformers.[49]
 An RNN may process data with more than one dimension. PixelRNN processes two-dimensional data, with many possible directions.[50] For example, the row-by-row direction processes an 



n
×
n


{\displaystyle n\times n}

 grid of vectors 




x

i
,
j




{\displaystyle x_{i,j}}

 in the following order: 




x

1
,
1


,

x

1
,
2


,
…
,

x

1
,
n


,

x

2
,
1


,

x

2
,
2


,
…
,

x

2
,
n


,
…
,

x

n
,
n




{\displaystyle x_{1,1},x_{1,2},\dots ,x_{1,n},x_{2,1},x_{2,2},\dots ,x_{2,n},\dots ,x_{n,n}}

The diagonal BiLSTM uses two LSTMs to process the same grid. One processes it from the top-left corner to the bottom-right, such that it processes 




x

i
,
j




{\displaystyle x_{i,j}}

 depending on its hidden state and cell state on the top and the left side: 




h

i
−
1
,
j


,

c

i
−
1
,
j




{\displaystyle h_{i-1,j},c_{i-1,j}}

 and 




h

i
,
j
−
1


,

c

i
,
j
−
1




{\displaystyle h_{i,j-1},c_{i,j-1}}

. The other processes it from the top-right corner to the bottom-left.
 Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. In other words, it is a fully connected network. This is the most general neural network topology, because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons.
 The Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN, as it does not process sequences of patterns. However, it guarantees that it will converge. If the connections are trained using Hebbian learning, then the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration.
 An Elman network is a three-layer network (arranged horizontally as x, y, and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one.[51] At each time step, the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state, allowing it to perform tasks such as sequence-prediction that are beyond the power of a standard multilayer perceptron.
 Jordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also called the state layer. They have a recurrent connection to themselves.[51]
 Elman and Jordan networks are also known as ""Simple recurrent networks"" (SRN).
 Variables and functions
 Long short-term memory (LSTM) is the most widely used RNN architecture. It was designed to solve the vanishing gradient problem. LSTM is normally augmented by recurrent gates called ""forget gates"".[54] LSTM prevents backpropagated errors from vanishing or exploding.[55] Instead, errors can flow backward through unlimited numbers of virtual layers unfolded in space. That is, LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved.[56] LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components.
 Many applications use stacks of LSTMs,[57] for which it is called ""deep LSTM"". LSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts.[58]
 Gated recurrent unit (GRU), introduced in 2014, was designed as a simplification of LSTM. They are used in the full form and several further simplified variants.[59][60] They have fewer parameters than LSTM, as they lack an output gate.[61]
 Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory.[62] There does not appear to be particular performance difference between LSTM and GRU.[62][63]
 Introduced by Bart Kosko,[64] a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bidirectionality comes from passing information through a matrix and its transpose. Typically, bipolar encoding is preferred to binary encoding of the associative pairs. Recently, stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.[65]
 A BAM network has two layers, either of which can be driven as an input to recall an association and produce an output on the other layer.[66]
 Echo state networks (ESN) have a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series.[67] A variant for spiking neurons is known as a liquid state machine.[68]
 A recursive neural network[69] is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation.[70][71] They can process distributed representations of structure, such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing.[72] The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree.[73]
 Neural Turing machines (NTMs) are a method of extending recurrent neural networks by coupling them to external memory resources with which they interact. The combined system is analogous to a Turing machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent.[74]
 Differentiable neural computers (DNCs) are an extension of Neural Turing machines, allowing for the usage of fuzzy amounts of each memory address and a record of chronology.[75]
 Neural network pushdown automata (NNPDA) are similar to NTMs, but tapes are replaced by analog stacks that are differentiable and trained. In this way, they are similar in complexity to recognizers of context free grammars (CFGs).[76]
 Recurrent neural networks are Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.[77]
 An RNN can be trained into a conditionally generative model of sequences, aka autoregression.
 Concretely, let us consider the problem of machine translation, that is, given a sequence 



(

x

1


,

x

2


,
…
,

x

n


)


{\displaystyle (x_{1},x_{2},\dots ,x_{n})}

 of English words, the model is to produce a sequence 



(

y

1


,
…
,

y

m


)


{\displaystyle (y_{1},\dots ,y_{m})}

 of French words. It is to be solved by a seq2seq model.
 Now, during training, the encoder half of the model would first ingest 



(

x

1


,

x

2


,
…
,

x

n


)


{\displaystyle (x_{1},x_{2},\dots ,x_{n})}

, then the decoder half would start generating a sequence 



(




y
^




1


,




y
^




2


,
…
,




y
^




l


)


{\displaystyle ({\hat {y}}_{1},{\hat {y}}_{2},\dots ,{\hat {y}}_{l})}

. The problem is that if the model makes a mistake early on, say at 







y
^




2




{\displaystyle {\hat {y}}_{2}}

, then subsequent tokens are likely to also be mistakes. This makes it inefficient for the model to obtain a learning signal, since the model would mostly learn to shift 







y
^




2




{\displaystyle {\hat {y}}_{2}}

 towards 




y

2




{\displaystyle y_{2}}

, but not the others.
 Teacher forcing makes it so that the decoder uses the correct output sequence for generating the next entry in the sequence. So for example, it would see 



(

y

1


,
…
,

y

k


)


{\displaystyle (y_{1},\dots ,y_{k})}

 in order to generate 







y
^




k
+
1




{\displaystyle {\hat {y}}_{k+1}}

.
 Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. In neural networks, it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight, provided the non-linear activation functions are differentiable.
 The standard method for training RNN by gradient descent is the ""backpropagation through time"" (BPTT) algorithm, which is a special case of the general algorithm of backpropagation. A more computationally expensive online variant is called ""Real-Time Recurrent Learning"" or RTRL,[78][79] which is an instance of automatic differentiation in the forward accumulation mode with stacked tangent vectors. Unlike BPTT, this algorithm is local in time but not local in space.
 In this context, local in space means that a unit's weight vector can be updated using only information stored in the connected units and the unit itself such that update complexity of a single unit is linear in the dimensionality of the weight vector. Local in time means that the updates take place continually (on-line) and depend only on the most recent time step rather than on multiple time steps within a given time horizon as in BPTT. Biological neural networks appear to be local with respect to both time and space.[80][81]
 For recursively computing the partial derivatives, RTRL has a time-complexity of O(number of hidden x number of weights) per time step for computing the Jacobian matrices, while BPTT only takes O(number of weights) per time step, at the cost of storing all forward activations within the given time horizon.[82] An online hybrid between BPTT and RTRL with intermediate complexity exists,[83][84] along with variants for continuous time.[85]
 A major problem with gradient descent for standard RNN architectures is that error gradients vanish exponentially quickly with the size of the time lag between important events.[55][86] LSTM combined with a BPTT/RTRL hybrid learning method attempts to overcome these problems.[36] This problem is also solved in the independently recurrent neural network (IndRNN)[87] by reducing the context of a neuron to its own past state and the cross-neuron information can then be explored in the following layers. Memories of different ranges including long-term memory can be learned without the gradient vanishing and exploding problem.
 The on-line algorithm called causal recursive backpropagation (CRBP), implements and combines BPTT and RTRL paradigms for locally recurrent networks.[88] It works with the most general locally recurrent networks. The CRBP algorithm can minimize the global error term. This fact improves the stability of the algorithm, providing a unifying view of gradient calculation techniques for recurrent networks with local feedback.
 One approach to gradient information computation in RNNs with arbitrary architectures is based on signal-flow graphs diagrammatic derivation.[89] It uses the BPTT batch algorithm, based on Lee's theorem for network sensitivity calculations.[90] It was proposed by Wan and Beaufays, while its fast online version was proposed by Campolucci, Uncini and Piazza.[90]
 The connectionist temporal classification (CTC)[91] is a specialized loss function for training RNNs for sequence modeling problems where the timing is variable.[92]
 Training the weights in a neural network can be modeled as a non-linear global optimization problem. A target function can be formed to evaluate the fitness or error of a particular weight vector as follows: First, the weights in the network are set according to the weight vector. Next, the network is evaluated against the training sequence. Typically, the sum-squared difference between the predictions and the target values specified in the training sequence is used to represent the error of the current weight vector. Arbitrary global optimization techniques may then be used to minimize this target function.
 The most common global optimization method for training RNNs is genetic algorithms, especially in unstructured networks.[93][94][95]
 Initially, the genetic algorithm is encoded with the neural network weights in a predefined manner where one gene in the chromosome represents one weight link. The whole network is represented as a single chromosome. The fitness function is evaluated as follows:
 Many chromosomes make up the population; therefore, many different neural networks are evolved until a stopping criterion is satisfied. A common stopping scheme is: 
 The fitness function evaluates the stopping criterion as it receives the mean-squared error reciprocal from each network during training. Therefore, the goal of the genetic algorithm is to maximize the fitness function, reducing the mean-squared error.
 Other global (and/or evolutionary) optimization techniques may be used to seek a good set of weights, such as simulated annealing or particle swarm optimization.
 The independently recurrent neural network (IndRNN)[87] addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections.
 The neural history compressor is an unsupervised stack of RNNs.[96] At the input level, it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN, which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level.
 The system effectively minimizes the description length or the negative logarithm of the probability of the data.[97] Given a lot of learnable predictability in the incoming data sequence, the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events.
 It is possible to distill the RNN hierarchy into two RNNs: the ""conscious"" chunker (higher level) and the ""subconscious"" automatizer (lower level).[96] Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer, then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate, rarely changing memories across long intervals. In turn, this helps the automatizer to make many of its once unpredictable inputs predictable, such that the chunker can focus on the remaining unpredictable events.[96]
 A generative model partially overcame the vanishing gradient problem[55] of automatic differentiation or backpropagation in neural networks in 1992. In 1993, such a system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time.[34]
 Second-order RNNs use higher order weights 



w




i
j
k




{\displaystyle w{}_{ijk}}

 instead of the standard 



w




i
j




{\displaystyle w{}_{ij}}

 weights, and states can be a product. This allows a direct mapping to a finite-state machine both in training, stability, and representation.[98][99] Long short-term memory is an example of this but has no such formal mappings or proof of stability.
 Hierarchical recurrent neural networks (HRNN) connect their neurons in various ways to decompose hierarchical behavior into useful subprograms.[96][100] Such hierarchical structures of cognition are present in theories of memory presented by philosopher Henri Bergson, whose philosophical views have inspired hierarchical models.[101]
 Hierarchical recurrent neural networks are useful in forecasting, helping to predict disaggregated inflation components of the consumer price index (CPI). The HRNN model leverages information from higher levels in the CPI hierarchy to enhance lower-level predictions. Evaluation of a substantial dataset from the US CPI-U index demonstrates the superior performance of the HRNN model compared to various established inflation prediction methods.[102]
 Generally, a recurrent multilayer perceptron network (RMLP network) consists of cascaded subnetworks, each containing multiple layers of nodes. Each subnetwork is feed-forward except for the last layer, which can have feedback connections. Each of these subnets is connected only by feed-forward connections.[103]
 A multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization depending on the spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties.[104][105] With such varied neuronal activities, continuous sequences of any set of behaviors are segmented into reusable primitives, which in turn are flexibly integrated into diverse sequential behaviors. The biological approval of such a type of hierarchy was discussed in the memory-prediction theory of brain function by Hawkins in his book On Intelligence.[citation needed] Such a hierarchy also agrees with theories of memory posited by philosopher Henri Bergson, which have been incorporated into an MTRNN model.[101][106]
 Greg Snider of HP Labs describes a system of cortical computing with memristive nanodevices.[107] The memristors (memory resistors) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. DARPA's SyNAPSE project has funded IBM Research and HP Labs, in collaboration with the Boston University Department of Cognitive and Neural Systems (CNS), to develop neuromorphic architectures that may be based on memristive systems.
Memristive networks are a particular type of physical neural network that have very similar properties to (Little-)Hopfield networks, as they have continuous dynamics, a limited memory capacity and natural relaxation via the minimization of a function which is asymptotic to the Ising model. In this sense, the dynamics of a memristive circuit have the advantage compared to a Resistor-Capacitor network to have a more interesting non-linear behavior. From this point of view, engineering analog memristive networks account for a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring or topology.
The evolution of these networks can be studied analytically using variations of the Caravelli–Traversa–Di Ventra equation.[108]
 A continuous-time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs. They are typically analyzed by dynamical systems theory. Many RNN models in neuroscience are continuous-time.[16]
 For a neuron 



i


{\displaystyle i}

 in the network with activation 




y

i




{\displaystyle y_{i}}

, the rate of change of activation is given by:
 Where:
 CTRNNs have been applied to evolutionary robotics where they have been used to address vision,[109] co-operation,[110] and minimal cognitive behaviour.[111]
 Note that, by the Shannon sampling theorem, discrete-time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations.[112] This transformation can be thought of as occurring after the post-synaptic node activation functions 




y

i


(
t
)


{\displaystyle y_{i}(t)}

 have been low-pass filtered but prior to sampling.
 They are in fact recursive neural networks with a particular structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step.
 From a time-series perspective, RNNs can appear as nonlinear versions of finite impulse response and infinite impulse response filters and also as a nonlinear autoregressive exogenous model (NARX).[113] RNN has infinite impulse response whereas convolutional neural networks have finite impulse response. Both classes of networks exhibit temporal dynamic behavior.[114] A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that cannot be unrolled.
 The effect of memory-based learning for the recognition of sequences can also be implemented by a more biological-based model which uses the silencing mechanism exhibited in neurons with a relatively high frequency spiking activity.[115]
 Additional stored states and the storage under direct control by the network can be added to both infinite-impulse and finite-impulse networks. Another network or graph can also replace the storage if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated states or gated memory and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedback Neural Network (FNN).
 Modern libraries provide runtime-optimized implementations of the above functionality or allow to speed up the slow loop by just-in-time compilation.
 Applications of recurrent neural networks include:
",recurr neural network rnn class artifici neural network commonli use sequenti data process unlik feedforward neural network process data singl pass rnn process data across multipl time step make model process text speech time seri build block rnn recurr unit unit maintain hidden state essenti form memori updat time step base current input previou hidden state feedback loop allow network learn past input incorpor knowledg current process earli rnn suffer vanish gradient problem limit abil learn depend solv long memori lstm variant thu make standard architectur rnn rnn appli task unseg connect handwrit recognit speech recognit natur languag process neural machin translat one origin rnn neurosci word recurr use describ structur anatomi cajal observ recurr semicircl cerebellar cortex form parallel fiber purkinj cell granul cell lorent de nó discov recurr reciproc connect golgi method propos excitatori loop explain certain aspect reflex multipl peopl propos exist feedback brain contrast previou understand neural system pure feedforward structur hebb consid reverber circuit explan memori mcculloch pitt paper propos neuron model consid network contain cycl current activ network affect activ indefinit far past interest close loop possibl explan epilepsi causalgia recurr inhibit propos neg feedback mechan motor control neural feedback loop common topic discuss maci confer see extens review recurr neural network model neurosci frank rosenblatt publish perceptron perceptron network whose middl layer contain recurr connect chang hebbian learn rule later principl neurodynam describ perceptron network made theoret experiment studi hebbian learn network chapter note fulli perceptron network equival infinit deep feedforward network section similar network publish kaoru nakano amari william littl de acknowledg hopfield paper anoth origin rnn statist mechan ise model develop wilhelm lenz ernst ise simpl statist mechan model magnet equilibrium glauber studi ise model evolv time process toward equilibrium glauber dynam ad compon time model spin glass publish hopfield network random initi sherrington kirkpatrick found highli like energi function sk model mani local minima paper hopfield appli recent develop theori studi hopfield network binari activ function paper extend continu activ function becam standard model studi neural network statist mechan modern rnn network mainli base two architectur lstm brnn resurg neural network recurr network studi sometim call iter net two earli influenti work jordan network elman network appli rnn studi cognit psycholog neural histori compressor system solv deep learn task requir subsequ layer rnn unfold time long memori lstm network invent hochreit schmidhub set accuraci record multipl applic domain becam default choic rnn architectur bidirect recurr neural network brnn use two rnn process input opposit direct two often combin give bidirect lstm architectur around bidirect lstm start revolution speech recognit outperform tradit model certain speech applic also improv speech recognit synthesi use googl voic search dictat android devic broke record improv machin translat languag model multilingu languag process also lstm combin convolut neural network cnn improv automat imag caption idea sequenc transduct develop earli paper commonli cite origin produc two paper architectur employ two rnn typic lstm encod decod sequenc transduct machin translat becam state art machin translat instrument develop attent mechan transform model factor two part configur architectur multipl rnn combin data flow data flow configur rnn may architectur includ lstm gru etc rnn come mani variant abstractli speak rnn function f θ type x h h word neural network map input x output hidden vector h play role memori partial record previou pair step transform input output modifi memori help better perform futur process illustr right may mislead mani practic neural network topolog frequent organ layer draw give appear howev appear layer fact differ step time unfold produc appear layer stack rnn deep rnn compos multipl rnn stack one abstractli structur follow layer oper rnn layer output sequenc use input sequenc layer conceptu limit depth stack rnn bidirect rnn birnn compos two rnn one process input sequenc one direct anoth opposit direct abstractli structur follow two output sequenc concaten give total output n n n n bidirect rnn allow model process token context came came stack multipl bidirect rnn togeth model process token increasingli contextu elmo model stack bidirect lstm take input produc embed two rnn run configur encod rnn process input sequenc sequenc hidden vector decod rnn process sequenc hidden vector output sequenc option attent mechan use construct state art neural machin translat period instrument step toward develop transform rnn may process data one dimens pixelrnn process data mani possibl direct exampl direct process n n n grid vector x j j follow order x x x n x x x n x n n n n n n diagon bilstm use two lstm process grid one process corner process x j j depend hidden state cell state top left side h j c j j j h j c j process corner fulli recurr neural network frnn connect output neuron input neuron word fulli connect network gener neural network topolog topolog repres set connect weight zero simul lack connect neuron hopfield network rnn connect across layer equal size requir stationari input thu gener rnn process sequenc pattern howev guarante converg connect train use hebbian learn hopfield network perform robust memori resist connect alter elman network network arrang horizont x z illustr addit set context unit u illustr middl hidden layer connect context unit fix weight one time step input fed forward learn rule appli fix save copi previou valu hidden unit context unit sinc propag connect learn rule appli thu network maintain sort state allow perform task beyond power standard multilay perceptron jordan network similar elman network context unit fed output layer instead hidden layer context unit jordan network also call state layer recurr connect elman jordan network also known simpl recurr network srn variabl function long memori lstm wide use rnn architectur design solv vanish gradient problem lstm normal augment recurr gate call forget gate lstm prevent backpropag error vanish explod instead error flow backward unlimit number virtual layer unfold space lstm learn task requir memori event happen thousand even million discret time step earlier topolog evolv lstm work even given long delay signific event handl signal mix low compon mani applic use stack lstm call deep lstm lstm learn recogn languag unlik previou model base hidden markov model hmm similar concept gate recurr unit gru introduc design simplif lstm use full form sever simplifi variant fewer paramet lstm lack output gate perform polyphon music model speech signal model found similar long memori appear particular perform differ lstm gru introduc bart kosko bidirect associ memori bam network variant hopfield network store associ data vector bidirection come pass inform matrix transpos typic bipolar encod prefer binari encod associ pair recent stochast bam model use markov step optim increas network stabil relev applic bam network two layer either driven input recal associ produc output layer echo state network esn spars connect random hidden layer weight output neuron part network chang train esn good reproduc certain time seri variant spike neuron known liquid state machin recurs neural network creat appli set weight recurs differenti structur travers structur topolog order network typic also train revers mode automat differenti process distribut represent structur logic term special case recurs neural network rnn whose structur correspond linear chain recurs neural network appli natur languag process recurs neural tensor network use composit function node tree neural ture machin ntm method extend recurr neural network coupl extern memori resourc interact combin system analog ture machin von neumann architectur differenti allow effici train gradient descent differenti neural comput dnc extens neural ture machin allow usag fuzzi amount memori address record chronolog neural network pushdown automata nnpda similar ntm tape replac analog stack differenti train way similar complex recogn context free grammar cfg recurr neural network ture complet run arbitrari program process arbitrari sequenc input rnn train condit gener model sequenc aka autoregress concret let us consid problem machin translat given sequenc x x x n n english word model produc sequenc french word solv model train encod half model would first ingest x x x n n decod half would start gener sequenc l l problem model make mistak earli say subsequ token like also mistak make ineffici model obtain learn signal sinc model would mostli learn shift toward other teacher forc make decod use correct output sequenc gener next entri sequenc exampl would see k k order gener k gradient descent iter optim algorithm find minimum function neural network use minim error term chang weight proport deriv error respect weight provid activ function differenti standard method train rnn gradient descent backpropag time bptt algorithm special case gener algorithm backpropag comput expens onlin variant call recurr learn rtrl instanc automat differenti forward accumul mode stack tangent vector unlik bptt algorithm local time local space context local space mean unit weight vector updat use inform store connect unit unit updat complex singl unit linear dimension weight vector local time mean updat take place continu depend recent time step rather multipl time step within given time horizon bptt biolog neural network appear local respect time space recurs comput partial deriv rtrl number hidden x number weight per time step comput jacobian matric bptt take number weight per time step cost store forward activ within given time horizon onlin hybrid bptt rtrl intermedi complex exist along variant continu time major problem gradient descent standard rnn architectur error gradient vanish exponenti quickli size time lag import event lstm combin hybrid learn method attempt overcom problem problem also solv independ recurr neural network indrnn reduc context neuron past state inform explor follow layer memori differ rang includ memori learn without gradient vanish explod problem algorithm call causal recurs backpropag crbp implement combin bptt rtrl paradigm local recurr network work gener local recurr network crbp algorithm minim global error term fact improv stabil algorithm provid unifi view gradient calcul techniqu recurr network local feedback one approach gradient inform comput rnn arbitrari architectur base graph diagrammat deriv use bptt batch algorithm base lee theorem network sensit calcul propos wan beaufay fast onlin version propos campolucci uncini piazza connectionist tempor classif ctc special loss function train rnn sequenc model problem time variabl train weight neural network model global optim problem target function form evalu fit error particular weight vector follow first weight network set accord weight vector next network evalu train sequenc typic differ predict target valu specifi train sequenc use repres error current weight vector arbitrari global optim techniqu may use minim target function common global optim method train rnn genet algorithm especi unstructur network initi genet algorithm encod neural network weight predefin manner one gene chromosom repres one weight link whole network repres singl chromosom fit function evalu follow mani chromosom make popul therefor mani differ neural network evolv stop criterion satisfi common stop scheme fit function evalu stop criterion receiv error reciproc network train therefor goal genet algorithm maxim fit function reduc error global evolutionari optim techniqu may use seek good set weight simul anneal particl swarm optim independ recurr neural network indrnn address gradient vanish explod problem tradit fulli connect rnn neuron one layer receiv past state context inform instead full connect neuron layer thu neuron independ histori gradient backpropag regul avoid gradient vanish explod order keep long memori inform explor next layer indrnn robustli train nonlinear function relu deep network train use skip connect neural histori compressor unsupervis stack rnn input level learn predict next input previou input unpredict input rnn hierarchi becom input next higher level rnn therefor recomput intern state rare higher level rnn thu studi compress represent inform rnn done input sequenc precis reconstruct represent highest level system effect minim descript length neg logarithm probabl data given lot learnabl predict incom data sequenc highest level rnn use supervis learn easili classifi even deep sequenc long interv import event possibl distil rnn hierarchi two rnn consciou chunker higher level subconsci automat lower level chunker learn predict compress input unpredict automat automat forc next learn phase predict imit addit unit hidden unit slowli chang chunker make easi automat learn appropri rare chang memori across long interv turn help automat make mani unpredict input predict chunker focu remain unpredict event gener model partial overcam vanish gradient problem automat differenti backpropag neural network system solv deep learn task requir subsequ layer rnn unfold time rnn use higher order weight w j k w ijk instead standard w j w ij weight state product allow direct map machin train stabil represent long memori exampl formal map proof stabil hierarch recurr neural network hrnn connect neuron variou way decompos hierarch behavior use subprogram hierarch structur cognit present theori memori present philosoph henri bergson whose philosoph view inspir hierarch model hierarch recurr neural network use forecast help predict disaggreg inflat compon consum price index cpi hrnn model leverag inform higher level cpi hierarchi enhanc predict evalu substanti dataset us index demonstr superior perform hrnn model compar variou establish inflat predict method gener recurr multilay perceptron network rmlp network consist cascad subnetwork contain multipl layer node subnetwork except last layer feedback connect subnet connect connect multipl timescal recurr neural network mtrnn comput model simul function hierarchi brain depend spatial connect neuron distinct type neuron activ distinct time properti vari neuron activ continu sequenc set behavior segment reusabl primit turn flexibl integr divers sequenti behavior biolog approv type hierarchi discuss theori brain function hawkin book intellig citat need hierarchi also agre theori memori posit philosoph henri bergson incorpor mtrnn model greg snider hp lab describ system cortic comput memrist nanodevic memristor memori resistor implement thin film materi resist electr tune via transport ion oxygen vacanc within film darpa synaps project fund ibm research hp lab collabor boston univers depart cognit neural system cn develop neuromorph architectur may base memrist system memrist network particular type physic neural network similar properti hopfield network continu dynam limit memori capac natur relax via minim function asymptot ise model sens dynam memrist circuit advantag compar network interest behavior point view engin analog memrist network account peculiar type neuromorph engin devic behavior depend circuit wire topolog evolut network studi analyt use variat ventra equat recurr neural network ctrnn use system ordinari differenti equat model effect neuron incom input typic analyz dynam system theori mani rnn model neurosci neuron network activ rate chang activ given ctrnn appli evolutionari robot use address vision minim cognit behaviour note shannon sampl theorem recurr neural network view recurr neural network differenti equat transform equival differ equat transform thought occur node activ function filter prior sampl fact recurs neural network particular structur linear chain wherea recurs neural network oper hierarch structur combin child represent parent represent recurr neural network oper linear progress time combin previou time step hidden represent represent current time step perspect rnn appear nonlinear version finit impuls respons infinit impuls respons filter also nonlinear autoregress exogen model narx rnn infinit impuls respons wherea convolut neural network finit impuls respons class network exhibit tempor dynam behavior finit impuls recurr network direct acycl graph unrol replac strictli feedforward neural network infinit impuls recurr network direct cyclic graph unrol effect learn recognit sequenc also implement model use silenc mechan exhibit neuron rel high frequenc spike activ addit store state storag direct control network ad network anoth network graph also replac storag incorpor time delay feedback loop control state refer gate state gate memori part long memori network lstm gate recurr unit also call feedback neural network fnn modern librari provid implement function allow speed slow loop compil applic recurr neural network includ
Long short-term memory,https://en.wikipedia.org/wiki/Long_short-term_memory,"Long short-term memory (LSTM)[1] is a type of recurrent neural network (RNN) aimed at mitigating the vanishing gradient problem[2] commonly encountered by traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models, and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps (thus ""long short-term memory"").[1] The name is made in analogy with long-term memory and short-term memory and their relationship, studied by cognitive psychologists since the early 20th century.
 An LSTM unit is typically composed of a cell and three gates: an input gate, an output gate,[3] and a forget gate.[4] The cell remembers values over arbitrary time intervals, and the gates regulate the flow of information into and out of the cell. Forget gates decide what information to discard from the previous state, by mapping the previous state and the current input to a value between 0 and 1. A (rounded) value of 1 signifies retention of the information, and a value of 0 represents discarding. Input gates decide which pieces of new information to store in the current cell state, using the same system as forget gates. Output gates control which pieces of information in the current cell state to output, by assigning a value from 0 to 1 to the information, considering the previous and current states. Selectively outputting relevant information from the current state allows the LSTM network to maintain useful, long-term dependencies to make predictions, both in current and future time-steps.
 LSTM has wide applications in classification,[5][6] data processing, time series analysis tasks,[7] speech recognition,[8][9] machine translation,[10][11] speech activity detection,[12] robot control,[13][14] video games,[15][16] and healthcare.[17]
 In theory, classic RNNs can keep track of arbitrary long-term dependencies in the input sequences. The problem with classic RNNs is computational (or practical) in nature: when training a classic RNN using back-propagation, the long-term gradients which are back-propagated can ""vanish"", meaning they can tend to zero due to very small numbers creeping into the computations, causing the model to effectively stop learning. RNNs using LSTM units partially solve the vanishing gradient problem, because LSTM units allow gradients to also flow with little to no attenuation. However, LSTM networks can still suffer from the exploding gradient problem.[18]
 The intuition behind the LSTM architecture is to create an additional module in a neural network that learns when to remember and when to forget pertinent information.[4] In other words, the network effectively learns which information might be needed later on in a sequence and when that information is no longer needed. For instance, in the context of natural language processing, the network can learn grammatical dependencies.[19] An LSTM might process the sentence ""Dave, as a result of his controversial claims, is now a pariah"" by remembering the (statistically likely) grammatical gender and number of the subject Dave, note that this information is pertinent for the pronoun his and note that this information is no longer important after the verb is.
 In the equations below, the lowercase variables represent vectors. Matrices 




W

q




{\displaystyle W_{q}}

 and 




U

q




{\displaystyle U_{q}}

 contain, respectively, the weights of the input and recurrent connections, where the subscript 






q




{\displaystyle _{q}}

 can either be the input gate 



i


{\displaystyle i}

, output gate 



o


{\displaystyle o}

, the forget gate 



f


{\displaystyle f}

 or the memory cell 



c


{\displaystyle c}

, depending on the activation being calculated. In this section, we are thus using a ""vector notation"". So, for example, 




c

t


∈


R


h




{\displaystyle c_{t}\in \mathbb {R} ^{h}}

 is not just one unit of one LSTM cell, but contains 



h


{\displaystyle h}

 LSTM cell's units. 
 See [20] for an empirical study of 8 architectural variants of LSTM.
 The compact forms of the equations for the forward pass of an LSTM cell with a forget gate are:[1][4]
 where the initial values are 




c

0


=
0


{\displaystyle c_{0}=0}

 and 




h

0


=
0


{\displaystyle h_{0}=0}

 and the operator 



⊙


{\displaystyle \odot }

 denotes the Hadamard product (element-wise product). The subscript 



t


{\displaystyle t}

 indexes the time step.
 Letting the superscripts 



d


{\displaystyle d}

 and 



h


{\displaystyle h}

 refer to the number of input features and number of hidden units, respectively:
 The figure on the right is a graphical representation of an LSTM unit with peephole connections (i.e. a peephole LSTM).[21][22] Peephole connections allow the gates to access the constant error carousel (CEC), whose activation is the cell state.[21] 




h

t
−
1




{\displaystyle h_{t-1}}

 is not used, 




c

t
−
1




{\displaystyle c_{t-1}}

 is used instead in most places.
 Each of the gates can be thought as a ""standard"" neuron in a feed-forward (or multi-layer) neural network: that is, they compute an activation (using an activation function) of a weighted sum. 




i

t


,

o

t




{\displaystyle i_{t},o_{t}}

 and 




f

t




{\displaystyle f_{t}}

 represent the activations of respectively the input, output and forget gates, at time step 



t


{\displaystyle t}

.
 The 3 exit arrows from the memory cell 



c


{\displaystyle c}

 to the 3 gates 



i
,
o


{\displaystyle i,o}

 and 



f


{\displaystyle f}

 represent the peephole connections. These peephole connections actually denote the contributions of the activation of the memory cell 



c


{\displaystyle c}

 at time step 



t
−
1


{\displaystyle t-1}

, i.e. the contribution of 




c

t
−
1




{\displaystyle c_{t-1}}

 (and not 




c

t




{\displaystyle c_{t}}

, as the picture may suggest). In other words, the gates 



i
,
o


{\displaystyle i,o}

 and 



f


{\displaystyle f}

 calculate their activations at time step 



t


{\displaystyle t}

 (i.e., respectively, 




i

t


,

o

t




{\displaystyle i_{t},o_{t}}

 and 




f

t




{\displaystyle f_{t}}

) also considering the activation of the memory cell 



c


{\displaystyle c}

 at time step 



t
−
1


{\displaystyle t-1}

, i.e. 




c

t
−
1




{\displaystyle c_{t-1}}

.
 The single left-to-right arrow exiting the memory cell is not a peephole connection and denotes 




c

t




{\displaystyle c_{t}}

.
 The little circles containing a 



×


{\displaystyle \times }

 symbol represent an element-wise multiplication between its inputs. The big circles containing an S-like curve represent the application of a differentiable function (like the sigmoid function) to a weighted sum.
 Peephole convolutional LSTM.[23] The 



∗


{\displaystyle *}

 denotes the convolution operator.
 An RNN using LSTM units can be trained in a supervised fashion on a set of training sequences, using an optimization algorithm like gradient descent combined with backpropagation through time to compute the gradients needed during the optimization process, in order to change each weight of the LSTM network in proportion to the derivative of the error (at the output layer of the LSTM network) with respect to corresponding weight.
 A problem with using gradient descent for standard RNNs is that error gradients vanish exponentially quickly with the size of the time lag between important events. This is due to 




lim

n
→
∞



W

n


=
0


{\displaystyle \lim _{n\to \infty }W^{n}=0}

 if the spectral radius of 



W


{\displaystyle W}

 is smaller than 1.[2][24]
 However, with LSTM units, when error values are back-propagated from the output layer, the error remains in the LSTM unit's cell. This ""error carousel"" continuously feeds error back to each of the LSTM unit's gates, until they learn to cut off the value.
 Many applications use stacks of LSTM RNNs[25] and train them by connectionist temporal classification (CTC)[5] to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.
 Sometimes, it can be advantageous to train (parts of) an LSTM by neuroevolution[7] or by policy gradient methods, especially when there is no ""teacher"" (that is, training labels).
 Applications of LSTM include:
 2015: Google started using an LSTM trained by CTC for speech recognition on Google Voice.[50][51] According to the official blog post, the new model cut transcription errors by 49%.[52]
 2016: Google started using an LSTM to suggest messages in the Allo conversation app.[53] In the same year, Google released the Google Neural Machine Translation system for Google Translate which used LSTMs to reduce translation errors by 60%.[10][54][55]
 Apple announced in its Worldwide Developers Conference that it would start using the LSTM for quicktype[56][57][58] in the iPhone and for Siri.[59][60]
 Amazon released Polly, which generates the voices behind Alexa, using a bidirectional LSTM for the text-to-speech technology.[61]
 2017:  Facebook performed some 4.5 billion automatic translations every day using long short-term memory networks.[11]
 Microsoft reported reaching 94.9% recognition accuracy on the Switchboard corpus, incorporating a vocabulary of 165,000 words. The approach used ""dialog session-based long-short-term memory"".[62]
 2018: OpenAI used LSTM trained by policy gradients to beat humans in the complex video game of Dota 2,[15] and to control a human-like robot hand that manipulates physical objects with unprecedented dexterity.[14][63]
 2019: DeepMind used LSTM trained by policy gradients to excel at the complex video game of Starcraft II.[16][63]
 Aspects of LSTM were anticipated by ""focused back-propagation"" (Mozer, 1989),[64] cited by the LSTM paper.[1]
 Sepp Hochreiter's 1991 German diploma thesis analyzed the vanishing gradient problem and developed principles of the method.[2] His supervisor, Jürgen Schmidhuber, considered the thesis highly significant.[65]
 An early version of LSTM was published in 1995 in a technical report by Sepp Hochreiter and Jürgen Schmidhuber,[66] then published in the NIPS 1996 conference.[3]
 The most commonly used reference point for LSTM was published in 1997 in the journal Neural Computation.[1] By introducing Constant Error Carousel (CEC) units, LSTM deals with the vanishing gradient problem. The initial version of LSTM block included cells, input and output gates.[20]
 (Felix Gers, Jürgen Schmidhuber, and Fred Cummins, 1999)[67] introduced the forget gate (also called ""keep gate"") into the LSTM architecture in 1999, enabling the LSTM to reset its own state.[20] This is the most commonly used version of LSTM nowadays.
 (Gers, Schmidhuber, and Cummins, 2000) added peephole connections.[21][22] Additionally, the output activation function was omitted.[20]
 (Graves, Fernandez, Gomez, and Schmidhuber, 2006)[5] introduce a new error function for LSTM: Connectionist Temporal Classification (CTC) for simultaneous alignment and recognition of sequences. 
 (Graves, Schmidhuber, 2005)[26] published LSTM with full backpropagation through time and bidirectional LSTM.
 (Kyunghyun Cho et al., 2014)[68] published a simplified variant of the forget gate LSTM[67] called Gated recurrent unit (GRU).
 (Rupesh Kumar Srivastava, Klaus Greff, and Schmidhuber, 2015) used LSTM principles[67] to create the Highway network, a feedforward neural network with hundreds of layers, much deeper than previous networks.[69][70][71] Concurrently, the ResNet architecture was developed. It is equivalent to an open-gated or gateless highway network.[72]
 A modern upgrade of LSTM called xLSTM is published by a team leaded by Sepp Hochreiter (Maximilian et al, 2024).[73][74] One of the 2 blocks (mLSTM) of the architecture are parallelizable like the Transformer architecture, the other ones (sLSTM) allow state tracking.
 2004: First successful application of LSTM to speech Alex Graves et al.[75][63]
 2001: Gers and Schmidhuber trained LSTM to learn languages unlearnable by traditional models such as Hidden Markov Models.[21][63]
 Hochreiter et al. used LSTM for meta-learning (i.e. learning a learning algorithm).[76]
 2005: Daan Wierstra, Faustino Gomez, and Schmidhuber trained LSTM by neuroevolution without a teacher.[7]
 Mayer et al. trained LSTM to control robots.[13]
 2007: Wierstra, Foerster, Peters, and Schmidhuber trained LSTM by policy gradients for reinforcement learning without a teacher.[77]
 Hochreiter, Heuesel, and Obermayr applied LSTM to protein homology detection the field of biology.[37]
 2009: Justin Bayer et al. introduced neural architecture search for LSTM.[78][63]
 2009: An LSTM trained by CTC won the ICDAR connected handwriting recognition competition. Three such models were submitted by a team led by Alex Graves.[79] One was the most accurate model in the competition and another was the fastest.[80] This was the first time an RNN won international competitions.[63]
 2013: Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton used LSTM networks as a major component of a network that achieved a record 17.7% phoneme error rate on the classic TIMIT natural speech dataset.[28]
 Researchers from Michigan State University, IBM Research, and Cornell University published a study in the Knowledge Discovery and Data Mining (KDD) conference.[81][82][83] Their Time-Aware LSTM (T-LSTM) performs better on certain data sets than standard LSTM.
",long memori lstm type recurr neural network rnn aim mitig vanish gradient problem commonli encount tradit rnn rel insensit gap length advantag rnn hidden markov model sequenc learn method aim provid memori rnn last thousand timestep thu long memori name made analog memori memori relationship studi cognit psychologist sinc earli centuri lstm unit typic compos cell three gate input gate output gate forget gate cell rememb valu arbitrari time interv gate regul flow inform cell forget gate decid inform discard previou state map previou state current input valu round valu signifi retent inform valu repres discard input gate decid piec new inform store current cell state use system forget gate output gate control piec inform current cell state output assign valu inform consid previou current state select output relev inform current state allow lstm network maintain use depend make predict current futur lstm wide applic classif data process time seri analysi task speech recognit machin translat speech activ detect robot control video game healthcar theori classic rnn keep track arbitrari depend input sequenc problem classic rnn comput practic natur train classic rnn use gradient vanish mean tend zero due small number creep comput caus model effect stop learn rnn use lstm unit partial solv vanish gradient problem lstm unit allow gradient also flow littl attenu howev lstm network still suffer explod gradient problem intuit behind lstm architectur creat addit modul neural network learn rememb forget pertin inform word network effect learn inform might need later sequenc inform longer need instanc context natur languag process network learn grammat depend lstm might process sentenc dave result controversi claim pariah rememb statist like grammat gender number subject dave note inform pertin pronoun note inform longer import verb equat lowercas variabl repres vector matric w q q u q q contain respect weight input recurr connect subscript q q either input gate output gate forget gate f f memori cell c c depend activ calcul section thu use vector notat exampl c r h r h one unit one lstm cell contain h h lstm cell unit see empir studi architectur variant lstm compact form equat forward pass lstm cell forget gate initi valu c h oper denot hadamard product product subscript index time step let superscript h h refer number input featur number hidden unit respect figur right graphic represent lstm unit peephol connect peephol lstm peephol connect allow gate access constant error carousel cec whose activ cell state h use c use instead place gate thought standard neuron neural network comput activ use activ function weight sum f repres activ respect input output forget gate time step exit arrow memori cell c c gate f f repres peephol connect peephol connect actual denot contribut activ memori cell c c time step contribut c c pictur may suggest word gate f f calcul activ time step respect f also consid activ memori cell c c time step c singl arrow exit memori cell peephol connect denot c littl circl contain symbol repres multipl input big circl contain curv repres applic differenti function like sigmoid function weight sum peephol convolut lstm denot convolut oper rnn use lstm unit train supervis fashion set train sequenc use optim algorithm like gradient descent combin backpropag time comput gradient need optim process order chang weight lstm network proport deriv error output layer lstm network respect correspond weight problem use gradient descent standard rnn error gradient vanish exponenti quickli size time lag import event due lim n w n n spectral radiu w w smaller howev lstm unit error valu output layer error remain lstm unit cell error carousel continu feed error back lstm unit gate learn cut valu mani applic use stack lstm rnn train connectionist tempor classif ctc find rnn weight matrix maxim probabl label sequenc train set given correspond input sequenc ctc achiev align recognit sometim advantag train part lstm neuroevolut polici gradient method especi teacher train label applic lstm includ googl start use lstm train ctc speech recognit googl voic accord offici blog post new model cut transcript error googl start use lstm suggest messag allo convers app year googl releas googl neural machin translat system googl translat use lstm reduc translat error appl announc worldwid develop confer would start use lstm quicktyp iphon siri amazon releas polli gener voic behind alexa use bidirect lstm technolog facebook perform billion automat translat everi day use long memori network microsoft report reach recognit accuraci switchboard corpu incorpor vocabulari word approach use dialog memori openai use lstm train polici gradient beat human complex video game dota control robot hand manipul physic object unpreced dexter deepmind use lstm train polici gradient excel complex video game starcraft ii aspect lstm anticip focus mozer cite lstm paper sepp hochreit german diploma thesi analyz vanish gradient problem develop principl method supervisor jürgen schmidhub consid thesi highli signific earli version lstm publish technic report sepp hochreit jürgen schmidhub publish nip confer commonli use refer point lstm publish journal neural comput introduc constant error carousel cec unit lstm deal vanish gradient problem initi version lstm block includ cell input output gate felix ger jürgen schmidhub fred cummin introduc forget gate also call keep gate lstm architectur enabl lstm reset state commonli use version lstm nowaday ger schmidhub cummin ad peephol connect addit output activ function omit grave fernandez gomez schmidhub introduc new error function lstm connectionist tempor classif ctc simultan align recognit sequenc grave schmidhub publish lstm full backpropag time bidirect lstm kyunghyun cho et publish simplifi variant forget gate lstm call gate recurr unit gru rupesh kumar srivastava klau greff schmidhub use lstm principl creat highway network feedforward neural network hundr layer much deeper previou network concurr resnet architectur develop equival gateless highway network modern upgrad lstm call xlstm publish team lead sepp hochreit maximilian et al one block mlstm architectur paralleliz like transform architectur one slstm allow state track first success applic lstm speech alex grave et al ger schmidhub train lstm learn languag unlearn tradit model hidden markov model hochreit et al use lstm learn learn algorithm daan wierstra faustino gomez schmidhub train lstm neuroevolut without teacher mayer et al train lstm control robot wierstra foerster peter schmidhub train lstm polici gradient reinforc learn without teacher hochreit heuesel obermayr appli lstm protein homolog detect field biolog justin bayer et al introduc neural architectur search lstm lstm train ctc icdar connect handwrit recognit competit three model submit team led alex grave one accur model competit anoth fastest first time rnn intern competit alex grave moham geoffrey hinton use lstm network major compon network achiev record phonem error rate classic timit natur speech dataset research michigan state univers ibm research cornel univers publish studi knowledg discoveri data mine kdd confer lstm perform better certain data set standard lstm
Gated recurrent unit,https://en.wikipedia.org/wiki/Gated_recurrent_unit,"Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al.[1] The GRU is like a long short-term memory (LSTM) with a gating mechanism to input or forget certain features,[2] but lacks a context vector or output gate, resulting in fewer parameters than LSTM.[3] 
GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM.[4][5] GRUs showed that gating is indeed helpful in general, and Bengio's team came to no concrete conclusion on which of the two gating units was better.[6][7]
 There are several variations on the full gated unit, with gating done using the previous hidden state and the bias in various combinations, and a simplified form called minimal gated unit.[8]
 The operator 



⊙


{\displaystyle \odot }

 denotes the Hadamard product in the following.
 Initially, for 



t
=
0


{\displaystyle t=0}

, the output vector is 




h

0


=
0


{\displaystyle h_{0}=0}

. 
 Variables (



d


{\displaystyle d}

 denotes the number of input features and 



e


{\displaystyle e}

 the number of output features):
 Activation functions
 Alternative activation functions are possible, provided that 



σ
(
x
)
∈
[
0
,
1
]


{\displaystyle \sigma (x)\in [0,1]}

.
 Alternate forms can be created by changing 




z

t




{\displaystyle z_{t}}

 and 




r

t




{\displaystyle r_{t}}

[9]
 The minimal gated unit (MGU) is similar to the fully gated unit, except the update and reset gate vector is merged into a forget gate. This also implies that the equation for the output vector must be changed:[10]
 Variables
 The light gated recurrent unit (LiGRU)[4] removes the reset gate altogether, replaces tanh with the ReLU activation, and applies batch normalization (BN):
 LiGRU has been studied from a Bayesian perspective.[11] This analysis yielded a variant called light Bayesian recurrent unit (LiBRU), which showed slight improvements over the LiGRU on speech recognition tasks.
",gate recurr unit gru gate mechan recurr neural network introduc kyunghyun cho et al gru like long memori lstm gate mechan input forget certain featur lack context vector output gate result fewer paramet lstm gru perform certain task polyphon music model speech signal model natur languag process found similar lstm gru show gate inde help gener bengio team came concret conclus two gate unit better sever variat full gate unit gate done use previou hidden state bia variou combin simplifi form call minim gate unit oper denot hadamard product follow initi output vector h variabl denot number input featur e e number output featur activ function altern activ function possibl provid σ x x altern form creat chang z r minim gate unit mgu similar fulli gate unit except updat reset gate vector merg forget gate also impli equat output vector must chang variabl light gate recurr unit ligru remov reset gate altogeth replac tanh relu activ appli batch normal bn ligru studi bayesian perspect analysi yield variant call light bayesian recurr unit libru show slight improv ligru speech recognit task
Echo state network,https://en.wikipedia.org/wiki/Echo_state_network,"An echo state network (ESN)[1][2] is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can produce or reproduce specific temporal patterns. The main interest of this network is that although its behavior is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.
 Alternatively, one may consider a nonparametric Bayesian formulation of the output layer, under which: (i) a prior distribution is imposed over the output weights; and (ii) the output weights are marginalized out in the context of prediction generation, given the training data. This idea has been demonstrated in[3] by using Gaussian priors, whereby a Gaussian process model with ESN-driven kernel function is obtained. Such a solution was shown to outperform ESNs with trainable (finite) sets of weights in several benchmarks.
 Some publicly available efficient implementations of ESNs are aureservoir (a C++ library for various kinds with python/numpy bindings), MATLAB, ReservoirComputing.jl (a Julia-based implementation of various types) and pyESN (for simple ESNs in Python).
 The Echo State Network (ESN)[4] belongs to the Recurrent Neural Network (RNN) family and provide their architecture and supervised learning principle. Unlike Feedforward Neural Networks, Recurrent Neural Networks are dynamic systems and not functions. Recurrent Neural Networks are typically used for: 
 For the training of RNNs a number of learning algorithms are available: backpropagation through time, real-time recurrent learning. Convergence is not guaranteed due to instability and bifurcation phenomena.[4]  
 The main approach of the ESN is firstly to operate a random, large, fixed, recurring neural network with the input signal, which induces a nonlinear response signal in each neuron within this ""reservoir"" network, and secondly connect a desired output signal by a trainable linear combination of all these response signals.[2]
 Another feature of the ESN is the autonomous operation in prediction: if it is trained with an input that is a backshifted version of the output, then it can be used for signal generation/prediction by using the previous output as input.[4][5]
 The main idea of ESNs is tied to liquid state machines, which were independently and simultaneously developed with ESNs by Wolfgang Maass.[6] They, ESNs and the newly researched backpropagation decorrelation learning rule for RNNs[7] are more and more summarized under the name Reservoir Computing.
 Schiller and Steil[7] also demonstrated that in conventional training approaches for RNNs, in which all weights (not only output weights) are adapted, the dominant changes are in output weights. In cognitive neuroscience, Peter F. Dominey analysed a related process related to the modelling of sequence processing in the mammalian brain, in particular speech recognition in the human brain.[8] The basic idea also included a model of temporal input discrimination in biological neuronal networks.[9] An early clear formulation of the reservoir computing idea is due to K. Kirby, who disclosed this concept in a largely forgotten conference contribution.[10] The first formulation of the reservoir computing idea known today stems from L. Schomaker,[11] who described how a desired target output could be obtained from an RNN by learning to combine signals from a randomly configured ensemble of spiking neural oscillators.[2]
 Echo state networks can be built in different ways. They can be set up with or without directly trainable input-to-output connections, with or without output reservation feedback, with different neurotypes, different reservoir internal connectivity patterns etc. The output weight can be calculated for linear regression with all algorithms whether they are online or offline. In addition to the solutions for errors with smallest squares, margin maximization criteria, so-called training support vector machines, are used to determine the output values.[12] Other variants of echo state networks seek to change the formulation to better match common models of physical systems, such as those typically those defined by differential equations. Work in this direction includes echo state networks which partially include physical models,[13] hybrid echo state networks,[14] and continuous-time echo state networks.[15]
 The fixed RNN acts as a random, nonlinear medium whose dynamic response, the ""echo"", is used as a signal base.  The linear combination of this base can be trained to reconstruct the desired output by minimizing some error criteria.[2]
 RNNs were rarely used in practice before the introduction of the ESN, because of the complexity involved in adjusting their connections (e.g., lack of autodifferentiation, susceptibility to vanishing/exploding gradients, etc.). RNN training algorithms were slow and often vulnerable to issues, such as branching errors.[16]  Convergence could therefore not be guaranteed. On the other hand, ESN training does not have a problem with branching and is easy to implement. In early studies, ESNs were shown to perform well on time series prediction tasks from synthetic datasets.[1][17]
 Today, many of the problems that made RNNs slow and error-prone have been addressed with the advent of autodifferentiation (deep learning) libraries, as well as more stable architectures such as long short-term memory and Gated recurrent unit; thus, the unique selling point of ESNs has been lost. RNNs have also proven themselves in several practical areas, such as language processing. To cope with tasks of similar complexity using reservoir calculation methods requires memory of excessive size.
 ESNs are used in some areas, such as signal processing applications. In particular, they have been widely used as a computing principle that mixes well with non-digital computer substrates. Since ESNs do not need to modify the parameters of the RNN, they make it possible to use many different objects as their nonlinear ""reservoir″. For example, optical microchips, mechanical nanooscillators, polymer mixtures, or even artificial soft limbs.[2]
",echo state network esn type reservoir comput use recurr neural network spars connect hidden layer typic connect connect weight hidden neuron fix randomli assign weight output neuron learn network produc reproduc specif tempor pattern main interest network although behavior weight modifi train synaps connect hidden neuron output neuron thu error function quadrat respect paramet vector differenti easili linear system altern one may consid nonparametr bayesian formul output layer prior distribut impos output weight ii output weight margin context predict gener given train data idea demonstr use gaussian prior wherebi gaussian process model kernel function obtain solut shown outperform esn trainabl finit set weight sever benchmark publicli avail effici implement esn aureservoir librari variou kind bind matlab implement variou type pyesn simpl esn python echo state network esn belong recurr neural network rnn famili provid architectur supervis learn principl unlik feedforward neural network recurr neural network dynam system function recurr neural network typic use train rnn number learn algorithm avail backpropag time recurr learn converg guarante due instabl bifurc phenomena main approach esn firstli oper random larg fix recur neural network input signal induc nonlinear respons signal neuron within reservoir network secondli connect desir output signal trainabl linear combin respons signal anoth featur esn autonom oper predict train input backshift version output use signal use previou output input main idea esn tie liquid state machin independ simultan develop esn wolfgang maass esn newli research backpropag decorrel learn rule rnn summar name reservoir comput schiller steil also demonstr convent train approach rnn weight output weight adapt domin chang output weight cognit neurosci peter dominey analys relat process relat model sequenc process mammalian brain particular speech recognit human brain basic idea also includ model tempor input discrimin biolog neuron network earli clear formul reservoir comput idea due kirbi disclos concept larg forgotten confer contribut first formul reservoir comput idea known today stem schomak describ desir target output could obtain rnn learn combin signal randomli configur ensembl spike neural oscil echo state network built differ way set without directli trainabl connect without output reserv feedback differ neurotyp differ reservoir intern connect pattern etc output weight calcul linear regress algorithm whether onlin offlin addit solut error smallest squar margin maxim criteria train support vector machin use determin output valu variant echo state network seek chang formul better match common model physic system typic defin differenti equat work direct includ echo state network partial includ physic model hybrid echo state network echo state network fix rnn act random nonlinear medium whose dynam respons echo use signal base linear combin base train reconstruct desir output minim error criteria rnn rare use practic introduct esn complex involv adjust connect lack autodifferenti suscept gradient rnn train algorithm slow often vulner issu branch error converg could therefor guarante hand esn train problem branch easi implement earli studi esn shown perform well time seri predict task synthet dataset today mani problem made rnn slow address advent autodifferenti deep learn librari well stabl architectur long memori gate recurr unit thu uniqu sell point esn lost rnn also proven sever practic area languag process cope task similar complex use reservoir calcul method requir memori excess size esn use area signal process applic particular wide use comput principl mix well comput substrat sinc esn need modifi paramet rnn make possibl use mani differ object nonlinear exampl optic microchip mechan nanooscil polym mixtur even artifici soft limb
Reservoir computing,https://en.wikipedia.org/wiki/Reservoir_computing,"Reservoir computing is a framework for computation derived from recurrent neural network theory that maps input signals into higher dimensional computational spaces through the dynamics of a fixed, non-linear system called a reservoir.[1] After the input signal is fed into the reservoir, which is treated as a ""black box,"" a simple readout mechanism is trained to read the state of the reservoir and map it to the desired output.[1] The first key benefit of this framework is that training is performed only at the readout stage, as the reservoir dynamics are fixed.[1] The second is that the computational power of naturally available systems, both classical and quantum mechanical, can be used to reduce the effective computational cost.[2]
 The first examples of reservoir neural networks demonstrated that randomly connected recurrent neural networks could be used for simple forms of interval and speech discrimination[3][4]. In these early models the memory in the network took the form of both short-term synaptic plasticity and activity mediated by recurrent connections. In other early reservoir neural network models the memory of the recent stimulus history was provided solely by the recurrent activity[5][6]. Overall, the general concept of reservoir computing stems from the use of recursive connections within neural networks to create a complex dynamical system.[7] It is a generalisation of earlier neural network architectures such as recurrent neural networks, liquid-state machines and echo-state networks. Reservoir computing also extends to physical systems that are not networks in the classical sense, but rather continuous systems in space and/or time: e.g. a literal ""bucket of water"" can serve as a reservoir that performs computations on inputs given as perturbations of the surface.[8] The resultant complexity of such recurrent neural networks was found to be useful in solving a variety of problems including language processing and dynamic system modeling.[7] However, training of recurrent neural networks is challenging and computationally expensive.[7] Reservoir computing reduces those training-related challenges by fixing the dynamics of the reservoir and only training the linear output layer.[7]
 A large variety of nonlinear dynamical systems can serve as a reservoir that performs computations. In recent years semiconductor lasers have attracted considerable interest as computation can be fast and energy efficient compared to electrical components.
 Recent advances in both AI and quantum information theory have given rise to the concept of quantum neural networks.[9] These hold promise in quantum information processing, which is challenging to classical networks, but can also find application in solving classical problems.[9][10]  In 2018, a physical realization of a quantum reservoir computing architecture was demonstrated in the form of nuclear spins within a molecular solid.[10] However, the nuclear spin experiments in [10] did not demonstrate quantum reservoir computing per se as they did not involve processing of sequential data. Rather the data were vector inputs, which makes this more accurately a demonstration of quantum implementation of a random kitchen sink[11] algorithm (also going by the name of extreme learning machines in some communities). In 2019, another possible implementation of quantum reservoir processors was proposed in the form of two-dimensional fermionic lattices.[10] In 2020, realization of reservoir computing on gate-based quantum computers was proposed and demonstrated on cloud-based IBM superconducting near-term quantum computers.[12]
 Reservoir computers have been used for time-series analysis purposes. In particular, some of their usages involve chaotic time-series prediction,[13][14] separation of chaotic signals,[15] and link inference of networks from their dynamics.[16]
 The 'reservoir' in reservoir computing is the internal structure of the computer, and must have two properties: it must be made up of individual, non-linear units, and it must be capable of storing information. The non-linearity describes the response of each unit to input, which is what allows reservoir computers to solve complex problems. Reservoirs are able to store information by connecting the units in recurrent loops, where the previous input affects the next response. The change in reaction due to the past allows the computers to be trained to complete specific tasks.[17]
 Reservoirs can be virtual or physical.[17] Virtual reservoirs are typically randomly generated and are designed like neural networks.[17][7] Virtual reservoirs can be designed to have non-linearity and recurrent loops, but, unlike neural networks, the connections between units are randomized and remain unchanged throughout computation.[17] Physical reservoirs are possible because of the inherent non-linearity of certain natural systems. The interaction between ripples on the surface of water contains the nonlinear dynamics required in reservoir creation, and a pattern recognition RC was developed by first inputting ripples with electric motors then recording and analyzing the ripples in the readout.[1]
 The readout is a neural network layer that performs a linear transformation on the output of the reservoir.[1] The weights of the readout layer are trained by analyzing the spatiotemporal patterns of the reservoir after excitation by known inputs, and by utilizing a training method such as a linear regression or a Ridge regression.[1] As its implementation depends on spatiotemporal reservoir patterns, the details of readout methods are tailored to each type of reservoir.[1] For example, the readout for a reservoir computer using a container of liquid as its reservoir might entail observing spatiotemporal patterns on the surface of the liquid.[1]
 An early example of reservoir computing was the context reverberation network.[18]
In this architecture, an input layer feeds into a high dimensional dynamical system which is read out by a trainable single-layer perceptron. Two kinds of dynamical system were described: a recurrent neural network with fixed random weights, and a continuous reaction–diffusion system inspired by Alan Turing’s model of morphogenesis. At the trainable layer, the perceptron associates current inputs with the signals that reverberate in the dynamical system; the latter were said to provide a dynamic ""context"" for the inputs.  In the language of later work, the reaction–diffusion system served as the reservoir.
 The Tree Echo State Network (TreeESN) model represents a generalization of the reservoir computing framework to tree structured data.[19]
 Chaotic Liquid State Machine
 The liquid (i.e. reservoir) of a Chaotic Liquid State Machine (CLSM),[20][21] or chaotic reservoir, is made from chaotic spiking neurons but which stabilize their activity by settling to a single hypothesis that describes the trained inputs of the machine. This is in contrast to general types of reservoirs that don’t stabilize. The liquid stabilization occurs via synaptic plasticity and chaos control that govern neural connections inside the liquid. CLSM showed promising results in learning sensitive time series data.[20][21]
 This type of information processing is most relevant when time-dependent input signals depart from the mechanism’s internal dynamics.[22] These departures cause transients or temporary altercations which are represented in the device’s output.[22]
 The extension of the reservoir computing framework towards Deep Learning, with the introduction of Deep Reservoir Computing and of the Deep Echo State Network (DeepESN) model[23][24][25][26] allows to develop efficiently trained models for hierarchical processing of temporal data, at the same time enabling the investigation on the inherent role of layered composition in recurrent neural networks.
 Quantum reservoir computing may use the nonlinear nature of quantum mechanical interactions or processes to form the characteristic nonlinear reservoirs[9][10][27][12] but may also be done with linear reservoirs when the injection of the input to the reservoir creates the nonlinearity.[28] The marriage of machine learning and quantum devices is leading to the emergence of quantum neuromorphic computing as a new research area.[29]
 Gaussian states are a paradigmatic class of states of continuous variable quantum systems.[30] Although they can nowadays be created and manipulated in, e.g, state-of-the-art optical platforms,[31] naturally robust to decoherence, it is well-known that they are not sufficient for, e.g., universal quantum computing because transformations that preserve the Gaussian nature of a state are linear.[32] Normally, linear dynamics would not be sufficient for nontrivial reservoir computing either. It is nevertheless possible to harness such dynamics for reservoir computing purposes by considering a network of interacting quantum harmonic oscillators and injecting the input by periodical state resets of a subset of the oscillators. With a suitable choice of how the states of this subset of oscillators depends on the input, the observables of the rest of the oscillators can become nonlinear functions of the input suitable for reservoir computing; indeed, thanks to the properties of these functions, even universal reservoir computing becomes possible by combining the observables with a polynomial readout function.[28] In principle, such reservoir computers could be implemented with controlled multimode optical parametric processes,[33] however efficient extraction of the output from the system is challenging especially in the quantum regime where measurement back-action must be taken into account.
 In this architecture, randomized coupling between lattice sites grants the reservoir the “black box” property inherent to reservoir processors.[9] The reservoir is then excited, which acts as the input, by an incident optical field. Readout occurs in the form of occupational numbers of lattice sites, which are naturally nonlinear functions of the input.[9]
 In this architecture, quantum mechanical coupling between spins of neighboring atoms within the molecular solid provides the non-linearity required to create the higher-dimensional computational space.[10] The reservoir is then excited by radiofrequency electromagnetic radiation tuned to the resonance frequencies of relevant nuclear spins.[10] Readout occurs by measuring the nuclear spin states.[10]
 The most prevalent model of quantum computing is the gate-based model where quantum computation is performed by sequential applications of unitary quantum gates on qubits of a quantum computer.[34] A theory for the implementation of reservoir computing on a gate-based quantum computer with proof-of-principle demonstrations on a number of IBM superconducting noisy intermediate-scale quantum (NISQ) computers[35] has been reported in.[12]
",reservoir comput framework comput deriv recurr neural network theori map input signal higher dimension comput space dynam fix system call reservoir input signal fed reservoir treat black box simpl readout mechan train read state reservoir map desir output first key benefit framework train perform readout stage reservoir dynam fix second comput power natur avail system classic quantum mechan use reduc effect comput cost first exampl reservoir neural network demonstr randomli connect recurr neural network could use simpl form interv speech discrimin earli model memori network took form synapt plastic activ mediat recurr connect earli reservoir neural network model memori recent stimulu histori provid sole recurr activ overal gener concept reservoir comput stem use recurs connect within neural network creat complex dynam system generalis earlier neural network architectur recurr neural network machin network reservoir comput also extend physic system network classic sens rather continu system space time liter bucket water serv reservoir perform comput input given perturb surfac result complex recurr neural network found use solv varieti problem includ languag process dynam system model howev train recurr neural network challeng comput expens reservoir comput reduc challeng fix dynam reservoir train linear output layer larg varieti nonlinear dynam system serv reservoir perform comput recent year semiconductor laser attract consider interest comput fast energi effici compar electr compon recent advanc ai quantum inform theori given rise concept quantum neural network hold promis quantum inform process challeng classic network also find applic solv classic problem physic realiz quantum reservoir comput architectur demonstr form nuclear spin within molecular solid howev nuclear spin experi demonstr quantum reservoir comput per se involv process sequenti data rather data vector input make accur demonstr quantum implement random kitchen sink algorithm also go name extrem learn machin commun anoth possibl implement quantum reservoir processor propos form fermion lattic realiz reservoir comput quantum comput propos demonstr ibm superconduct quantum comput reservoir comput use analysi purpos particular usag involv chaotic predict separ chaotic signal link infer network dynam reservoir comput intern structur comput must two properti must made individu unit must capabl store inform describ respons unit input allow reservoir comput solv complex problem reservoir abl store inform connect unit recurr loop previou input affect next respons chang reaction due past allow comput train complet specif task reservoir virtual physic virtual reservoir typic randomli gener design like neural network virtual reservoir design recurr loop unlik neural network connect unit random remain unchang throughout comput physic reservoir possibl inher certain natur system interact rippl surfac water contain nonlinear dynam requir reservoir creation pattern recognit rc develop first input rippl electr motor record analyz rippl readout readout neural network layer perform linear transform output reservoir weight readout layer train analyz spatiotempor pattern reservoir excit known input util train method linear regress ridg regress implement depend spatiotempor reservoir pattern detail readout method tailor type reservoir exampl readout reservoir comput use contain liquid reservoir might entail observ spatiotempor pattern surfac liquid earli exampl reservoir comput context reverber network architectur input layer feed high dimension dynam system read trainabl perceptron two kind dynam system describ recurr neural network fix random weight continu system inspir alan ture model morphogenesi trainabl layer perceptron associ current input signal reverber dynam system latter said provid dynam context input languag later work system serv reservoir tree echo state network treeesn model repres gener reservoir comput framework tree structur data chaotic liquid state machin liquid reservoir chaotic liquid state machin clsm chaotic reservoir made chaotic spike neuron stabil activ settl singl hypothesi describ train input machin contrast gener type reservoir stabil liquid stabil occur via synapt plastic chao control govern neural connect insid liquid clsm show promis result learn sensit time seri data type inform process relev input signal depart mechan intern dynam departur caus transient temporari alterc repres devic output extens reservoir comput framework toward deep learn introduct deep reservoir comput deep echo state network deepesn model allow develop effici train model hierarch process tempor data time enabl investig inher role layer composit recurr neural network quantum reservoir comput may use nonlinear natur quantum mechan interact process form characterist nonlinear reservoir may also done linear reservoir inject input reservoir creat nonlinear marriag machin learn quantum devic lead emerg quantum neuromorph comput new research area gaussian state paradigmat class state continu variabl quantum system although nowaday creat manipul optic platform natur robust decoher suffici univers quantum comput transform preserv gaussian natur state linear normal linear dynam would suffici nontrivi reservoir comput either nevertheless possibl har dynam reservoir comput purpos consid network interact quantum harmon oscil inject input period state reset subset oscil suitabl choic state subset oscil depend input observ rest oscil becom nonlinear function input suitabl reservoir comput inde thank properti function even univers reservoir comput becom possibl combin observ polynomi readout function principl reservoir comput could implement control multimod optic parametr process howev effici extract output system challeng especi quantum regim measur must taken account architectur random coupl lattic site grant reservoir black box properti inher reservoir processor reservoir excit act input incid optic field readout occur form occup number lattic site natur nonlinear function input architectur quantum mechan coupl spin neighbor atom within molecular solid provid requir creat comput space reservoir excit radiofrequ electromagnet radiat tune reson frequenc relev nuclear spin readout occur measur nuclear spin state preval model quantum comput model quantum comput perform sequenti applic unitari quantum gate qubit quantum comput theori implement reservoir comput quantum comput demonstr number ibm superconduct noisi quantum nisq comput report
Boltzmann machine,https://en.wikipedia.org/wiki/Boltzmann_machine,"A Boltzmann machine (also called Sherrington–Kirkpatrick model with external field or stochastic Ising model), named after Ludwig Boltzmann is a spin-glass model with an external field, i.e., a Sherrington–Kirkpatrick model,[1] that is a stochastic Ising model. It is a statistical physics technique applied in the context of cognitive science.[2] It is also classified as a Markov random field.[3]
 Boltzmann machines are theoretically intriguing because of the locality and Hebbian nature of their training algorithm (being trained by Hebb's rule), and because of their parallelism and the resemblance of their dynamics to simple physical processes.  Boltzmann machines with unconstrained connectivity have not been proven useful for practical problems in machine learning or inference, but if the connectivity is properly constrained, the learning can be made efficient enough to be useful for practical problems.[4]
 They are named after the Boltzmann distribution in statistical mechanics, which is used in their sampling function.  They were heavily popularized and promoted by Geoffrey Hinton, Terry Sejnowski and Yann LeCun in cognitive sciences communities, particularly in machine learning,[2]  as part of ""energy-based models"" (EBM), because Hamiltonians of spin glasses as energy are used as a starting point to define the learning task.[5]
 A Boltzmann machine, like a Sherrington–Kirkpatrick model, is a network of units with a total ""energy"" (Hamiltonian) defined for the overall network. Its units produce binary results. Boltzmann machine weights are stochastic. The global energy 



E


{\displaystyle E}

 in a Boltzmann machine is identical in form to that of Hopfield networks and Ising models:
 Where:
 Often the weights 




w

i
j




{\displaystyle w_{ij}}

 are represented as a symmetric matrix 



W
=
[

w

i
j


]


{\displaystyle W=[w_{ij}]}

 with zeros along the diagonal.
 The difference in the global energy that results from a single unit 



i


{\displaystyle i}

 equaling 0 (off) versus 1 (on), written 



Δ

E

i




{\displaystyle \Delta E_{i}}

, assuming a symmetric matrix of weights, is given by:
 This can be expressed as the difference of energies of two states:
 Substituting the energy of each state with its relative probability according to the Boltzmann factor
(the property of a Boltzmann distribution that the energy of a state is proportional to the negative log probability of that state)
yields:
 where 




k

B




{\displaystyle k_{B}}

 is the Boltzmann constant and is absorbed into the artificial notion of temperature 



T


{\displaystyle T}

.
Noting that the probabilities of the unit being on or off sum to 



1


{\displaystyle 1}

 allows for the simplification:
 whence the probability that the 



i


{\displaystyle i}

-th unit is given by
 where the scalar 



T


{\displaystyle T}

 is referred to as the temperature of the system.
This relation is the source of the logistic function found in probability expressions in variants of the Boltzmann machine.
 The network runs by repeatedly choosing a unit and resetting its state. After running for long enough at a certain temperature, the probability of a global state of the network depends only upon that global state's energy, according to a Boltzmann distribution, and not on the initial state from which the process was started. This means that log-probabilities of global states become linear in their energies. This relationship is true when the machine is ""at thermal equilibrium"", meaning that the probability distribution of global states has converged. Running the network beginning from a high temperature, its temperature gradually decreases until reaching a thermal equilibrium at a lower temperature. It then may converge to a distribution where the energy level fluctuates around the global minimum. This process is called simulated annealing.
 To train the network so that the chance it will converge to a global state according to an external distribution over these states, the weights must be set so that the global states with the highest probabilities get the lowest energies. This is done by training.
 The units in the Boltzmann machine are divided into 'visible' units, V, and 'hidden' units, H. The visible units are those that receive information from the 'environment', i.e. the training set is a set of binary vectors over the set V. The distribution over the training set is denoted 




P

+


(
V
)


{\displaystyle P^{+}(V)}

.  
 The distribution over global states converges as the Boltzmann machine reaches thermal equilibrium. We denote this distribution, after we marginalize it over the hidden units, as 




P

−


(
V
)


{\displaystyle P^{-}(V)}

.
 Our goal is to approximate the ""real"" distribution 




P

+


(
V
)


{\displaystyle P^{+}(V)}

 using the 




P

−


(
V
)


{\displaystyle P^{-}(V)}

 produced by the machine. The similarity of the two distributions is measured by the Kullback–Leibler divergence, 



G


{\displaystyle G}

:
 where the sum is over all the possible states of 



V


{\displaystyle V}

. 



G


{\displaystyle G}

 is a function of the weights, since they determine the energy of a state, and the energy determines 




P

−


(
v
)


{\displaystyle P^{-}(v)}

, as promised by the Boltzmann distribution. A gradient descent algorithm over 



G


{\displaystyle G}

 changes a given weight, 




w

i
j




{\displaystyle w_{ij}}

,  by subtracting the partial derivative of 



G


{\displaystyle G}

 with respect to the weight.
 Boltzmann machine training involves two alternating phases. One is the ""positive"" phase where the visible units' states are clamped to a particular binary state vector sampled from the training set (according to 




P

+




{\displaystyle P^{+}}

). The other is the ""negative"" phase where the network is allowed to run freely, i.e. only the input nodes have their state determined by external data, but the output nodes are allowed to float. The gradient with respect to a given weight, 




w

i
j




{\displaystyle w_{ij}}

, is given by the equation:[2]
 where:
 This result follows from the fact that at thermal equilibrium the probability 




P

−


(
s
)


{\displaystyle P^{-}(s)}

 of any global state 



s


{\displaystyle s}

 when the network is free-running is given by the Boltzmann distribution.
 This learning rule is biologically plausible because the only information needed to change the weights is provided by ""local"" information. That is, the connection (synapse, biologically) does not need information about anything other than the two neurons it connects. This is more biologically realistic than the information needed by a connection in many other neural network training algorithms, such as backpropagation.
 The training of a Boltzmann machine does not use the EM algorithm, which is heavily used in machine learning. By minimizing the KL-divergence, it is equivalent to maximizing the log-likelihood of the data. Therefore, the training procedure performs gradient ascent on the log-likelihood of the observed data. This is in contrast to the EM algorithm, where the posterior distribution of the hidden nodes must be calculated before the maximization of the expected value of the complete data likelihood during the M-step.
 Training the biases is similar, but uses only single node activity:
 Theoretically the Boltzmann machine is a rather general computational medium. For instance, if trained on photographs, the machine would theoretically model the distribution of photographs, and could use that model to, for example, complete a partial photograph.
 Unfortunately, Boltzmann machines experience a serious practical problem, namely that it seems to stop learning correctly when the machine is scaled up to anything larger than a trivial size.[citation needed] This is due to important effects, specifically:
 Although learning is impractical in general Boltzmann machines, it can be made quite efficient in a restricted Boltzmann machine (RBM) which does not allow intralayer connections between hidden units and visible units, i.e. there is no connection between visible to visible and hidden to hidden units. After training one RBM, the activities of its hidden units can be treated as data for training a higher-level RBM. This method of stacking RBMs makes it possible to train many layers of hidden units efficiently and is one of the most common deep learning strategies. As each new layer is added the generative model improves.
 An extension to the restricted Boltzmann machine allows using real valued data rather than binary data.[6]
 One example of a practical RBM application is in speech recognition.[7]
 A deep Boltzmann machine (DBM) is a type of binary pairwise Markov random field (undirected probabilistic graphical model) with multiple layers of hidden random variables. It is a network of symmetrically coupled stochastic binary units. It comprises a set of visible units 




ν

∈
{
0
,
1

}

D




{\displaystyle {\boldsymbol {\nu }}\in \{0,1\}^{D}}

 and layers of hidden units 





h


(
1
)


∈
{
0
,
1

}


F

1




,


h


(
2
)


∈
{
0
,
1

}


F

2




,
…
,


h


(
L
)


∈
{
0
,
1

}


F

L






{\displaystyle {\boldsymbol {h}}^{(1)}\in \{0,1\}^{F_{1}},{\boldsymbol {h}}^{(2)}\in \{0,1\}^{F_{2}},\ldots ,{\boldsymbol {h}}^{(L)}\in \{0,1\}^{F_{L}}}

. No connection links units of the same layer (like RBM). For the DBM, the probability assigned to vector ν is
 where 




h

=
{


h


(
1
)


,


h


(
2
)


,


h


(
3
)


}


{\displaystyle {\boldsymbol {h}}=\{{\boldsymbol {h}}^{(1)},{\boldsymbol {h}}^{(2)},{\boldsymbol {h}}^{(3)}\}}

 are the set of hidden units, and 



θ
=
{


W


(
1
)


,


W


(
2
)


,


W


(
3
)


}


{\displaystyle \theta =\{{\boldsymbol {W}}^{(1)},{\boldsymbol {W}}^{(2)},{\boldsymbol {W}}^{(3)}\}}

 are the model parameters, representing visible-hidden and hidden-hidden interactions.[8] In a DBN only the top two layers form a restricted Boltzmann machine (which is an undirected graphical model), while lower layers form a directed generative model. In a DBM all layers are symmetric and undirected.
 Like DBNs, DBMs can learn complex and abstract internal representations of the input in tasks such as object or speech recognition, using limited, labeled data to fine-tune the representations built using a large set of unlabeled sensory input data. However, unlike DBNs and deep convolutional neural networks, they pursue the inference and training procedure in both directions, bottom-up and top-down, which allow the DBM to better unveil the representations of the input structures.[9][10][11]
 However, the slow speed of DBMs limits their performance and functionality. Because exact maximum likelihood learning is intractable for DBMs, only approximate maximum likelihood learning is possible. Another option is to use mean-field inference to estimate data-dependent expectations and approximate the expected sufficient statistics by using Markov chain Monte Carlo (MCMC).[8] This approximate inference, which must be done for each test input, is about 25 to 50 times slower than a single bottom-up pass in DBMs. This makes joint optimization impractical for large data sets, and restricts the use of DBMs for tasks such as feature representation.
 The need for deep learning with real-valued inputs, as in Gaussian RBMs, led to the spike-and-slab RBM (ssRBM), which models continuous-valued inputs with binary latent variables.[12] Similar to basic RBMs and its variants, a spike-and-slab RBM is a bipartite graph, while like GRBMs, the visible units (input) are real-valued. The difference is in the hidden layer, where each hidden unit has a binary spike variable and a real-valued slab variable. A spike is a discrete probability mass at zero, while a slab is a density over continuous domain;[13] their mixture forms a prior.[14]
 An extension of ssRBM called μ-ssRBM provides extra modeling capacity using additional terms in the energy function. One of these terms enables the model to form a conditional distribution of the spike variables by marginalizing out the slab variables given an observation.
 In more general mathematical setting, the Boltzmann distribution is also known as the Gibbs measure. In statistics and machine learning it is called a log-linear model. In deep learning the Boltzmann distribution is used in the sampling distribution of stochastic neural networks such as the Boltzmann machine.
 The Boltzmann machine is based on the Sherrington–Kirkpatrick spin glass model by David Sherrington and Scott Kirkpatrick.[15] The seminal publication by John Hopfield (1982) applied methods of statistical mechanics, mainly the recently developed (1970s) theory of spin glasses, to study associative memory (later named the ""Hopfield network"").[16]
 The original contribution in applying such energy-based models in cognitive science appeared in papers by Geoffrey Hinton and Terry Sejnowski.[17][18][19] In a 1995 interview, Hinton stated that in 1983 February or March, he was going to give a talk on simulated annealing in Hopfield networks, so he had to design a learning algorithm for the talk, resulting in the Boltzmann machine learning algorithm.[20]
 The idea of applying the Ising model with annealed Gibbs sampling was used in Douglas Hofstadter's Copycat project (1984).[21][22]
 The explicit analogy drawn with statistical mechanics in the Boltzmann machine formulation led to the use of terminology borrowed from physics (e.g., ""energy""), which became standard in the field. The widespread adoption of this terminology may have been encouraged by the fact that its use led to the adoption of a variety of concepts and methods from statistical mechanics. The various proposals to use simulated annealing for inference were apparently independent.
 Similar ideas (with a change of sign in the energy function) are found in Paul Smolensky's ""Harmony Theory"".[23] Ising models can be generalized to Markov random fields, which find widespread application in linguistics, robotics, computer vision and artificial intelligence.
 In 2024, Hopfield and Hinton were awarded Nobel Prize in Physics for their foundational contributions to machine learning, such as the Boltzmann machine.[24]
",boltzmann machin also call model extern field stochast ise model name ludwig boltzmann model extern field model stochast ise model statist physic techniqu appli context cognit scienc also classifi markov random field boltzmann machin theoret intrigu local hebbian natur train algorithm train hebb rule parallel resembl dynam simpl physic process boltzmann machin unconstrain connect proven use practic problem machin learn infer connect properli constrain learn made effici enough use practic problem name boltzmann distribut statist mechan use sampl function heavili popular promot geoffrey hinton terri sejnowski yann lecun cognit scienc commun particularli machin learn part model ebm hamiltonian spin glass energi use start point defin learn task boltzmann machin like model network unit total energi hamiltonian defin overal network unit produc binari result boltzmann machin weight stochast global energi e e boltzmann machin ident form hopfield network ise model often weight w j ij repres symmetr matrix w w j ij zero along diagon differ global energi result singl unit equal versu written δ e assum symmetr matrix weight given express differ energi two state substitut energi state rel probabl accord boltzmann factor properti boltzmann distribut energi state proport neg log probabl state yield k b b boltzmann constant absorb artifici notion temperatur note probabl unit sum allow simplif whenc probabl unit given scalar refer temperatur system relat sourc logist function found probabl express variant boltzmann machin network run repeatedli choos unit reset state run long enough certain temperatur probabl global state network depend upon global state energi accord boltzmann distribut initi state process start mean global state becom linear energi relationship true machin thermal equilibrium mean probabl distribut global state converg run network begin high temperatur temperatur gradual decreas reach thermal equilibrium lower temperatur may converg distribut energi level fluctuat around global minimum process call simul anneal train network chanc converg global state accord extern distribut state weight must set global state highest probabl get lowest energi done train unit boltzmann machin divid unit v unit visibl unit receiv inform train set set binari vector set distribut train set denot p v v distribut global state converg boltzmann machin reach thermal equilibrium denot distribut margin hidden unit p v v goal approxim real distribut p v v use p v v produc machin similar two distribut measur diverg g g sum possibl state v v g g function weight sinc determin energi state energi determin p v v promis boltzmann distribut gradient descent algorithm g g chang given weight w j ij subtract partial deriv g g respect weight boltzmann machin train involv two altern phase one posit phase visibl unit state clamp particular binari state vector sampl train set accord p neg phase network allow run freeli input node state determin extern data output node allow float gradient respect given weight w j ij given equat result follow fact thermal equilibrium probabl p global state network given boltzmann distribut learn rule biolog plausibl inform need chang weight provid local inform connect synaps biolog need inform anyth two neuron connect biolog realist inform need connect mani neural network train algorithm backpropag train boltzmann machin use em algorithm heavili use machin learn minim equival maxim data therefor train procedur perform gradient ascent observ data contrast em algorithm posterior distribut hidden node must calcul maxim expect valu complet data likelihood train bias similar use singl node activ theoret boltzmann machin rather gener comput medium instanc train photograph machin would theoret model distribut photograph could use model exampl complet partial photograph unfortun boltzmann machin experi seriou practic problem name seem stop learn correctli machin scale anyth larger trivial size citat need due import effect specif although learn impract gener boltzmann machin made quit effici restrict boltzmann machin rbm allow intralay connect hidden unit visibl unit connect visibl visibl hidden hidden unit train one rbm activ hidden unit treat data train rbm method stack rbm make possibl train mani layer hidden unit effici one common deep learn strategi new layer ad gener model improv extens restrict boltzmann machin allow use real valu data rather binari data one exampl practic rbm applic speech recognit deep boltzmann machin dbm type binari pairwis markov random field undirect probabilist graphic model multipl layer hidden random variabl network symmetr coupl stochast binari unit compris set visibl unit ν layer hidden unit h f h f h l f l h h h l l connect link unit layer like rbm dbm probabl assign vector ν h h h h h h h h set hidden unit θ w w w w w w model paramet repres interact dbn top two layer form restrict boltzmann machin undirect graphic model lower layer form direct gener model dbm layer symmetr undirect like dbn dbm learn complex abstract intern represent input task object speech recognit use limit label data represent built use larg set unlabel sensori input data howev unlik dbn deep convolut neural network pursu infer train procedur direct allow dbm better unveil represent input structur howev slow speed dbm limit perform function exact maximum likelihood learn intract dbm approxim maximum likelihood learn possibl anoth option use infer estim expect approxim expect suffici statist use markov chain mont carlo mcmc approxim infer must done test input time slower singl pass dbm make joint optim impract larg data set restrict use dbm task featur represent need deep learn input gaussian rbm led rbm ssrbm model input binari latent variabl similar basic rbm variant rbm bipartit graph like grbm visibl unit input differ hidden layer hidden unit binari spike variabl slab variabl spike discret probabl mass zero slab densiti continu domain mixtur form prior extens ssrbm call provid extra model capac use addit term energi function one term enabl model form condit distribut spike variabl margin slab variabl given observ gener mathemat set boltzmann distribut also known gibb measur statist machin learn call model deep learn boltzmann distribut use sampl distribut stochast neural network boltzmann machin boltzmann machin base spin glass model david sherrington scott kirkpatrick semin public john hopfield appli method statist mechan mainli recent develop theori spin glass studi associ memori later name hopfield network origin contribut appli model cognit scienc appear paper geoffrey hinton terri sejnowski interview hinton state februari march go give talk simul anneal hopfield network design learn algorithm talk result boltzmann machin learn algorithm idea appli ise model anneal gibb sampl use dougla hofstadt copycat project explicit analog drawn statist mechan boltzmann machin formul led use terminolog borrow physic energi becam standard field widespread adopt terminolog may encourag fact use led adopt varieti concept method statist mechan variou propos use simul anneal infer appar independ similar idea chang sign energi function found paul smolenski harmoni theori ise model gener markov random field find widespread applic linguist robot comput vision artifici intellig hopfield hinton award nobel prize physic foundat contribut machin learn boltzmann machin
Restricted Boltzmann machine,https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine,"A restricted Boltzmann machine (RBM) (also called a restricted Sherrington–Kirkpatrick model with external field or restricted stochastic Ising–Lenz–Little model) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.[1]
 RBMs were initially proposed under the name Harmonium by Paul Smolensky in 1986,[2] and rose to prominence after Geoffrey Hinton and collaborators used fast learning algorithms for them in the mid-2000s. RBMs have found applications in dimensionality reduction,[3] classification,[4] collaborative filtering,[5] feature learning,[6] topic modelling,[7] immunology,[8] and even many‑body quantum mechanics.[9]
[10]
[11]
 
They can be trained in either supervised or unsupervised ways, depending on the task.[citation needed]
 As their name implies, RBMs are a variant of Boltzmann machines, with the restriction that their neurons must form a bipartite graph:
 By contrast, ""unrestricted"" Boltzmann machines may have connections between hidden units. This restriction allows for more efficient training algorithms than are available for the general class of Boltzmann machines, in particular the gradient-based contrastive divergence algorithm.[12]
 Restricted Boltzmann machines can also be used in deep learning networks. In particular, deep belief networks can be formed by ""stacking"" RBMs and optionally fine-tuning the resulting deep network with gradient descent and backpropagation.[13]
 The standard type of RBM has binary-valued (Boolean) hidden and visible units, and consists of a matrix of weights 



W


{\displaystyle W}

 of size 



m
×
n


{\displaystyle m\times n}

. Each weight element 



(

w

i
,
j


)


{\displaystyle (w_{i,j})}

 of the matrix is associated with the connection between the visible (input) unit 




v

i




{\displaystyle v_{i}}

 and the hidden unit 




h

j




{\displaystyle h_{j}}

. In addition, there are bias weights (offsets) 




a

i




{\displaystyle a_{i}}

 for 




v

i




{\displaystyle v_{i}}

 and 




b

j




{\displaystyle b_{j}}

 for 




h

j




{\displaystyle h_{j}}

. Given the weights and biases, the energy of a configuration (pair of Boolean vectors) (v,h) is defined as
 or, in matrix notation,
 This energy function is analogous to that of a Hopfield network. As with general Boltzmann machines, the joint probability distribution for the visible and hidden vectors is defined in terms of the energy function as follows,[14]
 where 



Z


{\displaystyle Z}

 is a partition function defined as the sum of 




e

−
E
(
v
,
h
)




{\displaystyle e^{-E(v,h)}}

 over all possible configurations, which can be interpreted as a normalizing constant to ensure that the probabilities sum to 1. The marginal probability of a visible vector is the sum of 



P
(
v
,
h
)


{\displaystyle P(v,h)}

 over all possible hidden layer configurations,[14]
 and vice versa. Since the underlying graph structure of the RBM is bipartite (meaning there are no intra-layer connections), the hidden unit activations are mutually independent given the visible unit activations. Conversely, the visible unit activations are mutually independent given the hidden unit activations.[12] That is, for m visible units and n hidden units, the conditional probability of a configuration of the visible units v, given a configuration of the hidden units h, is
 Conversely, the conditional probability of h given v is
 The individual activation probabilities are given by
 where 



σ


{\displaystyle \sigma }

 denotes the logistic sigmoid.
 The visible units of Restricted Boltzmann Machine can be multinomial, although the hidden units are Bernoulli.[clarification needed] In this case, the logistic function for visible units is replaced by the softmax function
 where K is the number of discrete values that the visible values have. They are applied in topic modeling,[7] and recommender systems.[5]
 Restricted Boltzmann machines are a special case of Boltzmann machines and Markov random fields.[15][16]
 The graphical model of RBMs corresponds to that of factor analysis.[17]
 
 Restricted Boltzmann machines are trained to maximize the product of probabilities assigned to some training set 



V


{\displaystyle V}

 (a matrix, each row of which is treated as a visible vector 



v


{\displaystyle v}

),
 or equivalently, to maximize the expected log probability of a training sample 



v


{\displaystyle v}

 selected randomly from 



V


{\displaystyle V}

:[15][16]
 The algorithm most often used to train RBMs, that is, to optimize the weight matrix 



W


{\displaystyle W}

, is the contrastive divergence (CD) algorithm due to Hinton, originally developed to train PoE (product of experts) models.[18][19]
The algorithm performs Gibbs sampling and is used inside a gradient descent procedure (similar to the way backpropagation is used inside such a procedure when training feedforward neural nets) to compute weight update.
 The basic, single-step contrastive divergence (CD-1) procedure for a single sample can be summarized as follows:
 A Practical Guide to Training RBMs written by Hinton can be found on his homepage.[14]
",restrict boltzmann machin rbm also call restrict model extern field restrict stochast model gener stochast artifici neural network learn probabl distribut set input rbm initi propos name harmonium paul smolenski rose promin geoffrey hinton collabor use fast learn algorithm rbm found applic dimension reduct classif collabor filter featur learn topic model immunolog even quantum mechan train either supervis unsupervis way depend task citat need name impli rbm variant boltzmann machin restrict neuron must form bipartit graph contrast unrestrict boltzmann machin may connect hidden unit restrict allow effici train algorithm avail gener class boltzmann machin particular contrast diverg algorithm restrict boltzmann machin also use deep learn network particular deep belief network form stack rbm option result deep network gradient descent backpropag standard type rbm boolean hidden visibl unit consist matrix weight w w size n n weight element w j j matrix associ connect visibl input unit v hidden unit h j j addit bia weight offset v b j j h j j given weight bias energi configur pair boolean vector v h defin matrix notat energi function analog hopfield network gener boltzmann machin joint probabl distribut visibl hidden vector defin term energi function follow z z partit function defin sum e e v h v h possibl configur interpret normal constant ensur probabl sum margin probabl visibl vector sum p v h p v h possibl hidden layer configur vice versa sinc underli graph structur rbm bipartit mean connect hidden unit activ mutual independ given visibl unit activ convers visibl unit activ mutual independ given hidden unit activ visibl unit n hidden unit condit probabl configur visibl unit v given configur hidden unit h convers condit probabl h given v individu activ probabl given σ denot logist sigmoid visibl unit restrict boltzmann machin multinomi although hidden unit bernoulli clarif need case logist function visibl unit replac softmax function k number discret valu visibl valu appli topic model recommend system restrict boltzmann machin special case boltzmann machin markov random field graphic model rbm correspond factor analysi restrict boltzmann machin train maxim product probabl assign train set v v matrix row treat visibl vector v v equival maxim expect log probabl train sampl v v select randomli v v algorithm often use train rbm optim weight matrix w w contrast diverg cd algorithm due hinton origin develop train poe product expert model algorithm perform gibb sampl use insid gradient descent procedur similar way backpropag use insid procedur train feedforward neural net comput weight updat basic contrast diverg procedur singl sampl summar follow practic guid train rbm written hinton found homepag
Generative adversarial network,https://en.wikipedia.org/wiki/Generative_adversarial_network,"
 A generative adversarial network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative artificial intelligence.[1][2] The concept was initially developed by Ian Goodfellow and his colleagues in June 2014.[3] In a GAN, two neural networks contest with each other in the form of a zero-sum game, where one agent's gain is another agent's loss.
 Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proved useful for semi-supervised learning,[4] fully supervised learning,[5] and reinforcement learning.[6]
 The core idea of a GAN is based on the ""indirect"" training through the discriminator, another neural network that can tell how ""realistic"" the input seems, which itself is also being updated dynamically.[7] This means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner.
 GANs are similar to mimicry in evolutionary biology, with an evolutionary arms race between both networks.
 
The original GAN is defined as the following game:[3] Each probability space 



(
Ω
,

μ

ref


)


{\displaystyle (\Omega ,\mu _{\text{ref}})}

 defines a GAN game.
 There are 2 players: generator and discriminator.
 The generator's strategy set is 





P


(
Ω
)


{\displaystyle {\mathcal {P}}(\Omega )}

, the set of all probability measures 




μ

G




{\displaystyle \mu _{G}}

 on 



Ω


{\displaystyle \Omega }

.
 The discriminator's strategy set is the set of Markov kernels 




μ

D


:
Ω
→


P


[
0
,
1
]


{\displaystyle \mu _{D}:\Omega \to {\mathcal {P}}[0,1]}

, where 





P


[
0
,
1
]


{\displaystyle {\mathcal {P}}[0,1]}

 is the set of probability measures on 



[
0
,
1
]


{\displaystyle [0,1]}

.
 The GAN game is a zero-sum game, with objective function



L
(

μ

G


,

μ

D


)
:=

E

x
∼

μ

ref


,
y
∼

μ

D


(
x
)


⁡
[
ln
⁡
y
]
+

E

x
∼

μ

G


,
y
∼

μ

D


(
x
)


⁡
[
ln
⁡
(
1
−
y
)
]
.


{\displaystyle L(\mu _{G},\mu _{D}):=\operatorname {E} _{x\sim \mu _{\text{ref}},y\sim \mu _{D}(x)}[\ln y]+\operatorname {E} _{x\sim \mu _{G},y\sim \mu _{D}(x)}[\ln(1-y)].}


The generator aims to minimize the objective, and the discriminator aims to maximize the objective.
 The generator's task is to approach 




μ

G


≈

μ

ref




{\displaystyle \mu _{G}\approx \mu _{\text{ref}}}

, that is, to match its own output distribution as closely as possible to the reference distribution. The discriminator's task is to output a value close to 1 when the input appears to be from the reference distribution, and to output a value close to 0 when the input looks like it came from the generator distribution.
 The generative network generates candidates while the discriminative network evaluates them.[3] The contest operates in terms of data distributions. Typically, the generative network learns to map from a latent space to a data distribution of interest, while the discriminative network distinguishes candidates produced by the generator from the true data distribution. The generative network's training objective is to increase the error rate of the discriminative network (i.e., ""fool"" the discriminator network by producing novel candidates that the discriminator thinks are not synthesized (are part of the true data distribution)).[3][8]
 A known dataset serves as the initial training data for the discriminator. Training involves presenting it with samples from the training dataset until it achieves acceptable accuracy. The generator is trained based on whether it succeeds in fooling the discriminator. Typically, the generator is seeded with randomized input that is sampled from a predefined latent space (e.g. a multivariate normal distribution). Thereafter, candidates synthesized by the generator are evaluated by the discriminator. Independent backpropagation procedures are applied to both networks so that the generator produces better samples, while the discriminator becomes more skilled at flagging synthetic samples.[9] When used for image generation, the generator is typically a deconvolutional neural network, and the discriminator is a convolutional neural network.
 GANs are implicit generative models,[10] which means that they do not explicitly model the likelihood function nor provide a means for finding the latent variable corresponding to a given sample, unlike alternatives such as flow-based generative model.
 Compared to fully visible belief networks such as WaveNet and PixelRNN and autoregressive models in general, GANs can generate one complete sample in one pass, rather than multiple passes through the network.
 Compared to Boltzmann machines and linear ICA, there is no restriction on the type of function used by the network.
 Since neural networks are universal approximators, GANs are asymptotically consistent. Variational autoencoders might be universal approximators, but it is not proven as of 2017.[11]
 This section provides some of the mathematical theory behind these methods.
 
In modern probability theory based on measure theory, a probability space also needs to be equipped with a σ-algebra. As a result, a more rigorous definition of the GAN game would make the following changes: Each probability space 



(
Ω
,


B


,

μ

ref


)


{\displaystyle (\Omega ,{\mathcal {B}},\mu _{\text{ref}})}

 defines a GAN game.
 The generator's strategy set is 





P


(
Ω
,


B


)


{\displaystyle {\mathcal {P}}(\Omega ,{\mathcal {B}})}

, the set of all probability measures 




μ

G




{\displaystyle \mu _{G}}

 on the measure-space 



(
Ω
,


B


)


{\displaystyle (\Omega ,{\mathcal {B}})}

.
 
The discriminator's strategy set is the set of Markov kernels 




μ

D


:
(
Ω
,


B


)
→


P


(
[
0
,
1
]
,


B


(
[
0
,
1
]
)
)


{\displaystyle \mu _{D}:(\Omega ,{\mathcal {B}})\to {\mathcal {P}}([0,1],{\mathcal {B}}([0,1]))}

, where 





B


(
[
0
,
1
]
)


{\displaystyle {\mathcal {B}}([0,1])}

 is the Borel σ-algebra on 



[
0
,
1
]


{\displaystyle [0,1]}

. Since issues of measurability never arise in practice, these will not concern us further.
 In the most generic version of the GAN game described above, the strategy set for the discriminator contains all Markov kernels 




μ

D


:
Ω
→


P


[
0
,
1
]


{\displaystyle \mu _{D}:\Omega \to {\mathcal {P}}[0,1]}

, and the strategy set for the generator contains arbitrary probability distributions 




μ

G




{\displaystyle \mu _{G}}

 on 



Ω


{\displaystyle \Omega }

.
 However, as shown below, the optimal discriminator strategy against any 




μ

G




{\displaystyle \mu _{G}}

 is deterministic, so there is no loss of generality in restricting the discriminator's strategies to deterministic functions 



D
:
Ω
→
[
0
,
1
]


{\displaystyle D:\Omega \to [0,1]}

. In most applications, 



D


{\displaystyle D}

 is a deep neural network function.
 As for the generator, while 




μ

G




{\displaystyle \mu _{G}}

 could theoretically be any computable probability distribution, in practice, it is usually implemented as a pushforward: 




μ

G


=

μ

Z


∘

G

−
1




{\displaystyle \mu _{G}=\mu _{Z}\circ G^{-1}}

. That is, start with a random variable 



z
∼

μ

Z




{\displaystyle z\sim \mu _{Z}}

, where 




μ

Z




{\displaystyle \mu _{Z}}

 is a probability distribution that is easy to compute (such as the uniform distribution, or the Gaussian distribution), then define a function 



G
:

Ω

Z


→
Ω


{\displaystyle G:\Omega _{Z}\to \Omega }

. Then the distribution 




μ

G




{\displaystyle \mu _{G}}

 is the distribution of 



G
(
z
)


{\displaystyle G(z)}

.
 Consequently, the generator's strategy is usually defined as just 



G


{\displaystyle G}

, leaving 



z
∼

μ

Z




{\displaystyle z\sim \mu _{Z}}

 implicit. In this formalism, the GAN game objective is



L
(
G
,
D
)
:=

E

x
∼

μ

ref




⁡
[
ln
⁡
D
(
x
)
]
+

E

z
∼

μ

Z




⁡
[
ln
⁡
(
1
−
D
(
G
(
z
)
)
)
]
.


{\displaystyle L(G,D):=\operatorname {E} _{x\sim \mu _{\text{ref}}}[\ln D(x)]+\operatorname {E} _{z\sim \mu _{Z}}[\ln(1-D(G(z)))].}


 The GAN architecture has two main components. One is casting optimization into a game, of form 




min

G



max

D


L
(
G
,
D
)


{\displaystyle \min _{G}\max _{D}L(G,D)}

, which is different from the usual kind of optimization, of form 




min

θ


L
(
θ
)


{\displaystyle \min _{\theta }L(\theta )}

. The other is the decomposition of 




μ

G




{\displaystyle \mu _{G}}

 into 




μ

Z


∘

G

−
1




{\displaystyle \mu _{Z}\circ G^{-1}}

, which can be understood as a reparametrization trick.
 To see its significance, one must compare GAN with previous methods for learning generative models, which were plagued with ""intractable probabilistic computations that arise in maximum likelihood estimation and related strategies"".[3]
 At the same time, Kingma and Welling[12] and Rezende et al.[13] developed the same idea of reparametrization into a general stochastic backpropagation method. Among its first applications was the variational autoencoder.
 In the original paper, as well as most subsequent papers, it is usually assumed that the generator moves first, and the discriminator moves second, thus giving the following minimax game:




min


μ

G





max


μ

D




L
(

μ

G


,

μ

D


)
:=

E

x
∼

μ

ref


,
y
∼

μ

D


(
x
)


⁡
[
ln
⁡
y
]
+

E

x
∼

μ

G


,
y
∼

μ

D


(
x
)


⁡
[
ln
⁡
(
1
−
y
)
]
.


{\displaystyle \min _{\mu _{G}}\max _{\mu _{D}}L(\mu _{G},\mu _{D}):=\operatorname {E} _{x\sim \mu _{\text{ref}},y\sim \mu _{D}(x)}[\ln y]+\operatorname {E} _{x\sim \mu _{G},y\sim \mu _{D}(x)}[\ln(1-y)].}


 If both the generator's and the discriminator's strategy sets are spanned by a finite number of strategies, then by the minimax theorem,




min


μ

G





max


μ

D




L
(

μ

G


,

μ

D


)
=

max


μ

D





min


μ

G




L
(

μ

G


,

μ

D


)


{\displaystyle \min _{\mu _{G}}\max _{\mu _{D}}L(\mu _{G},\mu _{D})=\max _{\mu _{D}}\min _{\mu _{G}}L(\mu _{G},\mu _{D})}

that is, the move order does not matter.
 However, since the strategy sets are both not finitely spanned, the minimax theorem does not apply, and the idea of an ""equilibrium"" becomes delicate. To wit, there are the following different concepts of equilibrium:
 For general games, these equilibria do not have to agree, or even to exist. For the original GAN game, these equilibria all exist, and are all equal. However, for more general GAN games, these do not necessarily exist, or agree.[14]
 
The original GAN paper proved the following two theorems:[3] Theorem (the optimal discriminator computes the Jensen–Shannon divergence) — For any fixed generator strategy 




μ

G




{\displaystyle \mu _{G}}

, let the optimal reply be 




D

∗


=
arg
⁡

max

D


L
(

μ

G


,
D
)


{\displaystyle D^{*}=\arg \max _{D}L(\mu _{G},D)}

, then
 








D

∗


(
x
)



=



d

μ

ref




d
(

μ

ref


+

μ

G


)







L
(

μ

G


,

D

∗


)



=
2

D

J
S


(

μ

ref


;

μ

G


)
−
2
ln
⁡
2






{\displaystyle {\begin{aligned}D^{*}(x)&={\frac {d\mu _{\text{ref}}}{d(\mu _{\text{ref}}+\mu _{G})}}\\[6pt]L(\mu _{G},D^{*})&=2D_{JS}(\mu _{\text{ref}};\mu _{G})-2\ln 2\end{aligned}}}


 where the derivative is the Radon–Nikodym derivative, and 




D

J
S




{\displaystyle D_{JS}}

 is the Jensen–Shannon divergence.
 By Jensen's inequality,
 




E

x
∼

μ

ref


,
y
∼

μ

D


(
x
)


⁡
[
ln
⁡
y
]
≤

E

x
∼

μ

ref




⁡
[
ln
⁡

E

y
∼

μ

D


(
x
)


⁡
[
y
]
]


{\displaystyle \operatorname {E} _{x\sim \mu _{\text{ref}},y\sim \mu _{D}(x)}[\ln y]\leq \operatorname {E} _{x\sim \mu _{\text{ref}}}[\ln \operatorname {E} _{y\sim \mu _{D}(x)}[y]]}


and similarly for the other term. Therefore, the optimal reply can be deterministic, i.e. 




μ

D


(
x
)
=

δ

D
(
x
)




{\displaystyle \mu _{D}(x)=\delta _{D(x)}}

 for some function 



D
:
Ω
→
[
0
,
1
]


{\displaystyle D:\Omega \to [0,1]}

, in which case
 



L
(

μ

G


,

μ

D


)
:=

E

x
∼

μ

ref




⁡
[
ln
⁡
D
(
x
)
]
+

E

x
∼

μ

G




⁡
[
ln
⁡
(
1
−
D
(
x
)
)
]
.


{\displaystyle L(\mu _{G},\mu _{D}):=\operatorname {E} _{x\sim \mu _{\text{ref}}}[\ln D(x)]+\operatorname {E} _{x\sim \mu _{G}}[\ln(1-D(x))].}


 To define suitable density functions, we define a base measure 



μ
:=

μ

ref


+

μ

G




{\displaystyle \mu :=\mu _{\text{ref}}+\mu _{G}}

, which allows us to take the Radon–Nikodym derivatives
 




ρ

ref


=



d

μ

ref




d
μ





ρ

G


=



d

μ

G




d
μ





{\displaystyle \rho _{\text{ref}}={\frac {d\mu _{\text{ref}}}{d\mu }}\quad \rho _{G}={\frac {d\mu _{G}}{d\mu }}}


with 




ρ

ref


+

ρ

G


=
1


{\displaystyle \rho _{\text{ref}}+\rho _{G}=1}

.
 We then have
 



L
(

μ

G


,

μ

D


)
:=
∫
μ
(
d
x
)

[


ρ

ref


(
x
)
ln
⁡
(
D
(
x
)
)
+

ρ

G


(
x
)
ln
⁡
(
1
−
D
(
x
)
)

]

.


{\displaystyle L(\mu _{G},\mu _{D}):=\int \mu (dx)\left[\rho _{\text{ref}}(x)\ln(D(x))+\rho _{G}(x)\ln(1-D(x))\right].}


 The integrand is just the negative cross-entropy between two Bernoulli random variables with parameters 




ρ

ref


(
x
)


{\displaystyle \rho _{\text{ref}}(x)}

 and 



D
(
x
)


{\displaystyle D(x)}

. We can write this as 



−
H
(

ρ

ref


(
x
)
)
−

D

K
L


(

ρ

ref


(
x
)
∥
D
(
x
)
)


{\displaystyle -H(\rho _{\text{ref}}(x))-D_{KL}(\rho _{\text{ref}}(x)\parallel D(x))}

, where 



H


{\displaystyle H}

 is the binary entropy function, so
 



L
(

μ

G


,

μ

D


)
=
−
∫
μ
(
d
x
)
(
H
(

ρ

ref


(
x
)
)
+

D

K
L


(

ρ

ref


(
x
)
∥
D
(
x
)
)
)
.


{\displaystyle L(\mu _{G},\mu _{D})=-\int \mu (dx)(H(\rho _{\text{ref}}(x))+D_{KL}(\rho _{\text{ref}}(x)\parallel D(x))).}


 This means that the optimal strategy for the discriminator is 



D
(
x
)
=

ρ

ref


(
x
)


{\displaystyle D(x)=\rho _{\text{ref}}(x)}

, with   




L
(

μ

G


,

μ

D


∗


)
=
−
∫
μ
(
d
x
)
H
(

ρ

ref


(
x
)
)
=

D

J
S


(

μ

ref


∥

μ

G


)
−
2
ln
⁡
2


{\displaystyle L(\mu _{G},\mu _{D}^{*})=-\int \mu (dx)H(\rho _{\text{ref}}(x))=D_{JS}(\mu _{\text{ref}}\parallel \mu _{G})-2\ln 2}


 after routine calculation.
 Interpretation: For any fixed generator strategy 




μ

G




{\displaystyle \mu _{G}}

, the optimal discriminator keeps track of the likelihood ratio between the reference distribution and the generator distribution:






D
(
x
)


1
−
D
(
x
)



=



d

μ

ref




d

μ

G





(
x
)
=




μ

ref


(
d
x
)



μ

G


(
d
x
)



;

D
(
x
)
=
σ
(
ln
⁡

μ

ref


(
d
x
)
−
ln
⁡

μ

G


(
d
x
)
)


{\displaystyle {\frac {D(x)}{1-D(x)}}={\frac {d\mu _{\text{ref}}}{d\mu _{G}}}(x)={\frac {\mu _{\text{ref}}(dx)}{\mu _{G}(dx)}};\quad D(x)=\sigma (\ln \mu _{\text{ref}}(dx)-\ln \mu _{G}(dx))}

where 



σ


{\displaystyle \sigma }

 is the logistic function.
In particular, if the prior probability for an image 



x


{\displaystyle x}

 to come from the reference distribution is equal to 





1
2




{\displaystyle {\frac {1}{2}}}

, then 



D
(
x
)


{\displaystyle D(x)}

 is just the posterior probability that 



x


{\displaystyle x}

 came from the reference distribution:



D
(
x
)
=
Pr
(
x

 came from reference distribution

∣
x
)
.


{\displaystyle D(x)=\Pr(x{\text{ came from reference distribution}}\mid x).}


 Theorem (the unique equilibrium point) — For any GAN game, there exists a pair 



(




μ
^




D


,




μ
^




G


)


{\displaystyle ({\hat {\mu }}_{D},{\hat {\mu }}_{G})}

 that is both a sequential equilibrium and a Nash equilibrium:
 








L
(




μ
^




G


,




μ
^




D


)
=

min


μ

G





max


μ

D




L
(

μ

G


,

μ

D


)
=



max


μ

D





min


μ

G




L
(

μ

G


,

μ

D


)
=
−
2
ln
⁡
2









μ
^




D


∈
arg
⁡

max


μ

D





min


μ

G




L
(

μ

G


,

μ

D


)
,







μ
^




G


∈
arg
⁡

min


μ

G





max


μ

D




L
(

μ

G


,

μ

D


)









μ
^




D


∈
arg
⁡

max


μ

D




L
(




μ
^




G


,

μ

D


)
,







μ
^




G


∈
arg
⁡

min


μ

G




L
(

μ

G


,




μ
^




D


)





∀
x
∈
Ω
,




μ
^




D


(
x
)
=

δ


1
2



,







μ
^




G


=

μ

ref








{\displaystyle {\begin{aligned}&L({\hat {\mu }}_{G},{\hat {\mu }}_{D})=\min _{\mu _{G}}\max _{\mu _{D}}L(\mu _{G},\mu _{D})=&\max _{\mu _{D}}\min _{\mu _{G}}L(\mu _{G},\mu _{D})=-2\ln 2\\[6pt]&{\hat {\mu }}_{D}\in \arg \max _{\mu _{D}}\min _{\mu _{G}}L(\mu _{G},\mu _{D}),&\quad {\hat {\mu }}_{G}\in \arg \min _{\mu _{G}}\max _{\mu _{D}}L(\mu _{G},\mu _{D})\\[6pt]&{\hat {\mu }}_{D}\in \arg \max _{\mu _{D}}L({\hat {\mu }}_{G},\mu _{D}),&\quad {\hat {\mu }}_{G}\in \arg \min _{\mu _{G}}L(\mu _{G},{\hat {\mu }}_{D})\\[6pt]&\forall x\in \Omega ,{\hat {\mu }}_{D}(x)=\delta _{\frac {1}{2}},&\quad {\hat {\mu }}_{G}=\mu _{\text{ref}}\end{aligned}}}


 That is, the generator perfectly mimics the reference, and the discriminator outputs 





1
2




{\displaystyle {\frac {1}{2}}}

 deterministically on all inputs.
 From the previous proposition,
 



arg
⁡

min


μ

G





max


μ

D




L
(

μ

G


,

μ

D


)
=

μ

ref


;


min


μ

G





max


μ

D




L
(

μ

G


,

μ

D


)
=
−
2
ln
⁡
2.


{\displaystyle \arg \min _{\mu _{G}}\max _{\mu _{D}}L(\mu _{G},\mu _{D})=\mu _{\text{ref}};\quad \min _{\mu _{G}}\max _{\mu _{D}}L(\mu _{G},\mu _{D})=-2\ln 2.}


 For any fixed discriminator strategy 




μ

D




{\displaystyle \mu _{D}}

, any 




μ

G




{\displaystyle \mu _{G}}

 concentrated on the set
 



{
x
∣

E

y
∼

μ

D


(
x
)


⁡
[
ln
⁡
(
1
−
y
)
]
=

inf

x



E

y
∼

μ

D


(
x
)


⁡
[
ln
⁡
(
1
−
y
)
]
}


{\displaystyle \{x\mid \operatorname {E} _{y\sim \mu _{D}(x)}[\ln(1-y)]=\inf _{x}\operatorname {E} _{y\sim \mu _{D}(x)}[\ln(1-y)]\}}


is an optimal strategy for the generator. Thus,
 



arg
⁡

max


μ

D





min


μ

G




L
(

μ

G


,

μ

D


)
=
arg
⁡

max


μ

D





E

x
∼

μ

ref


,
y
∼

μ

D


(
x
)


⁡
[
ln
⁡
y
]
+

inf

x



E

y
∼

μ

D


(
x
)


⁡
[
ln
⁡
(
1
−
y
)
]
.


{\displaystyle \arg \max _{\mu _{D}}\min _{\mu _{G}}L(\mu _{G},\mu _{D})=\arg \max _{\mu _{D}}\operatorname {E} _{x\sim \mu _{\text{ref}},y\sim \mu _{D}(x)}[\ln y]+\inf _{x}\operatorname {E} _{y\sim \mu _{D}(x)}[\ln(1-y)].}


 By Jensen's inequality, the discriminator can only improve by adopting the deterministic strategy of always playing 



D
(
x
)
=

E

y
∼

μ

D


(
x
)


⁡
[
y
]


{\displaystyle D(x)=\operatorname {E} _{y\sim \mu _{D}(x)}[y]}

. Therefore,
 



arg
⁡

max


μ

D





min


μ

G




L
(

μ

G


,

μ

D


)
=
arg
⁡

max

D



E

x
∼

μ

ref




⁡
[
ln
⁡
D
(
x
)
]
+

inf

x


ln
⁡
(
1
−
D
(
x
)
)


{\displaystyle \arg \max _{\mu _{D}}\min _{\mu _{G}}L(\mu _{G},\mu _{D})=\arg \max _{D}\operatorname {E} _{x\sim \mu _{\text{ref}}}[\ln D(x)]+\inf _{x}\ln(1-D(x))}


 By Jensen's inequality,
 








ln
⁡

E

x
∼

μ

ref




⁡
[
D
(
x
)
]
+

inf

x


ln
⁡
(
1
−
D
(
x
)
)




=




ln
⁡

E

x
∼

μ

ref




⁡
[
D
(
x
)
]
+
ln
⁡
(
1
−

sup

x


D
(
x
)
)




=




ln
⁡
[

E

x
∼

μ

ref




⁡
[
D
(
x
)
]
(
1
−

sup

x


D
(
x
)
)
]
≤
ln
⁡
[

sup

x


D
(
x
)
)
(
1
−

sup

x


D
(
x
)
)
]
≤
ln
⁡


1
4


,






{\displaystyle {\begin{aligned}&\ln \operatorname {E} _{x\sim \mu _{\text{ref}}}[D(x)]+\inf _{x}\ln(1-D(x))\\[6pt]={}&\ln \operatorname {E} _{x\sim \mu _{\text{ref}}}[D(x)]+\ln(1-\sup _{x}D(x))\\[6pt]={}&\ln[\operatorname {E} _{x\sim \mu _{\text{ref}}}[D(x)](1-\sup _{x}D(x))]\leq \ln[\sup _{x}D(x))(1-\sup _{x}D(x))]\leq \ln {\frac {1}{4}},\end{aligned}}}


 with equality if 



D
(
x
)
=


1
2




{\displaystyle D(x)={\frac {1}{2}}}

, so
 



∀
x
∈
Ω
,




μ
^




D


(
x
)
=

δ


1
2



;


max


μ

D





min


μ

G




L
(

μ

G


,

μ

D


)
=
−
2
ln
⁡
2.


{\displaystyle \forall x\in \Omega ,{\hat {\mu }}_{D}(x)=\delta _{\frac {1}{2}};\quad \max _{\mu _{D}}\min _{\mu _{G}}L(\mu _{G},\mu _{D})=-2\ln 2.}


 Finally, to check that this is a Nash equilibrium, note that when 




μ

G


=

μ

ref




{\displaystyle \mu _{G}=\mu _{\text{ref}}}

, we have
 



L
(

μ

G


,

μ

D


)
:=

E

x
∼

μ

ref


,
y
∼

μ

D


(
x
)


⁡
[
ln
⁡
(
y
(
1
−
y
)
)
]


{\displaystyle L(\mu _{G},\mu _{D}):=\operatorname {E} _{x\sim \mu _{\text{ref}},y\sim \mu _{D}(x)}[\ln(y(1-y))]}


which is always maximized by 



y
=


1
2




{\displaystyle y={\frac {1}{2}}}

.
 When 



∀
x
∈
Ω
,

μ

D


(
x
)
=

δ


1
2





{\displaystyle \forall x\in \Omega ,\mu _{D}(x)=\delta _{\frac {1}{2}}}

, any strategy is optimal for the generator.
 While the GAN game has a unique global equilibrium point when both the generator and discriminator have access to their entire strategy sets, the equilibrium is no longer guaranteed when they have a restricted strategy set.[14]
 In practice, the generator has access only to measures of form 




μ

Z


∘

G

θ


−
1




{\displaystyle \mu _{Z}\circ G_{\theta }^{-1}}

, where 




G

θ




{\displaystyle G_{\theta }}

 is a function computed by a neural network with parameters 



θ


{\displaystyle \theta }

, and 




μ

Z




{\displaystyle \mu _{Z}}

 is an easily sampled distribution, such as the uniform or normal distribution. Similarly, the discriminator has access only to functions of form 




D

ζ




{\displaystyle D_{\zeta }}

, a function computed by a neural network with parameters 



ζ


{\displaystyle \zeta }

. These restricted strategy sets take up a vanishingly small proportion of their entire strategy sets.[15]
 Further, even if an equilibrium still exists, it can only be found by searching in the high-dimensional space of all possible neural network functions. The standard strategy of using gradient descent to find the equilibrium often does not work for GAN, and often the game ""collapses"" into one of several failure modes. To improve the convergence stability, some training strategies start with an easier task, such as generating low-resolution images[16] or simple images (one object with uniform background),[17] and gradually increase the difficulty of the task during training. This essentially translates to applying a curriculum learning scheme.[18]
 GANs often suffer from mode collapse where they fail to generalize properly, missing entire modes from the input data. For example, a GAN trained on the MNIST dataset containing many samples of each digit might only generate pictures of digit 0. This was termed ""the Helvetica scenario"".[3]
 One way this can happen is if the generator learns too fast compared to the discriminator. If the discriminator 



D


{\displaystyle D}

 is held constant, then the optimal generator would only output elements of 



arg
⁡

max

x


D
(
x
)


{\displaystyle \arg \max _{x}D(x)}

.[19] So for example, if during GAN training for generating MNIST dataset, for a few epochs, the discriminator somehow prefers the digit 0 slightly more than other digits, the generator may seize the opportunity to generate only digit 0, then be unable to escape the local minimum after the discriminator improves.
 Some researchers perceive the root problem to be a weak discriminative network that fails to notice the pattern of omission, while others assign blame to a bad choice of objective function. Many solutions have been proposed, but it is still an open problem.[20][21]
 Even the state-of-the-art architecture, BigGAN (2019), could not avoid mode collapse. The authors resorted to ""allowing collapse to occur at the later stages of training, by which time a model is sufficiently trained to achieve good results"".[22]
 The two time-scale update rule (TTUR) is proposed to make GAN convergence more stable by making the learning rate of the generator lower than that of the discriminator. The authors argued that the generator should move slower than the discriminator, so that it does not ""drive the discriminator steadily into new regions without capturing its gathered information"".
 They proved that a general class of games that included the GAN game, when trained under TTUR, ""converges under mild assumptions to a stationary local Nash equilibrium"".[23]
 They also proposed using the Adam stochastic optimization[24] to avoid mode collapse, as well as the Fréchet inception distance for evaluating GAN performances.
 Conversely, if the discriminator learns too fast compared to the generator, then the discriminator could almost perfectly distinguish 




μ


G

θ




,

μ

ref




{\displaystyle \mu _{G_{\theta }},\mu _{\text{ref}}}

. In such case, the generator 




G

θ




{\displaystyle G_{\theta }}

 could be stuck with a very high loss no matter which direction it changes its 



θ


{\displaystyle \theta }

, meaning that the gradient 




∇

θ


L
(

G

θ


,

D

ζ


)


{\displaystyle \nabla _{\theta }L(G_{\theta },D_{\zeta })}

 would be close to zero. In such case, the generator cannot learn, a case of the vanishing gradient problem.[15]
 Intuitively speaking, the discriminator is too good, and since the generator cannot take any small step (only small steps are considered in gradient descent) to improve its payoff, it does not even try.
 One important method for solving this problem is the Wasserstein GAN.
 GANs are usually evaluated by Inception score (IS), which measures how varied the generator's outputs are (as classified by an image classifier, usually Inception-v3), or Fréchet inception distance (FID), which measures how similar the generator's outputs are to a reference set (as classified by a learned image featurizer, such as Inception-v3 without its final layer). Many papers that propose new GAN architectures for image generation report how their architectures break the state of the art on FID or IS.
 Another evaluation method is the Learned Perceptual Image Patch Similarity (LPIPS), which starts with a learned image featurizer 




f

θ


:

Image

→


R


n




{\displaystyle f_{\theta }:{\text{Image}}\to \mathbb {R} ^{n}}

, and finetunes it by supervised learning on a set of 



(
x
,

x
′

,

p
e
r
c
e
p
t
u
a
l
 
d
i
f
f
e
r
e
n
c
e

⁡
(
x
,

x
′

)
)


{\displaystyle (x,x',\operatorname {perceptual~difference} (x,x'))}

, where 



x


{\displaystyle x}

 is an image, 




x
′



{\displaystyle x'}

 is a perturbed version of it, and 




p
e
r
c
e
p
t
u
a
l
 
d
i
f
f
e
r
e
n
c
e

⁡
(
x
,

x
′

)


{\displaystyle \operatorname {perceptual~difference} (x,x')}

 is how much they differ, as reported by human subjects. The model is finetuned so that it can approximate 



‖

f

θ


(
x
)
−

f

θ


(

x
′

)
‖
≈

p
e
r
c
e
p
t
u
a
l
 
d
i
f
f
e
r
e
n
c
e

⁡
(
x
,

x
′

)


{\displaystyle \|f_{\theta }(x)-f_{\theta }(x')\|\approx \operatorname {perceptual~difference} (x,x')}

. This finetuned model is then used to define 



LPIPS
⁡
(
x
,

x
′

)
:=
‖

f

θ


(
x
)
−

f

θ


(

x
′

)
‖


{\displaystyle \operatorname {LPIPS} (x,x'):=\|f_{\theta }(x)-f_{\theta }(x')\|}

.[25]
 Other evaluation methods are reviewed in.[26]
 There is a veritable zoo of GAN variants.[27] Some of the most prominent are as follows:
 Conditional GANs are similar to standard GANs except they allow the model to conditionally generate samples based on additional information. For example, if we want to generate a cat face given a dog picture, we could use a conditional GAN.
 The generator in a GAN game generates 




μ

G




{\displaystyle \mu _{G}}

, a probability distribution on the probability space 



Ω


{\displaystyle \Omega }

. This leads to the idea of a conditional GAN, where instead of generating one probability distribution on 



Ω


{\displaystyle \Omega }

, the generator generates a different probability distribution 




μ

G


(
c
)


{\displaystyle \mu _{G}(c)}

 on 



Ω


{\displaystyle \Omega }

, for each given class label 



c


{\displaystyle c}

.
 For example, for generating images that look like ImageNet, the generator should be able to generate a picture of cat when given the class label ""cat"".
 In the original paper,[3] the authors noted that GAN can be trivially extended to conditional GAN by providing the labels to both the generator and the discriminator.
 Concretely, the conditional GAN game is just the GAN game with class labels provided:



L
(

μ

G


,
D
)
:=

E

c
∼

μ

C


,
x
∼

μ

ref


(
c
)


⁡
[
ln
⁡
D
(
x
,
c
)
]
+

E

c
∼

μ

C


,
x
∼

μ

G


(
c
)


⁡
[
ln
⁡
(
1
−
D
(
x
,
c
)
)
]


{\displaystyle L(\mu _{G},D):=\operatorname {E} _{c\sim \mu _{C},x\sim \mu _{\text{ref}}(c)}[\ln D(x,c)]+\operatorname {E} _{c\sim \mu _{C},x\sim \mu _{G}(c)}[\ln(1-D(x,c))]}

where 




μ

C




{\displaystyle \mu _{C}}

 is a probability distribution over classes, 




μ

ref


(
c
)


{\displaystyle \mu _{\text{ref}}(c)}

 is the probability distribution of real images of class 



c


{\displaystyle c}

, and 




μ

G


(
c
)


{\displaystyle \mu _{G}(c)}

 the probability distribution of images generated by the generator when given class label 



c


{\displaystyle c}

.
 In 2017, a conditional GAN learned to generate 1000 image classes of ImageNet.[28]
 The GAN game is a general framework and can be run with any reasonable parametrization of the generator 



G


{\displaystyle G}

 and discriminator 



D


{\displaystyle D}

. In the original paper, the authors demonstrated it using multilayer perceptron networks and convolutional neural networks. Many alternative architectures have been tried.
 Deep convolutional GAN (DCGAN):[29] For both generator and discriminator, uses only deep networks consisting entirely of convolution-deconvolution layers, that is, fully convolutional networks.[30]
 Self-attention GAN (SAGAN):[31] Starts with the DCGAN, then adds residually-connected standard self-attention modules to the generator and discriminator.
 Variational autoencoder GAN (VAEGAN):[32] Uses a variational autoencoder (VAE) for the generator.
 Transformer GAN (TransGAN):[33] Uses the pure transformer architecture for both the generator and discriminator, entirely devoid of convolution-deconvolution layers.
 Flow-GAN:[34] Uses flow-based generative model for the generator, allowing efficient computation of the likelihood function.
 Many GAN variants are merely obtained by changing the loss functions for the generator and discriminator.
 Original GAN:
 We recast the original GAN objective into a form more convenient for comparison:





{




min

D



L

D


(
D
,

μ

G


)
=
−

E

x
∼

μ

G




⁡
[
ln
⁡
D
(
x
)
]
−

E

x
∼

μ

ref




⁡
[
ln
⁡
(
1
−
D
(
x
)
)
]





min

G



L

G


(
D
,

μ

G


)
=
−

E

x
∼

μ

G




⁡
[
ln
⁡
(
1
−
D
(
x
)
)
]








{\displaystyle {\begin{cases}\min _{D}L_{D}(D,\mu _{G})=-\operatorname {E} _{x\sim \mu _{G}}[\ln D(x)]-\operatorname {E} _{x\sim \mu _{\text{ref}}}[\ln(1-D(x))]\\\min _{G}L_{G}(D,\mu _{G})=-\operatorname {E} _{x\sim \mu _{G}}[\ln(1-D(x))]\end{cases}}}


 Original GAN, non-saturating loss:
 This objective for generator was recommended in the original paper for faster convergence.[3]




L

G


=

E

x
∼

μ

G




⁡
[
ln
⁡
D
(
x
)
]


{\displaystyle L_{G}=\operatorname {E} _{x\sim \mu _{G}}[\ln D(x)]}

The effect of using this objective is analyzed in Section 2.2.2 of Arjovsky et al.[35]
 Original GAN, maximum likelihood:
 




L

G


=

E

x
∼

μ

G




⁡
[
(

exp

∘

σ

−
1


∘
D
)
(
x
)
]


{\displaystyle L_{G}=\operatorname {E} _{x\sim \mu _{G}}[({\exp }\circ \sigma ^{-1}\circ D)(x)]}

where 



σ


{\displaystyle \sigma }

 is the logistic function. When the discriminator is optimal, the generator gradient is the same as in maximum likelihood estimation, even though GAN cannot perform maximum likelihood estimation itself.[36][37]
 Hinge loss GAN:[38]




L

D


=
−

E

x
∼

p

ref




⁡

[

min

(

0
,
−
1
+
D
(
x
)

)


]

−

E

x
∼

μ

G




⁡

[

min

(

0
,
−
1
−
D

(
x
)


)


]



{\displaystyle L_{D}=-\operatorname {E} _{x\sim p_{\text{ref}}}\left[\min \left(0,-1+D(x)\right)\right]-\operatorname {E} _{x\sim \mu _{G}}\left[\min \left(0,-1-D\left(x\right)\right)\right]}






L

G


=
−

E

x
∼

μ

G




⁡
[
D
(
x
)
]


{\displaystyle L_{G}=-\operatorname {E} _{x\sim \mu _{G}}[D(x)]}

Least squares GAN:[39]




L

D


=

E

x
∼

μ

ref




⁡
[
(
D
(
x
)
−
b

)

2


]
+

E

x
∼

μ

G




⁡
[
(
D
(
x
)
−
a

)

2


]


{\displaystyle L_{D}=\operatorname {E} _{x\sim \mu _{\text{ref}}}[(D(x)-b)^{2}]+\operatorname {E} _{x\sim \mu _{G}}[(D(x)-a)^{2}]}






L

G


=

E

x
∼

μ

G




⁡
[
(
D
(
x
)
−
c

)

2


]


{\displaystyle L_{G}=\operatorname {E} _{x\sim \mu _{G}}[(D(x)-c)^{2}]}

where 



a
,
b
,
c


{\displaystyle a,b,c}

 are parameters to be chosen. The authors recommended 



a
=
−
1
,
b
=
1
,
c
=
0


{\displaystyle a=-1,b=1,c=0}

.
 The Wasserstein GAN modifies the GAN game at two points:
 One of its purposes is to solve the problem of mode collapse (see above).[15] The authors claim ""In no experiment did we see evidence of mode collapse for the WGAN algorithm"".
 An adversarial autoencoder (AAE)[40] is more autoencoder than GAN. The idea is to start with a plain autoencoder, but train a discriminator to discriminate the latent vectors from a reference distribution (often the normal distribution).
 In conditional GAN, the generator receives both a noise vector 



z


{\displaystyle z}

 and a label 



c


{\displaystyle c}

, and produces an image 



G
(
z
,
c
)


{\displaystyle G(z,c)}

. The discriminator receives image-label pairs 



(
x
,
c
)


{\displaystyle (x,c)}

, and computes 



D
(
x
,
c
)


{\displaystyle D(x,c)}

.
 When the training dataset is unlabeled, conditional GAN does not work directly.
 The idea of InfoGAN is to decree that every latent vector in the latent space can be decomposed as 



(
z
,
c
)


{\displaystyle (z,c)}

: an incompressible noise part 



z


{\displaystyle z}

, and an informative label part 



c


{\displaystyle c}

, and encourage the generator to comply with the decree, by encouraging it to maximize 



I
(
c
,
G
(
z
,
c
)
)


{\displaystyle I(c,G(z,c))}

, the mutual information between 



c


{\displaystyle c}

 and 



G
(
z
,
c
)


{\displaystyle G(z,c)}

, while making no demands on the mutual information 



z


{\displaystyle z}

 between 



G
(
z
,
c
)


{\displaystyle G(z,c)}

.
 Unfortunately, 



I
(
c
,
G
(
z
,
c
)
)


{\displaystyle I(c,G(z,c))}

 is intractable in general, The key idea of InfoGAN is Variational Mutual Information Maximization:[41] indirectly maximize it by maximizing a lower bound






I
^



(
G
,
Q
)
=


E


z
∼

μ

Z


,
c
∼

μ

C




[
ln
⁡
Q
(
c
∣
G
(
z
,
c
)
)
]
;

I
(
c
,
G
(
z
,
c
)
)
≥

sup

Q





I
^



(
G
,
Q
)


{\displaystyle {\hat {I}}(G,Q)=\mathbb {E} _{z\sim \mu _{Z},c\sim \mu _{C}}[\ln Q(c\mid G(z,c))];\quad I(c,G(z,c))\geq \sup _{Q}{\hat {I}}(G,Q)}

where 



Q


{\displaystyle Q}

 ranges over all Markov kernels of type 



Q
:

Ω

Y


→


P


(

Ω

C


)


{\displaystyle Q:\Omega _{Y}\to {\mathcal {P}}(\Omega _{C})}

.
 
The InfoGAN game is defined as follows:[42] Three probability spaces define an InfoGAN game:
 There are 3 players in 2 teams: generator, Q, and discriminator. The generator and Q are on one team, and the discriminator on the other team.
 The objective function is



L
(
G
,
Q
,
D
)
=

L

G
A
N


(
G
,
D
)
−
λ



I
^



(
G
,
Q
)


{\displaystyle L(G,Q,D)=L_{GAN}(G,D)-\lambda {\hat {I}}(G,Q)}

where 




L

G
A
N


(
G
,
D
)
=

E

x
∼

μ

ref


,


⁡
[
ln
⁡
D
(
x
)
]
+

E

z
∼

μ

Z




⁡
[
ln
⁡
(
1
−
D
(
G
(
z
,
c
)
)
)
]


{\displaystyle L_{GAN}(G,D)=\operatorname {E} _{x\sim \mu _{\text{ref}},}[\ln D(x)]+\operatorname {E} _{z\sim \mu _{Z}}[\ln(1-D(G(z,c)))]}

 is the original GAN game objective, and 






I
^



(
G
,
Q
)
=


E


z
∼

μ

Z


,
c
∼

μ

C




[
ln
⁡
Q
(
c
∣
G
(
z
,
c
)
)
]


{\displaystyle {\hat {I}}(G,Q)=\mathbb {E} _{z\sim \mu _{Z},c\sim \mu _{C}}[\ln Q(c\mid G(z,c))]}


 
Generator-Q team aims to minimize the objective, and discriminator aims to maximize it:




min

G
,
Q



max

D


L
(
G
,
Q
,
D
)


{\displaystyle \min _{G,Q}\max _{D}L(G,Q,D)}

 The standard GAN generator is a function of type 



G
:

Ω

Z


→

Ω

X




{\displaystyle G:\Omega _{Z}\to \Omega _{X}}

, that is, it is a mapping from a latent space 




Ω

Z




{\displaystyle \Omega _{Z}}

 to the image space 




Ω

X




{\displaystyle \Omega _{X}}

. This can be understood as a ""decoding"" process, whereby every latent vector 



z
∈

Ω

Z




{\displaystyle z\in \Omega _{Z}}

 is a code for an image 



x
∈

Ω

X




{\displaystyle x\in \Omega _{X}}

, and the generator performs the decoding. This naturally leads to the idea of training another network that performs ""encoding"", creating an autoencoder out of the encoder-generator pair.
 Already in the original paper,[3] the authors noted that ""Learned approximate inference can be performed by training an auxiliary network to predict 



z


{\displaystyle z}

 given 



x


{\displaystyle x}

"". The bidirectional GAN architecture performs exactly this.[43]
 
The BiGAN is defined as follows:  Two probability spaces define a BiGAN game:
 There are 3 players in 2 teams: generator, encoder, and discriminator. The generator and encoder are on one team, and the discriminator on the other team.
 The generator's strategies are functions 



G
:

Ω

Z


→

Ω

X




{\displaystyle G:\Omega _{Z}\to \Omega _{X}}

, and the encoder's strategies are functions 



E
:

Ω

X


→

Ω

Z




{\displaystyle E:\Omega _{X}\to \Omega _{Z}}

. The discriminator's strategies are functions 



D
:

Ω

X


→
[
0
,
1
]


{\displaystyle D:\Omega _{X}\to [0,1]}

.
 The objective function is



L
(
G
,
E
,
D
)
=


E


x
∼

μ

X




[
ln
⁡
D
(
x
,
E
(
x
)
)
]
+


E


z
∼

μ

Z




[
ln
⁡
(
1
−
D
(
G
(
z
)
,
z
)
)
]


{\displaystyle L(G,E,D)=\mathbb {E} _{x\sim \mu _{X}}[\ln D(x,E(x))]+\mathbb {E} _{z\sim \mu _{Z}}[\ln(1-D(G(z),z))]}


 
Generator-encoder team aims to minimize the objective, and discriminator aims to maximize it:




min

G
,
E



max

D


L
(
G
,
E
,
D
)


{\displaystyle \min _{G,E}\max _{D}L(G,E,D)}

  In the paper, they gave a more abstract definition of the objective as:



L
(
G
,
E
,
D
)
=


E


(
x
,
z
)
∼

μ

E
,
X




[
ln
⁡
D
(
x
,
z
)
]
+


E


(
x
,
z
)
∼

μ

G
,
Z




[
ln
⁡
(
1
−
D
(
x
,
z
)
)
]


{\displaystyle L(G,E,D)=\mathbb {E} _{(x,z)\sim \mu _{E,X}}[\ln D(x,z)]+\mathbb {E} _{(x,z)\sim \mu _{G,Z}}[\ln(1-D(x,z))]}

where 




μ

E
,
X


(
d
x
,
d
z
)
=

μ

X


(
d
x
)
⋅

δ

E
(
x
)


(
d
z
)


{\displaystyle \mu _{E,X}(dx,dz)=\mu _{X}(dx)\cdot \delta _{E(x)}(dz)}

 is the probability distribution on 




Ω

X


×

Ω

Z




{\displaystyle \Omega _{X}\times \Omega _{Z}}

 obtained by pushing 




μ

X




{\displaystyle \mu _{X}}

 forward via 



x
↦
(
x
,
E
(
x
)
)


{\displaystyle x\mapsto (x,E(x))}

, and 




μ

G
,
Z


(
d
x
,
d
z
)
=

δ

G
(
z
)


(
d
x
)
⋅

μ

Z


(
d
z
)


{\displaystyle \mu _{G,Z}(dx,dz)=\delta _{G(z)}(dx)\cdot \mu _{Z}(dz)}

 is the probability distribution on 




Ω

X


×

Ω

Z




{\displaystyle \Omega _{X}\times \Omega _{Z}}

 obtained by pushing 




μ

Z




{\displaystyle \mu _{Z}}

 forward via 



z
↦
(
G
(
x
)
,
z
)


{\displaystyle z\mapsto (G(x),z)}

.
 Applications of bidirectional models include semi-supervised learning,[44] interpretable machine learning,[45] and neural machine translation.[46]
 CycleGAN is an architecture for performing translations between two domains, such as between photos of horses and photos of zebras, or photos of night cities and photos of day cities.
 
The CycleGAN game is defined as follows:[47] There are two probability spaces 



(

Ω

X


,

μ

X


)
,
(

Ω

Y


,

μ

Y


)


{\displaystyle (\Omega _{X},\mu _{X}),(\Omega _{Y},\mu _{Y})}

, corresponding to the two domains needed for translations fore-and-back.
 There are 4 players in 2 teams: generators 




G

X


:

Ω

X


→

Ω

Y


,

G

Y


:

Ω

Y


→

Ω

X




{\displaystyle G_{X}:\Omega _{X}\to \Omega _{Y},G_{Y}:\Omega _{Y}\to \Omega _{X}}

, and discriminators 




D

X


:

Ω

X


→
[
0
,
1
]
,

D

Y


:

Ω

Y


→
[
0
,
1
]


{\displaystyle D_{X}:\Omega _{X}\to [0,1],D_{Y}:\Omega _{Y}\to [0,1]}

.
 The objective function is



L
(

G

X


,

G

Y


,

D

X


,

D

Y


)
=

L

G
A
N


(

G

X


,

D

X


)
+

L

G
A
N


(

G

Y


,

D

Y


)
+
λ

L

c
y
c
l
e


(

G

X


,

G

Y


)


{\displaystyle L(G_{X},G_{Y},D_{X},D_{Y})=L_{GAN}(G_{X},D_{X})+L_{GAN}(G_{Y},D_{Y})+\lambda L_{cycle}(G_{X},G_{Y})}


 
where 



λ


{\displaystyle \lambda }

 is a positive adjustable parameter, 




L

G
A
N




{\displaystyle L_{GAN}}

 is the GAN game objective, and 




L

c
y
c
l
e




{\displaystyle L_{cycle}}

 is the cycle consistency loss:




L

c
y
c
l
e


(

G

X


,

G

Y


)
=

E

x
∼

μ

X




‖

G

X


(

G

Y


(
x
)
)
−
x
‖
+

E

y
∼

μ

Y




‖

G

Y


(

G

X


(
y
)
)
−
y
‖


{\displaystyle L_{cycle}(G_{X},G_{Y})=E_{x\sim \mu _{X}}\|G_{X}(G_{Y}(x))-x\|+E_{y\sim \mu _{Y}}\|G_{Y}(G_{X}(y))-y\|}

The generators aim to minimize the objective, and the discriminators aim to maximize it:




min


G

X


,

G

Y





max


D

X


,

D

Y




L
(

G

X


,

G

Y


,

D

X


,

D

Y


)


{\displaystyle \min _{G_{X},G_{Y}}\max _{D_{X},D_{Y}}L(G_{X},G_{Y},D_{X},D_{Y})}

  Unlike previous work like pix2pix,[48] which requires paired training data, cycleGAN requires no paired data. For example, to train a pix2pix model to turn a summer scenery photo to winter scenery photo and back, the dataset must contain pairs of the same place in summer and winter, shot at the same angle; cycleGAN would only need a set of summer scenery photos, and an unrelated set of winter scenery photos.
 The BigGAN is essentially a self-attention GAN trained on a large scale (up to 80 million parameters) to generate large images of ImageNet (up to 512 x 512 resolution), with numerous engineering tricks to make it converge.[22][49]
 When there is insufficient training data, the reference distribution 




μ

ref




{\displaystyle \mu _{\text{ref}}}

 cannot be well-approximated by the empirical distribution given by the training dataset. In such cases, data augmentation can be applied, to allow training GAN on smaller datasets. Naïve data augmentation, however, brings its problems.
 Consider the original GAN game, slightly reformulated as follows:





{




min

D



L

D


(
D
,

μ

G


)
=
−

E

x
∼

μ

ref




⁡
[
ln
⁡
D
(
x
)
]
−

E

x
∼

μ

G




⁡
[
ln
⁡
(
1
−
D
(
x
)
)
]





min

G



L

G


(
D
,

μ

G


)
=
−

E

x
∼

μ

G




⁡
[
ln
⁡
(
1
−
D
(
x
)
)
]








{\displaystyle {\begin{cases}\min _{D}L_{D}(D,\mu _{G})=-\operatorname {E} _{x\sim \mu _{\text{ref}}}[\ln D(x)]-\operatorname {E} _{x\sim \mu _{G}}[\ln(1-D(x))]\\\min _{G}L_{G}(D,\mu _{G})=-\operatorname {E} _{x\sim \mu _{G}}[\ln(1-D(x))]\end{cases}}}

Now we use data augmentation by randomly sampling semantic-preserving transforms 



T
:
Ω
→
Ω


{\displaystyle T:\Omega \to \Omega }

 and applying them to the dataset, to obtain the reformulated GAN game:





{




min

D



L

D


(
D
,

μ

G


)
=
−

E

x
∼

μ

ref


,
T
∼

μ

trans




⁡
[
ln
⁡
D
(
T
(
x
)
)
]
−

E

x
∼

μ

G




⁡
[
ln
⁡
(
1
−
D
(
x
)
)
]





min

G



L

G


(
D
,

μ

G


)
=
−

E

x
∼

μ

G




⁡
[
ln
⁡
(
1
−
D
(
x
)
)
]








{\displaystyle {\begin{cases}\min _{D}L_{D}(D,\mu _{G})=-\operatorname {E} _{x\sim \mu _{\text{ref}},T\sim \mu _{\text{trans}}}[\ln D(T(x))]-\operatorname {E} _{x\sim \mu _{G}}[\ln(1-D(x))]\\\min _{G}L_{G}(D,\mu _{G})=-\operatorname {E} _{x\sim \mu _{G}}[\ln(1-D(x))]\end{cases}}}

This is equivalent to a GAN game with a different distribution 




μ

ref

′



{\displaystyle \mu _{\text{ref}}'}

, sampled by 



T
(
x
)


{\displaystyle T(x)}

, with 



x
∼

μ

ref


,
T
∼

μ

trans




{\displaystyle x\sim \mu _{\text{ref}},T\sim \mu _{\text{trans}}}

. For example, if 




μ

ref




{\displaystyle \mu _{\text{ref}}}

 is the distribution of images in ImageNet, and 




μ

trans




{\displaystyle \mu _{\text{trans}}}

 samples identity-transform with probability 0.5, and horizontal-reflection with probability 0.5, then 




μ

ref

′



{\displaystyle \mu _{\text{ref}}'}

 is the distribution of images in ImageNet and horizontally-reflected ImageNet, combined.
 The result of such training would be a generator that mimics 




μ

ref

′



{\displaystyle \mu _{\text{ref}}'}

. For example, it would generate images that look like they are randomly cropped, if the data augmentation uses random cropping.
 The solution is to apply data augmentation to both generated and real images:





{




min

D



L

D


(
D
,

μ

G


)
=
−

E

x
∼

μ

ref


,
T
∼

μ

trans




⁡
[
ln
⁡
D
(
T
(
x
)
)
]
−

E

x
∼

μ

G


,
T
∼

μ

trans




⁡
[
ln
⁡
(
1
−
D
(
T
(
x
)
)
)
]





min

G



L

G


(
D
,

μ

G


)
=
−

E

x
∼

μ

G


,
T
∼

μ

trans




⁡
[
ln
⁡
(
1
−
D
(
T
(
x
)
)
)
]








{\displaystyle {\begin{cases}\min _{D}L_{D}(D,\mu _{G})=-\operatorname {E} _{x\sim \mu _{\text{ref}},T\sim \mu _{\text{trans}}}[\ln D(T(x))]-\operatorname {E} _{x\sim \mu _{G},T\sim \mu _{\text{trans}}}[\ln(1-D(T(x)))]\\\min _{G}L_{G}(D,\mu _{G})=-\operatorname {E} _{x\sim \mu _{G},T\sim \mu _{\text{trans}}}[\ln(1-D(T(x)))]\end{cases}}}

The authors demonstrated high-quality generation using just 100-picture-large datasets.[50]
 The StyleGAN-2-ADA paper points out a further point on data augmentation: it must be invertible.[51] Continue with the example of generating ImageNet pictures. If the data augmentation is ""randomly rotate the picture by 0, 90, 180, 270 degrees with equal probability"", then there is no way for the generator to know which is the true orientation: Consider two generators 



G
,

G
′



{\displaystyle G,G'}

, such that for any latent 



z


{\displaystyle z}

, the generated image 



G
(
z
)


{\displaystyle G(z)}

 is a 90-degree rotation of 




G
′

(
z
)


{\displaystyle G'(z)}

. They would have exactly the same expected loss, and so neither is preferred over the other.
 The solution is to only use invertible data augmentation: instead of ""randomly rotate the picture by 0, 90, 180, 270 degrees with equal probability"", use ""randomly rotate the picture by 90, 180, 270 degrees with 0.1 probability, and keep the picture as it is with 0.7 probability"". This way, the generator is still rewarded  to keep images oriented the same way as un-augmented ImageNet pictures.
 Abstractly, the effect of randomly sampling transformations 



T
:
Ω
→
Ω


{\displaystyle T:\Omega \to \Omega }

 from the distribution 




μ

trans




{\displaystyle \mu _{\text{trans}}}

 is to define a Markov kernel 




K

trans


:
Ω
→


P


(
Ω
)


{\displaystyle K_{\text{trans}}:\Omega \to {\mathcal {P}}(\Omega )}

. Then, the data-augmented GAN game pushes the generator to find some 







μ
^




G


∈


P


(
Ω
)


{\displaystyle {\hat {\mu }}_{G}\in {\mathcal {P}}(\Omega )}

, such that 




K

trans


∗

μ

ref


=

K

trans


∗




μ
^




G




{\displaystyle K_{\text{trans}}*\mu _{\text{ref}}=K_{\text{trans}}*{\hat {\mu }}_{G}}

where 



∗


{\displaystyle *}

 is the Markov kernel convolution.
A data-augmentation method is defined to be invertible if its Markov kernel 




K

trans




{\displaystyle K_{\text{trans}}}

 satisfies




K

trans


∗
μ
=

K

trans


∗

μ
′


⟹

μ
=

μ
′


∀
μ
,

μ
′

∈


P


(
Ω
)


{\displaystyle K_{\text{trans}}*\mu =K_{\text{trans}}*\mu '\implies \mu =\mu '\quad \forall \mu ,\mu '\in {\mathcal {P}}(\Omega )}

Immediately by definition, we see that composing multiple invertible data-augmentation methods results in yet another invertible method. Also by definition, if the data-augmentation method is invertible, then using it in a GAN game does not change the optimal strategy 







μ
^




G




{\displaystyle {\hat {\mu }}_{G}}

 for the generator, which is still 




μ

ref




{\displaystyle \mu _{\text{ref}}}

.
 There are two prototypical examples of invertible Markov kernels:
 Discrete case: Invertible stochastic matrices, when 



Ω


{\displaystyle \Omega }

 is finite.
 For example, if 



Ω
=
{
↑
,
↓
,
←
,
→
}


{\displaystyle \Omega =\{\uparrow ,\downarrow ,\leftarrow ,\rightarrow \}}

 is the set of four images of an arrow, pointing in 4 directions, and the data augmentation is ""randomly rotate the picture by 90, 180, 270 degrees with probability 



p


{\displaystyle p}

, and keep the picture as it is with probability 



(
1
−
3
p
)


{\displaystyle (1-3p)}

"", then the Markov kernel 




K

trans




{\displaystyle K_{\text{trans}}}

 can be represented as a stochastic matrix:



[

K

trans


]
=


[



(
1
−
3
p
)


p


p


p




p


(
1
−
3
p
)


p


p




p


p


(
1
−
3
p
)


p




p


p


p


(
1
−
3
p
)



]




{\displaystyle [K_{\text{trans}}]={\begin{bmatrix}(1-3p)&p&p&p\\p&(1-3p)&p&p\\p&p&(1-3p)&p\\p&p&p&(1-3p)\end{bmatrix}}}

 and 




K

trans




{\displaystyle K_{\text{trans}}}

 is an invertible kernel iff 



[

K

trans


]


{\displaystyle [K_{\text{trans}}]}

 is an invertible matrix, that is, 



p
≠
1

/

4


{\displaystyle p\neq 1/4}

.
 Continuous case: The gaussian kernel, when 



Ω
=


R


n




{\displaystyle \Omega =\mathbb {R} ^{n}}

 for some 



n
≥
1


{\displaystyle n\geq 1}

.
 For example, if 



Ω
=


R



256

2






{\displaystyle \Omega =\mathbb {R} ^{256^{2}}}

 is the space of 256x256 images, and the data-augmentation method is ""generate a gaussian noise 



z
∼


N


(
0
,

I


256

2




)


{\displaystyle z\sim {\mathcal {N}}(0,I_{256^{2}})}

, then add 



ϵ
z


{\displaystyle \epsilon z}

 to the image"", then 




K

trans




{\displaystyle K_{\text{trans}}}

 is just convolution by the density function of 





N


(
0
,

ϵ

2



I


256

2




)


{\displaystyle {\mathcal {N}}(0,\epsilon ^{2}I_{256^{2}})}

. This is invertible, because convolution by a gaussian is just convolution by the heat kernel, so given any 



μ
∈


P


(


R


n


)


{\displaystyle \mu \in {\mathcal {P}}(\mathbb {R} ^{n})}

, the convolved distribution 




K

trans


∗
μ


{\displaystyle K_{\text{trans}}*\mu }

 can be obtained by heating up 





R


n




{\displaystyle \mathbb {R} ^{n}}

 precisely according to 



μ


{\displaystyle \mu }

, then wait for time 




ϵ

2



/

4


{\displaystyle \epsilon ^{2}/4}

. With that, we can recover 



μ


{\displaystyle \mu }

 by running the heat equation backwards in time for 




ϵ

2



/

4


{\displaystyle \epsilon ^{2}/4}

.
 More examples of invertible data augmentations are found in the paper.[51]
 SinGAN pushes data augmentation to the limit, by using only a single image as training data and performing data augmentation on it. The GAN architecture is adapted to this training method by using a multi-scale pipeline.
 The generator 



G


{\displaystyle G}

 is decomposed into a pyramid of generators 



G
=

G

1


∘

G

2


∘
⋯
∘

G

N




{\displaystyle G=G_{1}\circ G_{2}\circ \cdots \circ G_{N}}

, with the lowest one generating the image 




G

N


(

z

N


)


{\displaystyle G_{N}(z_{N})}

 at the lowest resolution, then the generated image is scaled up to 



r
(

G

N


(

z

N


)
)


{\displaystyle r(G_{N}(z_{N}))}

, and fed to the next level to generate an image 




G

N
−
1


(

z

N
−
1


+
r
(

G

N


(

z

N


)
)
)


{\displaystyle G_{N-1}(z_{N-1}+r(G_{N}(z_{N})))}

 at a higher resolution, and so on. The discriminator is decomposed into a pyramid as well.[52]
 The StyleGAN family is a series of architectures published by Nvidia's research division.
 Progressive GAN[16] is a method for training GAN for large-scale image generation stably, by growing a GAN generator from small to large scale in a pyramidal fashion. Like SinGAN, it decomposes the generator as



G
=

G

1


∘

G

2


∘
⋯
∘

G

N




{\displaystyle G=G_{1}\circ G_{2}\circ \cdots \circ G_{N}}

, and the discriminator as 



D
=

D

1


∘

D

2


∘
⋯
∘

D

N




{\displaystyle D=D_{1}\circ D_{2}\circ \cdots \circ D_{N}}

.
 During training, at first only 




G

N


,

D

N




{\displaystyle G_{N},D_{N}}

 are used in a GAN game to generate 4x4 images. Then 




G

N
−
1


,

D

N
−
1




{\displaystyle G_{N-1},D_{N-1}}

 are added to reach the second stage of GAN game, to generate 8x8 images, and so on, until we reach a GAN game to generate 1024x1024 images.
 To avoid shock between stages of the GAN game, each new layer is ""blended in"" (Figure 2 of the paper[16]). For example, this is how the second stage GAN game starts:
 StyleGAN-1 is designed as a combination of Progressive GAN with neural style transfer.[53]
 The key architectural choice of StyleGAN-1 is a progressive growth mechanism, similar to Progressive GAN. Each generated image starts as a constant 



4
×
4
×
512


{\displaystyle 4\times 4\times 512}

 array, and repeatedly passed through style blocks. Each style block applies a ""style latent vector"" via affine transform (""adaptive instance normalization""), similar to how neural style transfer uses Gramian matrix. It then adds noise, and normalize (subtract the mean, then divide by the variance).
 At training time, usually only one style latent vector is used per image generated, but sometimes two (""mixing regularization"") in order to encourage each style block to independently perform its stylization without expecting help from other style blocks (since they might receive an entirely different style latent vector).
 After training, multiple style latent vectors can be fed into each style block. Those fed to the lower layers control the large-scale styles, and those fed to the higher layers control the fine-detail styles.
 Style-mixing between two images 



x
,

x
′



{\displaystyle x,x'}

 can be performed as well. First, run a gradient descent to find 



z
,

z
′



{\displaystyle z,z'}

 such that 



G
(
z
)
≈
x
,
G
(

z
′

)
≈

x
′



{\displaystyle G(z)\approx x,G(z')\approx x'}

. This is called ""projecting an image back to style latent space"". Then, 



z


{\displaystyle z}

 can be fed to the lower style blocks, and 




z
′



{\displaystyle z'}

 to the higher style blocks, to generate a composite image that has the large-scale style of 



x


{\displaystyle x}

, and the fine-detail style of 




x
′



{\displaystyle x'}

. Multiple images can also be composed this way.
 StyleGAN-2 improves upon StyleGAN-1, by using the style latent vector to transform the convolution layer's weights instead, thus solving the ""blob"" problem.[54]
 This was updated by the StyleGAN-2-ADA (""ADA"" stands for ""adaptive""),[51] which uses invertible data augmentation as described above. It also tunes the amount of data augmentation applied by starting at zero, and gradually increasing it until an ""overfitting heuristic"" reaches a target level, thus the name ""adaptive"".
 StyleGAN-3[55] improves upon StyleGAN-2 by solving the ""texture sticking"" problem, which can be seen in the official videos.[56] They analyzed the problem by the Nyquist–Shannon sampling theorem, and argued that the layers in the generator learned to exploit the high-frequency signal in the pixels they operate upon.
 To solve this, they proposed imposing strict lowpass filters between each generator's layers, so that the generator is forced to operate on the pixels in a way faithful to the continuous signals they represent, rather than operate on them as merely discrete signals. They further imposed rotational and translational invariance by using more signal filters. The resulting StyleGAN-3 is able to solve the texture sticking problem, as well as generating images that rotate and translate smoothly.
 Other than for generative and discriminative modelling of data, GANs have been used for other things.
 GANs have been used for transfer learning to enforce the alignment of the latent feature space, such as in deep reinforcement learning.[57] This works by feeding the embeddings of the source and target task to the discriminator which tries to guess the context. The resulting loss is then (inversely) backpropagated through the encoder.
 GAN-generated molecules were validated experimentally in mice.[74][75]
 One of the major concerns in medical imaging is preserving patient privacy. Due to these reasons, researchers often face difficulties in obtaining medical images for their research purposes. GAN has been used for generating synthetic medical images, such as MRI and PET images to address this challenge. [76]
 GAN can be used to detect glaucomatous images helping the early diagnosis which is essential to avoid partial or total loss of vision.[77]
 GANs have been used to create forensic facial reconstructions of deceased historical figures.[78]
 Concerns have been raised about the potential use of GAN-based human image synthesis for sinister purposes, e.g., to produce fake, possibly incriminating, photographs and videos.[79]
GANs can be used to generate unique, realistic profile photos of people who do not exist, in order to automate creation of fake social media profiles.[80]
 In 2019 the state of California considered[81] and passed on October 3, 2019, the bill AB-602, which bans the use of human image synthesis technologies to make fake pornography without the consent of the people depicted, and bill AB-730, which prohibits distribution of manipulated videos of a political candidate within 60 days of an election. Both bills were authored by Assembly member Marc Berman and signed by Governor Gavin Newsom. The laws went into effect in 2020.[82]
 DARPA's Media Forensics program studies ways to counteract fake media, including fake media produced using GANs.[83]
 GANs can be used to generate art; The Verge wrote in March 2019 that ""The images created by GANs have become the defining look of contemporary AI art.""[84] GANs can also be used to
 Some have worked with using GAN for artistic creativity, as ""creative adversarial network"".[90][91] A GAN, trained on a set of 15,000 portraits from WikiArt from the 14th to the 19th century, created the 2018 painting Edmond de Belamy, which sold for US$432,500.[92]
 GANs were used by the video game modding community to up-scale low-resolution 2D textures in old video games by recreating them in 4k or higher resolutions via image training, and then down-sampling them to fit the game's native resolution (resembling supersampling anti-aliasing).[93]
 In 2020, Artbreeder was used to create the main antagonist in the sequel to the psychological web horror series Ben Drowned. The author would later go on to praise GAN applications for their ability to help generate assets for independent artists who are short on budget and manpower.[94][95]
 In May 2020, Nvidia researchers taught an AI system (termed ""GameGAN"") to recreate the game of Pac-Man simply by watching it being played.[96][97]
 In August 2019, a large dataset consisting of 12,197 MIDI songs each with paired lyrics and melody alignment was created for neural melody generation from lyrics using conditional GAN-LSTM (refer to sources at GitHub AI Melody Generation from Lyrics).[98]
 GANs have been used to 
 In 1991, Juergen Schmidhuber published ""artificial curiosity"", neural networks in a zero-sum game.[110] The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. GANs can be regarded as a case where the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set.[111]
 Other people had similar ideas but did not develop them similarly. An idea involving adversarial networks was published in a 2010 blog post by Olli Niemitalo.[112] This idea was never implemented and did not involve stochasticity in the generator and thus was not a generative model. It is now known as a conditional GAN or cGAN.[113] An idea similar to GANs was used to model animal behavior by Li, Gauci and Gross in 2013.[114]
 Another inspiration for GANs was noise-contrastive estimation,[115] which uses the same loss function as GANs and which Goodfellow studied during his PhD in 2010–2014.
 Adversarial machine learning has other uses besides generative modeling and can be applied to models other than neural networks. In control theory, adversarial learning based on neural networks was used in 2006 to train robust controllers in a game theoretic sense, by alternating the iterations between a minimizer policy, the controller, and a maximizer policy, the disturbance.[116][117]
 In 2017, a GAN was used for image enhancement focusing on realistic textures rather than pixel-accuracy, producing a higher image quality at high magnification.[118] In 2017, the first faces were generated.[119] These were exhibited in February 2018 at the Grand Palais.[120][121] Faces generated by StyleGAN[122] in 2019 drew comparisons with Deepfakes.[123][124][125]
",gener adversari network gan class machin learn framework promin framework approach gener artifici intellig concept initi develop ian goodfellow colleagu june gan two neural network contest form game one agent gain anoth agent loss given train set techniqu learn gener new data statist train set exampl gan train photograph gener new photograph look least superfici authent human observ mani realist characterist though origin propos form gener model unsupervis learn gan also prove use learn fulli supervis learn reinforc learn core idea gan base indirect train discrimin anoth neural network tell realist input seem also updat dynam mean gener train minim distanc specif imag rather fool discrimin enabl model learn unsupervis manner gan similar mimicri evolutionari biolog evolutionari arm race network origin gan defin follow game probabl space ω μ ref ref defin gan game player gener discrimin gener strategi set p ω p set probabl measur μ g g ω discrimin strategi set set markov kernel μ ω p p p p set probabl measur gan game game object function l μ g μ e x μ ref μ x ln e x μ g μ x ln l g e ref x e g x gener aim minim object discrimin aim maxim object gener task approach μ g μ ref g ref match output distribut close possibl refer distribut discrimin task output valu close input appear refer distribut output valu close input look like came gener distribut gener network gener candid discrimin network evalu contest oper term data distribut typic gener network learn map latent space data distribut interest discrimin network distinguish candid produc gener true data distribut gener network train object increas error rate discrimin network fool discrimin network produc novel candid discrimin think synthes part true data distribut known dataset serv initi train data discrimin train involv present sampl train dataset achiev accept accuraci gener train base whether succe fool discrimin typic gener seed random input sampl predefin latent space multivari normal distribut thereaft candid synthes gener evalu discrimin independ backpropag procedur appli network gener produc better sampl discrimin becom skill flag synthet sampl use imag gener gener typic deconvolut neural network discrimin convolut neural network gan implicit gener model mean explicitli model likelihood function provid mean find latent variabl correspond given sampl unlik altern gener model compar fulli visibl belief network wavenet pixelrnn autoregress model gener gan gener one complet sampl one pass rather multipl pass network compar boltzmann machin linear ica restrict type function use network sinc neural network univers approxim gan asymptot consist variat autoencod might univers approxim proven section provid mathemat theori behind method modern probabl theori base measur theori probabl space also need equip result rigor definit gan game would make follow chang probabl space ω b μ ref b ref defin gan game gener strategi set p ω b p b set probabl measur μ g g ω b b discrimin strategi set set markov kernel μ ω b p b b p b b b borel sinc issu measur never aris practic concern us gener version gan game describ strategi set discrimin contain markov kernel μ ω p p strategi set gener contain arbitrari probabl distribut μ g g ω howev shown optim discrimin strategi μ g g determinist loss gener restrict discrimin strategi determinist function ω applic deep neural network function gener μ g g could theoret comput probabl distribut practic usual implement pushforward μ g μ z g g z start random variabl z μ z z μ z z probabl distribut easi comput uniform distribut gaussian distribut defin function g ω z ω g z distribut μ g g distribut g z g z consequ gener strategi usual defin g g leav z μ z z implicit formal gan game object l g e x μ ref ln x e z μ z ln g z l g e ref x e z g z gan architectur two main compon one cast optim game form min g max l g g l g differ usual kind optim form min θ l θ l decomposit μ g g μ z g z understood reparametr trick see signific one must compar gan previou method learn gener model plagu intract probabilist comput aris maximum likelihood estim relat strategi time kingma well rezend et al develop idea reparametr gener stochast backpropag method among first applic variat autoencod origin paper well subsequ paper usual assum gener move first discrimin move second thu give follow minimax game min μ g max μ l μ g μ e x μ ref μ x ln e x μ g μ x ln g l g e ref x e g x gener discrimin strategi set span finit number strategi minimax theorem min μ g max μ l μ g μ max μ min μ g l μ g μ g l g g l g move order matter howev sinc strategi set finit span minimax theorem appli idea equilibrium becom delic wit follow differ concept equilibrium gener game equilibria agre even exist origin gan game equilibria exist equal howev gener gan game necessarili exist agre origin gan paper prove follow two theorem theorem optim discrimin comput diverg fix gener strategi μ g g let optim repli arg max l μ g l g x μ ref μ ref μ g l μ g j μ ref μ g ln align x ref ref g l g js ref g align deriv deriv j js diverg jensen inequ e x μ ref μ x ln e x μ ref ln e μ x e ref x e ref e x similarli term therefor optim repli determinist μ x δ x x x function ω case l μ g μ e x μ ref ln x e x μ g ln x l g e ref x e g x defin suitabl densiti function defin base measur μ μ ref μ g ref g allow us take deriv ρ ref μ ref μ ρ g μ g μ ref ref g g ρ ref ρ g ref g l μ g μ μ x ρ ref x ln x ρ g x ln x l g dx ref x x g x x integrand neg two bernoulli random variabl paramet ρ ref x ref x x x write h ρ ref x k l ρ ref x x ref x kl ref x x h h binari entropi function l μ g μ μ x h ρ ref x k l ρ ref x x l g dx h ref x kl ref x x mean optim strategi discrimin x ρ ref x x ref x l μ g μ μ x h ρ ref x j μ ref μ g ln l g dx h ref x js ref g routin calcul interpret fix gener strategi μ g g optim discrimin keep track likelihood ratio refer distribut gener distribut x x μ ref μ g x μ ref x μ g x x σ ln μ ref x ln μ g x x x ref g x ref dx g dx x ref dx g dx σ logist function particular prior probabl imag x x come refer distribut equal x x posterior probabl x x came refer distribut x pr x came refer distribut x x x came refer distribut x theorem uniqu equilibrium point gan game exist pair μ μ g g sequenti equilibrium nash equilibrium l μ g μ min μ g max μ l μ g μ max μ min μ g l μ g μ ln μ arg max μ min μ g l μ g μ μ g arg min μ g max μ l μ g μ μ arg max μ l μ g μ μ g arg min μ g l μ g μ x ω μ x δ μ g μ ref align l g g l g g l g g l g g g l g l g g g l g x g ref align gener perfectli mimic refer discrimin output determinist input previou proposit arg min μ g max μ l μ g μ μ ref min μ g max μ l μ g μ ln g l g ref g l g fix discrimin strategi μ μ g g concentr set x e μ x ln inf x e μ x ln e x x e x optim strategi gener thu arg max μ min μ g l μ g μ arg max μ e x μ ref μ x ln inf x e μ x ln g l g e ref x x e x jensen inequ discrimin improv adopt determinist strategi alway play x e μ x x e x therefor arg max μ min μ g l μ g μ arg max e x μ ref ln x inf x ln x g l g e ref x x x jensen inequ ln e x μ ref x inf x ln x ln e x μ ref x ln sup x x ln e x μ ref x sup x x ln sup x x sup x x ln align e ref x x x e ref x x x e ref x x x x x x x align equal x x x ω μ x δ max μ min μ g l μ g μ ln x g l g final check nash equilibrium note μ g μ ref g ref l μ g μ e x μ ref μ x ln l g e ref x alway maxim x ω μ x δ x strategi optim gener gan game uniqu global equilibrium point gener discrimin access entir strategi set equilibrium longer guarante restrict strategi set practic gener access measur form μ z g θ z g θ function comput neural network paramet θ μ z z easili sampl distribut uniform normal distribut similarli discrimin access function form ζ function comput neural network paramet ζ restrict strategi set take vanishingli small proport entir strategi set even equilibrium still exist found search space possibl neural network function standard strategi use gradient descent find equilibrium often work gan often game collaps one sever failur mode improv converg stabil train strategi start easier task gener imag simpl imag one object uniform background gradual increas difficulti task train essenti translat appli curriculum learn scheme gan often suffer mode collaps fail gener properli miss entir mode input data exampl gan train mnist dataset contain mani sampl digit might gener pictur digit term helvetica scenario one way happen gener learn fast compar discrimin discrimin held constant optim gener would output element arg max x x x x exampl gan train gener mnist dataset epoch discrimin somehow prefer digit slightli digit gener may seiz opportun gener digit unabl escap local minimum discrimin improv research perceiv root problem weak discrimin network fail notic pattern omiss other assign blame bad choic object function mani solut propos still open problem even architectur biggan could avoid mode collaps author resort allow collaps occur later stage train time model suffici train achiev good result two updat rule ttur propos make gan converg stabl make learn rate gener lower discrimin author argu gener move slower discrimin drive discrimin steadili new region without captur gather inform prove gener class game includ gan game train ttur converg mild assumpt stationari local nash equilibrium also propos use adam stochast optim avoid mode collaps well fréchet incept distanc evalu gan perform convers discrimin learn fast compar gener discrimin could almost perfectli distinguish μ g θ μ ref ref case gener g θ could stuck high loss matter direct chang θ mean gradient θ l g θ ζ l would close zero case gener learn case vanish gradient problem intuit speak discrimin good sinc gener take small step small step consid gradient descent improv payoff even tri one import method solv problem wasserstein gan gan usual evalu incept score measur vari gener output classifi imag classifi usual fréchet incept distanc fid measur similar gener output refer set classifi learn imag featur without final layer mani paper propos new gan architectur imag gener report architectur break state art fid anoth evalu method learn perceptu imag patch similar lpip start learn imag featur f θ imag r n imag r n finetun supervis learn set x x p e r c e p u l f f e r e n c e x x x x x x x x imag x x perturb version p e r c e p u l f f e r e n c e x x x x much differ report human subject model finetun approxim f θ x f θ x p e r c e p u l f f e r e n c e x x x x x x finetun model use defin lpip x x f θ x f θ x lpip x x x x evalu method review verit zoo gan variant promin follow condit gan similar standard gan except allow model condit gener sampl base addit inform exampl want gener cat face given dog pictur could use condit gan gener gan game gener μ g g probabl distribut probabl space ω lead idea condit gan instead gener one probabl distribut ω gener gener differ probabl distribut μ g c g c ω given class label c c exampl gener imag look like imagenet gener abl gener pictur cat given class label cat origin paper author note gan trivial extend condit gan provid label gener discrimin concret condit gan game gan game class label provid l μ g e c μ c x μ ref c ln x c e c μ c x μ g c ln x c l g e c ref c x c e c g c x c μ c c probabl distribut class μ ref c ref c probabl distribut real imag class c c μ g c g c probabl distribut imag gener gener given class label c c condit gan learn gener imag class imagenet gan game gener framework run reason parametr gener g g discrimin origin paper author demonstr use multilay perceptron network convolut neural network mani altern architectur tri deep convolut gan dcgan gener discrimin use deep network consist entir layer fulli convolut network gan sagan start dcgan add standard modul gener discrimin variat autoencod gan vaegan use variat autoencod vae gener transform gan transgan use pure transform architectur gener discrimin entir devoid layer use gener model gener allow effici comput likelihood function mani gan variant mere obtain chang loss function gener discrimin origin gan recast origin gan object form conveni comparison min l μ g e x μ g ln x e x μ ref ln x min g l g μ g e x μ g ln x case g e g x e ref x g g g e g x case origin gan loss object gener recommend origin paper faster converg l g e x μ g ln x g e g x effect use object analyz section arjovski et al origin gan maximum likelihood l g e x μ g exp σ x g e g x σ logist function discrimin optim gener gradient maximum likelihood estim even though gan perform maximum likelihood estim hing loss gan l e x p ref min x e x μ g min x e ref x e g l g e x μ g x g e g x least squar gan l e x μ ref x b e x μ g x e ref x e g x l g e x μ g x c g e g x b c b c paramet chosen author recommend b c wasserstein gan modifi gan game two point one purpos solv problem mode collaps see author claim experi see evid mode collaps wgan algorithm adversari autoencod aae autoencod gan idea start plain autoencod train discrimin discrimin latent vector refer distribut often normal distribut condit gan gener receiv nois vector z z label c c produc imag g z c g z c discrimin receiv pair x c x c comput x c x c train dataset unlabel condit gan work directli idea infogan decre everi latent vector latent space decompos z c z c incompress nois part z z inform label part c c encourag gener compli decre encourag maxim c g z c c g z c mutual inform c c g z c g z c make demand mutual inform z z g z c g z c unfortun c g z c c g z c intract gener key idea infogan variat mutual inform maxim indirectli maxim maxim lower bound g q e z μ z c μ c ln q c g z c c g z c sup q g q g q e z c q g z c c g z c q g q q q rang markov kernel type q ω p ω c q p c infogan game defin follow three probabl space defin infogan game player team gener q discrimin gener q one team discrimin team object function l g q l g n g λ g q l g q gan g g q l g n g e x μ ref ln x e z μ z ln g z c gan g e ref x e z g z c origin gan game object g q e z μ z c μ c ln q c g z c g q e z c q g z c team aim minim object discrimin aim maxim min g q max l g q g q l g q standard gan gener function type g ω z ω x g z x map latent space ω z z imag space ω x x understood decod process wherebi everi latent vector z ω z z code imag x ω x x gener perform decod natur lead idea train anoth network perform encod creat autoencod pair alreadi origin paper author note learn approxim infer perform train auxiliari network predict z z given x x bidirect gan architectur perform exactli bigan defin follow two probabl space defin bigan game player team gener encod discrimin gener encod one team discrimin team gener strategi function g ω z ω x g z x encod strategi function e ω x ω z e x z discrimin strategi function ω x x object function l g e e x μ x ln x e x e z μ z ln g z z l g e e x x e x e z g z z team aim minim object discrimin aim maxim min g e max l g e g e l g e paper gave abstract definit object l g e e x z μ e x ln x z e x z μ g z ln x z l g e e x z e x x z e x z g z x z μ e x x z μ x x δ e x z e x dx dz x dx e x dz probabl distribut ω x ω z x z obtain push μ x x forward via x x e x x e x μ g z x z δ g z x μ z z g z dx dz g z dx z dz probabl distribut ω x ω z x z obtain push μ z z forward via z g x z g x z applic bidirect model includ learn interpret machin learn neural machin translat cyclegan architectur perform translat two domain photo hors photo zebra photo night citi photo day citi cyclegan game defin follow two probabl space ω x μ x ω μ x x correspond two domain need translat player team gener g x ω x ω g ω ω x x x x discrimin x ω x ω x x object function l g x g x l g n g x x l g n g λ l c c l e g x g l x x gan x x gan cycl x λ posit adjust paramet l g n gan gan game object l c c l e cycl cycl consist loss l c c l e g x g e x μ x g x g x x e μ g g x cycl x x x x x gener aim minim object discrimin aim maxim min g x g max x l g x g x x x l x x unlik previou work like requir pair train data cyclegan requir pair data exampl train model turn summer sceneri photo winter sceneri photo back dataset must contain pair place summer winter shot angl cyclegan would need set summer sceneri photo unrel set winter sceneri photo biggan essenti gan train larg scale million paramet gener larg imag imagenet x resolut numer engin trick make converg insuffici train data refer distribut μ ref ref empir distribut given train dataset case data augment appli allow train gan smaller dataset naïv data augment howev bring problem consid origin gan game slightli reformul follow min l μ g e x μ ref ln x e x μ g ln x min g l g μ g e x μ g ln x case g e ref x e g x g g g e g x case use data augment randomli sampl transform ω ω appli dataset obtain reformul gan game min l μ g e x μ ref μ tran ln x e x μ g ln x min g l g μ g e x μ g ln x case g e ref tran x e g x g g g e g x case equival gan game differ distribut μ ref ref sampl x x x μ ref μ tran ref tran exampl μ ref ref distribut imag imagenet μ tran tran sampl probabl probabl μ ref ref distribut imag imagenet imagenet combin result train would gener mimic μ ref ref exampl would gener imag look like randomli crop data augment use random crop solut appli data augment gener real imag min l μ g e x μ ref μ tran ln x e x μ g μ tran ln x min g l g μ g e x μ g μ tran ln x case g e ref tran x e g tran x g g g e g tran x case author demonstr gener use dataset paper point point data augment must invert continu exampl gener imagenet pictur data augment randomli rotat pictur degre equal probabl way gener know true orient consid two gener g g g g latent z z gener imag g z g z rotat g z g z would exactli expect loss neither prefer solut use invert data augment instead randomli rotat pictur degre equal probabl use randomli rotat pictur degre probabl keep pictur probabl way gener still reward keep imag orient way imagenet pictur abstractli effect randomli sampl transform ω ω distribut μ tran tran defin markov kernel k tran ω p ω tran p gan game push gener find μ g p ω g p k tran μ ref k tran μ g tran ref tran g markov kernel convolut method defin invert markov kernel k tran tran satisfi k tran μ k tran μ μ μ μ μ p ω tran tran p immedi definit see compos multipl invert method result yet anoth invert method also definit method invert use gan game chang optim strategi μ g g gener still μ ref ref two prototyp exampl invert markov kernel discret case invert stochast matric ω finit exampl ω set four imag arrow point direct data augment randomli rotat pictur degre probabl p p keep pictur probabl p markov kernel k tran tran repres stochast matrix k tran p p p p p p p p p p p p p p p p tran bmatrix p p p p p p bmatrix k tran tran invert kernel iff k tran tran invert matrix p continu case gaussian kernel ω r n r n n exampl ω r r space imag method gener gaussian nois z n n add ϵ z z imag k tran tran convolut densiti function n ϵ n invert convolut gaussian convolut heat kernel given μ p r n p r n convolv distribut k tran μ tran obtain heat r n r n precis accord μ wait time ϵ recov μ run heat equat backward time ϵ exampl invert data augment found paper singan push data augment limit use singl imag train data perform data augment gan architectur adapt train method use pipelin gener g g decompos pyramid gener g g g g n n lowest one gener imag g n z n n n lowest resolut gener imag scale r g n z n r n n fed next level gener imag g n z n r g n z n n n higher resolut discrimin decompos pyramid well stylegan famili seri architectur publish nvidia research divis progress gan method train gan imag gener stabli grow gan gener small larg scale pyramid fashion like singan decompos gener g g g g n n discrimin n n train first g n n n n use gan game gener imag g n n ad reach second stage gan game gener imag reach gan game gener imag avoid shock stage gan game new layer blend figur paper exampl second stage gan game start design combin progress gan neural style transfer key architectur choic progress growth mechan similar progress gan gener imag start constant array repeatedli pass style block style block appli style latent vector via affin transform adapt instanc normal similar neural style transfer use gramian matrix add nois normal subtract mean divid varianc train time usual one style latent vector use per imag gener sometim two mix regular order encourag style block independ perform styliz without expect help style block sinc might receiv entir differ style latent vector train multipl style latent vector fed style block fed lower layer control style fed higher layer control style two imag x x x x perform well first run gradient descent find z z z z g z x g z x g z x g z x call project imag back style latent space z z fed lower style block z z higher style block gener composit imag style x x style x x multipl imag also compos way improv upon use style latent vector transform convolut layer weight instead thu solv blob problem updat ada stand adapt use invert data augment describ also tune amount data augment appli start zero gradual increas overfit heurist reach target level thu name adapt improv upon solv textur stick problem seen offici video analyz problem sampl theorem argu layer gener learn exploit signal pixel oper upon solv propos impos strict lowpass filter gener layer gener forc oper pixel way faith continu signal repres rather oper mere discret signal impos rotat translat invari use signal filter result abl solv textur stick problem well gener imag rotat translat smoothli gener discrimin model data gan use thing gan use transfer learn enforc align latent featur space deep reinforc learn work feed embed sourc target task discrimin tri guess context result loss invers backpropag encod molecul valid experiment mice one major concern medic imag preserv patient privaci due reason research often face difficulti obtain medic imag research purpos gan use gener synthet medic imag mri pet imag address challeng gan use detect glaucomat imag help earli diagnosi essenti avoid partial total loss vision gan use creat forens facial reconstruct deceas histor figur concern rais potenti use human imag synthesi sinist purpos produc fake possibl incrimin photograph video gan use gener uniqu realist profil photo peopl exist order autom creation fake social media profil state california consid pass octob bill ban use human imag synthesi technolog make fake pornographi without consent peopl depict bill prohibit distribut manipul video polit candid within day elect bill author assembl member marc berman sign governor gavin newsom law went effect darpa media forens program studi way counteract fake media includ fake media produc use gan gan use gener art verg wrote march imag creat gan becom defin look contemporari ai art gan also use work use gan artist creativ creativ adversari network gan train set portrait wikiart centuri creat paint edmond de belami sold us gan use video game mod commun textur old video game recreat higher resolut via imag train fit game nativ resolut resembl supersampl artbreed use creat main antagonist sequel psycholog web horror seri ben drown author would later go prais gan applic abil help gener asset independ artist short budget manpow may nvidia research taught ai system term gamegan recreat game simpli watch play august larg dataset consist midi song pair lyric melodi align creat neural melodi gener lyric use condit refer sourc github ai melodi gener lyric gan use juergen schmidhub publish artifici curios neural network game first network gener model model probabl distribut output pattern second network learn gradient descent predict reaction environ pattern gan regard case environment reaction depend whether first network output given set peopl similar idea develop similarli idea involv adversari network publish blog post olli niemitalo idea never implement involv stochast gener thu gener model known condit gan cgan idea similar gan use model anim behavior li gauci gross anoth inspir gan estim use loss function gan goodfellow studi phd adversari machin learn use besid gener model appli model neural network control theori adversari learn base neural network use train robust control game theoret sens altern iter minim polici control maxim polici disturb gan use imag enhanc focus realist textur rather produc higher imag qualiti high magnif first face gener exhibit februari grand palai face gener stylegan drew comparison deepfak
Diffusion model,https://en.wikipedia.org/wiki/Diffusion_model,"In machine learning, diffusion models, also known as diffusion probabilistic models or score-based generative models, are a class of latent variable generative models. A diffusion model consists of three major components: the forward process, the reverse process, and the sampling procedure.[1] The goal of diffusion models is to learn a diffusion process for a given dataset, such that the process can generate new elements that are distributed similarly as the original dataset. A diffusion model models data as generated by a diffusion process, whereby a new datum performs a random walk with drift through the space of all possible data.[2] A trained diffusion model can be sampled in many ways, with different efficiency and quality.
 There are various equivalent formalisms, including Markov chains, denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations.[3] They are typically trained using variational inference.[4] The model responsible for denoising is typically called its ""backbone"". The backbone may be of any kind, but they are typically U-nets or transformers.
 As of 2024[update], diffusion models are mainly used for computer vision tasks, including image denoising, inpainting, super-resolution, image generation, and video generation. These typically involve training a neural network to sequentially denoise images blurred with Gaussian noise.[2][5] The model is trained to reverse the process of adding noise to an image. After training to convergence, it can be used for image generation by starting with an image composed of random noise, and applying the network iteratively to denoise the image.
 Diffusion-based image generators have seen widespread commercial interest, such as Stable Diffusion and DALL-E. These models typically combine diffusion models with other models, such as text-encoders and cross-attention modules to allow text-conditioned generation.[6]
 Other than computer vision, diffusion models have also found applications in natural language processing[7][8] such as text generation[9][10] and summarization,[11] sound generation,[12] and reinforcement learning.[13][14]
 Diffusion models were introduced in 2015 as a method to learn a model that can sample from a highly complex probability distribution. They used techniques from non-equilibrium thermodynamics, especially diffusion.[15]
 Consider, for example, how one might model the distribution of all naturally-occurring photos. Each image is a point in the space of all images, and the distribution of naturally-occurring photos is a ""cloud"" in space, which, by repeatedly adding noise to the images, diffuses out to the rest of the image space, until the cloud becomes all but indistinguishable from a Gaussian distribution 



N
(
0
,
I
)


{\displaystyle N(0,I)}

. A model that can approximately undo the diffusion can then be used to sample from the original distribution. This is studied in ""non-equilibrium"" thermodynamics, as the starting distribution is not in equilibrium, unlike the final distribution.
 The equilibrium distribution is the Gaussian distribution 



N
(
0
,
I
)


{\displaystyle N(0,I)}

, with pdf 



ρ
(
x
)
∝

e

−


1
2


‖
x

‖

2






{\displaystyle \rho (x)\propto e^{-{\frac {1}{2}}\|x\|^{2}}}

. This is just the Maxwell–Boltzmann distribution of particles in a potential well 



V
(
x
)
=


1
2


‖
x

‖

2




{\displaystyle V(x)={\frac {1}{2}}\|x\|^{2}}

 at temperature 1. The initial distribution, being very much out of equilibrium, would diffuse towards the equilibrium distribution, making biased random steps that are a sum of pure randomness (like a Brownian walker) and gradient descent down the potential well. The randomness is necessary: if the particles were to undergo only gradient descent, then they will all fall to the origin, collapsing the distribution.
 The 2020 paper proposed the Denoising Diffusion Probabilistic Model (DDPM), which improves upon the previous method by variational inference.[4][16]
 To present the model, we need some notation.
 A forward diffusion process starts at some starting point 




x

0


∼
q


{\displaystyle x_{0}\sim q}

, where 



q


{\displaystyle q}

 is the probability distribution to be learned, then repeatedly adds noise to it by




x

t


=


1
−

β

t





x

t
−
1


+



β

t





z

t




{\displaystyle x_{t}={\sqrt {1-\beta _{t}}}x_{t-1}+{\sqrt {\beta _{t}}}z_{t}}

where 




z

1


,
.
.
.
,

z

T




{\displaystyle z_{1},...,z_{T}}

 are IID samples from 



N
(
0
,
I
)


{\displaystyle N(0,I)}

. This is designed so that for any starting distribution of 




x

0




{\displaystyle x_{0}}

, we have 




lim

t



x

t



|


x

0




{\displaystyle \lim _{t}x_{t}|x_{0}}

 converging to 



N
(
0
,
I
)


{\displaystyle N(0,I)}

.
 The entire diffusion process then satisfies



q
(

x

0
:
T


)
=
q
(

x

0


)
q
(

x

1



|


x

0


)
⋯
q
(

x

T



|


x

T
−
1


)
=
q
(

x

0


)
N
(

x

1



|




α

1





x

0


,

β

1


I
)
⋯
N
(

x

T



|




α

T





x

T
−
1


,

β

T


I
)


{\displaystyle q(x_{0:T})=q(x_{0})q(x_{1}|x_{0})\cdots q(x_{T}|x_{T-1})=q(x_{0})N(x_{1}|{\sqrt {\alpha _{1}}}x_{0},\beta _{1}I)\cdots N(x_{T}|{\sqrt {\alpha _{T}}}x_{T-1},\beta _{T}I)}

or



ln
⁡
q
(

x

0
:
T


)
=
ln
⁡
q
(

x

0


)
−

∑

t
=
1


T




1

2

β

t





‖

x

t


−


1
−

β

t





x

t
−
1



‖

2


+
C


{\displaystyle \ln q(x_{0:T})=\ln q(x_{0})-\sum _{t=1}^{T}{\frac {1}{2\beta _{t}}}\|x_{t}-{\sqrt {1-\beta _{t}}}x_{t-1}\|^{2}+C}

where 



C


{\displaystyle C}

 is a normalization constant and often omitted. In particular, we note that 




x

1
:
T



|


x

0




{\displaystyle x_{1:T}|x_{0}}

 is a gaussian process, which affords us considerable freedom in reparameterization. For example, by standard manipulation with gaussian process, 




x

t



|


x

0


∼
N

(







α
¯




t





x

0


,

σ

t


2


I

)



{\displaystyle x_{t}|x_{0}\sim N\left({\sqrt {{\bar {\alpha }}_{t}}}x_{0},\sigma _{t}^{2}I\right)}






x

t
−
1



|


x

t


,

x

0


∼
N
(




μ
~




t


(

x

t


,

x

0


)
,




σ
~




t


2


I
)


{\displaystyle x_{t-1}|x_{t},x_{0}\sim N({\tilde {\mu }}_{t}(x_{t},x_{0}),{\tilde {\sigma }}_{t}^{2}I)}

In particular, notice that for large 



t


{\displaystyle t}

, the variable 




x

t



|


x

0


∼
N

(







α
¯




t





x

0


,

σ

t


2


I

)



{\displaystyle x_{t}|x_{0}\sim N\left({\sqrt {{\bar {\alpha }}_{t}}}x_{0},\sigma _{t}^{2}I\right)}

 converges to 



N
(
0
,
I
)


{\displaystyle N(0,I)}

. That is, after a long enough diffusion process, we end up with some 




x

T




{\displaystyle x_{T}}

 that is very close to 



N
(
0
,
I
)


{\displaystyle N(0,I)}

, with all traces of the original 




x

0


∼
q


{\displaystyle x_{0}\sim q}

 gone.
 For example, since




x

t



|


x

0


∼
N

(







α
¯




t





x

0


,

σ

t


2


I

)



{\displaystyle x_{t}|x_{0}\sim N\left({\sqrt {{\bar {\alpha }}_{t}}}x_{0},\sigma _{t}^{2}I\right)}

we can sample 




x

t



|


x

0




{\displaystyle x_{t}|x_{0}}

 directly ""in one step"", instead of going through all the intermediate steps 




x

1


,

x

2


,
.
.
.
,

x

t
−
1




{\displaystyle x_{1},x_{2},...,x_{t-1}}

.
 We know 




x

t
−
1



|


x

0




{\textstyle x_{t-1}|x_{0}}

 is a gaussian, and 




x

t



|


x

t
−
1




{\textstyle x_{t}|x_{t-1}}

 is another gaussian. We also know that these are independent. Thus we can perform a reparameterization: 




x

t
−
1


=






α
¯




t
−
1





x

0


+


1
−




α
¯




t
−
1




z


{\displaystyle x_{t-1}={\sqrt {{\bar {\alpha }}_{t-1}}}x_{0}+{\sqrt {1-{\bar {\alpha }}_{t-1}}}z}

 




x

t


=



α

t





x

t
−
1


+


1
−

α

t





z
′



{\displaystyle x_{t}={\sqrt {\alpha _{t}}}x_{t-1}+{\sqrt {1-\alpha _{t}}}z'}

 where 



z
,

z
′



{\textstyle z,z'}

 are IID gaussians.
 There are 5 variables 




x

0


,

x

t
−
1


,

x

t


,
z
,

z
′



{\textstyle x_{0},x_{t-1},x_{t},z,z'}

 and two linear equations. The two sources of randomness are 



z
,

z
′



{\textstyle z,z'}

, which can be reparameterized by rotation, since the IID gaussian distribution is rotationally symmetric.
 By plugging in the equations, we can solve for the first reparameterization: 




x

t


=






α
¯




t





x

0


+







α

t


−




α
¯




t




z
+


1
−

α

t





z
′


⏟



=

σ

t



z
″





{\displaystyle x_{t}={\sqrt {{\bar {\alpha }}_{t}}}x_{0}+\underbrace {{\sqrt {\alpha _{t}-{\bar {\alpha }}_{t}}}z+{\sqrt {1-\alpha _{t}}}z'} _{=\sigma _{t}z''}}

 where 




z
″



{\textstyle z''}

 is a gaussian with mean zero and variance one.
 To find the second one, we complete the rotational matrix: 





[




z
″






z
‴




]


=


[







α

t


−




α
¯




t




σ

t










β

t




σ

t








?


?



]




[



z





z
′




]




{\displaystyle {\begin{bmatrix}z''\\z'''\end{bmatrix}}={\begin{bmatrix}{\frac {\sqrt {\alpha _{t}-{\bar {\alpha }}_{t}}}{\sigma _{t}}}&{\frac {\sqrt {\beta _{t}}}{\sigma _{t}}}\\?&?\end{bmatrix}}{\begin{bmatrix}z\\z'\end{bmatrix}}}


 Since rotational matrices are all of the form 





[



cos
⁡
θ


sin
⁡
θ




−
sin
⁡
θ


cos
⁡
θ



]




{\textstyle {\begin{bmatrix}\cos \theta &\sin \theta \\-\sin \theta &\cos \theta \end{bmatrix}}}

, we know the matrix must be 





[




z
″






z
‴




]


=


[







α

t


−




α
¯




t




σ

t










β

t




σ

t








−




β

t




σ

t










α

t


−




α
¯




t




σ

t







]




[



z





z
′




]




{\displaystyle {\begin{bmatrix}z''\\z'''\end{bmatrix}}={\begin{bmatrix}{\frac {\sqrt {\alpha _{t}-{\bar {\alpha }}_{t}}}{\sigma _{t}}}&{\frac {\sqrt {\beta _{t}}}{\sigma _{t}}}\\-{\frac {\sqrt {\beta _{t}}}{\sigma _{t}}}&{\frac {\sqrt {\alpha _{t}-{\bar {\alpha }}_{t}}}{\sigma _{t}}}\end{bmatrix}}{\begin{bmatrix}z\\z'\end{bmatrix}}}

 and since the inverse of rotational matrix is its transpose,






[



z





z
′




]


=


[







α

t


−




α
¯




t




σ

t






−




β

t




σ

t












β

t




σ

t










α

t


−




α
¯




t




σ

t







]




[




z
″






z
‴




]




{\displaystyle {\begin{bmatrix}z\\z'\end{bmatrix}}={\begin{bmatrix}{\frac {\sqrt {\alpha _{t}-{\bar {\alpha }}_{t}}}{\sigma _{t}}}&-{\frac {\sqrt {\beta _{t}}}{\sigma _{t}}}\\{\frac {\sqrt {\beta _{t}}}{\sigma _{t}}}&{\frac {\sqrt {\alpha _{t}-{\bar {\alpha }}_{t}}}{\sigma _{t}}}\end{bmatrix}}{\begin{bmatrix}z''\\z'''\end{bmatrix}}}


 Plugging back, and simplifying, we have 




x

t


=






α
¯




t





x

0


+

σ

t



z
″



{\displaystyle x_{t}={\sqrt {{\bar {\alpha }}_{t}}}x_{0}+\sigma _{t}z''}

 




x

t
−
1


=




μ
~




t


(

x

t


,

x

0


)
−




σ
~




t



z
‴



{\displaystyle x_{t-1}={\tilde {\mu }}_{t}(x_{t},x_{0})-{\tilde {\sigma }}_{t}z'''}


 The key idea of DDPM is to use a neural network parametrized by 



θ


{\displaystyle \theta }

. The network takes in two arguments 




x

t


,
t


{\displaystyle x_{t},t}

, and outputs a vector 




μ

θ


(

x

t


,
t
)


{\displaystyle \mu _{\theta }(x_{t},t)}

 and a matrix 




Σ

θ


(

x

t


,
t
)


{\displaystyle \Sigma _{\theta }(x_{t},t)}

, such that each step in the forward diffusion process can be approximately undone by 




x

t
−
1


∼
N
(

μ

θ


(

x

t


,
t
)
,

Σ

θ


(

x

t


,
t
)
)


{\displaystyle x_{t-1}\sim N(\mu _{\theta }(x_{t},t),\Sigma _{\theta }(x_{t},t))}

. This then gives us a backward diffusion process 




p

θ




{\displaystyle p_{\theta }}

 defined by




p

θ


(

x

T


)
=
N
(

x

T



|

0
,
I
)


{\displaystyle p_{\theta }(x_{T})=N(x_{T}|0,I)}






p

θ


(

x

t
−
1



|


x

t


)
=
N
(

x

t
−
1



|


μ

θ


(

x

t


,
t
)
,

Σ

θ


(

x

t


,
t
)
)


{\displaystyle p_{\theta }(x_{t-1}|x_{t})=N(x_{t-1}|\mu _{\theta }(x_{t},t),\Sigma _{\theta }(x_{t},t))}

The goal now is to learn the parameters such that 




p

θ


(

x

0


)


{\displaystyle p_{\theta }(x_{0})}

 is as close to 



q
(

x

0


)


{\displaystyle q(x_{0})}

 as possible. To do that, we use maximum likelihood estimation with variational inference.
 The ELBO inequality states that 



ln
⁡

p

θ


(

x

0


)
≥

E


x

1
:
T


∼
q
(
⋅

|


x

0


)


[
ln
⁡

p

θ


(

x

0
:
T


)
−
ln
⁡
q
(

x

1
:
T



|


x

0


)
]


{\displaystyle \ln p_{\theta }(x_{0})\geq E_{x_{1:T}\sim q(\cdot |x_{0})}[\ln p_{\theta }(x_{0:T})-\ln q(x_{1:T}|x_{0})]}

, and taking one more expectation, we get




E


x

0


∼
q


[
ln
⁡

p

θ


(

x

0


)
]
≥

E


x

0
:
T


∼
q


[
ln
⁡

p

θ


(

x

0
:
T


)
−
ln
⁡
q
(

x

1
:
T



|


x

0


)
]


{\displaystyle E_{x_{0}\sim q}[\ln p_{\theta }(x_{0})]\geq E_{x_{0:T}\sim q}[\ln p_{\theta }(x_{0:T})-\ln q(x_{1:T}|x_{0})]}

We see that maximizing the quantity on the right would give us a lower bound on the likelihood of observed data. This allows us to perform variational inference.
 Define the loss function



L
(
θ
)
:=
−

E


x

0
:
T


∼
q


[
ln
⁡

p

θ


(

x

0
:
T


)
−
ln
⁡
q
(

x

1
:
T



|


x

0


)
]


{\displaystyle L(\theta ):=-E_{x_{0:T}\sim q}[\ln p_{\theta }(x_{0:T})-\ln q(x_{1:T}|x_{0})]}

and now the goal is to minimize the loss by stochastic gradient descent. The expression may be simplified to[17]



L
(
θ
)
=

∑

t
=
1


T



E


x

t
−
1


,

x

t


∼
q


[
−
ln
⁡

p

θ


(

x

t
−
1



|


x

t


)
]
+

E


x

0


∼
q


[

D

K
L


(
q
(

x

T



|


x

0


)
‖

p

θ


(

x

T


)
)
]
+
C


{\displaystyle L(\theta )=\sum _{t=1}^{T}E_{x_{t-1},x_{t}\sim q}[-\ln p_{\theta }(x_{t-1}|x_{t})]+E_{x_{0}\sim q}[D_{KL}(q(x_{T}|x_{0})\|p_{\theta }(x_{T}))]+C}

where 



C


{\displaystyle C}

 does not depend on the parameter, and thus can be ignored. Since 




p

θ


(

x

T


)
=
N
(

x

T



|

0
,
I
)


{\displaystyle p_{\theta }(x_{T})=N(x_{T}|0,I)}

 also does not depend on the parameter, the term 




E


x

0


∼
q


[

D

K
L


(
q
(

x

T



|


x

0


)
‖

p

θ


(

x

T


)
)
]


{\displaystyle E_{x_{0}\sim q}[D_{KL}(q(x_{T}|x_{0})\|p_{\theta }(x_{T}))]}

 can also be ignored. This leaves just 



L
(
θ
)
=

∑

t
=
1


T



L

t




{\displaystyle L(\theta )=\sum _{t=1}^{T}L_{t}}

 with 




L

t


=

E


x

t
−
1


,

x

t


∼
q


[
−
ln
⁡

p

θ


(

x

t
−
1



|


x

t


)
]


{\displaystyle L_{t}=E_{x_{t-1},x_{t}\sim q}[-\ln p_{\theta }(x_{t-1}|x_{t})]}

 to be minimized.
 Since 




x

t
−
1



|


x

t


,

x

0


∼
N
(




μ
~




t


(

x

t


,

x

0


)
,




σ
~




t


2


I
)


{\displaystyle x_{t-1}|x_{t},x_{0}\sim N({\tilde {\mu }}_{t}(x_{t},x_{0}),{\tilde {\sigma }}_{t}^{2}I)}

, this suggests that we should use 




μ

θ


(

x

t


,
t
)
=




μ
~




t


(

x

t


,

x

0


)


{\displaystyle \mu _{\theta }(x_{t},t)={\tilde {\mu }}_{t}(x_{t},x_{0})}

; however, the network does not have access to 




x

0




{\displaystyle x_{0}}

, and so it has to estimate it instead. Now, since 




x

t



|


x

0


∼
N

(







α
¯




t





x

0


,

σ

t


2


I

)



{\displaystyle x_{t}|x_{0}\sim N\left({\sqrt {{\bar {\alpha }}_{t}}}x_{0},\sigma _{t}^{2}I\right)}

, we may write 




x

t


=






α
¯




t





x

0


+

σ

t


z


{\displaystyle x_{t}={\sqrt {{\bar {\alpha }}_{t}}}x_{0}+\sigma _{t}z}

, where 



z


{\displaystyle z}

 is some unknown gaussian noise. Now we see that estimating 




x

0




{\displaystyle x_{0}}

 is equivalent to estimating 



z


{\displaystyle z}

.
 Therefore, let the network output a noise vector 




ϵ

θ


(

x

t


,
t
)


{\displaystyle \epsilon _{\theta }(x_{t},t)}

, and let it predict




μ

θ


(

x

t


,
t
)
=




μ
~




t



(


x

t


,




x

t


−

σ

t



ϵ

θ


(

x

t


,
t
)






α
¯




t






)

=




x

t


−

ϵ

θ


(

x

t


,
t
)

β

t



/


σ

t





α

t







{\displaystyle \mu _{\theta }(x_{t},t)={\tilde {\mu }}_{t}\left(x_{t},{\frac {x_{t}-\sigma _{t}\epsilon _{\theta }(x_{t},t)}{\sqrt {{\bar {\alpha }}_{t}}}}\right)={\frac {x_{t}-\epsilon _{\theta }(x_{t},t)\beta _{t}/\sigma _{t}}{\sqrt {\alpha _{t}}}}}

It remains to design 




Σ

θ


(

x

t


,
t
)


{\displaystyle \Sigma _{\theta }(x_{t},t)}

. The DDPM paper suggested not learning it (since it resulted in ""unstable training and poorer sample quality""), but fixing it at some value 




Σ

θ


(

x

t


,
t
)
=

ζ

t


2


I


{\displaystyle \Sigma _{\theta }(x_{t},t)=\zeta _{t}^{2}I}

, where either 




ζ

t


2


=

β

t



 or 





σ
~




t


2




{\displaystyle \zeta _{t}^{2}=\beta _{t}{\text{ or }}{\tilde {\sigma }}_{t}^{2}}

 yielded similar performance.
 With this, the loss simplifies to 




L

t


=



β

t


2



2

α

t



σ

t


2



ζ

t


2






E


x

0


∼
q
;
z
∼
N
(
0
,
I
)



[


‖


ϵ

θ


(

x

t


,
t
)
−
z

‖


2


]

+
C


{\displaystyle L_{t}={\frac {\beta _{t}^{2}}{2\alpha _{t}\sigma _{t}^{2}\zeta _{t}^{2}}}E_{x_{0}\sim q;z\sim N(0,I)}\left[\left\|\epsilon _{\theta }(x_{t},t)-z\right\|^{2}\right]+C}

which may be minimized by stochastic gradient descent. The paper noted empirically that an even simpler loss function




L

s
i
m
p
l
e
,
t


=

E


x

0


∼
q
;
z
∼
N
(
0
,
I
)



[


‖


ϵ

θ


(

x

t


,
t
)
−
z

‖


2


]



{\displaystyle L_{simple,t}=E_{x_{0}\sim q;z\sim N(0,I)}\left[\left\|\epsilon _{\theta }(x_{t},t)-z\right\|^{2}\right]}

resulted in better models.
 After a noise prediction network is trained, it can be used for generating data points in the original distribution in a loop as follows:
 Score-based generative model is another formulation of diffusion modelling. They are also called noise conditional score network (NCSN) or score-matching with Langevin dynamics (SMLD).[18][19][20][21]
 Consider the problem of image generation. Let 



x


{\displaystyle x}

 represent an image, and let 



q
(
x
)


{\displaystyle q(x)}

 be the probability distribution over all possible images. If we have 



q
(
x
)


{\displaystyle q(x)}

 itself, then we can say for certain how likely a certain image is. However, this is intractable in general.
 Most often, we are uninterested in knowing the absolute probability of a certain image. Instead, we are usually only interested in knowing how likely a certain image is compared to its immediate neighbors — e.g. how much more likely is an image of cat compared to some small variants of it? Is it more likely if the image contains two whiskers, or three, or with some Gaussian noise added?
 Consequently, we are actually quite uninterested in 



q
(
x
)


{\displaystyle q(x)}

 itself, but rather, 




∇

x


ln
⁡
q
(
x
)


{\displaystyle \nabla _{x}\ln q(x)}

. This has two major effects:
 Let the score function be 



s
(
x
)
:=

∇

x


ln
⁡
q
(
x
)


{\displaystyle s(x):=\nabla _{x}\ln q(x)}

; then consider what we can do with 



s
(
x
)


{\displaystyle s(x)}

.
 As it turns out, 



s
(
x
)


{\displaystyle s(x)}

 allows us to sample from 



q
(
x
)


{\displaystyle q(x)}

 using thermodynamics. Specifically, if we have a potential energy function 



U
(
x
)
=
−
ln
⁡
q
(
x
)


{\displaystyle U(x)=-\ln q(x)}

, and a lot of particles in the potential well, then the distribution at thermodynamic equilibrium is the Boltzmann distribution 




q

U


(
x
)
∝

e

−
U
(
x
)

/


k

B


T


=
q
(
x

)

1

/


k

B


T




{\displaystyle q_{U}(x)\propto e^{-U(x)/k_{B}T}=q(x)^{1/k_{B}T}}

. At temperature 




k

B


T
=
1


{\displaystyle k_{B}T=1}

, the Boltzmann distribution is exactly 



q
(
x
)


{\displaystyle q(x)}

.
 Therefore, to model 



q
(
x
)


{\displaystyle q(x)}

, we may start with a particle sampled at any convenient distribution (such as the standard gaussian distribution), then simulate the motion of the particle forwards according to the Langevin equation




d

x

t


=
−

∇


x

t




U
(

x

t


)
d
t
+
d

W

t




{\displaystyle dx_{t}=-\nabla _{x_{t}}U(x_{t})dt+dW_{t}}


and the Boltzmann distribution is, by Fokker-Planck equation, the unique thermodynamic equilibrium. So no matter what distribution 




x

0




{\displaystyle x_{0}}

 has, the distribution of 




x

t




{\displaystyle x_{t}}

 converges in distribution to 



q


{\displaystyle q}

 as 



t
→
∞


{\displaystyle t\to \infty }

.
 Given a density 



q


{\displaystyle q}

, we wish to learn a score function approximation 




f

θ


≈
∇
ln
⁡
q


{\displaystyle f_{\theta }\approx \nabla \ln q}

. This is score matching.[22] Typically, score matching is formalized as minimizing Fisher divergence function 




E

q


[
‖

f

θ


(
x
)
−
∇
ln
⁡
q
(
x
)

‖

2


]


{\displaystyle E_{q}[\|f_{\theta }(x)-\nabla \ln q(x)\|^{2}]}

. By expanding the integral, and performing an integration by parts, 




E

q


[
‖

f

θ


(
x
)
−
∇
ln
⁡
q
(
x
)

‖

2


]
=

E

q


[
‖

f

θ



‖

2


+
2

∇

2


⋅

f

θ


]
+
C


{\displaystyle E_{q}[\|f_{\theta }(x)-\nabla \ln q(x)\|^{2}]=E_{q}[\|f_{\theta }\|^{2}+2\nabla ^{2}\cdot f_{\theta }]+C}

giving us a loss function, also known as the Hyvärinen scoring rule, that can be minimized by stochastic gradient descent.
 Suppose we need to model the distribution of images, and we want 




x

0


∼
N
(
0
,
I
)


{\displaystyle x_{0}\sim N(0,I)}

, a white-noise image. Now, most white-noise images do not look like real images, so 



q
(

x

0


)
≈
0


{\displaystyle q(x_{0})\approx 0}

 for large swaths of 




x

0


∼
N
(
0
,
I
)


{\displaystyle x_{0}\sim N(0,I)}

. This presents a problem for learning the score function, because if there are no samples around a certain point, then we can't learn the score function at that point. If we do not know the score function 




∇


x

t




ln
⁡
q
(

x

t


)


{\displaystyle \nabla _{x_{t}}\ln q(x_{t})}

 at that point, then we cannot impose the time-evolution equation on a particle:



d

x

t


=

∇


x

t




ln
⁡
q
(

x

t


)
d
t
+
d

W

t




{\displaystyle dx_{t}=\nabla _{x_{t}}\ln q(x_{t})dt+dW_{t}}

To deal with this problem, we perform annealing. If 



q


{\displaystyle q}

 is too different from a white-noise distribution, then progressively add noise until it is indistinguishable from one. That is, we perform a forward diffusion, then learn the score function, then use the score function to perform a backward diffusion.
 Consider again the forward diffusion process, but this time in continuous time:




x

t


=


1
−

β

t





x

t
−
1


+



β

t





z

t




{\displaystyle x_{t}={\sqrt {1-\beta _{t}}}x_{t-1}+{\sqrt {\beta _{t}}}z_{t}}

By taking the 




β

t


→
β
(
t
)
d
t
,


d
t



z

t


→
d

W

t




{\displaystyle \beta _{t}\to \beta (t)dt,{\sqrt {dt}}z_{t}\to dW_{t}}

 limit, we obtain a continuous diffusion process, in the form of a stochastic differential equation:



d

x

t


=
−


1
2


β
(
t
)

x

t


d
t
+


β
(
t
)


d

W

t




{\displaystyle dx_{t}=-{\frac {1}{2}}\beta (t)x_{t}dt+{\sqrt {\beta (t)}}dW_{t}}

where 




W

t




{\displaystyle W_{t}}

 is a Wiener process (multidimensional Brownian motion).
 Now, the equation is exactly a special case of the overdamped Langevin equation



d

x

t


=
−


D


k

B


T



(

∇

x


U
)
d
t
+


2
D


d

W

t




{\displaystyle dx_{t}=-{\frac {D}{k_{B}T}}(\nabla _{x}U)dt+{\sqrt {2D}}dW_{t}}

where 



D


{\displaystyle D}

 is diffusion tensor, 



T


{\displaystyle T}

 is temperature, and 



U


{\displaystyle U}

 is potential energy field. If we substitute in 



D
=


1
2


β
(
t
)
I
,

k

B


T
=
1
,
U
=


1
2


‖
x

‖

2




{\displaystyle D={\frac {1}{2}}\beta (t)I,k_{B}T=1,U={\frac {1}{2}}\|x\|^{2}}

, we recover the above equation. This explains why the phrase ""Langevin dynamics"" is sometimes used in diffusion models.
 Now the above equation is for the stochastic motion of a single particle. Suppose we have a cloud of particles distributed according to 



q


{\displaystyle q}

 at time 



t
=
0


{\displaystyle t=0}

, then after a long time, the cloud of particles would settle into the stable distribution of 



N
(
0
,
I
)


{\displaystyle N(0,I)}

. Let 




ρ

t




{\displaystyle \rho _{t}}

 be the density of the cloud of particles at time 



t


{\displaystyle t}

, then we have




ρ

0


=
q
;


ρ

T


≈
N
(
0
,
I
)


{\displaystyle \rho _{0}=q;\quad \rho _{T}\approx N(0,I)}

and the goal is to somehow reverse the process, so that we can start at the end and diffuse back to the beginning.
 By Fokker-Planck equation, the density of the cloud evolves according to




∂

t


ln
⁡

ρ

t


=


1
2


β
(
t
)

(

n
+
(
x
+
∇
ln
⁡

ρ

t


)
⋅
∇
ln
⁡

ρ

t


+
Δ
ln
⁡

ρ

t



)



{\displaystyle \partial _{t}\ln \rho _{t}={\frac {1}{2}}\beta (t)\left(n+(x+\nabla \ln \rho _{t})\cdot \nabla \ln \rho _{t}+\Delta \ln \rho _{t}\right)}

where 



n


{\displaystyle n}

 is the dimension of space, and 



Δ


{\displaystyle \Delta }

 is the Laplace operator.
 If we have solved 




ρ

t




{\displaystyle \rho _{t}}

 for time 



t
∈
[
0
,
T
]


{\displaystyle t\in [0,T]}

, then we can exactly reverse the evolution of the cloud. Suppose we start with another cloud of particles with density 




ν

0


=

ρ

T




{\displaystyle \nu _{0}=\rho _{T}}

, and let the particles in the cloud evolve according to



d

y

t


=


1
2


β
(
T
−
t
)

y

t


d
t
+
β
(
T
−
t
)





∇


y

t




ln
⁡

ρ

T
−
t



(

y

t


)


⏟



score function 


d
t
+


β
(
T
−
t
)


d

W

t




{\displaystyle dy_{t}={\frac {1}{2}}\beta (T-t)y_{t}dt+\beta (T-t)\underbrace {\nabla _{y_{t}}\ln \rho _{T-t}\left(y_{t}\right)} _{\text{score function }}dt+{\sqrt {\beta (T-t)}}dW_{t}}

then by plugging into the Fokker-Planck equation, we find that 




∂

t



ρ

T
−
t


=

∂

t



ν

t




{\displaystyle \partial _{t}\rho _{T-t}=\partial _{t}\nu _{t}}

. Thus this cloud of points is the original cloud, evolving backwards.[23]
 At the continuous limit, 








α
¯




t


=
(
1
−

β

1


)
⋯
(
1
−

β

t


)
=

e


∑

i


ln
⁡
(
1
−

β

i


)


→

e

−

∫

0


t


β
(
t
)
d
t




{\displaystyle {\bar {\alpha }}_{t}=(1-\beta _{1})\cdots (1-\beta _{t})=e^{\sum _{i}\ln(1-\beta _{i})}\to e^{-\int _{0}^{t}\beta (t)dt}}


and so 





x

t



|


x

0


∼
N

(


e

−


1
2



∫

0


t


β
(
t
)
d
t



x

0


,

(

1
−

e

−

∫

0


t


β
(
t
)
d
t



)

I

)



{\displaystyle x_{t}|x_{0}\sim N\left(e^{-{\frac {1}{2}}\int _{0}^{t}\beta (t)dt}x_{0},\left(1-e^{-\int _{0}^{t}\beta (t)dt}\right)I\right)}


In particular, we see that we can directly sample from any point in the continuous diffusion process without going through the intermediate steps, by first sampling 




x

0


∼
q
,
z
∼
N
(
0
,
I
)


{\displaystyle x_{0}\sim q,z\sim N(0,I)}

, then get 




x

t


=

e

−


1
2



∫

0


t


β
(
t
)
d
t



x

0


+

(

1
−

e

−

∫

0


t


β
(
t
)
d
t



)

z


{\displaystyle x_{t}=e^{-{\frac {1}{2}}\int _{0}^{t}\beta (t)dt}x_{0}+\left(1-e^{-\int _{0}^{t}\beta (t)dt}\right)z}

. That is, we can quickly sample 




x

t


∼

ρ

t




{\displaystyle x_{t}\sim \rho _{t}}

 for any 



t
≥
0


{\displaystyle t\geq 0}

.
 Now, define a certain probability distribution 



γ


{\displaystyle \gamma }

 over 



[
0
,
∞
)


{\displaystyle [0,\infty )}

, then the score-matching loss function is defined as the expected Fisher divergence:




L
(
θ
)
=

E

t
∼
γ
,

x

t


∼

ρ

t




[
‖

f

θ


(

x

t


,
t
)

‖

2


+
2
∇
⋅

f

θ


(

x

t


,
t
)
]


{\displaystyle L(\theta )=E_{t\sim \gamma ,x_{t}\sim \rho _{t}}[\|f_{\theta }(x_{t},t)\|^{2}+2\nabla \cdot f_{\theta }(x_{t},t)]}


After training, 




f

θ


(

x

t


,
t
)
≈
∇
ln
⁡

ρ

t




{\displaystyle f_{\theta }(x_{t},t)\approx \nabla \ln \rho _{t}}

, so we can perform the backwards diffusion process by first sampling 




x

T


∼
N
(
0
,
I
)


{\displaystyle x_{T}\sim N(0,I)}

, then integrating the SDE from 



t
=
T


{\displaystyle t=T}

 to 



t
=
0


{\displaystyle t=0}

:





x

t
−
d
t


=

x

t


+


1
2


β
(
t
)

x

t


d
t
+
β
(
t
)

f

θ


(

x

t


,
t
)
d
t
+


β
(
t
)


d

W

t




{\displaystyle x_{t-dt}=x_{t}+{\frac {1}{2}}\beta (t)x_{t}dt+\beta (t)f_{\theta }(x_{t},t)dt+{\sqrt {\beta (t)}}dW_{t}}


This may be done by any SDE integration method, such as Euler–Maruyama method.
 The name ""noise conditional score network"" is explained thus:
 DDPM and score-based generative models are equivalent.[19][2][24] This means that a network trained using DDPM can be used as a NCSN, and vice versa.
 We know that 




x

t



|


x

0


∼
N

(







α
¯




t





x

0


,

σ

t


2


I

)



{\displaystyle x_{t}|x_{0}\sim N\left({\sqrt {{\bar {\alpha }}_{t}}}x_{0},\sigma _{t}^{2}I\right)}

, so by Tweedie's formula, we have





∇


x

t




ln
⁡
q
(

x

t


)
=


1

σ

t


2




(
−

x

t


+






α
¯




t





E

q


[

x

0



|


x

t


]
)


{\displaystyle \nabla _{x_{t}}\ln q(x_{t})={\frac {1}{\sigma _{t}^{2}}}(-x_{t}+{\sqrt {{\bar {\alpha }}_{t}}}E_{q}[x_{0}|x_{t}])}


As described previously, the DDPM loss function is 




∑

t



L

s
i
m
p
l
e
,
t




{\displaystyle \sum _{t}L_{simple,t}}

 with





L

s
i
m
p
l
e
,
t


=

E


x

0


∼
q
;
z
∼
N
(
0
,
I
)



[


‖


ϵ

θ


(

x

t


,
t
)
−
z

‖


2


]



{\displaystyle L_{simple,t}=E_{x_{0}\sim q;z\sim N(0,I)}\left[\left\|\epsilon _{\theta }(x_{t},t)-z\right\|^{2}\right]}


where 




x

t


=






α
¯




t





x

0


+

σ

t


z


{\displaystyle x_{t}={\sqrt {{\bar {\alpha }}_{t}}}x_{0}+\sigma _{t}z}

. By a change of variables,





L

s
i
m
p
l
e
,
t


=

E


x

0


,

x

t


∼
q



[


‖


ϵ

θ


(

x

t


,
t
)
−




x

t


−






α
¯




t





x

0




σ

t





‖


2


]

=

E


x

t


∼
q
,

x

0


∼
q
(
⋅

|


x

t


)



[


‖


ϵ

θ


(

x

t


,
t
)
−




x

t


−






α
¯




t





x

0




σ

t





‖


2


]



{\displaystyle L_{simple,t}=E_{x_{0},x_{t}\sim q}\left[\left\|\epsilon _{\theta }(x_{t},t)-{\frac {x_{t}-{\sqrt {{\bar {\alpha }}_{t}}}x_{0}}{\sigma _{t}}}\right\|^{2}\right]=E_{x_{t}\sim q,x_{0}\sim q(\cdot |x_{t})}\left[\left\|\epsilon _{\theta }(x_{t},t)-{\frac {x_{t}-{\sqrt {{\bar {\alpha }}_{t}}}x_{0}}{\sigma _{t}}}\right\|^{2}\right]}


and the term inside becomes a least squares regression, so if the network actually reaches the global minimum of loss, then we have 




ϵ

θ


(

x

t


,
t
)
=




x

t


−






α
¯




t





E

q


[

x

0



|


x

t


]


σ

t




=
−

σ

t



∇


x

t




ln
⁡
q
(

x

t


)


{\displaystyle \epsilon _{\theta }(x_{t},t)={\frac {x_{t}-{\sqrt {{\bar {\alpha }}_{t}}}E_{q}[x_{0}|x_{t}]}{\sigma _{t}}}=-\sigma _{t}\nabla _{x_{t}}\ln q(x_{t})}


 Thus, a score-based network can be used for denoising diffusion.
 Conversely, the continuous limit 




x

t
−
1


=

x

t
−
d
t


,

β

t


=
β
(
t
)
d
t
,

z

t




d
t


=
d

W

t




{\displaystyle x_{t-1}=x_{t-dt},\beta _{t}=\beta (t)dt,z_{t}{\sqrt {dt}}=dW_{t}}

 of the backward equation





x

t
−
1


=



x

t




α

t





−



β

t




σ

t





α

t








ϵ

θ


(

x

t


,
t
)
+



β

t





z

t


;


z

t


∼
N
(
0
,
I
)


{\displaystyle x_{t-1}={\frac {x_{t}}{\sqrt {\alpha _{t}}}}-{\frac {\beta _{t}}{\sigma _{t}{\sqrt {\alpha _{t}}}}}\epsilon _{\theta }(x_{t},t)+{\sqrt {\beta _{t}}}z_{t};\quad z_{t}\sim N(0,I)}


gives us precisely the same equation as score-based diffusion:





x

t
−
d
t


=

x

t


(
1
+
β
(
t
)
d
t

/

2
)
+
β
(
t
)

∇


x

t




ln
⁡
q
(

x

t


)
d
t
+


β
(
t
)


d

W

t




{\displaystyle x_{t-dt}=x_{t}(1+\beta (t)dt/2)+\beta (t)\nabla _{x_{t}}\ln q(x_{t})dt+{\sqrt {\beta (t)}}dW_{t}}

Thus, a denoising network can be used as for score-based diffusion.
 In DDPM, the sequence of numbers 



0
=

σ

0


<

σ

1


<
⋯
<

σ

T


<
1


{\displaystyle 0=\sigma _{0}<\sigma _{1}<\cdots <\sigma _{T}<1}

 is called a (discrete time) noise schedule. In general, consider a strictly increasing monotonic function 



σ


{\displaystyle \sigma }

 of type 




R

→
(
0
,
1
)


{\displaystyle \mathbb {R} \to (0,1)}

, such as the sigmoid function. In that case, a noise schedule is a sequence of real numbers 




λ

1


<

λ

2


<
⋯
<

λ

T




{\displaystyle \lambda _{1}<\lambda _{2}<\cdots <\lambda _{T}}

. It then defines a sequence of noises 




σ

t


:=
σ
(

λ

t


)


{\displaystyle \sigma _{t}:=\sigma (\lambda _{t})}

, which then derives the other quantities 




β

t


=
1
−



1
−

σ

t


2




1
−

σ

t
−
1


2







{\displaystyle \beta _{t}=1-{\frac {1-\sigma _{t}^{2}}{1-\sigma _{t-1}^{2}}}}

.
 In order to use arbitrary noise schedules, instead of training a noise prediction model 




ϵ

θ


(

x

t


,
t
)


{\displaystyle \epsilon _{\theta }(x_{t},t)}

, one trains 




ϵ

θ


(

x

t


,

σ

t


)


{\displaystyle \epsilon _{\theta }(x_{t},\sigma _{t})}

.
 Similarly, for the noise conditional score network, instead of training 




f

θ


(

x

t


,
t
)


{\displaystyle f_{\theta }(x_{t},t)}

, one trains 




f

θ


(

x

t


,

σ

t


)


{\displaystyle f_{\theta }(x_{t},\sigma _{t})}

.
 The original DDPM method for generating images is slow, since the forward diffusion process usually takes 



T
∼
1000


{\displaystyle T\sim 1000}

 to make the distribution of 




x

T




{\displaystyle x_{T}}

 to appear close to gaussian. However this means the backward diffusion process also take 1000 steps. Unlike the forward diffusion process, which can skip steps as 




x

t



|


x

0




{\displaystyle x_{t}|x_{0}}

 is gaussian for all 



t
≥
1


{\displaystyle t\geq 1}

, the backward diffusion process does not allow skipping steps. For example, to sample 




x

t
−
2



|


x

t
−
1


∼
N
(

μ

θ


(

x

t
−
1


,
t
−
1
)
,

Σ

θ


(

x

t
−
1


,
t
−
1
)
)


{\displaystyle x_{t-2}|x_{t-1}\sim N(\mu _{\theta }(x_{t-1},t-1),\Sigma _{\theta }(x_{t-1},t-1))}

 requires the model to first sample 




x

t
−
1




{\displaystyle x_{t-1}}

. Attempting to directly sample 




x

t
−
2



|


x

t




{\displaystyle x_{t-2}|x_{t}}

 would require us to marginalize out 




x

t
−
1




{\displaystyle x_{t-1}}

, which is generally intractable.
 DDIM[25] is a method to take any model trained on DDPM loss, and use it to sample with some steps skipped, sacrificing an adjustable amount of quality. If we generate the Markovian chain case in DDPM to non-Markovian case, DDIM corresponds to the case that the reverse process has variance equals to 0. In other words, the reverse process (and also the forward process) is deterministic. When using fewer sampling steps, DDIM outperforms DDPM.
 In detail, the DDIM sampling method is as follows. Start with the forward diffusion process 




x

t


=






α
¯




t





x

0


+

σ

t


ϵ


{\displaystyle x_{t}={\sqrt {{\bar {\alpha }}_{t}}}x_{0}+\sigma _{t}\epsilon }

. Then, during the backward denoising process, given 




x

t


,

ϵ

θ


(

x

t


,
t
)


{\displaystyle x_{t},\epsilon _{\theta }(x_{t},t)}

, the original data is estimated as 




x

0

′

=




x

t


−

σ

t



ϵ

θ


(

x

t


,
t
)






α
¯




t







{\displaystyle x_{0}'={\frac {x_{t}-\sigma _{t}\epsilon _{\theta }(x_{t},t)}{\sqrt {{\bar {\alpha }}_{t}}}}}

then the backward diffusion process can jump to any step 



0
≤
s
<
t


{\displaystyle 0\leq s<t}

, and the next denoised sample is 




x

s


=






α
¯




s





x

0

′

+



σ

s


2


−
(

σ

s

′


)

2





ϵ

θ


(

x

t


,
t
)
+

σ

s

′

ϵ


{\displaystyle x_{s}={\sqrt {{\bar {\alpha }}_{s}}}x_{0}'+{\sqrt {\sigma _{s}^{2}-(\sigma '_{s})^{2}}}\epsilon _{\theta }(x_{t},t)+\sigma _{s}'\epsilon }

where 




σ

s

′



{\displaystyle \sigma _{s}'}

 is an arbitrary real number within the range 



[
0
,

σ

s


]


{\displaystyle [0,\sigma _{s}]}

, and 



ϵ
∼
N
(
0
,
I
)


{\displaystyle \epsilon \sim N(0,I)}

 is a newly sampled gaussian noise.[17] If all 




σ

s

′

=
0


{\displaystyle \sigma _{s}'=0}

, then the backward process becomes deterministic, and this special case of DDIM is also called ""DDIM"". The original paper noted that when the process is deterministic, samples generated with only 20 steps are already very similar to ones generated with 1000 steps on the high-level.
 The original paper recommended defining a single ""eta value"" 



η
∈
[
0
,
1
]


{\displaystyle \eta \in [0,1]}

, such that 




σ

s

′

=
η




σ
~




s




{\displaystyle \sigma _{s}'=\eta {\tilde {\sigma }}_{s}}

. When 



η
=
1


{\displaystyle \eta =1}

, this is the original DDPM. When 



η
=
0


{\displaystyle \eta =0}

, this is the fully deterministic DDIM. For intermediate values, the process interpolates between them.
 By the equivalence, the DDIM algorithm also applies for score-based diffusion models.
 Since the diffusion model is a general method for modelling probability distributions, if one wants to model a distribution over images, one can first encode the images into a lower-dimensional space by an encoder, then use a diffusion model to model the distribution over encoded images. Then to generate an image, one can sample from the diffusion model, then use a decoder to decode it into an image.[26]
 The encoder-decoder pair is most often a variational autoencoder (VAE).
 [27] proposed various architectural improvements. For example, they proposed log-space interpolation during backward sampling. Instead of sampling from 




x

t
−
1


∼
N
(




μ
~




t


(

x

t


,




x
~




0


)
,




σ
~




t


2


I
)


{\displaystyle x_{t-1}\sim N({\tilde {\mu }}_{t}(x_{t},{\tilde {x}}_{0}),{\tilde {\sigma }}_{t}^{2}I)}

, they recommended sampling from 



N
(




μ
~




t


(

x

t


,




x
~




0


)
,
(

σ

t


v






σ
~




t


1
−
v



)

2


I
)


{\displaystyle N({\tilde {\mu }}_{t}(x_{t},{\tilde {x}}_{0}),(\sigma _{t}^{v}{\tilde {\sigma }}_{t}^{1-v})^{2}I)}

 for a learned parameter 



v


{\displaystyle v}

.
 In the v-prediction formalism, the noising formula 




x

t


=






α
¯




t





x

0


+


1
−




α
¯




t





ϵ

t




{\displaystyle x_{t}={\sqrt {{\bar {\alpha }}_{t}}}x_{0}+{\sqrt {1-{\bar {\alpha }}_{t}}}\epsilon _{t}}

 is reparameterised by an angle 




ϕ

t




{\displaystyle \phi _{t}}

 such that 



cos
⁡

ϕ

t


=






α
¯




t






{\displaystyle \cos \phi _{t}={\sqrt {{\bar {\alpha }}_{t}}}}

 and a ""velocity"" defined by 



cos
⁡

ϕ

t



ϵ

t


−
sin
⁡

ϕ

t



x

0




{\displaystyle \cos \phi _{t}\epsilon _{t}-\sin \phi _{t}x_{0}}

. The network is trained to predict the velocity 







v
^




θ




{\displaystyle {\hat {v}}_{\theta }}

, and denoising is by 




x


ϕ

t


−
δ


=
cos
⁡
(
δ
)


x


ϕ

t




−
sin
⁡
(
δ
)




v
^




θ



(

x


ϕ

t




)


{\displaystyle x_{\phi _{t}-\delta }=\cos(\delta )\;x_{\phi _{t}}-\sin(\delta ){\hat {v}}_{\theta }\;(x_{\phi _{t}})}

.[28] This parameterization was found to improve performance, as the model can be trained to reach total noise (i.e. 




ϕ

t


=

90

∘




{\displaystyle \phi _{t}=90^{\circ }}

) and then reverse it, whereas the standard parameterization never reaches total noise since 









α
¯




t




>
0


{\displaystyle {\sqrt {{\bar {\alpha }}_{t}}}>0}

 is always true.[29]
 Classifier guidance was proposed in 2021 to improve class-conditional generation by using a classifier. The original publication used CLIP text encoders to improve text-conditional image generation.[30]
 Suppose we wish to sample not from the entire distribution of images, but conditional on the image description. We don't want to sample a generic image, but an image that fits the description ""black cat with red eyes"". Generally, we want to sample from the distribution 



p
(
x

|

y
)


{\displaystyle p(x|y)}

, where 



x


{\displaystyle x}

 ranges over images, and 



y


{\displaystyle y}

 ranges over classes of images (a description ""black cat with red eyes"" is just a very detailed class, and a class ""cat"" is just a very vague description).
 Taking the perspective of the noisy channel model, we can understand the process as follows: To generate an image 



x


{\displaystyle x}

 conditional on description 



y


{\displaystyle y}

, we imagine that the requester really had in mind an image 



x


{\displaystyle x}

, but the image is passed through a noisy channel and came out garbled, as 



y


{\displaystyle y}

. Image generation is then nothing but inferring which 



x


{\displaystyle x}

 the requester had in mind.
 In other words, conditional image generation is simply ""translating from a textual language into a pictorial language"". Then, as in noisy-channel model, we use Bayes theorem to get




p
(
x

|

y
)
∝
p
(
y

|

x
)
p
(
x
)


{\displaystyle p(x|y)\propto p(y|x)p(x)}


in other words, if we have a good model of the space of all images, and a good image-to-class translator, we get a class-to-image translator ""for free"". In the equation for backward diffusion, the score 



∇
ln
⁡
p
(
x
)


{\displaystyle \nabla \ln p(x)}

 can be replaced by





∇

x


ln
⁡
p
(
x

|

y
)
=





∇

x


ln
⁡
p
(
x
)

⏟



score


+





∇

x


ln
⁡
p
(
y

|

x
)

⏟



classifier guidance




{\displaystyle \nabla _{x}\ln p(x|y)=\underbrace {\nabla _{x}\ln p(x)} _{\text{score}}+\underbrace {\nabla _{x}\ln p(y|x)} _{\text{classifier guidance}}}


where 




∇

x


ln
⁡
p
(
x
)


{\displaystyle \nabla _{x}\ln p(x)}

 is the score function, trained as previously described, and 




∇

x


ln
⁡
p
(
y

|

x
)


{\displaystyle \nabla _{x}\ln p(y|x)}

 is found by using a differentiable image classifier.
 During the diffusion process, we need to condition on the time, giving




∇


x

t




ln
⁡
p
(

x

t



|

y
,
t
)
=

∇


x

t




ln
⁡
p
(
y

|


x

t


,
t
)
+

∇


x

t




ln
⁡
p
(

x

t



|

t
)


{\displaystyle \nabla _{x_{t}}\ln p(x_{t}|y,t)=\nabla _{x_{t}}\ln p(y|x_{t},t)+\nabla _{x_{t}}\ln p(x_{t}|t)}

Although, usually the classifier model does not depend on time, in which case 



p
(
y

|


x

t


,
t
)
=
p
(
y

|


x

t


)


{\displaystyle p(y|x_{t},t)=p(y|x_{t})}

.
 Classifier guidance is defined for the gradient of score function, thus for score-based diffusion network, but as previously noted, score-based diffusion models are equivalent to denoising models by 




ϵ

θ


(

x

t


,
t
)
=
−

σ

t



∇


x

t




ln
⁡
p
(

x

t



|

t
)


{\displaystyle \epsilon _{\theta }(x_{t},t)=-\sigma _{t}\nabla _{x_{t}}\ln p(x_{t}|t)}

, and similarly, 




ϵ

θ


(

x

t


,
y
,
t
)
=
−

σ

t



∇


x

t




ln
⁡
p
(

x

t



|

y
,
t
)


{\displaystyle \epsilon _{\theta }(x_{t},y,t)=-\sigma _{t}\nabla _{x_{t}}\ln p(x_{t}|y,t)}

. Therefore, classifier guidance works for denoising diffusion as well, using the modified noise prediction:[30]




ϵ

θ


(

x

t


,
y
,
t
)
=

ϵ

θ


(

x

t


,
t
)
−





σ

t



∇


x

t




ln
⁡
p
(
y

|


x

t


,
t
)

⏟



classifier guidance




{\displaystyle \epsilon _{\theta }(x_{t},y,t)=\epsilon _{\theta }(x_{t},t)-\underbrace {\sigma _{t}\nabla _{x_{t}}\ln p(y|x_{t},t)} _{\text{classifier guidance}}}


 The classifier-guided diffusion model samples from 



p
(
x

|

y
)


{\displaystyle p(x|y)}

, which is concentrated around the maximum a posteriori estimate 



arg
⁡

max

x


p
(
x

|

y
)


{\displaystyle \arg \max _{x}p(x|y)}

. If we want to force the model to move towards the maximum likelihood estimate 



arg
⁡

max

x


p
(
y

|

x
)


{\displaystyle \arg \max _{x}p(y|x)}

, we can use 





p

γ


(
x

|

y
)
∝
p
(
y

|

x

)

γ


p
(
x
)


{\displaystyle p_{\gamma }(x|y)\propto p(y|x)^{\gamma }p(x)}


where 



γ
>
0


{\displaystyle \gamma >0}

 is interpretable as inverse temperature. In the context of diffusion models, it is usually called the guidance scale. A high 



γ


{\displaystyle \gamma }

 would force the model to sample from a distribution concentrated around 



arg
⁡

max

x


p
(
y

|

x
)


{\displaystyle \arg \max _{x}p(y|x)}

. This sometimes improves quality of generated images.[30]
 This gives a modification to the previous equation:




∇

x


ln
⁡

p

β


(
x

|

y
)
=

∇

x


ln
⁡
p
(
x
)
+
γ

∇

x


ln
⁡
p
(
y

|

x
)


{\displaystyle \nabla _{x}\ln p_{\beta }(x|y)=\nabla _{x}\ln p(x)+\gamma \nabla _{x}\ln p(y|x)}

For denoising models, it corresponds to[31]




ϵ

θ


(

x

t


,
y
,
t
)
=

ϵ

θ


(

x

t


,
t
)
−
γ

σ

t



∇


x

t




ln
⁡
p
(
y

|


x

t


,
t
)


{\displaystyle \epsilon _{\theta }(x_{t},y,t)=\epsilon _{\theta }(x_{t},t)-\gamma \sigma _{t}\nabla _{x_{t}}\ln p(y|x_{t},t)}


 If we do not have a classifier 



p
(
y

|

x
)


{\displaystyle p(y|x)}

, we could still extract one out of the image model itself:[31]





∇

x


ln
⁡

p

γ


(
x

|

y
)
=
(
1
−
γ
)

∇

x


ln
⁡
p
(
x
)
+
γ

∇

x


ln
⁡
p
(
x

|

y
)


{\displaystyle \nabla _{x}\ln p_{\gamma }(x|y)=(1-\gamma )\nabla _{x}\ln p(x)+\gamma \nabla _{x}\ln p(x|y)}


Such a model is usually trained by presenting it with both 



(
x
,
y
)


{\displaystyle (x,y)}

 and 



(
x
,


N
o
n
e


)


{\displaystyle (x,{\rm {None}})}

, allowing it to model both 




∇

x


ln
⁡
p
(
x

|

y
)


{\displaystyle \nabla _{x}\ln p(x|y)}

 and 




∇

x


ln
⁡
p
(
x
)


{\displaystyle \nabla _{x}\ln p(x)}

.
 Note that for CFG, the diffusion model cannot be merely a generative model of the entire data distribution 




∇

x


ln
⁡
p
(
x
)


{\displaystyle \nabla _{x}\ln p(x)}

. It must be a conditional generative model 




∇

x


ln
⁡
p
(
x

|

y
)


{\displaystyle \nabla _{x}\ln p(x|y)}

. For example, in stable diffusion, the diffusion backbone takes as input both a noisy model 




x

t




{\displaystyle x_{t}}

, a time 



t


{\displaystyle t}

, and a conditioning vector 



y


{\displaystyle y}

 (such as a vector encoding a text prompt), and produces a noise prediction 




ϵ

θ


(

x

t


,
y
,
t
)


{\displaystyle \epsilon _{\theta }(x_{t},y,t)}

.
 For denoising models, it corresponds to




ϵ

θ


(

x

t


,
y
,
t
,
γ
)
=

ϵ

θ


(

x

t


,
t
)
+
γ
(

ϵ

θ


(

x

t


,
y
,
t
)
−

ϵ

θ


(

x

t


,
t
)
)


{\displaystyle \epsilon _{\theta }(x_{t},y,t,\gamma )=\epsilon _{\theta }(x_{t},t)+\gamma (\epsilon _{\theta }(x_{t},y,t)-\epsilon _{\theta }(x_{t},t))}

As sampled by DDIM, the algorithm can be written as[32]








ϵ

uncond





←

ϵ

θ


(

x

t


,
t
)





ϵ

cond





←

ϵ

θ


(

x

t


,
t
,
c
)





ϵ

CFG





←

ϵ

uncond


+
γ
(

ϵ

cond


−

ϵ

uncond


)





x

0





←
(

x

t


−

σ

t



ϵ

CFG


)

/



1
−

σ

t


2









x

s





←


1
−

σ

s


2





x

0


+



σ

s


2


−
(

σ

s

′


)

2





ϵ

uncond


+

σ

s

′

ϵ






{\displaystyle {\begin{aligned}\epsilon _{\text{uncond}}&\leftarrow \epsilon _{\theta }(x_{t},t)\\\epsilon _{\text{cond}}&\leftarrow \epsilon _{\theta }(x_{t},t,c)\\\epsilon _{\text{CFG}}&\leftarrow \epsilon _{\text{uncond}}+\gamma (\epsilon _{\text{cond}}-\epsilon _{\text{uncond}})\\x_{0}&\leftarrow (x_{t}-\sigma _{t}\epsilon _{\text{CFG}})/{\sqrt {1-\sigma _{t}^{2}}}\\x_{s}&\leftarrow {\sqrt {1-\sigma _{s}^{2}}}x_{0}+{\sqrt {\sigma _{s}^{2}-(\sigma _{s}')^{2}}}\epsilon _{\text{uncond}}+\sigma _{s}'\epsilon \\\end{aligned}}}

A similar technique applies to language model sampling. Also, if the unconditional generation 




ϵ

uncond


←

ϵ

θ


(

x

t


,
t
)


{\displaystyle \epsilon _{\text{uncond}}\leftarrow \epsilon _{\theta }(x_{t},t)}

 is replaced by 




ϵ

neg cond


←

ϵ

θ


(

x

t


,
t
,

c
′

)


{\displaystyle \epsilon _{\text{neg cond}}\leftarrow \epsilon _{\theta }(x_{t},t,c')}

, then it results in negative prompting, which pushes the generation away from 




c
′



{\displaystyle c'}

 condition.[33][34]
 Given a diffusion model, one may regard it either as a continuous process, and sample from it by integrating a SDE, or one can regard it as a discrete process, and sample from it by iterating the discrete steps. The choice of the ""noise schedule"" 




β

t




{\displaystyle \beta _{t}}

 can also affect the quality of samples. A noise schedule is a function that sends a natural number to a noise level: 



t
↦

β

t


,

t
∈
{
1
,
2
,
…
}
,
β
∈
(
0
,
1
)


{\displaystyle t\mapsto \beta _{t},\quad t\in \{1,2,\dots \},\beta \in (0,1)}

A noise schedule is more often specified by a map 



t
↦

σ

t




{\displaystyle t\mapsto \sigma _{t}}

. The two definitions are equivalent, since 




β

t


=
1
−



1
−

σ

t


2




1
−

σ

t
−
1


2







{\displaystyle \beta _{t}=1-{\frac {1-\sigma _{t}^{2}}{1-\sigma _{t-1}^{2}}}}

.
 In the DDPM perspective, one can use the DDPM itself (with noise), or DDIM (with adjustable amount of noise). The case where one adds noise is sometimes called ancestral sampling.[35] One can interpolate between noise and no noise. The amount of noise is denoted 



η


{\displaystyle \eta }

 (""eta value"") in the DDIM paper, with 



η
=
0


{\displaystyle \eta =0}

 denoting no noise (as in deterministic DDIM), and 



η
=
1


{\displaystyle \eta =1}

 denoting full noise (as in DDPM).
 In the perspective of SDE, one can use any of the numerical integration methods, such as Euler–Maruyama method, Heun's method, linear multistep methods, etc. Just as in the discrete case, one can add an adjustable amount of noise during the integration.[36]
 A survey and comparison of samplers in the context of image generation is in.[37]
 Notable variants include[38] Poisson flow generative model,[39] consistency model,[40] critically-damped Langevin diffusion,[41] GenPhys,[42] cold diffusion,[43] discrete diffusion,[44][45] etc.
 Abstractly speaking, the idea of diffusion model is to take an unknown probability distribution (the distribution of natural-looking images), then progressively convert it to a known probability distribution (standard gaussian distribution), by building an absolutely continuous probability path connecting them. The probability path is in fact defined implicitly by the score function 



∇
ln
⁡

p

t




{\displaystyle \nabla \ln p_{t}}

.
 In denoising diffusion models, the forward process adds noise, and the backward process removes noise. Both the forward and backward processes are SDEs, though the forward process is integrable in closed-form, so it can be done at no computational cost. The backward process is not integrable in closed-form, so it must be integrated step-by-step by standard SDE solvers, which can be very expensive. The probability path in diffusions model is defined through an Itô process and one can retrieve the deterministic process by using the Probability ODE flow formulation.[2]
 In flow-based diffusion models, the forward process is a deterministic flow along a time-dependent vector field, and the backward process is also a deterministic flow along the same vector field, but going backwards. Both processes are solutions to ODEs. If the vector field is well-behaved, the ODE will also be well-behaved.
 Given two distributions 




π

0




{\displaystyle \pi _{0}}

 and 




π

1




{\displaystyle \pi _{1}}

, a flow-based model is a time-dependent velocity field 




v

t


(
x
)


{\displaystyle v_{t}(x)}

 in 



[
0
,
1
]
×


R


d




{\displaystyle [0,1]\times \mathbb {R} ^{d}}

, such that if we start by sampling a point 



x
∼

π

0




{\displaystyle x\sim \pi _{0}}

, and let it move according to the velocity field:






d

d
t




ϕ

t


(
x
)
=

v

t


(

ϕ

t


(
x
)
)

t
∈
[
0
,
1
]
,


starting from 


ϕ

0


(
x
)
=
x


{\displaystyle {\frac {d}{dt}}\phi _{t}(x)=v_{t}(\phi _{t}(x))\quad t\in [0,1],\quad {\text{starting from }}\phi _{0}(x)=x}


we end up with a point 




x

1


∼

π

1




{\displaystyle x_{1}\sim \pi _{1}}

. The solution 




ϕ

t




{\displaystyle \phi _{t}}

 of the above ODE define a probability path 




p

t


=
[

ϕ

t



]

#



π

0




{\displaystyle p_{t}=[\phi _{t}]_{\#}\pi _{0}}

 by the pushforward measure operator. In particular, 



[

ϕ

1



]

#



π

0


=

π

1




{\displaystyle [\phi _{1}]_{\#}\pi _{0}=\pi _{1}}

.
 The probability path and the velocity field also satisfy the continuity equation, in the sense of probability distribution:





∂

t



p

t


+
∇
⋅
(

v

t



p

t


)
=
0


{\displaystyle \partial _{t}p_{t}+\nabla \cdot (v_{t}p_{t})=0}


To construct a probability path, we start by construct a conditional probability path 




p

t


(
x
|
z
)


{\displaystyle p_{t}(x\vert z)}

 and the corresponding conditional velocity field 




v

t


(
x
|
z
)


{\displaystyle v_{t}(x\vert z)}

 on some conditional distribution 



q
(
z
)


{\displaystyle q(z)}

. A natural choice is the Gaussian conditional probability path:





p

t


(
x
|
z
)
=


N



(


m

t


(
z
)
,

ζ

t


2


I

)



{\displaystyle p_{t}(x\vert z)={\mathcal {N}}\left(m_{t}(z),\zeta _{t}^{2}I\right)}


The conditional velocity field which corresponds to the geodesic path between conditional Gaussian path is 





v

t


(
x
|
z
)
=



ζ

t

′


ζ

t




(
x
−

m

t


(
z
)
)
+

m

t

′

(
z
)


{\displaystyle v_{t}(x\vert z)={\frac {\zeta _{t}'}{\zeta _{t}}}(x-m_{t}(z))+m_{t}'(z)}


The probability path and velocity field are then computed by marginalizing
 




p

t


(
x
)
=
∫

p

t


(
x
|
z
)
q
(
z
)
d
z


 and 



v

t


(
x
)
=


E


q
(
z
)



[




v

t


(
x
|
z
)

p

t


(
x
|
z
)



p

t


(
x
)



]



{\displaystyle p_{t}(x)=\int p_{t}(x\vert z)q(z)dz\qquad {\text{ and }}\qquad v_{t}(x)=\mathbb {E} _{q(z)}\left[{\frac {v_{t}(x\vert z)p_{t}(x\vert z)}{p_{t}(x)}}\right]}


 The idea of optimal transport flow [46] is to construct a probability path minimizing the Wasserstein metric. The distribution on which we condition is the optimal transport plan between 




π

0




{\displaystyle \pi _{0}}

 and 




π

1




{\displaystyle \pi _{1}}

: 



z
=
(

x

0


,

x

1


)


{\displaystyle z=(x_{0},x_{1})}

 and 



q
(
z
)
=
Γ
(

π

0


,

π

1


)


{\displaystyle q(z)=\Gamma (\pi _{0},\pi _{1})}

, where 



Γ


{\displaystyle \Gamma }

 is the optimal transport plan, which can be approximated by mini-batch optimal transport.
 The idea of rectified flow[47][48] is to learn a flow model such that the velocity is nearly constant along each flow path. This is beneficial, because we can integrate along such a vector field with very few steps. For example, if an ODE 







ϕ

t


˙



(
x
)
=

v

t


(

ϕ

t


(
x
)
)


{\displaystyle {\dot {\phi _{t}}}(x)=v_{t}(\phi _{t}(x))}

 follows perfectly straight paths, it simplifies to 




ϕ

t


(
x
)
=

x

0


+
t
⋅

v

0


(

x

0


)


{\displaystyle \phi _{t}(x)=x_{0}+t\cdot v_{0}(x_{0})}

, allowing for exact solutions in one step. In practice, we cannot reach such perfection, but when the flow field is nearly so, we can take a few large steps instead of many little steps.  
 The general idea is to start with two distributions 




π

0




{\displaystyle \pi _{0}}

 and 




π

1




{\displaystyle \pi _{1}}

, then construct a flow field 




ϕ

0


=
{

ϕ

t


:
t
∈
[
0
,
1
]
}


{\displaystyle \phi ^{0}=\{\phi _{t}:t\in [0,1]\}}

 from it, then repeatedly apply a ""reflow"" operation to obtain successive flow fields 




ϕ

1


,

ϕ

2


,
…


{\displaystyle \phi ^{1},\phi ^{2},\dots }

, each straighter than the previous one. When the flow field is straight enough for the application, we stop.
 Generally, for any time-differentiable process 




ϕ

t




{\displaystyle \phi _{t}}

, 




v

t




{\displaystyle v_{t}}

 can be estimated by solving:





min

θ



∫

0


1




E


x
∼

p

t





[

‖


v

t


(
x
,
θ
)
−

v

t


(
x
)


‖

2



]



d

t
.


{\displaystyle \min _{\theta }\int _{0}^{1}\mathbb {E} _{x\sim p_{t}}\left[\lVert {v_{t}(x,\theta )-v_{t}(x)}\rVert ^{2}\right]\,\mathrm {d} t.}


 In rectified flow, by injecting strong priors that intermediate trajectories are straight, it can achieve both theoretical relevance for optimal transport  and computational efficiency, as ODEs with straight paths can be simulated precisely without time discretization.
 Specifically, rectified flow seeks to match an ODE with the marginal distributions of the linear interpolation between points from distributions 




π

0




{\displaystyle \pi _{0}}

 and 




π

1




{\displaystyle \pi _{1}}

. Given observations 




x

0


∼

π

0




{\displaystyle x_{0}\sim \pi _{0}}

 and 




x

1


∼

π

1




{\displaystyle x_{1}\sim \pi _{1}}

, the canonical linear interpolation 




x

t


=
t

x

1


+
(
1
−
t
)

x

0


,
t
∈
[
0
,
1
]


{\displaystyle x_{t}=tx_{1}+(1-t)x_{0},t\in [0,1]}

 yields a trivial case 







x
˙




t


=

x

1


−

x

0




{\displaystyle {\dot {x}}_{t}=x_{1}-x_{0}}

, which cannot be causally simulated without 




x

1




{\displaystyle x_{1}}

. To address this, 




x

t




{\displaystyle x_{t}}

 is ""projected"" into a space of causally simulatable ODEs, by minimizing the least squares loss with respect to the direction 




x

1


−

x

0




{\displaystyle x_{1}-x_{0}}

:





min

θ



∫

0


1




E



π

0


,

π

1


,

p

t





[

‖

(

x

1


−

x

0


)
−

v

t


(

x

t


)


‖

2



]



d

t
.


{\displaystyle \min _{\theta }\int _{0}^{1}\mathbb {E} _{\pi _{0},\pi _{1},p_{t}}\left[\lVert {(x_{1}-x_{0})-v_{t}(x_{t})}\rVert ^{2}\right]\,\mathrm {d} t.}


 The data pair 



(

x

0


,

x

1


)


{\displaystyle (x_{0},x_{1})}

 can be any coupling of 




π

0




{\displaystyle \pi _{0}}

 and 




π

1




{\displaystyle \pi _{1}}

, typically independent (i.e., 



(

x

0


,

x

1


)
∼

π

0


×

π

1




{\displaystyle (x_{0},x_{1})\sim \pi _{0}\times \pi _{1}}

) obtained by randomly combining observations from 




π

0




{\displaystyle \pi _{0}}

 and 




π

1




{\displaystyle \pi _{1}}

. This process ensures that the trajectories closely mirror the density map of 




x

t




{\displaystyle x_{t}}

 trajectories but reroute at intersections to ensure causality. This rectifying process is also known as Flow Matching,[49] Stochastic Interpolation,[50] and Alpha-Blending.[citation needed]
 A distinctive aspect of rectified flow is its capability for ""reflow"", which straightens the trajectory of ODE paths. Denote the rectified flow 




ϕ

0


=
{

ϕ

t


:
t
∈
[
0
,
1
]
}


{\displaystyle \phi ^{0}=\{\phi _{t}:t\in [0,1]\}}

 induced from 



(

x

0


,

x

1


)


{\displaystyle (x_{0},x_{1})}

 as 




ϕ

0


=


R
e
c
t
f
l
o
w


(
(

x

0


,

x

1


)
)


{\displaystyle \phi ^{0}={\mathsf {Rectflow}}((x_{0},x_{1}))}

. Recursively applying this 





R
e
c
t
f
l
o
w


(
⋅
)


{\displaystyle {\mathsf {Rectflow}}(\cdot )}

 operator generates a series of rectified flows 




ϕ

k
+
1


=


R
e
c
t
f
l
o
w


(
(

ϕ

0


k


(

x

0


)
,

ϕ

1


k


(

x

1


)
)
)


{\displaystyle \phi ^{k+1}={\mathsf {Rectflow}}((\phi _{0}^{k}(x_{0}),\phi _{1}^{k}(x_{1})))}

. This ""reflow"" process not only reduces transport costs but also straightens the paths of rectified flows, making 




ϕ

k




{\displaystyle \phi ^{k}}

 paths straighter with increasing 



k


{\displaystyle k}

.
 Rectified flow includes a nonlinear extension where linear interpolation 




x

t




{\displaystyle x_{t}}

 is replaced with any time-differentiable curve that connects 




x

0




{\displaystyle x_{0}}

 and 




x

1




{\displaystyle x_{1}}

, given by 




x

t


=

α

t



x

1


+

β

t



x

0




{\displaystyle x_{t}=\alpha _{t}x_{1}+\beta _{t}x_{0}}

. This framework encompasses DDIM and probability flow ODEs as special cases, with particular choices of 




α

t




{\displaystyle \alpha _{t}}

 and 




β

t




{\displaystyle \beta _{t}}

. However, in the case where the path of 




x

t




{\displaystyle x_{t}}

 is not straight, the reflow process no longer ensures a reduction in convex transport costs, and also no longer straighten the paths of 




ϕ

t




{\displaystyle \phi _{t}}

.[47]
 See [51] for a tutorial on flow matching, with animations.
 For generating images by DDPM, we need a neural network that takes a time 



t


{\displaystyle t}

 and a noisy image 




x

t




{\displaystyle x_{t}}

, and predicts a noise 




ϵ

θ


(

x

t


,
t
)


{\displaystyle \epsilon _{\theta }(x_{t},t)}

 from it. Since predicting the noise is the same as predicting the denoised image, then subtracting it from 




x

t




{\displaystyle x_{t}}

, denoising architectures tend to work well. For example, the U-Net, which was found to be good for denoising images, is often used for denoising diffusion models that generate images.[52]
 For DDPM, the underlying architecture (""backbone"") does not have to be a U-Net. It just has to predict the noise somehow. For example, the diffusion transformer (DiT) uses a Transformer to predict the mean and diagonal covariance of the noise, given the textual conditioning and the partially denoised image. It is the same as standard U-Net-based denoising diffusion model, with a Transformer replacing the U-Net.[53] Mixture of experts-Transformer can also be applied.[54]
 DDPM can be used to model general data distributions, not just natural-looking images. For example, Human Motion Diffusion[55] models human motion trajectory by DDPM. Each human motion trajectory is a sequence of poses, represented by either joint rotations or positions. It uses a Transformer network to generate a less noisy trajectory out of a noisy one.
 The base diffusion model can only generate unconditionally from the whole distribution. For example, a diffusion model learned on ImageNet would generate images that look like a random image from ImageNet. To generate images from just one category, one would need to impose the condition, and then sample from the conditional distribution. Whatever condition one wants to impose, one needs to first convert the conditioning into a vector of floating point numbers, then feed it into the underlying diffusion model neural network. However, one has freedom in choosing how to convert the conditioning into a vector.
 Stable Diffusion, for example, imposes conditioning in the form of cross-attention mechanism, where the query is an intermediate representation of the image in the U-Net, and both key and value are the conditioning vectors. The conditioning can be selectively applied to only parts of an image, and new kinds of conditionings can be finetuned upon the base model, as used in ControlNet.[56]
 As a particularly simple example, consider image inpainting. The conditions are 






x
~





{\displaystyle {\tilde {x}}}

, the reference image, and 



m


{\displaystyle m}

, the inpainting mask. The conditioning is imposed at each step of the backward diffusion process, by first sampling 







x
~




t


∼
N

(







α
¯




t







x
~



,

σ

t


2


I

)



{\displaystyle {\tilde {x}}_{t}\sim N\left({\sqrt {{\bar {\alpha }}_{t}}}{\tilde {x}},\sigma _{t}^{2}I\right)}

, a noisy version of 






x
~





{\displaystyle {\tilde {x}}}

, then replacing 




x

t




{\displaystyle x_{t}}

 with 



(
1
−
m
)
⊙

x

t


+
m
⊙




x
~




t




{\displaystyle (1-m)\odot x_{t}+m\odot {\tilde {x}}_{t}}

, where 



⊙


{\displaystyle \odot }

 means elementwise multiplication.[57] Another application of cross-attention mechanism is prompt-to-prompt image editing.[58]
 Conditioning is not limited to just generating images from a specific category, or according to a specific caption (as in text-to-image). For example,[55] demonstrated generating human motion, conditioned on an audio clip of human walking (allowing syncing motion to a soundtrack), or video of human running, or a text description of human motion, etc. For how conditional diffusion models are mathematically formulated, see a methodological summary in.[59]
 As generating an image takes a long time, one can try to generate a small image by a base diffusion model, then upscale it by other models. Upscaling can be done by GAN,[60] Transformer,[61] or signal processing methods like Lanczos resampling.
 Diffusion models themselves can be used to perform upscaling. Cascading diffusion model stacks multiple diffusion models one after another, in the style of Progressive GAN. The lowest level is a standard diffusion model that generate 32x32 image, then the image would be upscaled by a diffusion model specifically trained for upscaling, and the process repeats.[52]
 In more detail, the diffusion upscaler is trained as follows:[52]
 This section collects some notable diffusion models, and briefly describes their architecture.
 The DALL-E series by OpenAI are text-conditional diffusion models of images.
 The first version of DALL-E (2021) is not actually a diffusion model. Instead, it uses a Transformer architecture that autoregressively generates a sequence of tokens, which is then converted to an image by the decoder of a discrete VAE. Released with DALL-E was the CLIP classifier, which was used by DALL-E to rank generated images according to how close the image fits the text.
 GLIDE (2022-03)[62] is a 3.5-billion diffusion model, and a small version was released publicly.[6] Soon after, DALL-E 2 was released (2022-04).[63] DALL-E 2 is a 3.5-billion cascaded diffusion model that generates images from text by ""inverting the CLIP image encoder"", the technique which they termed ""unCLIP"".
 The unCLIP method contains 4 models: a CLIP image encoder, a CLIP text encoder, an image decoder, and a ""prior"" model (which can be a diffusion model, or an autoregressive model). During training, the prior model is trained to convert CLIP image encodings to CLIP text encodings. The image decoder is trained to convert CLIP image encodings back to images. During inference, a text is converted by the CLIP text encoder to a vector, then it is converted by the prior model to an image encoding, then it is converted by the image decoder to an image.
 Sora (2024-02) is a diffusion Transformer model (DiT).
 Stable Diffusion (2022-08), released by Stability AI, consists of a denoising latent diffusion model (860 million parameters), a VAE, and a text encoder. The denoising network is a U-Net, with cross-attention blocks to allow for conditional image generation.[64][26]
 Stable Diffusion 3 (2024-03)[65] changed the latent diffusion model from the UNet to a Transformer model, and so it is a DiT. It uses rectified flow.
 Stable Video 4D (2024-07)[66] is a latent diffusion model for videos of 3D objects.
 Imagen (2022)[67][68] uses a T5-XXL language model to encode the input text into an embedding vector. It is a cascaded diffusion model with three sub-models. The first step denoises a white noise to a 64×64 image, conditional on the embedding vector of the text. This model has 2B parameters. The second step upscales the image by 64×64→256×256, conditional on embedding. This model has 650M parameters. The third step is similar, upscaling by 256×256→1024×1024. This model has 400M parameters. The three denoising networks are all U-Nets.
 Muse (2023-01)[69] is not a diffusion model, but an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens.
 Imagen 2 (2023-12) is also diffusion-based. It can generate images based on a prompt that mixes images and text. No further information available.[70] Imagen 3 (2024-05) is too. No further information available.[71]
 Veo (2024) generates videos by latent diffusion. The diffusion is conditioned on a vector that encodes both a text prompt and an image prompt.[72]
 Make-A-Video (2022) is a text-to-video diffusion model.[73][74]
 CM3leon (2023) is not a diffusion model, but an autoregressive causally masked Transformer, with mostly the same architecture as LLaMa-2.[75][76]
 Transfusion (2024) is a Transformer that combines autoregressive text generation and denoising diffusion. Specifically, it generates text autoregressively (with causal masking), and generates images by denoising multiple times over image tokens (with all-to-all attention).[77]
 Movie Gen (2024) is a series of Diffusion Transformers operating on latent space and by flow matching.[78]
",machin learn diffus model also known diffus probabilist model gener model class latent variabl gener model diffus model consist three major compon forward process revers process sampl procedur goal diffus model learn diffus process given dataset process gener new element distribut similarli origin dataset diffus model model data gener diffus process wherebi new datum perform random walk drift space possibl data train diffus model sampl mani way differ effici qualiti variou equival formal includ markov chain denois diffus probabilist model nois condit score network stochast differenti equat typic train use variat infer model respons denois typic call backbon backbon may kind typic transform updat diffus model mainli use comput vision task includ imag denois inpaint imag gener video gener typic involv train neural network sequenti denois imag blur gaussian nois model train revers process ad nois imag train converg use imag gener start imag compos random nois appli network iter denois imag imag gener seen widespread commerci interest stabl diffus model typic combin diffus model model modul allow gener comput vision diffus model also found applic natur languag process text gener summar sound gener reinforc learn diffus model introduc method learn model sampl highli complex probabl distribut use techniqu thermodynam especi diffus consid exampl one might model distribut photo imag point space imag distribut photo cloud space repeatedli ad nois imag diffus rest imag space cloud becom indistinguish gaussian distribut n n model approxim undo diffus use sampl origin distribut studi thermodynam start distribut equilibrium unlik final distribut equilibrium distribut gaussian distribut n n pdf ρ x e x x distribut particl potenti well v x x v x temperatur initi distribut much equilibrium would diffus toward equilibrium distribut make bias random step sum pure random like brownian walker gradient descent potenti well random necessari particl undergo gradient descent fall origin collaps distribut paper propos denois diffus probabilist model ddpm improv upon previou method variat infer present model need notat forward diffus process start start point x q q q q probabl distribut learn repeatedli add nois x β x β z z z iid sampl n n design start distribut x lim x x converg n n entir diffus process satisfi q x q x q x x q x x q x n x α x β n x α x β q q q n n ln q x ln q x β x β x c q q c c normal constant often omit particular note x x gaussian process afford us consider freedom reparameter exampl standard manipul gaussian process x x n α x σ x x x n μ x x σ n particular notic larg variabl x x n α x σ converg n n long enough diffus process end x close n n trace origin x q q gone exampl sinc x x n α x σ sampl x x directli one step instead go intermedi step x x x know x x gaussian x x anoth gaussian also know independ thu perform reparameter x α x α z z x α x α z z z z z z iid gaussian variabl x x x z z z z two linear equat two sourc random z z z z reparameter rotat sinc iid gaussian distribut rotat symmetr plug equat solv first reparameter x α x α α z α z σ z z z z z gaussian mean zero varianc one find second one complet rotat matrix z z α α σ β σ z z bmatrix z bmatrix bmatrix bmatrix bmatrix bmatrix sinc rotat matric form co θ sin θ sin θ co θ bmatrix bmatrix know matrix must z z α α σ β σ β σ α α σ z z bmatrix z bmatrix bmatrix bmatrix bmatrix bmatrix sinc invers rotat matrix transpos z z α α σ β σ β σ α α σ z z bmatrix bmatrix bmatrix bmatrix bmatrix z bmatrix plug back simplifi x α x σ z z x μ x x σ z z key idea ddpm use neural network parametr θ network take two argument x output vector μ θ x matrix σ θ x step forward diffus process approxim undon x n μ θ x σ θ x n give us backward diffus process p θ defin p θ x n x p θ x x n x μ θ x σ θ x goal learn paramet p θ x close q x q possibl use maximum likelihood estim variat infer elbo inequ state ln p θ x e x q x ln p θ x ln q x x q q take one expect get e x q ln p θ x e x q ln p θ x ln q x x q q q see maxim quantiti right would give us lower bound likelihood observ data allow us perform variat infer defin loss function l θ e x q ln p θ x ln q x x l q q goal minim loss stochast gradient descent express may simplifi l θ e x x q ln p θ x x e x q k l q x x p θ x c l q q kl q c c depend paramet thu ignor sinc p θ x n x also depend paramet term e x q k l q x x p θ x q kl q also ignor leav l θ l l l e x x q ln p θ x x q minim sinc x x x n μ x x σ n suggest use μ θ x μ x x howev network access x estim instead sinc x x n α x σ may write x α x σ z z z z unknown gaussian nois see estim x equival estim z z therefor let network output nois vector ϵ θ x let predict μ θ x μ x x σ ϵ θ x α x ϵ θ x β σ α remain design σ θ x ddpm paper suggest learn sinc result unstabl train poorer sampl qualiti fix valu σ θ x ζ either ζ β σ yield similar perform loss simplifi l β α σ ζ e x q z n ϵ θ x z c q n may minim stochast gradient descent paper note empir even simpler loss function l p l e e x q z n ϵ θ x z simpl q n result better model nois predict network train use gener data point origin distribut loop follow gener model anoth formul diffus model also call nois condit score network ncsn langevin dynam smld consid problem imag gener let x x repres imag let q x q x probabl distribut possibl imag q x q x say certain like certain imag howev intract gener often uninterest know absolut probabl certain imag instead usual interest know like certain imag compar immedi neighbor much like imag cat compar small variant like imag contain two whisker three gaussian nois ad consequ actual quit uninterest q x q x rather x ln q x x q x two major effect let score function x x ln q x x x q x consid x x turn x x allow us sampl q x q x use thermodynam specif potenti energi function u x ln q x u x q x lot particl potenti well distribut thermodynam equilibrium boltzmann distribut q u x e u x k b q x k b u x x b x b temperatur k b b boltzmann distribut exactli q x q x therefor model q x q x may start particl sampl conveni distribut standard gaussian distribut simul motion particl forward accord langevin equat x x u x w u boltzmann distribut equat uniqu thermodynam equilibrium matter distribut x distribut x converg distribut q q given densiti q q wish learn score function approxim f θ ln q q score match typic score match formal minim fisher diverg function e q f θ x ln q x q x q x expand integr perform integr part e q f θ x ln q x e q f θ f θ c q x q x q give us loss function also known hyvärinen score rule minim stochast gradient descent suppos need model distribut imag want x n n imag imag look like real imag q x q larg swath x n n present problem learn score function sampl around certain point ca learn score function point know score function x ln q x q point impos equat particl x x ln q x w q deal problem perform anneal q q differ distribut progress add nois indistinguish one perform forward diffus learn score function use score function perform backward diffus consid forward diffus process time continu time x β x β z take β β z w dt dt limit obtain continu diffus process form stochast differenti equat x β x β w w wiener process multidimension brownian motion equat exactli special case overdamp langevin equat x k b x u w b x u diffus tensor temperatur u u potenti energi field substitut β k b u x b recov equat explain phrase langevin dynam sometim use diffus model equat stochast motion singl particl suppos cloud particl distribut accord q q time long time cloud particl would settl stabl distribut n n let ρ densiti cloud particl time ρ q ρ n n goal somehow revers process start end diffus back begin equat densiti cloud evolv accord ln ρ β n x ln ρ ln ρ δ ln ρ n n dimens space δ laplac oper solv ρ time exactli revers evolut cloud suppos start anoth cloud particl densiti ν ρ let particl cloud evolv accord β β ln ρ score function β w score function plug equat find ρ ν thu cloud point origin cloud evolv backward continu limit α β β e ln β e β dt x x n e β x e β dt dt particular see directli sampl point continu diffus process without go intermedi step first sampl x q z n q n get x e β x e β z dt dt z quickli sampl x ρ defin certain probabl distribut γ loss function defin expect fisher diverg l θ e γ x ρ f θ x f θ x l train f θ x ln ρ perform backward diffus process first sampl x n n integr sde x x β x β f θ x β w may done sde integr method method name nois condit score network explain thu ddpm gener model equival mean network train use ddpm use ncsn vice versa know x x n α x σ tweedi formula x ln q x σ x α e q x x q q describ previous ddpm loss function l p l e simpl l p l e e x q z n ϵ θ x z simpl q n x α x σ z z chang variabl l p l e e x x q ϵ θ x x α x σ e x q x q x ϵ θ x x α x σ simpl q q q term insid becom least squar regress network actual reach global minimum loss ϵ θ x x α e q x x σ σ x ln q x q q thu network use denois diffus convers continu limit x x β β z w dt dt backward equat x x α β σ α ϵ θ x β z z n n give us precis equat diffus x x β β x ln q x β w q thu denois network use diffus ddpm sequenc number σ σ σ call discret time nois schedul gener consid strictli increas monoton function σ type r r sigmoid function case nois schedul sequenc real number λ λ λ defin sequenc nois σ σ λ deriv quantiti β σ σ order use arbitrari nois schedul instead train nois predict model ϵ θ x one train ϵ θ x σ similarli nois condit score network instead train f θ x one train f θ x σ origin ddpm method gener imag slow sinc forward diffus process usual take make distribut x appear close gaussian howev mean backward diffus process also take step unlik forward diffus process skip step x x gaussian backward diffus process allow skip step exampl sampl x x n μ θ x σ θ x n requir model first sampl x attempt directli sampl x x would requir us margin x gener intract ddim method take model train ddpm loss use sampl step skip sacrif adjust amount qualiti gener markovian chain case ddpm case ddim correspond case revers process varianc equal word revers process also forward process determinist use fewer sampl step ddim outperform ddpm detail ddim sampl method follow start forward diffus process x α x σ ϵ backward denois process given x ϵ θ x origin data estim x x σ ϵ θ x α backward diffus process jump step next denois sampl x α x σ σ ϵ θ x σ ϵ σ arbitrari real number within rang σ ϵ n n newli sampl gaussian nois σ backward process becom determinist special case ddim also call ddim origin paper note process determinist sampl gener step alreadi similar one gener step origin paper recommend defin singl eta valu η σ η σ η origin ddpm η fulli determinist ddim intermedi valu process interpol equival ddim algorithm also appli diffus model sinc diffus model gener method model probabl distribut one want model distribut imag one first encod imag space encod use diffus model model distribut encod imag gener imag one sampl diffus model use decod decod imag pair often variat autoencod vae propos variou architectur improv exampl propos interpol backward sampl instead sampl x n μ x x σ n x recommend sampl n μ x x σ v σ v n x v learn paramet v v formal nois formula x α x α ϵ reparameteris angl ϕ co ϕ α veloc defin co ϕ ϵ sin ϕ x network train predict veloc v θ v denois x ϕ δ co δ x ϕ sin δ v θ x ϕ v parameter found improv perform model train reach total nois ϕ revers wherea standard parameter never reach total nois sinc α alway true classifi guidanc propos improv gener use classifi origin public use clip text encod improv imag gener suppos wish sampl entir distribut imag condit imag descript want sampl gener imag imag fit descript black cat red eye gener want sampl distribut p x p x x rang imag rang class imag descript black cat red eye detail class class cat vagu descript take perspect noisi channel model understand process follow gener imag x x condit descript imagin request realli mind imag x x imag pass noisi channel came garbl imag gener noth infer x x request mind word condit imag gener simpli translat textual languag pictori languag model use bay theorem get p x p x p x p p p x word good model space imag good translat get translat free equat backward diffus score ln p x p x replac x ln p x x ln p x score x ln p x classifi guidanc x p x p x score x p classifi guidanc x ln p x x p x score function train previous describ x ln p x x p found use differenti imag classifi diffus process need condit time give x ln p x x ln p x x ln p x p p p although usual classifi model depend time case p x p x p classifi guidanc defin gradient score function thu diffus network previous note diffus model equival denois model ϵ θ x σ x ln p x p similarli ϵ θ x σ x ln p x p therefor classifi guidanc work denois diffus well use modifi nois predict ϵ θ x ϵ θ x σ x ln p x classifi guidanc p classifi guidanc diffus model sampl p x p concentr around maximum posteriori estim arg max x p x x p want forc model move toward maximum likelihood estim arg max x p x x p use p γ x p x γ p x p p x γ interpret invers temperatur context diffus model usual call guidanc scale high γ would forc model sampl distribut concentr around arg max x p x x p sometim improv qualiti gener imag give modif previou equat x ln p β x x ln p x γ x ln p x x x p x x p denois model correspond ϵ θ x ϵ θ x γ σ x ln p x p classifi p x p could still extract one imag model x ln p γ x γ x ln p x γ x ln p x x x p x x p model usual train present x x x n n e x none allow model x ln p x x p x ln p x x p x note cfg diffus model mere gener model entir data distribut x ln p x x p x must condit gener model x ln p x x p exampl stabl diffus diffus backbon take input noisi model x time condit vector vector encod text prompt produc nois predict ϵ θ x denois model correspond ϵ θ x γ ϵ θ x γ ϵ θ x ϵ θ x sampl ddim algorithm written ϵ uncond ϵ θ x ϵ cond ϵ θ x c ϵ cfg ϵ uncond γ ϵ cond ϵ uncond x x σ ϵ cfg σ x σ x σ σ ϵ uncond σ ϵ align uncond cond c cfg uncond cond uncond cfg uncond align similar techniqu appli languag model sampl also uncondit gener ϵ uncond ϵ θ x uncond replac ϵ neg cond ϵ θ x c neg cond c result neg prompt push gener away c c condit given diffus model one may regard either continu process sampl integr sde one regard discret process sampl iter discret step choic nois schedul β also affect qualiti sampl nois schedul function send natur number nois level β β nois schedul often specifi map σ two definit equival sinc β σ σ ddpm perspect one use ddpm nois ddim adjust amount nois case one add nois sometim call ancestr sampl one interpol nois nois amount nois denot η eta valu ddim paper η denot nois determinist ddim η denot full nois ddpm perspect sde one use numer integr method method heun method linear multistep method etc discret case one add adjust amount nois integr survey comparison sampler context imag gener notabl variant includ poisson flow gener model consist model langevin diffus genphi cold diffus discret diffus etc abstractli speak idea diffus model take unknown probabl distribut distribut imag progress convert known probabl distribut standard gaussian distribut build absolut continu probabl path connect probabl path fact defin implicitli score function ln p denois diffus model forward process add nois backward process remov nois forward backward process sde though forward process integr done comput cost backward process integr must integr standard sde solver expens probabl path diffus model defin itô process one retriev determinist process use probabl ode flow formul diffus model forward process determinist flow along vector field backward process also determinist flow along vector field go backward process solut ode vector field ode also given two distribut π π model veloc field v x x r r start sampl point x π let move accord veloc field ϕ x v ϕ x start ϕ x x dt x x start x end point x π solut ϕ ode defin probabl path p ϕ π pushforward measur oper particular ϕ π π probabl path veloc field also satisfi continu equat sens probabl distribut p v p construct probabl path start construct condit probabl path p x z z correspond condit veloc field v x z z condit distribut q z q z natur choic gaussian condit probabl path p x z n z ζ z n z condit veloc field correspond geodes path condit gaussian path v x z ζ ζ x z z z z z probabl path veloc field comput margin p x p x z q z z v x e q z v x z p x z p x x z q z x e q z z z x idea optim transport flow construct probabl path minim wasserstein metric distribut condit optim transport plan π π z x x q z γ π π q z γ optim transport plan approxim optim transport idea rectifi flow learn flow model veloc nearli constant along flow path benefici integr along vector field step exampl ode ϕ x v ϕ x x x follow perfectli straight path simplifi ϕ x x v x x allow exact solut one step practic reach perfect flow field nearli take larg step instead mani littl step gener idea start two distribut π π construct flow field ϕ ϕ repeatedli appli reflow oper obtain success flow field ϕ ϕ straighter previou one flow field straight enough applic stop gener process ϕ v estim solv min θ e x p v x θ v x e x x rectifi flow inject strong prior intermedi trajectori straight achiev theoret relev optim transport comput effici ode straight path simul precis without time discret specif rectifi flow seek match ode margin distribut linear interpol point distribut π π given observ x π x π canon linear interpol x x x yield trivial case x x x x causal simul without x address x project space causal simulat ode minim least squar loss respect direct x x min θ e π π p x x v x e data pair x x coupl π π typic independ x x π π obtain randomli combin observ π π process ensur trajectori close mirror densiti map x trajectori rerout intersect ensur causal rectifi process also known flow match stochast interpol citat need distinct aspect rectifi flow capabl reflow straighten trajectori ode path denot rectifi flow ϕ ϕ induc x x ϕ r e c f l w x x rectflow recurs appli r e c f l w rectflow oper gener seri rectifi flow ϕ k r e c f l w ϕ k x ϕ k x rectflow k k reflow process reduc transport cost also straighten path rectifi flow make ϕ k k path straighter increas k k rectifi flow includ nonlinear extens linear interpol x replac curv connect x x given x α x β x framework encompass ddim probabl flow ode special case particular choic α β howev case path x straight reflow process longer ensur reduct convex transport cost also longer straighten path ϕ see tutori flow match anim gener imag ddpm need neural network take time noisi imag x predict nois ϵ θ x sinc predict nois predict denois imag subtract x denois architectur tend work well exampl found good denois imag often use denois diffus model gener imag ddpm underli architectur backbon predict nois somehow exampl diffus transform dit use transform predict mean diagon covari nois given textual condit partial denois imag standard denois diffus model transform replac mixtur also appli ddpm use model gener data distribut imag exampl human motion diffus model human motion trajectori ddpm human motion trajectori sequenc pose repres either joint rotat posit use transform network gener less noisi trajectori noisi one base diffus model gener uncondit whole distribut exampl diffus model learn imagenet would gener imag look like random imag imagenet gener imag one categori one would need impos condit sampl condit distribut whatev condit one want impos one need first convert condit vector float point number feed underli diffus model neural network howev one freedom choos convert condit vector stabl diffus exampl impos condit form mechan queri intermedi represent imag key valu condit vector condit select appli part imag new kind condit finetun upon base model use controlnet particularli simpl exampl consid imag inpaint condit x x refer imag inpaint mask condit impos step backward diffus process first sampl x n α x σ x x noisi version x x replac x x x x mean elementwis multipl anoth applic mechan imag edit condit limit gener imag specif categori accord specif caption exampl demonstr gener human motion condit audio clip human walk allow sync motion soundtrack video human run text descript human motion etc condit diffus model mathemat formul see methodolog summari gener imag take long time one tri gener small imag base diffus model upscal model upscal done gan transform signal process method like lanczo resampl diffus model use perform upscal cascad diffus model stack multipl diffus model one anoth style progress gan lowest level standard diffus model gener imag imag would upscal diffus model specif train upscal process repeat detail diffus upscal train follow section collect notabl diffus model briefli describ architectur seri openai diffus model imag first version actual diffus model instead use transform architectur autoregress gener sequenc token convert imag decod discret vae releas clip classifi use rank gener imag accord close imag fit text glide diffus model small version releas publicli soon releas cascad diffus model gener imag text invert clip imag encod techniqu term unclip unclip method contain model clip imag encod clip text encod imag decod prior model diffus model autoregress model train prior model train convert clip imag encod clip text encod imag decod train convert clip imag encod back imag infer text convert clip text encod vector convert prior model imag encod convert imag decod imag sora diffus transform model dit stabl diffus releas stabil ai consist denois latent diffus model million paramet vae text encod denois network block allow condit imag gener stabl diffus chang latent diffus model unet transform model dit use rectifi flow stabl video latent diffus model video object imagen use languag model encod input text embed vector cascad diffus model three first step denois white nois imag condit embed vector text model paramet second step upscal imag condit embed model paramet third step similar upscal model paramet three denois network muse diffus model transform train predict mask imag token unmask imag token imagen also gener imag base prompt mix imag text inform avail imagen inform avail veo gener video latent diffus diffus condit vector encod text prompt imag prompt diffus model diffus model autoregress causal mask transform mostli architectur transfus transform combin autoregress text gener denois diffus specif gener text autoregress causal mask gener imag denois multipl time imag token attent movi gen seri diffus transform oper latent space flow match
Self-organizing map,https://en.wikipedia.org/wiki/Self-organizing_map,"A self-organizing map (SOM) or self-organizing feature map (SOFM) is an unsupervised machine learning technique used to produce a low-dimensional (typically two-dimensional) representation of a higher-dimensional data set while preserving the topological structure of the data. For example, a data set with 



p


{\displaystyle p}

 variables measured in 



n


{\displaystyle n}

 observations could be represented as clusters of observations with similar values for the variables. These clusters then could be visualized as a two-dimensional ""map"" such that observations in proximal clusters have more similar values than observations in distal clusters. This can make high-dimensional data easier to visualize and analyze.
 An SOM is a type of artificial neural network but is trained using competitive learning rather than the error-correction learning (e.g., backpropagation with gradient descent) used by other artificial neural networks. The SOM was introduced by the Finnish professor Teuvo Kohonen in the 1980s and therefore is sometimes called a Kohonen map or Kohonen network.[1][2] The Kohonen map or network is a computationally convenient abstraction building on biological models of neural systems from the 1970s[3] and morphogenesis models dating back to Alan Turing in the 1950s.[4]
SOMs create internal representations reminiscent of the cortical homunculus[citation needed], a distorted representation of the human body, based on a neurological ""map"" of the areas and proportions of the human brain dedicated to processing sensory functions, for different parts of the body.
 Self-organizing maps, like most artificial neural networks, operate in two modes: training and mapping. First, training uses an input data set (the ""input space"") to generate a lower-dimensional representation of the input data (the ""map space""). Second, mapping classifies additional input data using the generated map.
 In most cases, the goal of training is to represent an input space with p dimensions as a map space with two dimensions. Specifically, an input space with p variables is said to have p dimensions. A map space consists of components called ""nodes"" or ""neurons"", which are arranged as a hexagonal or rectangular grid with two dimensions.[5] The number of nodes and their arrangement are specified beforehand based on the larger goals of the analysis and exploration of the data.
 Each node in the map space is associated with a ""weight"" vector, which is the position of the node in the input space. While nodes in the map space stay fixed, training consists in moving weight vectors toward the input data (reducing a distance metric such as Euclidean distance) without spoiling the topology induced from the map space. After training, the map can be used to classify additional observations for the input space by finding the node with the closest weight vector (smallest distance metric) to the input space vector.
 The goal of learning in the self-organizing map is to cause different parts of the network to respond similarly to certain input patterns. This is partly motivated by how visual, auditory or other sensory information is handled in separate parts of the cerebral cortex in the human brain.[6]
 The weights of the neurons are initialized either to small random values or sampled evenly from the subspace spanned by the two largest principal component eigenvectors. With the latter alternative, learning is much faster because the initial weights already give a good approximation of SOM weights.[7]
 The network must be fed a large number of example vectors that represent, as close as possible, the kinds of vectors expected during mapping. The examples are usually administered several times as iterations.
 The training utilizes competitive learning. When a training example is fed to the network, its Euclidean distance to all weight vectors is computed. The neuron whose weight vector is most similar to the input is called the best matching unit (BMU). The weights of the BMU and neurons close to it in the SOM grid are adjusted towards the input vector. The magnitude of the change decreases with time and with the grid-distance from the BMU. The update formula for a neuron v with weight vector Wv(s) is
 where s is the step index, t is an index into the training sample, u is the index of the BMU for the input vector D(t), α(s) is a monotonically decreasing learning coefficient; θ(u, v, s) is the neighborhood function which gives the distance between the neuron u and the neuron v in step s.[8] Depending on the implementations, t can scan the training data set systematically (t is 0, 1, 2...T-1, then repeat, T being the training sample's size), be randomly drawn from the data set (bootstrap sampling), or implement some other sampling method (such as jackknifing).
 The neighborhood function θ(u, v, s) (also called function of lateral interaction) depends on the grid-distance between the BMU (neuron u) and neuron v. In the simplest form, it is 1 for all neurons close enough to BMU and 0 for others, but the Gaussian and Mexican-hat[9] functions are common choices, too. Regardless of the functional form, the neighborhood function shrinks with time.[6] At the beginning when the neighborhood is broad, the self-organizing takes place on the global scale. When the neighborhood has shrunk to just a couple of neurons, the weights are converging to local estimates. In some implementations, the learning coefficient α and the neighborhood function θ decrease steadily with increasing s, in others (in particular those where t scans the training data set) they decrease in step-wise fashion, once every T steps.
 This process is repeated for each input vector for a (usually large) number of cycles λ. The network winds up associating output nodes with groups or patterns in the input data set. If these patterns can be named, the names can be attached to the associated nodes in the trained net.
 During mapping, there will be one single winning neuron: the neuron whose weight vector lies closest to the input vector. This can be simply determined by calculating the Euclidean distance between input vector and weight vector.
 While representing input data as vectors has been emphasized in this article, any kind of object which can be represented digitally, which has an appropriate distance measure associated with it, and in which the necessary operations for training are possible can be used to construct a self-organizing map. This includes matrices, continuous functions or even other self-organizing maps.
 The variable names mean the following, with vectors in bold,
 The key design choices are the shape of the SOM, the neighbourhood function, and the learning rate schedule. The idea of the neighborhood function is to make it such that the BMU is updated the most, its immediate neighbors are updated a little less, and so on. The idea of the learning rate schedule is to make it so that the map updates are large at the start, and gradually stop updating.
 For example, if we want to learn a SOM using a square grid, we can index it using 



(
i
,
j
)


{\displaystyle (i,j)}

 where both 



i
,
j
∈
1
:
N


{\displaystyle i,j\in 1:N}

. The neighborhood function can make it so that the BMU updates in full, the nearest neighbors update in half, and their neighbors update in half again, etc.



θ
(
(
i
,
j
)
,
(

i
′

,

j
′

)
,
s
)
=


1

2


|

i
−

i
′


|

+

|

j
−

j
′


|





=


{



1



if 

i
=

i
′

,
j
=

j
′





1

/

2



if 


|

i
−

i
′


|

+

|

j
−

j
′


|

=
1




1

/

4



if 


|

i
−

i
′


|

+

|

j
−

j
′


|

=
2




⋯


⋯








{\displaystyle \theta ((i,j),(i',j'),s)={\frac {1}{2^{|i-i'|+|j-j'|}}}={\begin{cases}1&{\text{if }}i=i',j=j'\\1/2&{\text{if }}|i-i'|+|j-j'|=1\\1/4&{\text{if }}|i-i'|+|j-j'|=2\\\cdots &\cdots \end{cases}}}

And we can use a simple linear learning rate schedule 



α
(
s
)
=
1
−
s

/

λ


{\displaystyle \alpha (s)=1-s/\lambda }

.
 Notice in particular, that the update rate does not depend on where the point is in the Euclidean space, only on where it is in the SOM itself. For example, the points 



(
1
,
1
)
,
(
1
,
2
)


{\displaystyle (1,1),(1,2)}

 are close on the SOM, so they will always update in similar ways, even when they are far apart on the Euclidean space. In contrast, even if the points 



(
1
,
1
)
,
(
1
,
100
)


{\displaystyle (1,1),(1,100)}

 end up overlapping each other (such as if the SOM looks like a folded towel), they still do not update in similar ways.
 Selection of initial weights as good approximations of the final weights is a well-known problem for all iterative methods of artificial neural networks, including self-organizing maps. Kohonen originally proposed random initiation of weights.[10] (This approach is reflected by the algorithms described above.) More recently, principal component initialization, in which initial map weights are chosen from the space of the first principal components, has become popular due to the exact reproducibility of the results.[11]
 A careful comparison of random initialization to principal component initialization for a one-dimensional map, however, found that the advantages of principal component initialization are not universal. The best initialization method depends on the geometry of the specific dataset. Principal component initialization was preferable (for a one-dimensional map) when the principal curve approximating the dataset could be univalently and linearly projected on the first principal component (quasilinear sets). For nonlinear datasets, however, random initiation performed better.[12]
 There are two ways to interpret a SOM. Because in the training phase weights of the whole neighborhood are moved in the same direction, similar items tend to excite adjacent neurons. Therefore, SOM forms a semantic map where similar samples are mapped close together and dissimilar ones apart. This may be visualized by a U-Matrix (Euclidean distance between weight vectors of neighboring cells) of the SOM.[14][15][16]
 The other way is to think of neuronal weights as pointers to the input space. They form a discrete approximation of the distribution of training samples. More neurons point to regions with high training sample concentration and fewer where the samples are scarce.
 SOM may be considered a nonlinear generalization of Principal components analysis (PCA).[17] It has been shown, using both artificial and real geophysical data, that SOM has many advantages[18][19] over the conventional feature extraction methods such as Empirical Orthogonal Functions (EOF) or PCA.
 Originally, SOM was not formulated as a solution to an optimisation problem. Nevertheless, there have been several attempts to modify the definition of SOM and to formulate an optimisation problem which gives similar results.[20] For example, Elastic maps use the mechanical metaphor of elasticity to approximate principal manifolds:[21] the analogy is an elastic membrane and plate.
",map som featur map sofm unsupervis machin learn techniqu use produc typic represent data set preserv topolog structur data exampl data set p p variabl measur n n observ could repres cluster observ similar valu variabl cluster could visual map observ proxim cluster similar valu observ distal cluster make data easier visual analyz som type artifici neural network train use competit learn rather learn backpropag gradient descent use artifici neural network som introduc finnish professor teuvo kohonen therefor sometim call kohonen map kohonen network kohonen map network comput conveni abstract build biolog model neural system morphogenesi model date back alan ture som creat intern represent reminisc cortic homunculu citat need distort represent human bodi base neurolog map area proport human brain dedic process sensori function differ part bodi map like artifici neural network oper two mode train map first train use input data set input space gener represent input data map space second map classifi addit input data use gener map case goal train repres input space p dimens map space two dimens specif input space p variabl said p dimens map space consist compon call node neuron arrang hexagon rectangular grid two dimens number node arrang specifi beforehand base larger goal analysi explor data node map space associ weight vector posit node input space node map space stay fix train consist move weight vector toward input data reduc distanc metric euclidean distanc without spoil topolog induc map space train map use classifi addit observ input space find node closest weight vector smallest distanc metric input space vector goal learn map caus differ part network respond similarli certain input pattern partli motiv visual auditori sensori inform handl separ part cerebr cortex human brain weight neuron initi either small random valu sampl evenli subspac span two largest princip compon eigenvector latter altern learn much faster initi weight alreadi give good approxim som weight network must fed larg number exampl vector repres close possibl kind vector expect map exampl usual administ sever time iter train util competit learn train exampl fed network euclidean distanc weight vector comput neuron whose weight vector similar input call best match unit bmu weight bmu neuron close som grid adjust toward input vector magnitud chang decreas time bmu updat formula neuron v weight vector wv step index index train sampl u index bmu input vector α monoton decreas learn coeffici θ u v neighborhood function give distanc neuron u neuron v step depend implement scan train data set systemat repeat train sampl size randomli drawn data set bootstrap sampl implement sampl method jackknif neighborhood function θ u v also call function later interact depend bmu neuron u neuron simplest form neuron close enough bmu other gaussian function common choic regardless function form neighborhood function shrink time begin neighborhood broad take place global scale neighborhood shrunk coupl neuron weight converg local estim implement learn coeffici α neighborhood function θ decreas steadili increas other particular scan train data set decreas fashion everi step process repeat input vector usual larg number cycl network wind associ output node group pattern input data set pattern name name attach associ node train net map one singl win neuron neuron whose weight vector lie closest input vector simpli determin calcul euclidean distanc input vector weight vector repres input data vector emphas articl kind object repres digit appropri distanc measur associ necessari oper train possibl use construct map includ matric continu function even map variabl name mean follow vector bold key design choic shape som neighbourhood function learn rate schedul idea neighborhood function make bmu updat immedi neighbor updat littl less idea learn rate schedul make map updat larg start gradual stop updat exampl want learn som use squar grid index use j j j n n neighborhood function make bmu updat full nearest neighbor updat half neighbor updat half etc θ j j j j j j j j j j j j case case use simpl linear learn rate schedul α λ notic particular updat rate depend point euclidean space som exampl point close som alway updat similar way even far apart euclidean space contrast even point end overlap som look like fold towel still updat similar way select initi weight good approxim final weight problem iter method artifici neural network includ map kohonen origin propos random initi weight approach reflect algorithm describ recent princip compon initi initi map weight chosen space first princip compon becom popular due exact reproduc result care comparison random initi princip compon initi map howev found advantag princip compon initi univers best initi method depend geometri specif dataset princip compon initi prefer map princip curv approxim dataset could unival linearli project first princip compon quasilinear set nonlinear dataset howev random initi perform better two way interpret som train phase weight whole neighborhood move direct similar item tend excit adjac neuron therefor som form semant map similar sampl map close togeth dissimilar one apart may visual euclidean distanc weight vector neighbor cell som way think neuron weight pointer input space form discret approxim distribut train sampl neuron point region high train sampl concentr fewer sampl scarc som may consid nonlinear gener princip compon analysi pca shown use artifici real geophys data som mani advantag convent featur extract method empir orthogon function eof pca origin som formul solut optimis problem nevertheless sever attempt modifi definit som formul optimis problem give similar result exampl elast map use mechan metaphor elast approxim princip manifold analog elast membran plate
Convolutional neural network,https://en.wikipedia.org/wiki/Convolutional_neural_network,"A convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns features by itself via filter (or kernel) optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio.[1] Convolution-based networks are the de-facto standard in deep learning-based approaches to computer vision and image processing, and have only recently been replaced -- in some cases -- by newer deep learning architectures such as the transformer. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections.[2][3] For example, for each neuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 × 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels,[4][5] only 25 neurons are required to process 5x5-sized tiles.[6][7] Higher-layer features are extracted from wider context windows, compared to lower-layer features.
 Some applications of CNNs include: 
 CNNs are also known as shift invariant or space invariant artificial neural networks, based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps.[13][14] Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input.[15]
 Feed-forward neural networks are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The ""full connectivity"" of these networks makes them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increase the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set.[16]
 Convolutional networks were inspired by biological processes[17][18][19][20] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.
 CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage.[to whom?]
 A convolutional neural network consists of an input layer, hidden layers and an output layer. In a convolutional neural network, the hidden layers include one or more layers that perform convolutions. Typically this includes a layer that performs a dot product of the convolution kernel with the layer's input matrix. This product is usually the Frobenius inner product, and its activation function is commonly ReLU. As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers, fully connected layers, and normalization layers.
Here it should be noted how close a convolutional neural network is to a matched filter.[21]
 In a CNN, the input is a tensor with shape:
 (number of inputs) × (input height) × (input width) × (input channels)
 After passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape:
 (number of inputs) × (feature map height) × (feature map width) × (feature map channels).
 Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus.[22] Each convolutional neuron processes data only for its receptive field. 
 Although fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs (e.g., high-resolution images), which would require massive numbers of neurons because each pixel is a relevant input feature. A fully connected layer for an image of size 100 × 100 has 10,000 weights for each neuron in the second layer. Convolution reduces the number of free parameters, allowing the network to be deeper.[6] For example, using a 5 × 5 tiling region, each with the same shared weights, requires only 25 neurons. Using regularized weights over fewer parameters avoids the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks.[2][3]
 To speed processing, standard convolutional layers can be replaced by depthwise separable convolutional layers,[23] which are based on a depthwise convolution followed by a pointwise convolution. The depthwise convolution is a spatial convolution applied independently over each channel of the input tensor, while the pointwise convolution is a standard convolution restricted to the use of 



1
×
1


{\displaystyle 1\times 1}

 kernels.
 Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, tiling sizes such as 2 × 2 are commonly used. Global pooling acts on all the neurons of the feature map.[24][25] There are two common types of pooling in popular use: max and average. Max pooling uses the maximum value of each local cluster of neurons in the feature map,[26][27] while average pooling takes the average value.
 Fully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional multilayer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.
 In neural networks, each neuron receives input from some number of locations in the previous layer. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. Typically the area is a square (e.g. 5 by 5 neurons). Whereas, in a fully connected layer, the receptive field is the entire previous layer. Thus, in each convolutional layer, each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over, which takes the value of a pixel into account, as well as its surrounding pixels. When using dilated layers, the number of pixels in the receptive field remains constant, but the field is more sparsely populated as its dimensions grow when combining the effect of several layers.
 To manipulate the receptive field size as desired, there are some alternatives to the standard convolutional layer. For example, atrous or dilated convolution[28][29] expands the receptive field size without increasing the number of parameters by interleaving visible and blind regions. Moreover, a single dilated convolutional layer can comprise filters with multiple dilation ratios,[30] thus having a variable receptive field size.
 Each neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights.
 The vectors of weights and biases are called filters and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter, as opposed to each receptive field having its own bias and vector weighting.[31]
 
 A deconvolutional neural network is essentially the reverse of a CNN. It consists of deconvolutional layers and unpooling layers.[32]
 A deconvolutional layer is the transpose of a convolutional layer. Specifically, a convolutional layer can be written as a multiplication with a matrix, and a deconvolutional layer is multiplication with the transpose of that matrix.[33]
 An unpooling layer expands the layer. The max-unpooling layer is the simplest, as it simply copies each entry multiple times. For example, a 2-by-2 max-unpooling layer is 



[
x
]
↦


[



x


x




x


x



]




{\displaystyle [x]\mapsto {\begin{bmatrix}x&x\\x&x\end{bmatrix}}}

.
 Deconvolution layers are used in image generators. By default, it creates periodic checkerboard artifact, which can be fixed by upscale-then-convolve.[34]
 CNN are often compared to the way the brain achieves vision processing in living organisms.[35]
 Work by Hubel and Wiesel in the 1950s and 1960s showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field.[36] Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space.[citation needed] The cortex in each hemisphere represents the contralateral visual field.[citation needed]
 Their 1968 paper identified two basic visual cell types in the brain:[18]
 Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.[37][36]
 Inspired by Hubel and Wiesel's work, in 1969, Kunihiko Fukushima published a deep CNN that uses ReLU activation function.[38] Unlike most modern networks, this network used hand-designed kernels. It was not used in his neocognitron, since all the weights were nonnegative; lateral inhibition was used instead. The rectifier has become the most popular activation function for CNNs and deep neural networks in general.[39]
 The ""neocognitron"" was introduced by Kunihiko Fukushima in 1979.[40][19][17] The kernels were trained by unsupervised learning. It was inspired by the above-mentioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers:
 In a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging with inhibition and saturation, J. Weng et al. in 1993 introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch.[41] Max-pooling is often used in modern CNNs.[42]
 Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron.[17] Today, however, the CNN architecture is usually trained through backpropagation.
 The term ""convolution"" first appears in neural networks in a paper by Toshiteru Homma, Les Atlas, and Robert Marks II at the first Conference on Neural Information Processing Systems in 1987. Their paper replaced multiplication with convolution in time, inherently providing shift invariance, motivated by and connecting more directly to the signal-processing concept of a filter, and demonstrated it on a speech recognition task.[7] They also pointed out that as a data-trainable system, convolution is essentially equivalent to correlation since reversal of the weights does not affect the final learned function (""For convenience, we denote * as correlation instead of convolution. Note that convolving a(t) with b(t) is equivalent to correlating a(-t) with b(t)."").[7] Modern CNN implementations typically do correlation and call it convolution, for convenience, as they did here.
 The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was one of the first convolutional networks, as it achieved shift-invariance.[43] A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation.[44] Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.[43]
 TDNNs are convolutional networks that share weights along the temporal dimension.[45] They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant that performs a two-dimensional convolution.[46] Since these TDNNs operated on spectrograms, the resulting phoneme recognition system was invariant to both time and frequency shifts, as with images processed by a neocognitron.
 TDNNs improved the performance of far-distance speech recognition.[47]
 Denker et al. (1989) designed a 2-D CNN system to recognize hand-written ZIP Code numbers.[48] However, the lack of an efficient training method to determine the kernel coefficients of the involved convolutions meant that all the coefficients had to be laboriously hand-designed.[49]
 Following the advances in the training of 1-D CNNs by Waibel et al. (1987), Yann LeCun et al. (1989)[49] used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types. 
Wei Zhang et al. (1988)[13][14] used back-propagation to train the convolution kernels of a CNN for alphabets recognition. The model was called shift-invariant pattern recognition neural network before the name CNN was coined later in the early 1990s. Wei Zhang et al. also applied the same CNN without the last fully connected layer for medical image object segmentation (1991)[50] and breast cancer detection in mammograms (1994).[51]
 This approach became a foundation of modern computer vision.
 In 1990 Yamaguchi et al. introduced the concept of max pooling, a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling to realize a speaker-independent isolated word recognition system.[26] In their system they used several TDNNs per word, one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification.
 LeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1995,[52] classifies hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources.
 It was superior than other commercial courtesy amount reading systems (as of 1995). The system was integrated in NCR's check reading systems, and fielded in several American banks since June 1996, reading millions of checks per day.[53]
 A shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988.[13][14] It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer. The model was trained with back-propagation. The training algorithm was further improved in 1991[54] to improve its generalization ability. The model architecture was modified by removing the last fully connected layer and applied for medical image segmentation (1991)[50] and automatic detection of breast cancer in mammograms (1994).[51]
 A different convolution-based design was proposed in 1988[55] for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.[56][57]
 Although CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs).
 In 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU.[58] In 2005, another paper also emphasised the value of GPGPU for machine learning.[59]
 The first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU.[60] In the same period, GPUs were also used for unsupervised training of deep belief networks.[61][62][63][64]
 In 2010, Dan Ciresan et al. at IDSIA trained deep feedforward networks on GPUs.[65] In 2011, they extended this to CNNs, accelerating by 60 compared to training CPU.[24] In 2011, the network win an image recognition contest where they achieved superhuman performance for the first time.[66] Then they won more competitions and achieved state of the art on several benchmarks.[67][42][27]
 Subsequently, AlexNet, a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012.[68] It was an early catalytic event for the AI boom.
 Compared to the training of CNNs using GPUs, not much attention was given to CPU. (Viebke et al 2019) parallelizes CNN by thread- and SIMD-level parallelism that is available on the Intel Xeon Phi.[69][70]
 In the past, traditional multilayer perceptron (MLP) models were used for image recognition.[example needed] However, the full connectivity between nodes caused the curse of dimensionality, and was computationally intractable with higher-resolution images. A 1000×1000-pixel image with RGB color channels has 3 million weights per fully-connected neuron, which is too high to feasibly process efficiently at scale.
 For example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.
 Also, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in data with a grid-topology (such as images), both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.
 Convolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:
 Together, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.
 
A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below. The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the filter entries and the input, producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.[73][nb 1]
 Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input. Each entry in an activation map use the same set of parameters that define the filter.
 Self-supervised learning has been adapted for use in convolutional layers by using sparse patches with a high-mask ratio and a global response normalization layer.[citation needed]
 When dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.
 The extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learned (British English: learnt) filters produce the strongest response to a spatially local input pattern.
 Three hyperparameters control the size of the output volume of the convolutional layer: the depth, stride, and padding size:
 The spatial size of the output volume is a function of the input volume size 



W


{\displaystyle W}

, the kernel field size 



K


{\displaystyle K}

 of the convolutional layer neurons, the stride 



S


{\displaystyle S}

, and the amount of zero padding 



P


{\displaystyle P}

 on the border. The number of neurons that ""fit"" in a given volume is then:
 If this number is not an integer, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be 



P
=
(
K
−
1
)

/

2


{\textstyle P=(K-1)/2}

 when the stride is 



S
=
1


{\displaystyle S=1}

 ensures that the input volume and output volume will have the same size spatially. However, it is not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding.
 A parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as a depth slice, the neurons in each depth slice are constrained to use the same weights and bias.
 Since all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neuron's weights with the input volume.[nb 2] Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.[15]
 Sometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a ""locally connected layer"".
 Another important concept of CNNs is pooling, which is used as a form of non-linear down-sampling. Pooling provides downsampling because it reduces the spatial dimensions (height and width) of the input feature maps while retaining the most important information. There are several non-linear functions to implement pooling, where max pooling and average pooling are the most common. Pooling aggregates information from small regions of the input creating partitions of the input feature map, typically using a fixed-size window (like 2x2) and applying a stride (often 2) to move the window across the input.[75] Note that without using a stride greater than 1, pooling would not perform downsampling, as it would simply move the pooling window across the input one step at a time, without reducing the size of the feature map. In other words, the stride is what actually causes the downsampling by determining how much the pooling window moves over the input.
 Intuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting. This is known as down-sampling. It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function, such as a ReLU layer) in a CNN architecture.[73]: 460–461  While pooling layers contribute to local translation invariance, they do not provide global translation invariance in a CNN, unless a form of global pooling is used.[15][72] The pooling layer commonly operates independently on every depth, or slice, of the input and resizes it spatially. A very common form of max pooling is a layer with filters of size 2×2, applied with a stride of 2, which subsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations:




f

X
,
Y


(
S
)
=

max

a
,
b
=
0


1



S

2
X
+
a
,
2
Y
+
b


.


{\displaystyle f_{X,Y}(S)=\max _{a,b=0}^{1}S_{2X+a,2Y+b}.}


In this case, every max operation is over 4 numbers. The depth dimension remains unchanged (this is true for other forms of pooling as well).
 In addition to max pooling, pooling units can use other functions, such as average pooling or ℓ2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which generally performs better in practice.[76]
 Due to the effects of fast spatial reduction of the size of the representation,[which?] there is a recent trend towards using smaller filters[77] or discarding pooling layers altogether.[78]
 A channel max pooling (CMP) operation layer conducts the MP operation along the channel side among the corresponding positions of the consecutive feature maps for the purpose of redundant information elimination. The CMP makes the significant features gather together within fewer channels, which is important for fine-grained image classification that needs more discriminating features. Meanwhile, another advantage of the CMP operation is to make the channel number of feature maps smaller before it connects to the first fully connected (FC) layer. Similar to the MP operation, we denote the input feature maps and output feature maps of a CMP layer as F ∈ R(C×M×N) and C ∈ R(c×M×N), respectively, where C and c are the channel numbers of the input and output feature maps, M and N are the widths and the height of the feature maps, respectively. Note that the CMP operation only changes the channel number of the feature maps. The width and the height of the feature maps are not changed, which is different from the MP operation.[79]
 See [80][81] for reviews for pooling methods.
 ReLU is the abbreviation of rectified linear unit. It was proposed by Alston Householder in 1941,[82] and used in CNN by Kunihiko Fukushima in 1969.[38] ReLU applies the non-saturating activation function 



f
(
x
)
=
max
(
0
,
x
)


{\textstyle f(x)=\max(0,x)}

.[68] It effectively removes negative values from an activation map by setting them to zero.[83] It introduces nonlinearity to the decision function and in the overall network without affecting the receptive fields of the convolution layers.
In 2011, Xavier Glorot, Antoine Bordes and Yoshua Bengio found that ReLU enables better training of deeper networks,[84] compared to widely used activation functions prior to 2011.
 Other functions can also be used to increase nonlinearity, for example the saturating hyperbolic tangent 



f
(
x
)
=
tanh
⁡
(
x
)


{\displaystyle f(x)=\tanh(x)}

, 



f
(
x
)
=

|

tanh
⁡
(
x
)

|



{\displaystyle f(x)=|\tanh(x)|}

, and the sigmoid function 



σ
(
x
)
=
(
1
+

e

−
x



)

−
1




{\textstyle \sigma (x)=(1+e^{-x})^{-1}}

. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.[85]
 After several convolutional and max pooling layers, the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation, with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term).
 The ""loss layer"", or ""loss function"", specifies how training penalizes the deviation between the predicted output of the network, and the true data labels (during supervised learning). Various loss functions can be used, depending on the specific task.
 The Softmax loss function is used for predicting a single class of K mutually exclusive classes.[nb 3] Sigmoid cross-entropy loss is used for predicting K independent probability values in 



[
0
,
1
]


{\displaystyle [0,1]}

. Euclidean loss is used for regressing to real-valued labels 



(
−
∞
,
∞
)


{\displaystyle (-\infty ,\infty )}

.
 Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP).
 The kernel is the number of pixels processed together. It is typically expressed as the kernel's dimensions, e.g., 2x2, or 3x3.
 Padding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example, a convolutional layer using 3x3 kernels would receive a 2-pixel pad, that is 1 pixel on each side of the image.[citation needed]
 The stride is the number of pixels that the analysis window moves on each iteration. A stride of 2 means that each kernel is offset by 2 pixels from its predecessor.
 Since feature map size decreases with depth, layers near the input layer tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature values va with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.
 The number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.
 Common filter sizes found in the literature vary greatly, and are usually chosen based on the data set. Typical filter sizes range from 1x1 to 7x7. As two famous examples, AlexNet used 3x3, 5x5, and 11x11. Inceptionv3 used 1x1, 3x3, and 5x5.
 The challenge is to find the right level of granularity so as to create abstractions at the proper scale, given a particular data set, and without overfitting.
 Max pooling is typically used, often with a 2x2 dimension. This implies that the input is drastically downsampled, reducing processing cost.
 Greater pooling reduces the dimension of the signal, and may result in unacceptable information loss. Often, non-overlapping pooling windows perform best.[76]
 Dilation involves ignoring pixels within a kernel. This reduces processing memory potentially without significant signal loss. A dilation of 2 on a 3x3 kernel expands the kernel to 5x5, while still processing 9 (evenly spaced) pixels. Specifically, the processed pixels after the dilation are the cells (1,1), (1,3), (1,5), (3,1), (3,3), (3,5), (5,1), (5,3), (5,5), where (i,j) denotes the cell of the i-th row and j-th column in the expanded 5x5 kernel. Accordingly, dilation of 4 expands the kernel to 7x7.[citation needed]
 It is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input.[72] However, layers with a stride greater than one ignore the Nyquist-Shannon sampling theorem and might lead to aliasing of the input signal[72] While, in principle, CNNs are capable of implementing anti-aliasing filters, it has been observed that this does not happen in practice [86] and yield models that are not equivariant to translations.
Furthermore, if a CNN makes use of fully connected layers, translation equivariance does not imply translation invariance, as the fully connected layers are not invariant to shifts of the input.[87][15] One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer.[72] Additionally, several other partial solutions have been proposed, such as anti-aliasing before downsampling operations,[88] spatial transformer networks,[89] data augmentation, subsampling combined with pooling,[15] and capsule neural networks.[90]
 The accuracy of the final model is based on a sub-part of the dataset set apart at the start, often called a test-set. Other times methods such as k-fold cross-validation are applied. Other strategies include using conformal prediction.[91][92]
 Regularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.
 Because a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting is dropout, introduced in 2014.[93] At each training stage, individual nodes are either ""dropped out"" of the net (ignored) with probability 



1
−
p


{\displaystyle 1-p}

 or kept with probability 



p


{\displaystyle p}

, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.
 In the training stages, 



p


{\displaystyle p}

 is usually 0.5; for input nodes, it is typically much higher because information is directly lost when input nodes are ignored.
 At testing time after training has finished, we would ideally like to find a sample average of all possible 




2

n




{\displaystyle 2^{n}}

 dropped-out networks; unfortunately this is unfeasible for large values of 



n


{\displaystyle n}

. However, we can find an approximation by using the full network with each node's output weighted by a factor of 



p


{\displaystyle p}

, so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates 




2

n




{\displaystyle 2^{n}}

 neural nets, and as such allows for model combination, at test time only a single network needs to be tested.
 By avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical, even for deep neural networks. The technique seems to reduce node interactions, leading them to learn more robust features[clarification needed] that better generalize to new data.
 DropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability 



1
−
p


{\displaystyle 1-p}

. Each unit thus receives input from a random subset of units in the previous layer.[94]
 DropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.
 A major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.
 Even before Dropout, in 2013 a technique called stochastic pooling,[95] the conventional deterministic pooling operations were replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout and data augmentation.
 An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images,[96] which delivers excellent performance on the MNIST data set.[96] Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.
 Because the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train, especially considering that some part should be spared for later testing, two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s.[52] For example, input images can be cropped, rotated, or rescaled to create new examples with the same labels as the original training set.[97]
 One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.
 Another simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a ""zero norm"".
 A simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant('alpha' hyperparameter), thus increasing the penalty for large weight vectors.
 L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.
 L1 regularization is also common. It makes the weight vectors sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined; this is called elastic net regularization.
 Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector 






w
→





{\displaystyle {\vec {w}}}

 of every neuron to satisfy 



‖



w
→




‖

2


<
c


{\displaystyle \|{\vec {w}}\|_{2}<c}

. Typical values of 



c


{\displaystyle c}

 are order of 3–4. Some papers report improvements[98] when using this form of regularization.
 Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.[99]
 An earlier common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.[100]
 Thus, one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). This approach ensures that the higher-level entity (e.g. face) is present when the lower-level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (""pose vectors"") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.[101]
 CNNs are often used in image recognition systems. In 2012, an error rate of 0.23% on the MNIST database was reported.[27] Another paper on using CNN for image classification reported that the learning process was ""surprisingly fast""; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database.[24] Subsequently, a similar CNN called AlexNet[102] won the ImageNet Large Scale Visual Recognition Challenge 2012.
 When applied to facial recognition, CNNs achieved a large decrease in error rate.[103] Another paper reported a 97.6% recognition rate on ""5,600 still images of more than 10 subjects"".[20] CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.[104]
 The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014,[105] a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet[106] (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans.[107] The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.[citation needed]
 In 2015, a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.[108]
 Compared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space.[109][110] Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream.[111][112][113] Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies.[114][115] Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines[116] and Independent Subspace Analysis.[117] Its application can be seen in text-to-video model.[citation needed]
 CNNs have also been explored for natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing,[118] search query retrieval,[119] sentence modeling,[120] classification,[121] prediction[122] and other traditional NLP tasks.[123]
Compared to traditional language processing methods such as recurrent neural networks, CNNs can represent different contextual realities of language that do not rely on a series-sequence assumption, while RNNs are better suitable when classical time series modeling is required.[124][125][126][127]
 A CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain.[128]
 CNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based drug design.[129] The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures,[130] AtomNet discovers chemical features, such as aromaticity, sp3 carbons, and hydrogen bonding. Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus[131] and multiple sclerosis.[132]
 CNNs have been used in the game of checkers. From 1999 to 2001, Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checker using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides. Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%.[133][134] It also earned a win against the program Chinook at its ""expert"" level of play.[135]
 CNNs have been used in computer Go. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play.[136] Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.[137]
 A couple of CNNs for choosing moves to try (""policy network"") and evaluating positions (""value network"") driving MCTS were used by AlphaGo, the first to beat the best human player at the time.[138]
 Recurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better.[139][12] Dilated convolutions[140] might enable one-dimensional convolutional neural networks to effectively learn time series dependences.[141] Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients.[142] Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from.[143] CNNs can also be applied to further tasks in time series analysis (e.g., time series classification[144] or quantile forecasting[145]).
 As archaeological findings such as clay tablets with cuneiform writing are increasingly acquired using 3D scanners, benchmark datasets are becoming available, including HeiCuBeDa[146] providing almost 2000 normalized 2-D and 3-D datasets prepared with the GigaMesh Software Framework.[147] So curvature-based measures are used in conjunction with geometric neural networks (GNNs), e.g. for period classification of those clay tablets being among the oldest documents of human history.[148][149]
 For many applications, training data is not very available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights, this is known as transfer learning. Furthermore, this technique allows convolutional network architectures to successfully be applied to problems with tiny training sets.[150]
 End-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars.[151] With recent advances in visual salience, spatial attention, and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.[152][153]
 A deep Q-network (DQN) is a type of deep learning model that combines a deep neural network with Q-learning, a form of reinforcement learning. Unlike earlier reinforcement learning agents, DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs via reinforcement learning.[154]
 Preliminary results were presented in 2014, with an accompanying paper in February 2015.[155] The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it.[156]
 Convolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR[157] have been obtained using CDBNs.[158] The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid[159] by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks.
",convolut neural network cnn regular type neural network learn featur via filter kernel optim type deep learn network appli process make predict mani differ type data includ text imag audio network standard deep approach comput vision imag process recent replac case newer deep learn architectur transform vanish gradient explod gradient seen backpropag earlier neural network prevent use regular weight fewer connect exampl neuron layer weight would requir process imag size pixel howev appli cascad convolut kernel neuron requir process tile featur extract wider context window compar featur applic cnn includ cnn also known shift invari space invari artifici neural network base architectur convolut kernel filter slide along input featur provid respons known featur map convolut neural network invari translat due downsampl oper appli input neural network usual fulli connect network neuron one layer connect neuron next layer full connect network make prone overfit data typic way regular prevent overfit includ penal paramet train weight decay trim connect skip connect dropout etc robust dataset also increas probabl cnn learn gener principl character given dataset rather bias set convolut network inspir biolog process connect pattern neuron resembl organ anim visual cortex individu cortic neuron respond stimuli restrict region visual field known recept field recept field differ neuron partial overlap cover entir visual field cnn use rel littl compar imag classif algorithm mean network learn optim filter kernel autom learn wherea tradit algorithm filter independ prior knowledg human intervent featur extract major advantag convolut neural network consist input layer hidden layer output layer convolut neural network hidden layer includ one layer perform convolut typic includ layer perform dot product convolut kernel layer input matrix product usual frobeniu inner product activ function commonli relu convolut kernel slide along input matrix layer convolut oper gener featur map turn contribut input next layer follow layer pool layer fulli connect layer normal layer note close convolut neural network match filter cnn input tensor shape number input input height input width input channel pass convolut layer imag becom abstract featur map also call activ map shape number input featur map height featur map width featur map channel convolut layer convolv input pass result next layer similar respons neuron visual cortex specif stimulu convolut neuron process data recept field although fulli connect feedforward neural network use learn featur classifi data architectur gener impract larger input imag would requir massiv number neuron pixel relev input featur fulli connect layer imag size weight neuron second layer convolut reduc number free paramet allow network deeper exampl use tile region share weight requir neuron use regular weight fewer paramet avoid vanish gradient explod gradient problem seen backpropag earlier neural network speed process standard convolut layer replac depthwis separ convolut layer base depthwis convolut follow pointwis convolut depthwis convolut spatial convolut appli independ channel input tensor pointwis convolut standard convolut restrict use kernel convolut network may includ local global pool layer along tradit convolut layer pool layer reduc dimens data combin output neuron cluster one layer singl neuron next layer local pool combin small cluster tile size commonli use global pool act neuron featur map two common type pool popular use max averag max pool use maximum valu local cluster neuron featur map averag pool take averag valu fulli connect layer connect everi neuron one layer everi neuron anoth layer tradit multilay perceptron neural network mlp flatten matrix goe fulli connect layer classifi imag neural network neuron receiv input number locat previou layer convolut layer neuron receiv input restrict area previou layer call neuron recept field typic area squar neuron wherea fulli connect layer recept field entir previou layer thu convolut layer neuron take input larger area input previou layer due appli convolut take valu pixel account well surround pixel use dilat layer number pixel recept field remain constant field spars popul dimens grow combin effect sever layer manipul recept field size desir altern standard convolut layer exampl atrou dilat convolut expand recept field size without increas number paramet interleav visibl blind region moreov singl dilat convolut layer compris filter multipl dilat ratio thu variabl recept field size neuron neural network comput output valu appli specif function input valu receiv recept field previou layer function appli input valu determin vector weight bia typic real number learn consist iter adjust bias weight vector weight bias call filter repres particular featur input particular shape distinguish featur cnn mani neuron share filter reduc memori footprint singl bia singl vector weight use across recept field share filter oppos recept field bia vector weight deconvolut neural network essenti revers cnn consist deconvolut layer unpool layer deconvolut layer transpos convolut layer specif convolut layer written multipl matrix deconvolut layer multipl transpos matrix unpool layer expand layer layer simplest simpli copi entri multipl time exampl layer x x x x x x bmatrix x bmatrix deconvolut layer use imag gener default creat period checkerboard artifact fix cnn often compar way brain achiev vision process live organ work hubel wiesel show cat visual cortic contain neuron individu respond small region visual field provid eye move region visual space within visual stimuli affect fire singl neuron known recept field neighbor cell similar overlap recept field recept field size locat vari systemat across cortex form complet map visual space citat need cortex hemispher repres contralater visual field citat need paper identifi two basic visual cell type brain hubel wiesel also propos cascad model two type cell use pattern recognit task inspir hubel wiesel work kunihiko fukushima publish deep cnn use relu activ function unlik modern network network use kernel use neocognitron sinc weight nonneg later inhibit use instead rectifi becom popular activ function cnn deep neural network gener neocognitron introduc kunihiko fukushima kernel train unsupervis learn inspir work hubel wiesel neocognitron introduc two basic type layer variant neocognitron call cresceptron instead use fukushima spatial averag inhibit satur weng et al introduc method call downsampl unit comput maximum activ unit patch often use modern cnn sever supervis unsupervis learn algorithm propos decad train weight neocognitron today howev cnn architectur usual train backpropag term convolut first appear neural network paper toshiteru homma le atla robert mark ii first confer neural inform process system paper replac multipl convolut time inher provid shift invari motiv connect directli concept filter demonstr speech recognit task also point system convolut essenti equival correl sinc revers weight affect final learn function conveni denot correl instead convolut note convolv b equival correl b modern cnn implement typic correl call convolut conveni time delay neural network tdnn introduc alex waibel et al phonem recognit one first convolut network achiev tdnn convolut neural net convolut perform along time axi data first cnn util weight share combin train gradient descent use backpropag thu also use pyramid structur neocognitron perform global optim weight instead local one tdnn convolut network share weight along tempor dimens allow speech signal process hampshir waibel introduc variant perform convolut sinc tdnn oper spectrogram result phonem recognit system invari time frequenc shift imag process neocognitron tdnn improv perform speech recognit denker et al design cnn system recogn zip code number howev lack effici train method determin kernel coeffici involv convolut meant coeffici labori follow advanc train cnn waibel et al yann lecun et al use learn convolut kernel coeffici directli imag number learn thu fulli automat perform better manual coeffici design suit broader rang imag recognit problem imag type wei zhang et al use train convolut kernel cnn alphabet recognit model call pattern recognit neural network name cnn coin later earli wei zhang et al also appli cnn without last fulli connect layer medic imag object segment breast cancer detect mammogram approach becam foundat modern comput vision yamaguchi et al introduc concept max pool fix filter oper calcul propag maximum valu given region combin tdnn max pool realiz isol word recognit system system use sever tdnn per word one syllabl result tdnn input signal combin use max pool output pool layer pass network perform actual word classif pioneer convolut network lecun et al classifi number check british english chequ digit pixel imag abil process imag requir larger layer convolut neural network techniqu constrain avail comput resourc superior commerci courtesi amount read system system integr ncr check read system field sever american bank sinc june read million check per day neural network propos wei zhang et al imag charact recognit modifi neocognitron keep convolut interconnect imag featur layer last fulli connect layer model train train algorithm improv improv gener abil model architectur modifi remov last fulli connect layer appli medic imag segment automat detect breast cancer mammogram differ design propos applic decomposit electromyographi convolv signal via design modifi design although cnn invent breakthrough requir fast implement graphic process unit gpu shown oh jung standard neural network greatli acceler gpu implement time faster equival implement cpu anoth paper also emphasis valu gpgpu machin learn first cnn describ chellapilla et al implement time faster equival implement cpu period gpu also use unsupervis train deep belief network dan ciresan et al idsia train deep feedforward network gpu extend cnn acceler compar train cpu network win imag recognit contest achiev superhuman perform first time competit achiev state art sever benchmark subsequ alexnet similar cnn alex krizhevski et al imagenet larg scale visual recognit challeng earli catalyt event ai boom compar train cnn use gpu much attent given cpu viebk et al parallel cnn parallel avail intel xeon phi past tradit multilay perceptron mlp model use imag recognit exampl need howev full connect node caus curs dimension comput intract imag imag rgb color channel million weight per neuron high feasibl process effici scale exampl imag size wide high color channel singl fulli connect neuron first hidden layer regular neural network would weight imag howev would lead neuron weight also network architectur take account spatial structur data treat input pixel far apart way pixel close togeth ignor local refer data imag comput semant thu full connect neuron wast purpos imag recognit domin spatial local input pattern convolut neural network variant multilay perceptron design emul behavior visual cortex model mitig challeng pose mlp architectur exploit strong spatial local correl present natur imag oppos mlp cnn follow distinguish featur togeth properti allow cnn achiev better gener vision problem weight share dramat reduc number free paramet learn thu lower memori requir run network allow train larger power network cnn architectur form stack distinct layer transform input volum output volum hold class score differenti function distinct type layer commonli use discuss convolut layer core build block cnn layer paramet consist set learnabl filter kernel small recept field extend full depth input volum forward pass filter convolv across width height input volum comput dot product filter entri input produc activ map filter result network learn filter activ detect specif type featur spatial posit input nb stack activ map filter along depth dimens form full output volum convolut layer everi entri output volum thu also interpret output neuron look small region input entri activ map use set paramet defin filter learn adapt use convolut layer use spars patch ratio global respons normal layer citat need deal input imag impract connect neuron neuron previou volum network architectur take spatial structur data account convolut network exploit spatial local correl enforc spars local connect pattern neuron adjac layer neuron connect small region input volum extent connect hyperparamet call recept field neuron connect local space along width height alway extend along entir depth input volum architectur ensur learn british english learnt filter produc strongest respons spatial local input pattern three hyperparamet control size output volum convolut layer depth stride pad size spatial size output volum function input volum size w w kernel field size k k convolut layer neuron stride amount zero pad p p border number neuron fit given volum number integ stride incorrect neuron tile fit across input volum symmetr way gener set zero pad p k stride ensur input volum output volum size spatial howev alway complet necessari use neuron previou layer exampl neural network design may decid use portion pad paramet share scheme use convolut layer control number free paramet reli assumpt patch featur use comput spatial posit also use comput posit denot singl slice depth depth slice neuron depth slice constrain use weight bia sinc neuron singl depth slice share paramet forward pass depth slice convolut layer comput convolut neuron weight input volum nb therefor common refer set weight filter kernel convolv input result convolut activ map set activ map differ filter stack togeth along depth dimens produc output volum paramet share contribut translat invari cnn architectur sometim paramet share assumpt may make sens especi case input imag cnn specif center structur expect complet differ featur learn differ spatial locat one practic exampl input face center imag might expect differ featur learn differ part imag case common relax paramet share scheme instead simpli call layer local connect layer anoth import concept cnn pool use form pool provid downsampl reduc spatial dimens height width input featur map retain import inform sever function implement pool max pool averag pool common pool aggreg inform small region input creat partit input featur map typic use window like appli stride often move window across input note without use stride greater pool would perform downsampl would simpli move pool window across input one step time without reduc size featur map word stride actual caus downsampl determin much pool window move input intuit exact locat featur less import rough locat rel featur idea behind use pool convolut neural network pool layer serv progress reduc spatial size represent reduc number paramet memori footprint amount comput network henc also control overfit known common period insert pool layer success convolut layer one typic follow activ function relu layer cnn architectur pool layer contribut local translat invari provid global translat invari cnn unless form global pool use pool layer commonli oper independ everi depth slice input resiz spatial common form max pool layer filter size appli stride subsampl everi depth slice input along width height discard activ f x max b x b x case everi max oper number depth dimens remain unchang true form pool well addit max pool pool unit use function averag pool pool averag pool often use histor recent fallen favor compar max pool gener perform better practic due effect fast spatial reduct size represent recent trend toward use smaller filter discard pool layer altogeth channel max pool cmp oper layer conduct mp oper along channel side among correspond posit consecut featur map purpos redund inform elimin cmp make signific featur gather togeth within fewer channel import imag classif need discrimin featur meanwhil anoth advantag cmp oper make channel number featur map smaller connect first fulli connect fc layer similar mp oper denot input featur map output featur map cmp layer f r c r respect c c channel number input output featur map n width height featur map respect note cmp oper chang channel number featur map width height featur map chang differ mp oper see review pool method relu abbrevi rectifi linear unit propos alston household use cnn kunihiko fukushima relu appli activ function f x max x f x x effect remov neg valu activ map set zero introduc nonlinear decis function overal network without affect recept field convolut layer xavier glorot antoin bord yoshua bengio found relu enabl better train deeper network compar wide use activ function prior function also use increas nonlinear exampl satur hyperbol tangent f x tanh x f x x f x tanh x f x x sigmoid function σ x e x x relu often prefer function train neural network sever time faster without signific penalti gener accuraci sever convolut max pool layer final classif done via fulli connect layer neuron fulli connect layer connect activ previou layer seen regular artifici neural network activ thu comput affin transform matrix multipl follow bia offset vector addit learn fix bia term loss layer loss function specifi train penal deviat predict output network true data label supervis learn variou loss function use depend specif task softmax loss function use predict singl class k mutual exclus class nb sigmoid loss use predict k independ probabl valu euclidean loss use regress label hyperparamet variou set use control learn process cnn use hyperparamet standard multilay perceptron mlp kernel number pixel process togeth typic express kernel dimens pad addit typic pixel border imag done border pixel undervalu lost output would ordinarili particip singl recept field instanc pad appli typic one less correspond kernel dimens exampl convolut layer use kernel would receiv pad pixel side imag citat need stride number pixel analysi window move iter stride mean kernel offset pixel predecessor sinc featur map size decreas depth layer near input layer tend fewer filter higher layer equal comput layer product featur valu va pixel posit kept roughli constant across layer preserv inform input would requir keep total number activ number featur map time number pixel posit one layer next number featur map directli control capac depend number avail exampl task complex common filter size found literatur vari greatli usual chosen base data set typic filter size rang two famou exampl alexnet use use challeng find right level granular creat abstract proper scale given particular data set without overfit max pool typic use often dimens impli input drastic downsampl reduc process cost greater pool reduc dimens signal may result unaccept inform loss often pool window perform best dilat involv ignor pixel within kernel reduc process memori potenti without signific signal loss dilat kernel expand kernel still process evenli space pixel specif process pixel dilat cell j denot cell row column expand kernel accordingli dilat expand kernel citat need commonli assum cnn invari shift input convolut pool layer within cnn stride greater one inde equivari translat input howev layer stride greater one ignor sampl theorem might lead alias input signal principl cnn capabl implement filter observ happen practic yield model equivari translat furthermor cnn make use fulli connect layer translat equivari impli translat invari fulli connect layer invari shift input one solut complet translat invari avoid throughout network appli global averag pool last layer addit sever partial solut propos downsampl oper spatial transform network data augment subsampl combin pool capsul neural network accuraci final model base dataset set apart start often call time method appli strategi includ use conform predict regular process introduc addit inform solv problem prevent overfit cnn use variou type regular fulli connect layer occupi paramet prone overfit one method reduc overfit dropout introduc train stage individu node either drop net ignor probabl p kept probabl p p reduc network left incom outgo edg node also remov reduc network train data stage remov node reinsert network origin weight train stage p p usual input node typic much higher inform directli lost input node ignor test time train finish would ideal like find sampl averag possibl n n network unfortun unfeas larg valu n n howev find approxim use full network node output weight factor p p expect valu output node train stage biggest contribut dropout method although effect gener n n neural net allow model combin test time singl network need test avoid train node train data dropout decreas overfit method also significantli improv train speed make model combin practic even deep neural network techniqu seem reduc node interact lead learn robust featur clarif need better gener new data dropconnect gener dropout connect rather output unit drop probabl p unit thu receiv input random subset unit previou layer dropconnect similar dropout introduc dynam sparsiti within model differ sparsiti weight rather output vector layer word fulli connect layer dropconnect becom spars connect layer connect chosen random train stage major drawback dropout benefit convolut layer neuron fulli connect even dropout techniqu call stochast pool convent determinist pool oper replac stochast procedur activ within pool region pick randomli accord multinomi distribut given activ within pool region approach free hyperparamet combin regular approach dropout data augment altern view stochast pool equival standard max pool mani copi input imag small local deform similar explicit elast deform input imag deliv excel perform mnist data set use stochast pool multilay model give exponenti number deform sinc select higher layer independ degre model overfit determin power amount train receiv provid convolut network train exampl reduc overfit often enough avail data train especi consid part spare later test two approach either gener new data scratch possibl perturb exist data creat new one latter one use sinc exampl input imag crop rotat rescal creat new exampl label origin train set one simplest method prevent overfit network simpli stop train overfit chanc occur come disadvantag learn process halt anoth simpl way prevent overfit limit number paramet typic limit number hidden unit layer limit network depth convolut network filter size also affect number paramet limit number paramet restrict predict power network directli reduc complex function perform data thu limit amount overfit equival zero norm simpl form ad regular weight decay simpli add addit error proport sum weight norm squar magnitud norm weight vector error node level accept model complex reduc increas proportion constant hyperparamet thu increas penalti larg weight vector regular common form regular implement penal squar magnitud paramet directli object regular intuit interpret heavili penal peaki weight vector prefer diffus weight vector due multipl interact weight input use properti encourag network use input littl rather input lot regular also common make weight vector spars optim word neuron regular end use spars subset import input becom nearli invari noisi input regular combin call elast net regular anoth form regular enforc absolut upper bound magnitud weight vector everi neuron use project gradient descent enforc constraint practic correspond perform paramet updat normal enforc constraint clamp weight vector w w everi neuron satisfi w c w c typic valu c c order paper report improv use form regular pool lose precis spatial relationship part nose mouth face imag relationship need ident recognit overlap pool featur occur multipl pool help retain inform translat alon extrapol understand geometr relationship radic new viewpoint differ orient scale hand peopl good extrapol see new shape recogn differ viewpoint earlier common way deal problem train network transform data differ orient scale light etc network cope variat comput intens larg altern use hierarchi coordin frame use group neuron repres conjunct shape featur pose rel retina pose rel retina relationship coordin frame retina intrins featur coordin frame thu one way repres someth emb coordin frame within allow larg featur recogn use consist pose part nose mouth pose make consist predict pose whole face approach ensur entiti face present nose mouth agre predict pose vector neuron activ repres pose pose vector allow spatial transform model linear oper make easier network learn hierarchi visual entiti gener across viewpoint similar way human visual system impos coordin frame order repres shape cnn often use imag recognit system error rate mnist databas report anoth paper use cnn imag classif report learn process surprisingli fast paper best publish result achiev mnist databas norb databas subsequ similar cnn call alexnet imagenet larg scale visual recognit challeng appli facial recognit cnn achiev larg decreas error rate anoth paper report recognit rate still imag subject cnn use assess video qualiti object way manual train result system low root mean squar error imagenet larg scale visual recognit challeng benchmark object classif detect million imag hundr object class ilsvrc visual recognit challeng almost everi highli rank team use cnn basic framework winner googlenet foundat deepdream increas mean averag precis object detect reduc classif error best result date network appli layer perform convolut neural network imagenet test close human best algorithm still struggl object small thin small ant stem flower person hold quill hand also troubl imag distort filter increasingli common phenomenon modern digit camera contrast kind imag rare troubl human human howev tend troubl issu exampl good classifi object categori particular breed dog speci bird wherea convolut neural network handl citat need cnn demonstr abil spot face wide rang angl includ upsid even partial occlud competit perform network train databas imag includ face variou angl orient million imag without face use batch imag iter compar imag data domain rel littl work appli cnn video classif video complex imag sinc anoth tempor dimens howev extens cnn video domain explor one approach treat space time equival dimens input perform convolut time space anoth way fuse featur two convolut neural network one spatial one tempor stream long memori lstm recurr unit typic incorpor cnn account depend unsupervis learn scheme train featur introduc base convolut gate restrict boltzmann machin independ subspac analysi applic seen model citat need cnn also explor natur languag process cnn model effect variou nlp problem achiev excel result semant pars search queri retriev sentenc model classif predict tradit nlp task compar tradit languag process method recurr neural network cnn repres differ contextu realiti languag reli assumpt rnn better suitabl classic time seri model requir cnn convolut use time seri frequenc domain spectral residu unsupervis model detect anomali time domain cnn use drug discoveri predict interact molecul biolog protein identifi potenti treatment atomwis introduc atomnet first deep learn neural network drug design system train directli represent chemic interact similar imag recognit network learn compos smaller spatial proxim featur larger complex structur atomnet discov chemic featur aromat carbon hydrogen bond subsequ atomnet use predict novel candid biomolecul multipl diseas target notabl treatment ebola viru multipl sclerosi cnn use game checker fogel chellapilla publish paper show convolut neural network could learn play checker use learn process use prior human profession game rather focus minim set inform contain checkerboard locat type piec differ number piec two side ultim program test game player rank highest also earn win program chinook expert level play cnn use comput go decemb clark storkey publish paper show cnn train supervis learn databas human profession game could outperform gnu go win game mont carlo tree search fuego fraction time took fuego play later announc larg convolut neural network correctli predict profession move posit equal accuraci dan human player train convolut network use directli play game go without search beat tradit search program gnu go game match perform mont carlo tree search program fuego simul ten thousand playout million posit per move coupl cnn choos move tri polici network evalu posit valu network drive mct use alphago first beat best human player time recurr neural network gener consid best neural network architectur time seri forecast sequenc model gener recent studi show convolut network perform compar even better dilat convolut might enabl convolut neural network effect learn time seri depend convolut implement effici solut suffer vanish explod gradient convolut network provid improv forecast perform multipl similar time seri learn cnn also appli task time seri analysi time seri classif quantil forecast archaeolog find clay tablet cuneiform write increasingli acquir use scanner benchmark dataset becom avail includ heicubeda provid almost normal dataset prepar gigamesh softwar framework measur use conjunct geometr neural network gnn period classif clay tablet among oldest document human histori mani applic train data avail convolut neural network usual requir larg amount train data order avoid overfit common techniqu train network larger data set relat domain network paramet converg addit train step perform use data network weight known transfer learn furthermor techniqu allow convolut network architectur success appli problem tini train set train predict common practic comput vision howev human interpret explan requir critic system car recent advanc visual salienc spatial attent tempor attent critic spatial instant could visual justifi cnn predict deep dqn type deep learn model combin deep neural network form reinforc learn unlik earlier reinforc learn agent dqn util cnn learn directli sensori input via reinforc learn preliminari result present accompani paper februari research describ applic atari game deep reinforc learn model preced convolut deep belief network cdbn structur similar convolut neural network train similarli deep belief network therefor exploit structur imag like cnn make use like deep belief network provid gener structur use mani imag signal process task benchmark result standard imag dataset like cifar obtain use cdbn architectur convolut neural network extend neural abstract pyramid later feedback connect result recurr convolut network allow flexibl incorpor contextu inform iter resolv local ambigu contrast previou model output highest resolut gener semant segment imag reconstruct object local task
U-Net,https://en.wikipedia.org/wiki/U-Net,"U-Net is a convolutional neural network that was developed for image segmentation.[1] The network is based on a fully convolutional neural network[2] whose architecture was modified and extended to work with fewer training images and to yield more precise segmentation. Segmentation of a 512 × 512 image takes less than a second on a modern (2015) GPU using the U-Net architecture.[1] [3][4][5]
 The U-Net architecture has also been employed in diffusion models for iterative image denoising.[6] This technology underlies many modern image generation models, such as DALL-E, Midjourney, and Stable Diffusion.
 The U-Net architecture stems from the so-called ""fully convolutional network"".[2]
 The main idea is to supplement a usual contracting network by successive layers, where pooling operations are replaced by upsampling operators. Hence these layers increase the resolution of the output. A successive convolutional layer can then learn to assemble a precise output based on this information.[1]
 One important modification in U-Net is that there are a large number of feature channels in the upsampling part, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting part, and yields a u-shaped architecture. The network only uses the valid part of each convolution without any fully connected layers.[2] To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory.
 The network consists of a contracting path and an expansive path, which gives it the u-shaped architecture. The contracting path is a typical convolutional network that consists of repeated application of convolutions, each followed by a rectified linear unit (ReLU) and a max pooling operation. During the contraction, the spatial information is reduced while feature information is increased. The expansive pathway combines the feature and spatial information through a sequence of up-convolutions and concatenations with high-resolution features from the contracting path.[7]
 There are many applications of U-Net in biomedical image segmentation, such as brain image segmentation (''BRATS''[8]) and liver image segmentation (""siliver07""[9]) as well as protein binding site prediction.[10] U-Net implementations have also found use in the physical sciences, for example in the analysis of micrographs of materials.[11][12][13] Variations of the U-Net have also been applied for medical image reconstruction.[14] Here are some variants and applications of U-Net as follows:
 U-Net was created by Olaf Ronneberger, Philipp Fischer, Thomas Brox in 2015 and reported in the paper ""U-Net: Convolutional Networks for Biomedical Image Segmentation"".[1] It is an improvement and development of FCN: Evan Shelhamer, Jonathan Long, Trevor Darrell (2014). ""Fully convolutional networks for semantic segmentation"".[2]
",convolut neural network develop imag segment network base fulli convolut neural network whose architectur modifi extend work fewer train imag yield precis segment segment imag take less second modern gpu use architectur architectur also employ diffus model iter imag denois technolog underli mani modern imag gener model midjourney stabl diffus architectur stem fulli convolut network main idea supplement usual contract network success layer pool oper replac upsampl oper henc layer increas resolut output success convolut layer learn assembl precis output base inform one import modif larg number featur channel upsampl part allow network propag context inform higher resolut layer consequ expans path less symmetr contract part yield architectur network use valid part convolut without fulli connect layer predict pixel border region imag miss context extrapol mirror input imag tile strategi import appli network larg imag sinc otherwis resolut would limit gpu memori network consist contract path expans path give architectur contract path typic convolut network consist repeat applic convolut follow rectifi linear unit relu max pool oper contract spatial inform reduc featur inform increas expans pathway combin featur spatial inform sequenc concaten featur contract path mani applic biomed imag segment brain imag segment brat liver imag segment well protein bind site predict implement also found use physic scienc exampl analysi micrograph materi variat also appli medic imag reconstruct variant applic follow creat olaf ronneberg philipp fischer thoma brox report paper convolut network biomed imag segment improv develop fcn evan shelham jonathan long trevor darrel fulli convolut network semant segment
LeNet,https://en.wikipedia.org/wiki/LeNet,"LeNet is a series of convolutional neural network structure proposed by LeCun et al..[1] The earliest version, LeNet-1, was trained in 1989. In general, when ""LeNet"" is referred to without a number, it refers to LeNet-5 (1998), the most well-known version.
 Convolutional neural networks are a kind of feed-forward neural network whose artificial neurons can respond to a part of the surrounding cells in the coverage range and perform well in large-scale image processing. LeNet-5 was one of the earliest convolutional neural networks and was historically important during the development of deep learning.[2]
 In 1988, LeCun joined the Adaptive Systems Research Department at AT&T Bell Laboratories in Holmdel, New Jersey, United States, headed by Lawrence D. Jackel.
 In 1988, LeCun et al. published a neural network design that recognize handwritten zip code. However, its convolutional kernels were hand-designed.[3]
 In 1989, Yann LeCun et al. at Bell Labs first applied the backpropagation algorithm to practical applications, and believed that the ability to learn network generalization could be greatly enhanced by providing constraints from the task's domain. He combined a convolutional neural network trained by backpropagation algorithms to read handwritten numbers and successfully applied it in identifying handwritten zip code numbers provided by the US Postal Service. This was the prototype of what later came to be called LeNet-1.[4] In the same year, LeCun described a small handwritten digit recognition problem in another paper, and showed that even though the problem is linearly separable, single-layer networks exhibited poor generalization capabilities. When using shift-invariant feature detectors on a multi-layered, constrained network, the model could perform very well. He believed that these results proved that minimizing the number of free parameters in the neural network could enhance the generalization ability of the neural network.[5]
 In 1990, their paper described the application of backpropagation networks in handwritten digit recognition again. They only performed minimal preprocessing on the data, and the model was carefully designed for this task and it was highly constrained. The input data consisted of images, each containing a number, and the test results on the postal code digital data provided by the US Postal Service showed that the model had an error rate of only 1% and a rejection rate of about 9%.[6]
 Their research continued for the next four years, and in 1994 MNIST database was developed, for which LeNet-1 was too small, hence a new LeNet-4 was trained on it.[7]
 A year later the AT&T Bell Labs collective introduced LeNet-5 and reviewed various methods on handwritten character recognition in paper, using standard handwritten digits to identify benchmark tasks. These models were compared and the results showed that the latest network outperformed other models.[8]
 By 1998 Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner were able to provided examples of practical applications of neural networks, such as two systems for recognizing handwritten characters online and models that could read millions of checks per day.[1]
 The research achieved great success and aroused the interest of scholars in the study of neural networks. While the architecture of the best performing neural networks today are not the same as that of LeNet, the network was the starting point for a large number of neural network architectures, and also brought inspiration to the field.
 LeNet has several common motifs of modern convolutional neural networks, such as convolutional layer, pooling layer and full connection layer.[4]
 Before LeNet-1, the 1988 architecture[3] was a hybrid approach. The first stage scaled, deskewed, and skeletonized the input image. The second stage was a convolutional layer with 18 hand-designed kernels. The third stage was a fully connected network with one hidden layer.
 The LeNet-1 architecture has 3 hidden layers (H1-H3) and an output layer.[4] It has 1256 units, 64660 connections, and 9760 independent parameters.
 The dataset was 9298 grayscale images, digitized from handwritten zip codes that appeared on U.S. mail passing through the Buffalo, New York post office.[9] The training set had 7291 data points, and test set had 2007. Both training and test set contained ambiguous, unclassifiable, and misclassified data. Training took 3 days on a Sun workstation.
 Compared to the previous 1988 architecture, there was no skeletonization, and the convolutional kernels were learned automatically by backpropagation.
 A later version of LeNet-1 has four hidden layers (H1-H4) and an output layer. It takes a 28x28 pixel image as input, though the active region is 16x16 to avoid boundary effects.[10]
 The network 4635 units, 98442 connections, and 2578 trainable parameters. It was started by a previous CNN[11] with 4 times as many trainable parameters, then optimized by Optimal Brain Damage.[12] One forward pass requires about 140,000 multiply-add operations.[7]
 LeNet-4 was a larger version of LeNet-1 designed to fit the larger MNIST database. It had more feature maps in its convolutional layers, and had an additional layer of hidden units, fully connected to both the last convolutional layer and to the output units. It has 2 convolutions, 2 average poolings, and 2 fully connected layers. It has about 17000 trainable parameters.[7]
 One forward pass requires about 260,000 multiply-add operations.[7]
 LeNet-5 is similar to LeNet-4, but with more fully connected layers. Its architecture is shown in the image on the right. It has 2 convolutions, 2 average poolings, and 3 fully connected layers.
 LeNet-5 was trained for about 20 epoches over MNIST. It took 2 to 3 days of CPU time on a Silicon Graphics Origin 2000 server, using a single 200 MHz R10000 processor.[1]
 Recognizing simple digit images is the most classic application of LeNet as it was created because of that.
 Yann LeCun et al. created LeNet-1 in 1989. The paper Backpropagation Applied to Handwritten Zip Code Recognition[4] demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. And it had been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service.[4]
 After the development of LeNet-1, as a demonstration for real-time application, they loaded the neural network into a AT&T DSP-32C digital signal processor[13] with a peak performance of 12.5 million multiply-add operations per second. It could normalize-and-classify 10 digits a second, or classify 30 normalized digits a second. Shortly afterwards, the research group started working with a development group and a product group at NCR (acquired by AT&T in 1991). It resulted in ATM machines that could read the numerical amounts on checks using a LeNet loaded on DSP-32C. Later, NCR deployed a similar system in large cheque reading machines in bank back offices.[14]
 The LeNet-5 means the emergence of CNN and defines the basic components of CNN.[1] But it was not popular at that time because of the lack of hardware, especially since GPUs and other algorithms, such as SVM, could achieve similar effects or even exceed LeNet.
 Since the success of AlexNet in 2012, CNN has become the best choice for computer vision applications and many different types of CNN have been created, such as the R-CNN series. Nowadays, CNN models are quite different from LeNet, but they are all developed on the basis of LeNet.
 A three-layer tree architecture imitating LeNet-5 and consisting of only one convolutional layer, has achieved a similar success rate on the CIFAR-10 dataset.[15]
 Increasing the number of filters for the LeNet architecture results in a power law decay of the error rate. These results indicate that a shallow network can achieve the same performance as deep learning architectures.[16]
",lenet seri convolut neural network structur propos lecun et al earliest version train gener lenet refer without number refer version convolut neural network kind neural network whose artifici neuron respond part surround cell coverag rang perform well imag process one earliest convolut neural network histor import develop deep learn lecun join adapt system research depart bell laboratori holmdel new jersey unit state head lawrenc jackel lecun et al publish neural network design recogn handwritten zip code howev convolut kernel yann lecun et al bell lab first appli backpropag algorithm practic applic believ abil learn network gener could greatli enhanc provid constraint task domain combin convolut neural network train backpropag algorithm read handwritten number success appli identifi handwritten zip code number provid us postal servic prototyp later came call year lecun describ small handwritten digit recognit problem anoth paper show even though problem linearli separ network exhibit poor gener capabl use featur detector constrain network model could perform well believ result prove minim number free paramet neural network could enhanc gener abil neural network paper describ applic backpropag network handwritten digit recognit perform minim preprocess data model care design task highli constrain input data consist imag contain number test result postal code digit data provid us postal servic show model error rate reject rate research continu next four year mnist databas develop small henc new train year later bell lab collect introduc review variou method handwritten charact recognit paper use standard handwritten digit identifi benchmark task model compar result show latest network outperform model yann lecun leon bottou yoshua bengio patrick haffner abl provid exampl practic applic neural network two system recogn handwritten charact onlin model could read million check per day research achiev great success arous interest scholar studi neural network architectur best perform neural network today lenet network start point larg number neural network architectur also brought inspir field lenet sever common motif modern convolut neural network convolut layer pool layer full connect layer architectur hybrid approach first stage scale deskew skeleton input imag second stage convolut layer kernel third stage fulli connect network one hidden layer architectur hidden layer output layer unit connect independ paramet dataset grayscal imag digit handwritten zip code appear mail pass buffalo new york post offic train set data point test set train test set contain ambigu unclassifi misclassifi data train took day sun workstat compar previou architectur skeleton convolut kernel learn automat backpropag later version four hidden layer output layer take pixel imag input though activ region avoid boundari effect network unit connect trainabl paramet start previou cnn time mani trainabl paramet optim optim brain damag one forward pass requir oper larger version design fit larger mnist databas featur map convolut layer addit layer hidden unit fulli connect last convolut layer output unit convolut averag pool fulli connect layer trainabl paramet one forward pass requir oper similar fulli connect layer architectur shown imag right convolut averag pool fulli connect layer train epoch mnist took day cpu time silicon graphic origin server use singl mhz processor recogn simpl digit imag classic applic lenet creat yann lecun et al creat paper backpropag appli handwritten zip code recognit demonstr constraint integr backpropag network architectur network success appli recognit handwritten zip code digit provid postal servic develop demonstr applic load neural network digit signal processor peak perform million oper per second could digit second classifi normal digit second shortli afterward research group start work develop group product group ncr acquir result atm machin could read numer amount check use lenet load later ncr deploy similar system larg chequ read machin bank back offic mean emerg cnn defin basic compon cnn popular time lack hardwar especi sinc gpu algorithm svm could achiev similar effect even exceed lenet sinc success alexnet cnn becom best choic comput vision applic mani differ type cnn creat seri nowaday cnn model quit differ lenet develop basi lenet tree architectur imit consist one convolut layer achiev similar success rate dataset increas number filter lenet architectur result power law decay error rate result indic shallow network achiev perform deep learn architectur
AlexNet,https://en.wikipedia.org/wiki/AlexNet,"AlexNet is a convolutional neural network (CNN) architecture, designed by Alex Krizhevsky in collaboration with Ilya Sutskever and Geoffrey Hinton, who was Krizhevsky's Ph.D. advisor at the University of Toronto in 2012. It had 60 million parameters and 650,000 neurons.[1]
 The original paper's primary result was that the depth of the model was essential for its high performance, which was computationally expensive, but made feasible due to the utilization of graphics processing units (GPUs) during training.[1]
 The three formed team SuperVision and submitted AlexNet in the ImageNet Large Scale Visual Recognition Challenge on September 30, 2012.[2] The network achieved a top-5 error of 15.3%, more than 10.8 percentage points better than that of the runner-up.
 The architecture influenced a large number of subsequent work in deep learning, especially in applying neural networks to computer vision.
 
AlexNet contains eight layers: the first five are convolutional layers, some of them followed by max-pooling layers, and the last three are fully connected layers. The network, except the last layer, is split into two copies, each run on one GPU.[1] The entire structure can be written as (CNN → RN → MP)² → (CNN³ → MP) → (FC → DO)² → Linear → softmax where
 It used the non-saturating ReLU activation function, which trained better than tanh and sigmoid.[1]
 Because the network did not fit onto a single Nvidia GTX580 3GB GPU, it was split into two halves, one on each GPU.[1]: Section 3.2 
 The ImageNet training set had 1.2 million images. It was trained for 90 epochs, which took five to six days on two NVIDIA GTX 580 3GB GPUs,[1] which has a theoretical performance of 1.581 TFLOPS in float32 and release price 500 USD.[3] One forward pass of AlexNet takes about 4 GFLOPs.[4]
 It was trained with momentum gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005. Learning rate started at 




10

−
2




{\displaystyle 10^{-2}}

 and was manually decreased 10-fold whenever validation error appeared to stop decreasing. It was reduced three times during training, ending at 




10

−
5




{\displaystyle 10^{-5}}

.
 It used two forms of data augmentation, both computed on the fly on the CPU, thus ""computationally free"":
 It used local response normalization, and dropout regularization with drop probability 0.5.
 All weights were initialized as gaussians with 0 mean and 0.01 standard deviation. Biases in convolutional layers 2, 4, 5, and all fully-connected layers, were initialized to constant 1 to avoid the dying ReLU problem.
 AlexNet is a convolutional neural network. In 1980, Kunihiko Fukushima proposed an early CNN named neocognitron.[5][6] It was trained by an unsupervised learning algorithm. The LeNet-5 (Yann LeCun et al., 1989)[7][8] was trained by supervised learning with backpropagation algorithm, with an architecture that is essentially the same as AlexNet on a small scale. (J. Weng, 1993) added max-pooling.[9][10]
 During the 2000s, as GPU hardware improved, some researchers adapted these for general-purpose computing, including neural network training. (K. Chellapilla et al., 2006) trained a CNN on GPU that was 4 times faster than an equivalent CPU implementation.[11] A deep CNN of (Dan Cireșan et al., 2011) at IDSIA was 60 times faster than an equivalent CPU implementation.[12] Between May 15, 2011, and September 10, 2012, their CNN won four image competitions and achieved SOTA for multiple image databases.[13][14][15] According to the AlexNet paper,[1] Cireșan's earlier net is ""somewhat similar."" Both were written with CUDA to run on GPU.
 During the 1990 -- 2010 period, neural networks and were not better than other machine learning methods like kernel regression, support vector machines, AdaBoost, structured estimation,[16] among others. For computer vision in particular, much progress came from manual feature engineering, such as SIFT features, SURF features, HoG features, bags of visual words, etc. It was a minority position in computer vision that features can be learned directly from data, a position which became dominant after AlexNet.[17]
 In 2011, Geoffrey Hinton started reaching out to colleagues about ""What do I have to do to convince you that neural networks are the future?"", and Jitendra Malik, a sceptic of neural networks, recommended the PASCAL Visual Object Classes challenge. Hinton said its dataset was too small, so Malik recommended to him the ImageNet challenge.[18]
 While AlexNet and LeNet share essentially the same design and algorithm, AlexNet is much larger than LeNet and was trained on a much larger dataset on much faster hardware. Over the period of 20 years, both data and compute became cheaply available.[17]
 AlexNet is highly influential, resulting in much subsequent work in using CNNs for computer vision and using GPUs to accelerate deep learning. As of mid 2024, the AlexNet paper has been cited over 157,000 times according to Google Scholar.[19]
 At the time of publication, there was no framework available for GPU-based neural network training and inference. The codebase for AlexNet was released under a BSD license, and had been commonly used in neural network research for several subsequent years.[20][17]
 In one direction, subsequent works aimed to train increasingly deep CNNs that achieve increasingly higher performance on ImageNet. In this line of research are GoogLeNet (2014), VGGNet (2014), Highway network (2015), and ResNet (2015). Another direction aimed to reproduce the performance of AlexNet at a lower cost. In this line of research are SqueezeNet (2016), MobileNet (2017), EfficientNet (2019).
",alexnet convolut neural network cnn architectur design alex krizhevski collabor ilya sutskev geoffrey hinton krizhevski advisor univers toronto million paramet neuron origin paper primari result depth model essenti high perform comput expens made feasibl due util graphic process unit gpu train three form team supervis submit alexnet imagenet larg scale visual recognit challeng septemb network achiev error percentag point better architectur influenc larg number subsequ work deep learn especi appli neural network comput vision alexnet contain eight layer first five convolut layer follow layer last three fulli connect layer network except last layer split two copi run one gpu entir structur written cnn rn mp mp fc linear softmax use relu activ function train better tanh sigmoid network fit onto singl nvidia gpu split two halv one gpu section imagenet train set million imag train epoch took five six day two nvidia gtx gpu theoret perform tflop releas price usd one forward pass alexnet take gflop train momentum gradient descent batch size exampl momentum weight decay learn rate start manual decreas whenev valid error appear stop decreas reduc three time train end use two form data augment comput fli cpu thu comput free use local respons normal dropout regular drop probabl weight initi gaussian mean standard deviat bias convolut layer layer initi constant avoid die relu problem alexnet convolut neural network kunihiko fukushima propos earli cnn name neocognitron train unsupervis learn algorithm yann lecun et train supervis learn backpropag algorithm architectur essenti alexnet small scale weng ad gpu hardwar improv research adapt comput includ neural network train chellapilla et train cnn gpu time faster equival cpu implement deep cnn dan cireșan et idsia time faster equival cpu implement may septemb cnn four imag competit achiev sota multipl imag databas accord alexnet paper cireșan earlier net somewhat similar written cuda run gpu period neural network better machin learn method like kernel regress support vector machin adaboost structur estim among other comput vision particular much progress came manual featur engin sift featur surf featur hog featur bag visual word etc minor posit comput vision featur learn directli data posit becam domin alexnet geoffrey hinton start reach colleagu convinc neural network futur jitendra malik sceptic neural network recommend pascal visual object class challeng hinton said dataset small malik recommend imagenet challeng alexnet lenet share essenti design algorithm alexnet much larger lenet train much larger dataset much faster hardwar period year data comput becam cheapli avail alexnet highli influenti result much subsequ work use cnn comput vision use gpu acceler deep learn mid alexnet paper cite time accord googl scholar time public framework avail neural network train infer codebas alexnet releas bsd licens commonli use neural network research sever subsequ year one direct subsequ work aim train increasingli deep cnn achiev increasingli higher perform imagenet line research googlenet vggnet highway network resnet anoth direct aim reproduc perform alexnet lower cost line research squeezenet mobilenet efficientnet
DeepDream,https://en.wikipedia.org/wiki/DeepDream,"DeepDream is a computer vision program created by Google engineer Alexander Mordvintsev that uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dream-like appearance reminiscent of a psychedelic experience in the deliberately overprocessed images.[1][2][3]
 Google's program popularized the term (deep) ""dreaming"" to refer to the generation of images that produce desired activations in a trained deep network, and the term now refers to a collection of related approaches.
 The DeepDream software, originated in a deep convolutional network codenamed ""Inception"" after the film of the same name,[1][2][3] was developed for the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) in 2014[3] and released in July 2015.
 The dreaming idea and name became popular on the internet in 2015 thanks to Google's DeepDream program.  The idea dates from early in the history of neural networks,[4] and similar methods have been used to synthesize visual textures.[5]
Related visualization ideas were developed (prior to Google's work) by several research groups.[6][7]
 After Google published their techniques and made their code open-source,[8] a number of tools in the form of web services, mobile applications, and desktop software appeared on the market to enable users to transform their own photos.[9]
 The software is designed to detect faces and other patterns in images, with the aim of automatically classifying images.[10] However, once trained, the network can also be run in reverse, being asked to adjust the original image slightly so that a given output neuron (e.g. the one for faces or certain animals) yields a higher confidence score. This can be used for visualizations to understand the emergent structure of the neural network better, and is the basis for the DeepDream concept. This reversal procedure is never perfectly clear and unambiguous because it utilizes a one-to-many mapping process.[11] However, after enough reiterations, even imagery initially devoid of the sought features will be adjusted enough that a form of pareidolia results, by which psychedelic and surreal images are generated algorithmically. The optimization resembles backpropagation; however, instead of adjusting the network weights, the weights are held fixed and the input is adjusted.
 For example, an existing image can be altered so that it is ""more cat-like"", and the resulting enhanced image can be again input to the procedure.[2] This usage resembles the activity of looking for animals or other patterns in clouds.
 Applying gradient descent independently to each pixel of the input produces images in which
adjacent pixels have little relation and thus the image has too much high frequency information.
The generated images can be greatly improved by including a prior or regularizer that prefers inputs
that have natural image statistics (without a preference for any particular image), or are simply smooth.[7][12][13]
For example, Mahendran et al.[12] used the total variation regularizer that prefers images that are piecewise constant. Various regularizers are discussed further in Yosinski et al.[13] An in-depth, visual exploration of feature visualization and regularization techniques was published more recently.[14]
 The cited resemblance of the imagery to LSD- and psilocybin-induced hallucinations is suggestive of a functional resemblance between artificial neural networks and particular layers of the visual cortex.[15]
 Neural networks such as DeepDream have biological analogies providing insight into brain processing and the formation of consciousness. Hallucinogens such as DMT alter the function of the serotonergic system which is present within the layers of the visual cortex. Neural networks are trained on input vectors and are altered by internal variations during the training process. The input and internal modifications represent the processing of exogenous and endogenous signals respectively in the visual cortex. As internal variations are modified in deep neural networks the output image reflect these changes. This specific manipulation demonstrates how inner brain mechanisms are analogous to internal layers of neural networks. Internal noise level modifications represent how hallucinogens omit external sensory information leading internal preconceived conceptions to strongly influence visual perception.[16]
 The dreaming idea can be applied to hidden (internal) neurons other than those in the output, 
which allows exploration of the roles and representations of various parts of the network.[13]
It is also possible to optimize the input to satisfy either a single neuron (this usage is sometimes called Activity Maximization)[17] or an entire layer of neurons.
 While dreaming is most often used for visualizing networks or producing computer art, it has recently been proposed that adding ""dreamed"" inputs to the training set can improve training times for abstractions in Computer Science.[18]
 The DeepDream model has also been demonstrated to have application in the field of art history.[19]
 DeepDream was used for Foster the People's music video for the song ""Doing It for the Money"".[20]
 In 2017, a research group out of the University of Sussex created a Hallucination Machine, applying the DeepDream algorithm to a pre-recorded panoramic video, allowing users to explore virtual reality environments to mimic the experience of psychoactive substances and/or psychopathological conditions.[21]  They were able to demonstrate that the subjective experiences induced by the Hallucination Machine differed significantly from control (non-‘hallucinogenic’) videos, while bearing phenomenological similarities to the psychedelic state (following administration of psilocybin).
 In 2021, a study published in the journal Entropy demonstrated the similarity between DeepDream and actual psychedelic experience with neuroscientific evidence.[22] The authors recorded Electroencephalography (EEG) of human participants during passive vision of a movie clip and its DeepDream-generated counterpart. They found that DeepDream video triggered a higher entropy in the EEG signal and a higher level of functional connectivity between brain areas,[22] both well-known biomarkers of actual psychedelic experience.[23]
 In 2022, a research group coordinated by the University of Trento ""measure[d] participants’ cognitive flexibility and creativity after the exposure to virtual reality panoramic videos and their hallucinatory-like counterparts generated by the DeepDream algorithm ... following the simulated psychedelic exposure, individuals exhibited ... an attenuated contribution of the automatic process and chaotic dynamics underlying their decision processes, presumably due to a reorganization in the cognitive dynamics that facilitates the exploration of uncommon decision strategies and inhibits automated choices.""[24]
",deepdream comput vision program creat googl engin alexand mordvintsev use convolut neural network find enhanc pattern imag via algorithm pareidolia thu creat appear reminisc psychedel experi deliber overprocess imag googl program popular term deep dream refer gener imag produc desir activ train deep network term refer collect relat approach deepdream softwar origin deep convolut network codenam incept film name develop imagenet visual recognit challeng ilsvrc releas juli dream idea name becam popular internet thank googl deepdream program idea date earli histori neural network similar method use synthes visual textur relat visual idea develop prior googl work sever research group googl publish techniqu made code number tool form web servic mobil applic desktop softwar appear market enabl user transform photo softwar design detect face pattern imag aim automat classifi imag howev train network also run revers ask adjust origin imag slightli given output neuron one face certain anim yield higher confid score use visual understand emerg structur neural network better basi deepdream concept revers procedur never perfectli clear unambigu util map process howev enough reiter even imageri initi devoid sought featur adjust enough form pareidolia result psychedel surreal imag gener algorithm optim resembl backpropag howev instead adjust network weight weight held fix input adjust exampl exist imag alter result enhanc imag input procedur usag resembl activ look anim pattern cloud appli gradient descent independ pixel input produc imag adjac pixel littl relat thu imag much high frequenc inform gener imag greatli improv includ prior regular prefer input natur imag statist without prefer particular imag simpli smooth exampl mahendran et al use total variat regular prefer imag piecewis constant variou regular discuss yosinski et al visual explor featur visual regular techniqu publish recent cite resembl imageri hallucin suggest function resembl artifici neural network particular layer visual cortex neural network deepdream biolog analog provid insight brain process format conscious hallucinogen dmt alter function serotonerg system present within layer visual cortex neural network train input vector alter intern variat train process input intern modif repres process exogen endogen signal respect visual cortex intern variat modifi deep neural network output imag reflect chang specif manipul demonstr inner brain mechan analog intern layer neural network intern nois level modif repres hallucinogen omit extern sensori inform lead intern preconceiv concept strongli influenc visual percept dream idea appli hidden intern neuron output allow explor role represent variou part network also possibl optim input satisfi either singl neuron usag sometim call activ maxim entir layer neuron dream often use visual network produc comput art recent propos ad dream input train set improv train time abstract comput scienc deepdream model also demonstr applic field art histori deepdream use foster peopl music video song money research group univers sussex creat hallucin machin appli deepdream algorithm panoram video allow user explor virtual realiti environ mimic experi psychoact substanc psychopatholog condit abl demonstr subject experi induc hallucin machin differ significantli control hallucinogen video bear phenomenolog similar psychedel state follow administr psilocybin studi publish journal entropi demonstr similar deepdream actual psychedel experi neuroscientif evid author record electroencephalographi eeg human particip passiv vision movi clip counterpart found deepdream video trigger higher entropi eeg signal higher level function connect brain area biomark actual psychedel experi research group coordin univers trento measur particip cognit flexibl creativ exposur virtual realiti panoram video counterpart gener deepdream algorithm follow simul psychedel exposur individu exhibit attenu contribut automat process chaotic dynam underli decis process presum due reorgan cognit dynam facilit explor uncommon decis strategi inhibit autom choic
Neural radiance field,https://en.wikipedia.org/wiki/Neural_radiance_field,"A neural radiance field (NeRF) is a method based on deep learning for reconstructing a three-dimensional representation of a scene from two-dimensional images. The NeRF model enables downstream applications of novel view synthesis, scene geometry reconstruction, and obtaining the reflectance properties of the scene. Additional scene properties such as camera poses may also be jointly learned. First introduced in 2020,[1] it has since gained significant attention for its potential applications in computer graphics and content creation.[2]
 The NeRF algorithm represents a scene as a radiance field parametrized by a deep neural network (DNN). The network predicts a volume density and view-dependent emitted radiance given the spatial location (x, y, z) and viewing direction in Euler angles (θ, Φ) of the camera. By sampling many points along camera rays, traditional volume rendering techniques can produce an image.[1]
 A NeRF needs to be retrained for each unique scene. The first step is to collect images of the scene from different angles and their respective camera pose. These images are standard 2D images and do not require a specialized camera or software. Any camera is able to generate datasets, provided the settings and capture method meet the requirements for SfM (Structure from Motion).
 This requires tracking of the camera position and orientation, often through some combination of SLAM, GPS, or inertial estimation. Researchers often use synthetic data to evaluate NeRF and related techniques. For such data, images (rendered through traditional non-learned methods) and respective camera poses are reproducible and error-free.[3]
 For each sparse viewpoint (image and camera pose) provided, camera rays are marched through the scene, generating a set of 3D points with a given radiance direction (into the camera). For these points, volume density and emitted radiance are predicted using the multi-layer perceptron (MLP). An image is then generated through classical volume rendering. Because this process is fully differentiable, the error between the predicted image and the original image can be minimized with gradient descent over multiple viewpoints, encouraging the MLP to develop a coherent model of the scene.[1]
 Early versions of NeRF were slow to optimize and required that all input views were taken with the same camera in the same lighting conditions. These performed best when limited to orbiting around individual objects, such as a drum set, plants or small toys.[2] Since the original paper in 2020, many improvements have been made to the NeRF algorithm, with variations for special use cases. 
 In 2020, shortly after the release of NeRF, the addition of Fourier Feature Mapping improved training speed and image accuracy. Deep neural networks struggle to learn high frequency functions in low dimensional domains; a phenomenon known as spectral bias. To overcome this shortcoming, points are mapped to a higher dimensional feature space before being fed into the MLP. 
 



γ
(

v

)
=


[




a

1


cos
⁡
(
2

π




B



1


T



v

)





a

1


sin
⁡
(
2
π



B



1


T



v

)




⋮





a

m


cos
⁡
(
2

π




B



m


T



v

)





a

m


sin
⁡
(
2

π




B



m


T



v

)



]




{\displaystyle \gamma (\mathrm {v} )={\begin{bmatrix}a_{1}\cos(2{\pi }{\mathrm {B} }_{1}^{T}\mathrm {v} )\\a_{1}\sin(2\pi {\mathrm {B} }_{1}^{T}\mathrm {v} )\\\vdots \\a_{m}\cos(2{\pi }{\mathrm {B} }_{m}^{T}\mathrm {v} )\\a_{m}\sin(2{\pi }{\mathrm {B} }_{m}^{T}\mathrm {v} )\end{bmatrix}}}


 Where 




v



{\displaystyle \mathrm {v} }

 is the input point, 





B


i




{\displaystyle \mathrm {B} _{i}}

 are the frequency vectors, and 




a

i




{\displaystyle a_{i}}

 are coefficients.   
 This allows for rapid convergence to high frequency functions, such as pixels in a detailed image.[4]
 One limitation of NeRFs is the requirement of knowing accurate camera poses to train the model. Often times, pose estimation methods are not completely accurate, nor is the camera pose even possible to know. These imperfections result in artifacts and suboptimal convergence. So, a method was developed to optimize the camera pose along with the volumetric function itself. Called Bundle-Adjusting Neural Radiance Field (BARF), the technique uses a dynamic low-pass filter to go from coarse to fine adjustment, minimizing error by finding the geometric transformation to the desired image. This corrects imperfect camera poses and greatly improves the quality of NeRF renders.[5]
 Conventional NeRFs struggle to represent detail at all viewing distances, producing blurry images up close and overly aliased images from distant views. In 2021, researchers introduced a technique to improve the sharpness of details at different viewing scales known as mip-NeRF (comes from mipmap). Rather than sampling a single ray per pixel, the technique fits a gaussian to the conical frustum cast by the camera. This improvement effectively anti-aliases across all viewing scales. mip-NeRF also reduces overall image error and is faster to converge at ~half the size of ray-based NeRF.[6]
 In 2021, researchers applied meta-learning to assign initial weights to the MLP. This rapidly speeds up convergence by effectively giving the network a head start in gradient descent. Meta-learning also allowed the MLP to learn an underlying representation of certain scene types. For example, given a dataset of famous tourist landmarks, an initialized NeRF could partially reconstruct a scene given one image.[7]
 Conventional NeRFs are vulnerable to slight variations in input images (objects, lighting) often resulting in ghosting and artifacts. As a result, NeRFs struggle to represent dynamic scenes, such as bustling city streets with changes in lighting and dynamic objects. In 2021, researchers at Google[2] developed a new method for accounting for these variations, named NeRF in the Wild (NeRF-W). This method splits the neural network (MLP) into three separate models. The main MLP is retained to encode the static volumetric radiance. However, it operates in sequence with a separate MLP for appearance embedding (changes in lighting, camera properties) and an MLP for transient embedding (changes in scene objects). This allows the NeRF to be trained on diverse photo collections, such as those taken by mobile phones at different times of day.[8]
 In 2021, researchers added more outputs to the MLP at the heart of NeRFs. The output now included: volume density, surface normal, material parameters, distance to the first surface intersection (in any direction), and visibility of the external environment in any direction. The inclusion of these new parameters lets the MLP learn material properties, rather than pure radiance values. This facilitates a more complex rendering pipeline, calculating direct and global illumination, specular highlights, and shadows. As a result, the NeRF can render the scene under any lighting conditions with no re-training.[9]
 Although NeRFs had reached high levels of fidelity, their costly compute time made them useless for many applications requiring real-time rendering, such as VR/AR and interactive content. Introduced in 2021, Plenoctrees (plenoptic octrees) enabled real-time rendering of pre-trained NeRFs through division of the volumetric radiance function into an octree. Rather than assigning a radiance direction into the camera, viewing direction is taken out of the network input and spherical radiance is predicted for each region. This makes rendering over 3000x faster than conventional NeRFs.[10]
 Similar to Plenoctrees, this method enabled real-time rendering of pretrained NeRFs. To avoid querying the large MLP for each point, this method bakes NeRFs into Sparse Neural Radiance Grids (SNeRG). A SNeRG is a sparse voxel grid containing opacity and color, with learned feature vectors to encode view-dependent information. A lightweight, more efficient MLP is then used to produce view-dependent residuals to modify the color and opacity. To enable this compressive baking, small changes to the NeRF architecture were made, such as running the MLP once per pixel rather than for each point along the ray. These improvements make SNeRG extremely efficient, outperforming Plenoctrees.[11]
 In 2022, researchers at Nvidia enabled real-time training of NeRFs through a technique known as Instant Neural Graphics Primitives. An innovative input encoding reduces computation, enabling real-time training of a NeRF, an improvement orders of magnitude above previous methods. The speedup stems from the use of spatial hash functions, which have 



O
(
1
)


{\displaystyle O(1)}

 access times, and parallelized architectures which run fast on modern GPUs.[12]
 Plenoxel (plenoptic volume element) uses a sparse voxel representation instead of a volumetric approach as seen in NeRFs. Plenoxel also completely removes the MLP, instead directly performing gradient descent on the voxel coefficients. Plenoxel can match the fidelity of a conventional NeRF in orders of magnitude less training time. Published in 2022, this method disproved the importance of the MLP, showing that the differentiable rendering pipeline is the critical component.[13]
 Gaussian splatting is a newer method that can outperform NeRF in render time and fidelity. Rather than representing the scene as a volumetric function, it uses a sparse cloud of 3D gaussians. First, a point cloud is generated (through structure from motion) and converted to gaussians of initial covariance, color, and opacity. The gaussians are directly optimized through stochastic gradient descent to match the input image. This saves computation by removing empty space and foregoing the need to query a neural network for each point. Instead, simply ""splat"" all the gaussians onto the screen and they overlap to produce the desired image.[14]
 Traditional photogrammetry is not neural, instead using robust geometric equations to obtain 3D measurements. NeRFs, unlike photogrammetric methods, do not inherently produce dimensionally accurate 3D geometry. While their results are often sufficient for extracting accurate geometry (ex: via cube marching[1]), the process is fuzzy, as with most neural methods. This limits NeRF to cases where the output image is valued, rather than raw scene geometry. However, NeRFs excel in situations with unfavorable lighting. For example, photogrammetric methods completely break down when trying to reconstruct reflective or transparent objects in a scene, while a NeRF is able to infer the geometry.[15]
 NeRFs have a wide range of applications, and are starting to grow in popularity as they become integrated into user-friendly applications.[3]
 NeRFs have huge potential in content creation, where on-demand photorealistic views are extremely valuable.[16] The technology democratizes a space previously only accessible by teams of VFX artists with expensive assets. Neural radiance fields now allow anyone with a camera to create compelling 3D environments.[3] NeRF has been combined with generative AI, allowing users with no modelling experience to instruct changes in photorealistic 3D scenes.[17] NeRFs have potential uses in video production, computer graphics, and product design.  
 The photorealism of NeRFs make them appealing for applications where immersion is important, such as virtual reality or videogames. NeRFs can be combined with classical rendering techniques to insert synthetic objects and create believable virtual experiences.[18]
 NeRFs have been used to reconstruct 3D CT scans from sparse or even single X-ray views. The model demonstrated high fidelity renderings of chest and knee data. If adopted, this method can save patients from excess doses of ionizing radiation, allowing for safer diagnosis.[19]
 The unique ability of NeRFs to understand transparent and reflective objects makes them useful for robots interacting in such environments. The use of NeRF allowed a robot arm to precisely manipulate a transparent wine glass; a task where traditional computer vision would struggle.[20]
 NeRFs can also generate photorealistic human faces, making them valuable tools for human-computer interaction. Traditionally rendered faces can be uncanny, while other neural methods are too slow to run in real-time.[21]
",neural radianc field nerf method base deep learn reconstruct represent scene imag nerf model enabl downstream applic novel view synthesi scene geometri reconstruct obtain reflect properti scene addit scene properti camera pose may also jointli learn first introduc sinc gain signific attent potenti applic comput graphic content creation nerf algorithm repres scene radianc field parametr deep neural network dnn network predict volum densiti emit radianc given spatial locat x z view direct euler angl θ φ camera sampl mani point along camera ray tradit volum render techniqu produc imag nerf need retrain uniqu scene first step collect imag scene differ angl respect camera pose imag standard imag requir special camera softwar camera abl gener dataset provid set captur method meet requir sfm structur motion requir track camera posit orient often combin slam gp inerti estim research often use synthet data evalu nerf relat techniqu data imag render tradit method respect camera pose reproduc spars viewpoint imag camera pose provid camera ray march scene gener set point given radianc direct camera point volum densiti emit radianc predict use perceptron mlp imag gener classic volum render process fulli differenti error predict imag origin imag minim gradient descent multipl viewpoint encourag mlp develop coher model scene earli version nerf slow optim requir input view taken camera light condit perform best limit orbit around individu object drum set plant small toy sinc origin paper mani improv made nerf algorithm variat special use case shortli releas nerf addit fourier featur map improv train speed imag accuraci deep neural network struggl learn high frequenc function low dimension domain phenomenon known spectral bia overcom shortcom point map higher dimension featur space fed mlp γ v co π b v sin π b v co π b v sin π b v v bmatrix b v b v b v b v bmatrix v v input point b b frequenc vector coeffici allow rapid converg high frequenc function pixel detail imag one limit nerf requir know accur camera pose train model often time pose estim method complet accur camera pose even possibl know imperfect result artifact suboptim converg method develop optim camera pose along volumetr function call neural radianc field barf techniqu use dynam filter go coars fine adjust minim error find geometr transform desir imag correct imperfect camera pose greatli improv qualiti nerf render convent nerf struggl repres detail view distanc produc blurri imag close overli alias imag distant view research introduc techniqu improv sharp detail differ view scale known come mipmap rather sampl singl ray per pixel techniqu fit gaussian conic frustum cast camera improv effect across view scale also reduc overal imag error faster converg size nerf research appli assign initi weight mlp rapidli speed converg effect give network head start gradient descent also allow mlp learn underli represent certain scene type exampl given dataset famou tourist landmark initi nerf could partial reconstruct scene given one imag convent nerf vulner slight variat input imag object light often result ghost artifact result nerf struggl repres dynam scene bustl citi street chang light dynam object research googl develop new method account variat name nerf wild method split neural network mlp three separ model main mlp retain encod static volumetr radianc howev oper sequenc separ mlp appear embed chang light camera properti mlp transient embed chang scene object allow nerf train divers photo collect taken mobil phone differ time day research ad output mlp heart nerf output includ volum densiti surfac normal materi paramet distanc first surfac intersect direct visibl extern environ direct inclus new paramet let mlp learn materi properti rather pure radianc valu facilit complex render pipelin calcul direct global illumin specular highlight shadow result nerf render scene light condit although nerf reach high level fidel costli comput time made useless mani applic requir render interact content introduc plenoctre plenopt octre enabl render nerf divis volumetr radianc function octre rather assign radianc direct camera view direct taken network input spheric radianc predict region make render faster convent nerf similar plenoctre method enabl render pretrain nerf avoid queri larg mlp point method bake nerf spars neural radianc grid snerg snerg spars voxel grid contain opac color learn featur vector encod inform lightweight effici mlp use produc residu modifi color opac enabl compress bake small chang nerf architectur made run mlp per pixel rather point along ray improv make snerg extrem effici outperform plenoctre research nvidia enabl train nerf techniqu known instant neural graphic primit innov input encod reduc comput enabl train nerf improv order magnitud previou method speedup stem use spatial hash function access time parallel architectur run fast modern gpu plenoxel plenopt volum element use spars voxel represent instead volumetr approach seen nerf plenoxel also complet remov mlp instead directli perform gradient descent voxel coeffici plenoxel match fidel convent nerf order magnitud less train time publish method disprov import mlp show differenti render pipelin critic compon gaussian splat newer method outperform nerf render time fidel rather repres scene volumetr function use spars cloud gaussian first point cloud gener structur motion convert gaussian initi covari color opac gaussian directli optim stochast gradient descent match input imag save comput remov empti space forego need queri neural network point instead simpli splat gaussian onto screen overlap produc desir imag tradit photogrammetri neural instead use robust geometr equat obtain measur nerf unlik photogrammetr method inher produc dimension accur geometri result often suffici extract accur geometri ex via cube march process fuzzi neural method limit nerf case output imag valu rather raw scene geometri howev nerf excel situat unfavor light exampl photogrammetr method complet break tri reconstruct reflect transpar object scene nerf abl infer geometri nerf wide rang applic start grow popular becom integr applic nerf huge potenti content creation photorealist view extrem valuabl technolog democrat space previous access team vfx artist expens asset neural radianc field allow anyon camera creat compel environ nerf combin gener ai allow user model experi instruct chang photorealist scene nerf potenti use video product comput graphic product design photor nerf make appeal applic immers import virtual realiti videogam nerf combin classic render techniqu insert synthet object creat believ virtual experi nerf use reconstruct ct scan spars even singl view model demonstr high fidel render chest knee data adopt method save patient excess dose ioniz radiat allow safer diagnosi uniqu abil nerf understand transpar reflect object make use robot interact environ use nerf allow robot arm precis manipul transpar wine glass task tradit comput vision would struggl nerf also gener photorealist human face make valuabl tool interact tradit render face uncanni neural method slow run
Transformer (deep learning architecture),https://en.wikipedia.org/wiki/Transformer_(machine_learning_model),"A transformer is a deep learning architecture that was developed by researchers at Google and is based on the multi-head attention mechanism, which was proposed in the 2017 paper ""Attention Is All You Need"".[1] Text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.[1] At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.
 Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM).[2] Later variations have been widely adopted for training large language models (LLM) on large (language) datasets, such as the Wikipedia corpus and Common Crawl.[3]
 
Transformers were first developed as an improvement over previous architectures for machine translation,[4][5] but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning,[6][7] audio,[8] multimodal learning, robotics,[9] and even playing chess.[10] It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs)[11] and BERT[12] (bidirectional encoder representations from transformers). For many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.
 A key breakthrough was LSTM (1995),[note 1] a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units.[13] Neural networks using multiplicative units were later called sigma-pi networks[14] or higher-order networks.[15] LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.
However, LSTM still used sequential processing, like most other RNNs.[note 2] Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence. 
 Modern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window.  The linearly scaling fast weight controller (1992) learns  to compute a  weight matrix for further processing depending on the input.[16] One of its two networks has  ""fast weights"" or ""dynamic links"" (1981).[17][18][19] A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries.[16] This was later shown to be equivalent to the unnormalized linear Transformer.[20][21]
 The idea of encoder-decoder sequence transduction had been developed in the early 2010s (see previous papers[22][23]). The papers most commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.[22][23]
 A 380M-parameter model for machine translation uses two long short-term memories (LSTM).[23] Its architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM.[22] Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.[24][25]
 These early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, degrading the output. As evidence, reversing the input sentence improved seq2seq translation.[26]
 The RNNsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily. The name is because it ""emulates searching through a source sentence during decoding a translation"".[4]
 The relative performances were compared between global (that of RNNsearch) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.[27]
 In 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM.[28] It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.[29]
 Seq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them to be accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude less parameters than LSTMs.[30] One of its authors, Jakob Uszkoreit, suspected that attention without recurrence is sufficient for language translation, thus the title ""attention is all you need"".[31] That hypothesis was against conventional wisdom of the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical.[31] In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs.[32]
 In 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the ""Attention is all you need"" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance.[1] This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks.[33]

 Already in spring 2017, even before the ""Attention is all you need"" preprint was published, one of the co-authors applied the ""decoder-only"" variation of the architecture to generate fictitious Wikipedia articles.[34] Transformer architecture is now used in many generative models that contribute to the ongoing AI boom.
 In language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. It was followed by BERT (2018), an encoder-only Transformer model.[35] In 2019 October, Google started using BERT to process search queries.[36] In 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model.[37]
 Starting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly popular,[38] triggering a boom around large language models.[39][40]
 Since 2020, Transformers have been applied in modalities beyond text, including the vision transformer,[41] speech recognition,[42] robotics,[6] and multimodal.[43] The vision transformer, in turn, stimulated new developments in convolutional neural networks.[44] Image and video generators like DALL-E (2021), Stable Diffusion 3 (2024),[45] and Sora (2024), are based on the Transformer architecture.
 The plain transformer architecture had difficulty converging. In the original paper[1] the authors recommended using learning rate warmup. That is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2% of the total number of training steps), before decaying again.
 A 2020 paper found that using layer normalization before (instead of after) multiheaded attention and feedforward layers stabilizes training, not requiring learning rate warmup.[46]
 Transformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset. The pretrain dataset is typically an unlabeled large corpus, such as The Pile. Tasks for pretraining and fine-tuning commonly include:
 The T5 transformer report[47] documents a large number of natural language pretraining tasks. Some examples are:
 Note that while each of these tasks is trivial or obvious for human native speakers of the language (or languages), they have typically proved challenging for previous generations of machine learning architecture.
 In general, there are 3 classes of language modelling tasks: ""masked"",[49] ""autoregressive"",[50] and ""prefixLM"".[51] These classes are independent of a specific modeling architecture such as Transformer, but they are often discussed in the context of Transformer.
 In a masked task,[49] one or more of the tokens is masked out, and the model would produce a probability distribution predicting what the masked-out tokens are based on the context. The loss function for the task is typically sum of log-perplexities for the masked-out tokens: 




Loss

=
−

∑

t
∈

masked tokens



ln
⁡
(

probability of 

t

 conditional on its context

)


{\displaystyle {\text{Loss}}=-\sum _{t\in {\text{masked tokens}}}\ln({\text{probability of }}t{\text{ conditional on its context}})}

and the model is trained to minimize this loss function. The BERT series of models are trained for masked token prediction and another task.
 In an autoregressive task,[50] the entire sequence is masked at first, and the model produces a probability distribution for the first token. Then the first token is revealed and the model predicts the second token, and so on. The loss function for the task is still typically the same. The GPT series of models are trained by autoregressive tasks.
 In a prefixLM task,[51] the sequence is divided into two parts. The first part is presented as context, and the model predicts the first token of the second part. Then that would be revealed, and the model predicts the second token, and so on. The loss function for the task is still typically the same. The T5 series of models are trained by prefixLM tasks.
 Note that ""masked"" as in ""masked language modelling"" is not ""masked"" as in ""masked attention"", and ""prefixLM"" (prefix language modeling) is not ""prefixLM"" (prefix language model).
 All transformers have the same primary components:
 The following description follows exactly the Transformer as described in the original paper. There are variants, described in the following section.
 By convention, we write all vectors as row vectors. This, for example, means that pushing a vector through a linear layer means multiplying it by a weight matrix on the right, as 



x
W


{\displaystyle xW}

.
 As the Transformer architecture natively processes numerical data, not text, there must be a translation between text and tokens. A token is an integer that represents a character, or a short segment of characters. On the input side, the input text is parsed into a token sequence. Similarly, on the output side, the output tokens are parsed back to text. The module doing the conversion between texts and token sequences  is a tokenizer.
 The set of all tokens is the vocabulary of the tokenizer, and its size is the vocabulary size 




n

vocabulary




{\displaystyle n_{\text{vocabulary}}}

. When faced with tokens outside the vocabulary, typically a special token is used, written as ""[UNK]"" for ""unknown"".
 Some commonly used tokenizers are byte pair encoding, WordPiece, and SentencePiece.
 Each token is converted into an embedding vector via a lookup table. Equivalently stated, it multiplies a one-hot representation of the token by an embedding matrix 



M


{\displaystyle M}

. For example, if the input token is 



3


{\displaystyle 3}

, then the one-hot representation is 



[
0
,
0
,
0
,
1
,
0
,
0
,
…
]


{\displaystyle [0,0,0,1,0,0,\dots ]}

, and its embedding vector is




E
m
b
e
d

(
3
)
=
[
0
,
0
,
0
,
1
,
0
,
0
,
…
]
M


{\displaystyle \mathrm {Embed} (3)=[0,0,0,1,0,0,\dots ]M}

The token embedding vectors are added to their respective positional encoding vectors (see below), producing the sequence of input vectors. 
 The number of dimensions in an embedding vector is called hidden size or embedding size and written as 




d

emb




{\displaystyle d_{\text{emb}}}

.[35] This size is written as 




d

model




{\displaystyle d_{\text{model}}}

 in the original Transformer paper.[1]
 An un-embedding layer is almost the reverse of an embedding layer. Whereas an embedding layer converts a token into a vector, an un-embedding layer converts a vector into a probability distribution over tokens.
 The un-embedding layer is a linear-softmax layer:




U
n
E
m
b
e
d

(
x
)
=

s
o
f
t
m
a
x

(
x
W
+
b
)


{\displaystyle \mathrm {UnEmbed} (x)=\mathrm {softmax} (xW+b)}

The matrix has shape 



(

d

emb


,

n

vocabulary


)


{\displaystyle (d_{\text{emb}},n_{\text{vocabulary}})}

. The embedding matrix 



M


{\displaystyle M}

 and the un-embedding matrix 



W


{\displaystyle W}

 are sometimes required to be transposes of each other, a practice called weight tying.[52]
 A positional encoding is a fixed-size vector representation of the relative positions of tokens within a sequence: it provides the transformer model with information about where the words are in the input sequence. This shall induce a bias towards the order of the input sequence, so that, for example, the input sequence ""man bites dog"" is processed differently from ""dog bites man"".
 The positional encoding is defined as a function of type 



f
:

R

→


R


d


;
d
∈

Z

,
d
>
0


{\displaystyle f:\mathbb {R} \to \mathbb {R} ^{d};d\in \mathbb {Z} ,d>0}

, where 



d


{\displaystyle d}

 is a positive even integer. The full positional encoding defined in the original paper[1] is:



(
f
(
t

)

2
k


,
f
(
t

)

2
k
+
1


)
=
(
sin
⁡
(
θ
)
,
cos
⁡
(
θ
)
)

∀
k
∈
{
0
,
1
,
…
,
d

/

2
−
1
}


{\displaystyle (f(t)_{2k},f(t)_{2k+1})=(\sin(\theta ),\cos(\theta ))\quad \forall k\in \{0,1,\ldots ,d/2-1\}}

where 



θ
=


t

r

k




,
r
=

N

2

/

d




{\displaystyle \theta ={\frac {t}{r^{k}}},r=N^{2/d}}

.
 Here, 



N


{\displaystyle N}

 is a free parameter that should be significantly larger than the biggest 



k


{\displaystyle k}

 that would be input into the positional encoding function. The original paper uses 



N
=
10000


{\displaystyle N=10000}

.
 The function is in a simpler form when written as a complex function of type 



f
:

R

→


C


d

/

2




{\displaystyle f:\mathbb {R} \to \mathbb {C} ^{d/2}}





f
(
t
)
=


(

e

i
t

/


r

k




)


k
=
0
,
1
,
…
,


d
2


−
1




{\displaystyle f(t)=\left(e^{it/r^{k}}\right)_{k=0,1,\ldots ,{\frac {d}{2}}-1}}

where 



r
=

N

2

/

d




{\displaystyle r=N^{2/d}}

.
 The main reason for using this positional encoding function is that using it, shifts are linear transformations:



f
(
t
+
Δ
t
)
=

d
i
a
g

(
f
(
Δ
t
)
)
f
(
t
)


{\displaystyle f(t+\Delta t)=\mathrm {diag} (f(\Delta t))f(t)}

where 



Δ
t
∈

R



{\displaystyle \Delta t\in \mathbb {R} }

 is the distance one wishes to shift. This allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication.
 By taking a linear sum, any convolution can also be implemented as linear transformations:




∑

j



c

j


f
(
t
+
Δ

t

j


)
=

(


∑

j



c

j




d
i
a
g

(
f
(
Δ

t

j


)
)

)

f
(
t
)


{\displaystyle \sum _{j}c_{j}f(t+\Delta t_{j})=\left(\sum _{j}c_{j}\,\mathrm {diag} (f(\Delta t_{j}))\right)f(t)}

for any constants 




c

j




{\displaystyle c_{j}}

. This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors. This sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a convolutional neural network language model. In the author's words, ""we hypothesized it would allow the model to easily learn to attend by relative position.""
 In typical implementations, all operations are done over the real numbers, not the complex numbers, but since complex multiplication can be implemented as real 2-by-2 matrix multiplication, this is a mere notational difference.
 Like earlier seq2seq models, the original transformer model used an encoder-decoder architecture. The encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoder's output and the decoder's output tokens so far.
 The purpose of each encoder layer is to create contextualized representations of the tokens, where each representation corresponds to a token that ""mixes"" information from other input tokens via self-attention mechanism. Each decoder layer contains two attention sublayers: (1) cross-attention for incorporating the output of encoder (contextualized input token representations), and (2) self-attention for ""mixing"" information among the input tokens to the decoder (i.e. the tokens generated so far during inference time).[53][54]
 Both the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps.[54] These feed-forward layers contain most of the parameters in a Transformer model.
  The feedforward network (FFN) modules in a Transformer are 2-layered multilayer perceptrons:




F
F
N

(
x
)
=
ϕ
(
x

W

(
1
)


+

b

(
1
)


)

W

(
2
)


+

b

(
2
)




{\displaystyle \mathrm {FFN} (x)=\phi (xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}}

where 



ϕ


{\displaystyle \phi }

 is its activation function. The original Transformer used ReLU activation.
 The number of neurons in the middle layer is called intermediate size (GPT),[55] filter size (BERT),[35] or feedforward size (BERT).[35] It is typically larger than the embedding size. For example, in both GPT-2 series and BERT series, the intermediate size of a model is 4 times its embedding size: 




d

ffn


=
4

d

emb




{\displaystyle d_{\text{ffn}}=4d_{\text{emb}}}

.
 The attention mechanism used in the Transformer architecture are scaled dot-product attention units. For each unit, the transformer model learns three weight matrices: the query weights 




W

Q




{\displaystyle W^{Q}}

, the key weights 




W

K




{\displaystyle W^{K}}

, and the value weights 




W

V




{\displaystyle W^{V}}

.
 The module takes three sequences, a query sequence, a key sequence, and a value sequence. The query sequence is a sequence of length 




ℓ

seq, query




{\displaystyle \ell _{\text{seq, query}}}

, and each entry is a vector of dimension 




d

emb, query




{\displaystyle d_{\text{emb, query}}}

. Similarly for the key and value sequences.
 For each vector 




x

i
,

query





{\displaystyle x_{i,{\text{query}}}}

 in the query sequence, it is multiplied by a matrix 




W

Q




{\displaystyle W^{Q}}

 to produce a query vector 




q

i


=

x

i
,

query




W

Q




{\displaystyle q_{i}=x_{i,{\text{query}}}W^{Q}}

. The matrix of all query vectors is the query matrix:



Q
=

X

query



W

Q




{\displaystyle Q=X_{\text{query}}W^{Q}}

Similarly, we construct the key matrix 



K
=

X

key



W

K




{\displaystyle K=X_{\text{key}}W^{K}}

 and the value matrix 



V
=

X

value



W

V




{\displaystyle V=X_{\text{value}}W^{V}}

.
 It is usually the case that all 




W

Q


,

W

K


,

W

V




{\displaystyle W^{Q},W^{K},W^{V}}

 are square matrices, meaning 




d

emb, query


=

d

query




{\displaystyle d_{\text{emb, query}}=d_{\text{query}}}

, etc.
 Attention weights are calculated using the query and key vectors: the attention weight 




a

i
j




{\displaystyle a_{ij}}

 from token 



i


{\displaystyle i}

 to token 



j


{\displaystyle j}

 is the dot product between 




q

i




{\displaystyle q_{i}}

 and 




k

j




{\displaystyle k_{j}}

. The attention weights are divided by the square root of the dimension of the key vectors, 






d

k






{\displaystyle {\sqrt {d_{k}}}}

, which stabilizes gradients during training, and passed through a softmax which normalizes the weights. The fact that 




W

Q




{\displaystyle W^{Q}}

 and 




W

K




{\displaystyle W^{K}}

 are different matrices allows attention to be non-symmetric: if token 



i


{\displaystyle i}

 attends to token 



j


{\displaystyle j}

 (i.e. 




q

i


⋅

k

j




{\displaystyle q_{i}\cdot k_{j}}

 is large), this does not necessarily mean that token 



j


{\displaystyle j}

 will attend to token 



i


{\displaystyle i}

 (i.e. 




q

j


⋅

k

i




{\displaystyle q_{j}\cdot k_{i}}

 could be small). The output of the attention unit for token 



i


{\displaystyle i}

 is the weighted sum of the value vectors of all tokens, weighted by 




a

i
j




{\displaystyle a_{ij}}

, the attention from token 



i


{\displaystyle i}

 to each token.
 The attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations. The matrices 



Q


{\displaystyle Q}

, 



K


{\displaystyle K}

 and 



V


{\displaystyle V}

 are defined as the matrices where the 



i


{\displaystyle i}

th rows are vectors 




q

i




{\displaystyle q_{i}}

, 




k

i




{\displaystyle k_{i}}

, and 




v

i




{\displaystyle v_{i}}

 respectively. Then we can represent the attention as








Attention

(
Q
,
K
,
V
)
=

softmax


(



Q

K


T






d

k





)

V






{\displaystyle {\begin{aligned}{\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}\right)V\end{aligned}}}


 where the softmax is applied over each of the rows of the matrix.
 The number of dimensions in a query vector is query size 




d

query




{\displaystyle d_{\text{query}}}

 and similarly for the key size 




d

key




{\displaystyle d_{\text{key}}}

 and value size 




d

value




{\displaystyle d_{\text{value}}}

. The output dimension of an attention head is its head dimension 




d

head




{\displaystyle d_{\text{head}}}

. The attention mechanism requires the following three equalities to hold:




ℓ

seq, key


=

ℓ

seq, value


,


d

query


=

d

key


,


d

value


=

d

head




{\displaystyle \ell _{\text{seq, key}}=\ell _{\text{seq, value}},\;d_{\text{query}}=d_{\text{key}},\;d_{\text{value}}=d_{\text{head}}}

but is otherwise unconstrained.
 If the attention head is used in a self-attention fashion, then 




X

query


=

X

key


=

X

value




{\displaystyle X_{\text{query}}=X_{\text{key}}=X_{\text{value}}}

. If the attention head is used in a cross-attention fashion, then usually 




X

query


≠

X

key


=

X

value




{\displaystyle X_{\text{query}}\neq X_{\text{key}}=X_{\text{value}}}

. It is theoretically possible for all three to be different, but that is rarely the case in practice.
 One set of 




(


W

Q


,

W

K


,

W

V



)



{\displaystyle \left(W^{Q},W^{K},W^{V}\right)}

 matrices is called an attention head, and each layer in a transformer model has multiple attention heads. While each attention head attends to the tokens that are relevant to each token, multiple attention heads allow the model to do this for different definitions of ""relevance"". Specifically, the query and key projection matrices, 




W

Q




{\displaystyle W^{Q}}

 and 




W

K




{\displaystyle W^{K}}

 , which are involved in the attention score computation, defines the ""relevance"". Meanwhile, the value projection matrix 




W

V




{\displaystyle W^{V}}

, in combination with the part of the output projection matrix 




W

O




{\displaystyle W^{O}}

, determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits. In addition, the scope of attention, or the range of token relationships captured by each attention head, can expand as tokens pass through successive layers. This allows the model to capture more complex and long-range dependencies in deeper layers. Many transformer attention heads encode relevance relations that are meaningful to humans. For example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects.[56] The computations for each attention head can be performed in parallel, which allows for fast processing. The outputs for the attention layer are concatenated to pass into the feed-forward neural network layers.
 Concretely, let the multiple attention heads be indexed by 



i


{\displaystyle i}

, then we have




MultiheadedAttention

(
Q
,
K
,
V
)
=


Concat


i
∈
[

n

heads


]


(

Attention

(
X

W

i


Q


,
X

W

i


K


,
X

W

i


V


)
)

W

O




{\displaystyle {\text{MultiheadedAttention}}(Q,K,V)={\text{Concat}}_{i\in [n_{\text{heads}}]}({\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V}))W^{O}}

 where the matrix 



X


{\displaystyle X}

 is the concatenation of word embeddings, and the matrices 




W

i


Q


,

W

i


K


,

W

i


V




{\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}

 are ""projection matrices"" owned by individual attention head 



i


{\displaystyle i}

, and 




W

O




{\displaystyle W^{O}}

 is a final projection matrix owned by the whole multi-headed attention head.
 It is theoretically possible for each attention head to have a different head dimension 




d

head




{\displaystyle d_{\text{head}}}

, but that is rarely the case in practice.
 As an example, in the smallest GPT-2 model, there are only self-attention mechanisms. It has the following dimensions:




d

emb


=
768
,

n

head


=
12
,

d

head


=
64


{\displaystyle d_{\text{emb}}=768,n_{\text{head}}=12,d_{\text{head}}=64}

Since 



12
×
64
=
768


{\displaystyle 12\times 64=768}

, its output projection matrix 




W

O


∈


R


(
12
×
64
)
×
768




{\displaystyle W^{O}\in \mathbb {R} ^{(12\times 64)\times 768}}

 is a square matrix.
 The Transformer architecture is constructed to calculate output tokens iteratively. Assuming 



t
=
0


{\displaystyle t=0}

 refers to the calculation of the first output token 



i
=
0


{\displaystyle i=0}

, for step 



t
>
0


{\displaystyle t>0}

, the output token 



i
=
0


{\displaystyle i=0}

 shall remain constant. This ensures properties of the model similar to autoregressive models.[1] Therefore, at every time step 



t


{\displaystyle t}

, the calculation for all outputs 



i


{\displaystyle i}

 should not have access to tokens at position 



j


{\displaystyle j}

 for 



j
>=
i


{\displaystyle j>=i}

 (as it naturally is the case for time step 



t
=
i


{\displaystyle t=i}

, when tokens 



j
>
t


{\displaystyle j>t}

 are not yet calculated). This behavior may be accomplished before the softmax stage by adding a mask matrix 



M


{\displaystyle M}

 that is 



−
∞


{\displaystyle -\infty }

 at entries where the attention link must be cut, and 



0


{\displaystyle 0}

 at other places:








MaskedAttention

(
Q
,
K
,
V
)
=

softmax


(

M
+



Q

K


T






d

k






)

V






{\displaystyle {\begin{aligned}{\text{MaskedAttention}}(Q,K,V)={\text{softmax}}\left(M+{\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}\right)V\end{aligned}}}

 The following matrix is commonly used in decoder self-attention modules, called ""causal masking"":




M

causal


=


[



0


−
∞


−
∞


…


−
∞




0


0


−
∞


…


−
∞




0


0


0


…


−
∞




⋮


⋮


⋮


⋱


⋮




0


0


0


…


0



]




{\displaystyle M_{\text{causal}}={\begin{bmatrix}0&-\infty &-\infty &\dots &-\infty \\0&0&-\infty &\dots &-\infty \\0&0&0&\dots &-\infty \\\vdots &\vdots &\vdots &\ddots &\vdots \\0&0&0&\dots &0\end{bmatrix}}}


 In words, it means that each token can pay attention to itself, and every token before it, but not any after it. A non-masked attention module can be thought of as a masked attention module where the mask has all entries zero. As an example of an uncommon use of mask matrix, the XLNet considers all masks of the form 



P

M

causal



P

−
1




{\displaystyle PM_{\text{causal}}P^{-1}}

, where 



P


{\displaystyle P}

 is a random permutation matrix.[57]
 An encoder consists of an embedding layer, followed by multiple encoder layers.
 Each encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer. It takes an input as a sequence of input vectors, applies the self-attention mechanism, to produce an intermediate sequence of vectors, then applies the feed-forward layer for each vector individually. Schematically, we have:








given input vectors 




h

0


,

h

1


,
…





combine them into a matrix 

H



=


[




h

0







h

1






⋮



]







EncoderLayer

(
H
)



=


[




FFN

(

MultiheadedAttention

(
H
,
H
,
H

)

0


)





FFN

(

MultiheadedAttention

(
H
,
H
,
H

)

1


)




⋮



]








{\displaystyle {\begin{aligned}{\text{given input vectors }}&h_{0},h_{1},\dots \\{\text{combine them into a matrix }}H&={\begin{bmatrix}h_{0}\\h_{1}\\\vdots \end{bmatrix}}\\{\text{EncoderLayer}}(H)&={\begin{bmatrix}{\text{FFN}}({\text{MultiheadedAttention}}(H,H,H)_{0})\\{\text{FFN}}({\text{MultiheadedAttention}}(H,H,H)_{1})\\\vdots \end{bmatrix}}\\\end{aligned}}}


 where 




FFN



{\displaystyle {\text{FFN}}}

 stands for ""feed-forward network"". We can more succinctly write it as




EncoderLayer

(
H
)
=

FFN

(

MultiheadedAttention

(
H
,
H
,
H
)
)


{\displaystyle {\text{EncoderLayer}}(H)={\text{FFN}}({\text{MultiheadedAttention}}(H,H,H))}

with the implicit convention that the 




FFN



{\displaystyle {\text{FFN}}}

 is applied to each row of the matrix individually.
 The encoder layers are stacked. The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors. This sequence of vectors is processed by the second encoder, and so on. The output from the final encoder layer is then used by the decoder.
 As the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking.
 A decoder consists of an embedding layer, followed by multiple decoder layers, followed by an un-embedding layer.
 Each decoder consists of three major components: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders. This mechanism can also be called the encoder-decoder attention.[1][54]
 Like the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow.[1] This allows for autoregressive text generation. For decoding, all-to-all attention is inappropriate, because a token cannot attend to tokens not yet generated. Thus, the self-attention module in the decoder is causally masked.
 In contrast, the cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding. Consequently, there is no need for masking in the cross-attention mechanism.
 Schematically, we have:








H
′




=

MaskedMultiheadedAttention

(
H
,
H
,
H
)





DecoderLayer

(
H
)



=

FFN

(

MultiheadedAttention

(

H
′

,

H

E


,

H

E


)
)






{\displaystyle {\begin{aligned}H'&={\text{MaskedMultiheadedAttention}}(H,H,H)\\{\text{DecoderLayer}}(H)&={\text{FFN}}({\text{MultiheadedAttention}}(H',H^{E},H^{E}))\end{aligned}}}

where 




H

E




{\displaystyle H^{E}}

 is the matrix with rows being the output vectors from the encoder.
 The last decoder is followed by a final un-embedding layer. to produce the output probabilities over the vocabulary. Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc, autoregressively generating output text.
 Each encoder layer contains 2 sublayers: the self-attention and the feedforward network. Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network.
 The final points of detail are the residual connections and layer normalization (LayerNorm, or LN), which while conceptually unnecessary, are necessary for numerical stability and convergence. Similarly to how the feedforward network modules are applied individually to each vector, the LayerNorm is also applied individually to each vector. 
 There are two common conventions in use: the post-LN and the pre-LN convention. In the post-LN convention, the output of each sublayer is 




L
a
y
e
r
N
o
r
m

(
x
+

S
u
b
l
a
y
e
r

(
x
)
)


{\displaystyle \mathrm {LayerNorm} (x+\mathrm {Sublayer} (x))}

where 




S
u
b
l
a
y
e
r

(
x
)


{\displaystyle \mathrm {Sublayer} (x)}

 is the function implemented by the sublayer itself.
 In the pre-LN convention, the output of each sublayer is



x
+

S
u
b
l
a
y
e
r

(

L
a
y
e
r
N
o
r
m

(
x
)
)


{\displaystyle x+\mathrm {Sublayer} (\mathrm {LayerNorm} (x))}

The original 2017 Transformer used the post-LN convention. It was difficult to train and required careful hyperparameter tuning and a ""warm-up"" in learning rate, where it starts small and gradually increases. The pre-LN convention, proposed several times in 2018,[58] was found to be easier to train, requiring no warm-up, leading to faster convergence.[46]
 The following is the pseudocode for a standard pre-LN encoder-decoder Transformer, adapted from[59]
 The Transformer architecture, being modular, allows variations. Several common variations are described here.[60]
 An ""encoder-only"" Transformer applies the encoder to map an input text into a sequence of vectors that represent the input text. This is usually used for text embedding and representation learning for downstream applications. BERT is encoder-only. They are less often used currently, as they were found to be not significantly better than training an encoder-decoder Transformer, then taking just the encoder.[51]
 A ""decoder-only"" Transformer is not literally decoder-only, since without an encoder, the cross-attention mechanism has nothing to attend to. Thus, the decoder layers in a decoder-only Transformer is composed of just two sublayers: the causally masked self-attention, and the feedforward network. This is usually used for text generation and instruction following. The models in the GPT series and Chinchilla series are decoder-only.
 An ""encoder-decoder"" Transformer is generally the same as the original Transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. They might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. This is also usually used for text generation and instruction following. The models in the T5 series are encoder-decoder.[60]
 A ""prefixLM"" (prefix language model) is a decoder-only architecture, but with prefix masking, which is different from causal masking. Specifically, it has mask of the form[60]: Figure 3 




M

prefixLM


=


[




0



−
∞





0




M

causal





]




{\displaystyle M_{\text{prefixLM}}={\begin{bmatrix}\mathbf {0} &-\infty \\\mathbf {0} &M_{\text{causal}}\end{bmatrix}}}

where the first columns correspond to the ""prefix"", and the subsequent columns correspond to the autoregressively generated text based on the prefix. They resemble encoder-decoder models, but has less ""sparsity"". Such models are rarely used, though they are cited as theoretical possibilities and benchmarked comparisons.[51]
 There are also mixed seq2seq models. For example, in 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model, on the argument that an RNN-decoder runs much faster than Transformer-decoder when run autoregressively.[61]
 The original transformer uses ReLU activation function. Other activation functions were developed. The Llama series used SwiGLU;[62] both GPT-1 and BERT[35] used GELU.[63]
 Alternative activation functions are often used in combination with Gated Linear Units in the feedforward module.[64]
 The normalization used in the Transformer can be different from LayerNorm. One example is RMSNorm[65] which is used in the Llama series. Other examples include CapsuleNorm[66] ScaleNorm,[67] or FixNorm.[67]
 Transformers may use other positional encoding methods than sinusoidal.[68]
 The original Transformer paper reported using a learned positional encoding,[69] but finding it not superior to the sinusoidal one.[1] Later, [70] found that causal masking itself provides enough signal to a Transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module.
 RoPE (rotary positional embedding),[71] is best explained by considering a list of 2-dimensional vectors 



[
(

x

1


(
1
)


,

x

1


(
2
)


)
,
(

x

2


(
1
)


,

x

2


(
2
)


)
,
(

x

3


(
1
)


,

x

3


(
2
)


)
,
.
.
.
]


{\displaystyle [(x_{1}^{(1)},x_{1}^{(2)}),(x_{2}^{(1)},x_{2}^{(2)}),(x_{3}^{(1)},x_{3}^{(2)}),...]}

. Now pick some angle 



θ


{\displaystyle \theta }

. Then RoPE encoding is




RoPE



(



x

m


(
1
)


,

x

m


(
2
)


,
m


)


=


(



cos
⁡
m
θ


−
sin
⁡
m
θ




sin
⁡
m
θ


cos
⁡
m
θ



)




(




x

m


(
1
)







x

m


(
2
)





)


=


(




x

m


(
1
)


cos
⁡
m
θ
−

x

m


(
2
)


sin
⁡
m
θ





x

m


(
2
)


cos
⁡
m
θ
+

x

m


(
1
)


sin
⁡
m
θ



)




{\displaystyle {\text{RoPE}}{\big (}x_{m}^{(1)},x_{m}^{(2)},m{\big )}={\begin{pmatrix}\cos m\theta &-\sin m\theta \\\sin m\theta &\cos m\theta \end{pmatrix}}{\begin{pmatrix}x_{m}^{(1)}\\x_{m}^{(2)}\\\end{pmatrix}}={\begin{pmatrix}x_{m}^{(1)}\cos m\theta -x_{m}^{(2)}\sin m\theta \\x_{m}^{(2)}\cos m\theta +x_{m}^{(1)}\sin m\theta \\\end{pmatrix}}}

Equivalently, if we write the 2-dimensional vectors as complex numbers 




z

m


:=

x

m


(
1
)


+
i

x

m


(
2
)




{\displaystyle z_{m}:=x_{m}^{(1)}+ix_{m}^{(2)}}

, then RoPE encoding is just multiplication by an angle:




RoPE



(



z

m


,
m


)


=

e

i
m
θ



z

m




{\displaystyle {\text{RoPE}}{\big (}z_{m},m{\big )}=e^{im\theta }z_{m}}

For a list of 



2
n


{\displaystyle 2n}

-dimensional vectors, a RoPE encoder is defined by a sequence of angles 




θ

(
1
)


,
.
.
.
,

θ

(
n
)




{\displaystyle \theta ^{(1)},...,\theta ^{(n)}}

. Then the RoPE encoding is applied to each pair of coordinates.
 The benefit of RoPE is that the dot-product between two vectors depends on their relative location only:




RoPE



(


x
,
m



)



T



RoPE



(


y
,
n


)


=

RoPE



(


x
,
m
+
k



)



T



RoPE



(


y
,
n
+
k


)




{\displaystyle {\text{RoPE}}{\big (}x,m{\big )}^{T}{\text{RoPE}}{\big (}y,n{\big )}={\text{RoPE}}{\big (}x,m+k{\big )}^{T}{\text{RoPE}}{\big (}y,n+k{\big )}}


for any integer 



k


{\displaystyle k}

.
 ALiBi (Attention with Linear Biases)[72] is not a replacement for the positional encoder on the original transformer. Instead, it is an additional positional encoder that is directly plugged into the attention mechanism. Specifically, the ALiBi attention mechanism is








Attention

(
Q
,
K
,
V
)
=

softmax


(




Q

K


T






d

k





+
s
B

)

V






{\displaystyle {\begin{aligned}{\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}+sB\right)V\end{aligned}}}

Here, 



s


{\displaystyle s}

 is a real number (""scalar""), and 



B


{\displaystyle B}

 is the linear bias matrix defined by



B
=


(



0


1


2


3


⋯




−
1


0


1


2


⋯




−
2


−
1


0


1


⋯




−
3


−
2


−
1


0


⋯




⋮


⋮


⋮


⋮


⋱



)




{\displaystyle B={\begin{pmatrix}0&1&2&3&\cdots \\-1&0&1&2&\cdots \\-2&-1&0&1&\cdots \\-3&-2&-1&0&\cdots \\\vdots &\vdots &\vdots &\vdots &\ddots \\\end{pmatrix}}}

in other words, 




B

i
,
j


=
j
−
i


{\displaystyle B_{i,j}=j-i}

. The idea being that the linear bias matrix is a softened mask. Just as 



0


{\displaystyle 0}

 represent full attention paid, and 



−
∞


{\displaystyle -\infty }

 represents no attention paid, the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction.
 ALiBi allows pretraining on short context windows, then finetuning on longer context windows. Since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the ""bottom"" of the entire network (which is where the sinusoidal encoder on the original transformer, as well as RoPE and many others, are located).
 Relative Position Encodings[73] is similar to ALiBi, but more generic:








Attention

(
Q
,
K
,
V
)
=

softmax


(




Q

K


T






d

k





+
B

)

V






{\displaystyle {\begin{aligned}{\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}+B\right)V\end{aligned}}}

where 



B


{\displaystyle B}

 is a Toeplitz matrix, that is, 




B

i
,
j


=

B


i
′

,

j
′





{\displaystyle B_{i,j}=B_{i',j'}}

 whenever 



i
−
j
=

i
′

−

j
′



{\displaystyle i-j=i'-j'}

. This is contrasted with the original sinusoidal positional encoding, which is an ""absolute positional encoding"".[74]
 The transformer model has been implemented in standard deep learning frameworks such as TensorFlow and PyTorch. Transformers is a library produced by Hugging Face that supplies transformer-based architectures and pretrained models.[11]
 FlashAttention[75] is an algorithm that implements the transformer attention mechanism efficiently on a GPU. It performs matrix multiplications in blocks, such that each block fits within the cache of a GPU, and by careful management of the blocks it minimizes data copying between GPU caches (as data movement is slow).
 An improved version, FlashAttention-2,[76][77][78] was developed to cater to the rising demand for language models capable of handling longer context lengths. It offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 TFLOPs/s on A100 GPUs (FP16/BF16), a 2x speed increase over the original FlashAttention.
 Key advancements in FlashAttention-2 include the reduction of non-matmul FLOPs, improved parallelism over the sequence length dimension, better work partitioning between GPU warps, and added support for head dimensions up to 256 and multi-query attention (MQA) and grouped-query attention (GQA).[79]
 Benchmarks revealed FlashAttention-2 to be up to 2x faster than FlashAttention and up to 9x faster than a standard attention implementation in PyTorch. Future developments include optimization for new hardware like H100 GPUs and new data types like FP8.
 Multi-Query Attention changes the multiheaded attention mechanism.[80] Whereas normally,
 




MultiheadedAttention

(
Q
,
K
,
V
)
=


Concat


i
∈
[

n

heads


]



(


Attention

(
X

W

i


Q


,
X

W

i


K


,
X

W

i


V


)

)


W

O




{\displaystyle {\text{MultiheadedAttention}}(Q,K,V)={\text{Concat}}_{i\in [n_{\text{heads}}]}\left({\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V})\right)W^{O}}

with Multi-Query Attention, there is just one 




W

K


,

W

V




{\displaystyle W^{K},W^{V}}

, thus:
 




MultiQueryAttention

(
Q
,
K
,
V
)
=


Concat


i
∈
[

n

heads


]



(


Attention

(
X

W

i


Q


,
X

W

K


,
X

W

V


)

)


W

O




{\displaystyle {\text{MultiQueryAttention}}(Q,K,V)={\text{Concat}}_{i\in [n_{\text{heads}}]}\left({\text{Attention}}(XW_{i}^{Q},XW^{K},XW^{V})\right)W^{O}}


 This has a neutral effect on model quality and training speed, but increases inference speed. 
 More generally, grouped-query attention (GQA) partitions attention heads into groups, each of which shares the key-value pair. MQA is GQA with one group, while standard multiheaded atteniton is GQA with the maximal number of groups.[81]
 When an autoregressive transformer is used for inference, such as generating text, the query vector is different at each step, but the already-computed key and value vectors are always the same. The KV caching method saves the computed key and value vectors at each attention block, so that they are not recomputed at each new token. PagedAttention applies memory paging to KV caching.[82][83][84]
 If a transformer is used with a baked-in prompt, such as [""You are a customer support agent...""], then the key and value vectors can be computed for the prompt, and saved on disk. The saving in compute is significant when the model is used for many short interactions, such as in online chatbots.
 Transformers are used in large language models for autoregressive sequence generation: generating a stream of text, one token at a time. However, in most settings, decoding from language models is memory-bound, meaning that we have spare compute power available. Speculative decoding[85][86] uses this spare compute power by computing several tokens in parallel. Similarly to speculative execution in CPUs, future tokens are computed concurrently, by speculating on the value of previous tokens, and are later discarded if it turns out the speculation was incorrect.
 Specifically, consider a transformer model like GPT-3 with a context window size of 512. To generate an entire context window autoregressively with greedy decoding, it must be run for 512 times, each time generating a token 




x

1


,

x

2


,
.
.
.
,

x

512




{\displaystyle x_{1},x_{2},...,x_{512}}

. However, if we had some educated guess for the values of these tokens, we could verify all of them in parallel, in one run of the model, by checking that each 




x

t




{\displaystyle x_{t}}

 is indeed the token with the largest log-likelihood in the 



t


{\displaystyle t}

-th output.
 In speculative decoding, a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model. For example, suppose a small model generated four speculative tokens: 







x
~




1


,




x
~




2


,




x
~




3


,




x
~




4




{\displaystyle {\tilde {x}}_{1},{\tilde {x}}_{2},{\tilde {x}}_{3},{\tilde {x}}_{4}}

. These tokens are run through the larger model, and only 







x
~




1




{\displaystyle {\tilde {x}}_{1}}

 and 







x
~




2




{\displaystyle {\tilde {x}}_{2}}

 are accepted. The same run of the large model already generated a new token 




x

3




{\displaystyle x_{3}}

 to replace 







x
~




3




{\displaystyle {\tilde {x}}_{3}}

, and 







x
~




4




{\displaystyle {\tilde {x}}_{4}}

 is completely discarded. The process then repeats (starting from the 4th token) until all tokens are generated.
 For non-greedy decoding, similar ideas apply, except the speculative tokens are accepted or rejected stochastically, in a way that guarantees the final output distribution is the same as if speculative decoding was not used.[85][87]
 Training transformer-based architectures can be expensive, especially for long inputs.[88] Many methods have been developed to attempt to address the issue. Long Range Arena (2020)[89] is a standard benchmark for comparing the behavior of transformer architectures over long inputs.
 The standard attention graph is either all-to-all or causal, both of which scales as 



O
(

N

2


)


{\displaystyle O(N^{2})}

 where 



N


{\displaystyle N}

 is the number of tokens in a sequence.
 Reformer (2020)[88][90] reduces the computational load from 



O
(

N

2


)


{\displaystyle O(N^{2})}

 to 



O
(
N
ln
⁡
N
)


{\displaystyle O(N\ln N)}

 by using locality-sensitive hashing and reversible layers.[91]
 Sparse attention[92] uses attention graphs that grows slower than 



O
(

N

2


)


{\displaystyle O(N^{2})}

. For example, BigBird (2020)[93] uses random small-world networks which grows as 



O
(
N
)


{\displaystyle O(N)}

.
 Ordinary transformers require a memory size that is quadratic in the size of the context window. Attention-free transformers[94] reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value.
 Random Feature Attention (2021)[95] uses Fourier random features:



φ
(
x
)
=


1

D



[
cos
⁡
⟨

w

1


,
x
⟩
,
sin
⁡
⟨

w

1


,
x
⟩
,
⋯
cos
⁡
⟨

w

D


,
x
⟩
,
sin
⁡
⟨

w

D


,
x
⟩

]

T




{\displaystyle \varphi (x)={\frac {1}{\sqrt {D}}}[\cos \langle w_{1},x\rangle ,\sin \langle w_{1},x\rangle ,\cdots \cos \langle w_{D},x\rangle ,\sin \langle w_{D},x\rangle ]^{T}}

where 




w

1


,
.
.
.
,

w

D




{\displaystyle w_{1},...,w_{D}}

 are independent samples from the normal distribution 



N
(
0
,

σ

2


I
)


{\displaystyle N(0,\sigma ^{2}I)}

. This choice of parameters satisfy 




E

[
⟨
φ
(
x
)
,
φ
(
y
)
⟩
]
=

e

−



‖
x
−
y

‖

2




2

σ

2









{\displaystyle \mathbb {E} [\langle \varphi (x),\varphi (y)\rangle ]=e^{-{\frac {\|x-y\|^{2}}{2\sigma ^{2}}}}}

, or 




e

⟨
x
,
y
⟩

/


σ

2




=

E

[
⟨

e

‖
x

‖

2



/

2

σ

2




φ
(
x
)
,

e

‖
y

‖

2



/

2

σ

2




φ
(
y
)
⟩
]
≈
⟨

e

‖
x

‖

2



/

2

σ

2




φ
(
x
)
,

e

‖
y

‖

2



/

2

σ

2




φ
(
y
)
⟩


{\displaystyle e^{\langle x,y\rangle /\sigma ^{2}}=\mathbb {E} [\langle e^{\|x\|^{2}/2\sigma ^{2}}\varphi (x),e^{\|y\|^{2}/2\sigma ^{2}}\varphi (y)\rangle ]\approx \langle e^{\|x\|^{2}/2\sigma ^{2}}\varphi (x),e^{\|y\|^{2}/2\sigma ^{2}}\varphi (y)\rangle }

Consequently, the one-headed attention, with one query, can be written as 




Attention

(
q
,
K
,
V
)
=

softmax


(



q

K


T






d

k





)

V
≈



φ
(
q

)

T



∑

i



e

‖

k

i



‖

2



/

2

σ

2




φ
(

k

i


)

v

i


T




φ
(
q

)

T



∑

i



e

‖

k

i



‖

2



/

2

σ

2




φ
(

k

i


)





{\displaystyle {\text{Attention}}(q,K,V)={\text{softmax}}\left({\frac {qK^{\mathrm {T} }}{\sqrt {d_{k}}}}\right)V\approx {\frac {\varphi (q)^{T}\sum _{i}e^{\|k_{i}\|^{2}/2\sigma ^{2}}\varphi (k_{i})v_{i}^{T}}{\varphi (q)^{T}\sum _{i}e^{\|k_{i}\|^{2}/2\sigma ^{2}}\varphi (k_{i})}}}

where 



σ
=

d

K


1

/

4




{\displaystyle \sigma =d_{K}^{1/4}}

. Similarly for multiple queries, and for multiheaded attention.
 This approximation can be computed in linear time, as we can compute the matrix 



φ
(

k

i


)

v

i


T




{\displaystyle \varphi (k_{i})v_{i}^{T}}

 first, then multiply it with the query. In essence, we have managed to obtain a more precise version of 




Attention

(
Q
,
K
,
V
)
=

softmax


(



Q

K


T






d

k





)

V
≈
Q
(

K

T


V

/




d

k




)


{\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}\right)V\approx Q(K^{T}V/{\sqrt {d_{k}}})}

Performer (2022)[96] uses the same Random Feature Attention, but 




w

1


,
.
.
.
,

w

D




{\displaystyle w_{1},...,w_{D}}

 are first independently sampled from the normal distribution 



N
(
0
,

σ

2


I
)


{\displaystyle N(0,\sigma ^{2}I)}

, then they are Gram-Schmidt processed.
 Transformers can also be used/adapted for modalities (input or output) beyond just text, usually by finding a way to ""tokenize"" the modality.
 Multimodal models can either be trained from scratch, or by finetuning. A 2022 study found that Transformers pretrained only on natural language can be finetuned on only 0.03% of parameters and become competitive with LSTMs on a variety of logical and visual tasks, demonstrating transfer learning.[97] The LLaVA was a vision-language model composed of a language model (Vicuna-13B)[98] and a vision model (ViT-L/14), connected by a linear layer. Only the linear layer is finetuned.[99]
 Vision transformers[41] adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like tokens in a standard transformer.
 Conformer[42] and later Whisper[100] follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e. broken down into a series of patches, turned into vectors and treated like tokens in a standard transformer.
 Perceivers[101][102] are a variant of Transformers designed for multimodality.
 For image generation, notable architectures are DALL-E 1 (2021), Parti (2022),[103] Phenaki (2023),[104] and Muse (2023).[105] Unlike later models, DALL-E is not a diffusion model. Instead, it uses a decoder-only Transformer that autoregressively generates a text, followed by the token representation of an image, which is then converted by a variational autoencoder to an image.[106] Parti is an encoder-decoder Transformer, where the encoder processes a text prompt, and the decoder generates a token representation of an image.[107] Muse is an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens. During generation, all input tokens are masked, and the highest-confidence predictions are included for the next iteration, until all tokens are predicted.[105] Phenaki is a text-to-video model. It is a bidirectional masked transformer conditioned on pre-computed text tokens. The generated tokens are then decoded to a video.[104]
 The transformer has had great success in natural language processing (NLP). Many large language models such as GPT-2, GPT-3, GPT-4, AlbertAGPT, Claude, BERT, XLNet, RoBERTa and ChatGPT demonstrate the ability of transformers to perform a wide variety of NLP-related subtasks and their related real-world or practical applications, including:
 Beyond traditional NLP, the transformer architecture has had success in other applications, such as:
",transform deep learn architectur develop research googl base attent mechan propos paper attent need text convert numer represent call token token convert vector via lookup word embed tabl layer token contextu within scope context window unmask token via parallel attent mechan allow signal key token amplifi less import token diminish transform advantag recurr unit therefor requir less train time earlier recurr neural architectur rnn long memori lstm later variat wide adopt train larg languag model llm larg languag dataset wikipedia corpu common crawl transform first develop improv previou architectur machin translat found mani applic sinc use natur languag process comput vision vision transform reinforc learn audio multimod learn robot even play chess also led develop system gener transform gpt bert bidirect encod represent transform mani year sequenc model gener done use plain recurr neural network rnn earli exampl elman network theori inform one token propag arbitrarili far sequenc practic problem leav model state end long sentenc without precis extract inform preced token key breakthrough lstm note rnn use variou innov overcom vanish gradient problem allow effici learn model one key innov use attent mechan use neuron multipli output neuron multipl unit neural network use multipl unit later call network network lstm becam standard architectur long sequenc model public transform howev lstm still use sequenti process like rnn note specif rnn oper one token time first last oper parallel token sequenc modern transform overcom problem unlik rnn requir comput time quadrat size context window linearli scale fast weight control learn comput weight matrix process depend input one two network fast weight dynam link slow neural network learn gradient descent gener key valu comput weight chang fast neural network comput answer queri later shown equival unnorm linear transform idea sequenc transduct develop earli see previou paper paper commonli cite origin produc two concurr publish paper model machin translat use two long memori lstm architectur consist two part encod lstm take sequenc token turn vector decod anoth lstm convert vector sequenc token similarli anoth model use gate recurr unit gru instead lstm later research show gru neither better wors lstm earli model attent mechan state vector access last word sourc text process although theori vector retain inform whole origin sentenc practic inform poorli preserv input process sequenti one recurr network output vector process anoth recurr network output input long output vector would abl contain relev inform degrad output evid revers input sentenc improv translat rnnsearch model introduc attent mechan machin translat solv bottleneck problem output vector allow model process depend easili name emul search sourc sentenc decod translat rel perform compar global rnnsearch local slide window attent model architectur machin translat find mix attent higher qualiti global attent local attent reduc translat time googl translat revamp googl neural machin translat replac previou model base statist machin translat new model model encod decod layer bidirect lstm took nine month develop outperform statist approach took ten year develop model attent includ still suffer issu recurr network hard parallel prevent acceler gpu decompos attent appli mechan feedforward network easi parallel achiev sota result textual entail order magnitud less paramet lstm one author jakob uszkoreit suspect attent without recurr suffici languag translat thu titl attent need hypothesi convent wisdom time even father han uszkoreit comput linguist skeptic year call attent propos lstm origin transform model propos attent need paper time focu research improv machin translat remov recurr process token parallel preserv attent mechan keep text process perform led introduct attent model easier parallel due use independ head lack recurr paralleliz import factor widespread use larg neural network alreadi spring even attent need preprint publish one appli variat architectur gener fictiti wikipedia articl transform architectur use mani gener model contribut ongo ai boom languag model elmo lstm produc contextu word embed improv upon line research bag word follow bert transform model octob googl start use bert process search queri googl translat replac previou model model start openai gpt seri transform becam state art natur languag gener chatbot base chatgpt becam unexpectedli popular trigger boom around larg languag model sinc transform appli modal beyond text includ vision transform speech recognit robot multimod vision transform turn stimul new develop convolut neural network imag video gener like stabl diffus sora base transform architectur plain transform architectur difficulti converg origin paper author recommend use learn rate warmup learn rate linearli scale maxim valu first part train usual recommend total number train step decay paper found use layer normal instead multihead attent feedforward layer stabil train requir learn rate warmup transform typic first pretrain learn larg gener dataset follow supervis small dataset pretrain dataset typic unlabel larg corpu pile task pretrain commonli includ transform report document larg number natur languag pretrain task exampl note task trivial obviou human nativ speaker languag languag typic prove challeng previou gener machin learn architectur gener class languag model task mask autoregress prefixlm class independ specif model architectur transform often discuss context transform mask task one token mask model would produc probabl distribut predict token base context loss function task typic sum token loss mask token ln probabl condit context loss mask token probabl condit context model train minim loss function bert seri model train mask token predict anoth task autoregress task entir sequenc mask first model produc probabl distribut first token first token reveal model predict second token loss function task still typic gpt seri model train autoregress task prefixlm task sequenc divid two part first part present context model predict first token second part would reveal model predict second token loss function task still typic seri model train prefixlm task note mask mask languag model mask mask attent prefixlm prefix languag model prefixlm prefix languag model transform primari compon follow descript follow exactli transform describ origin paper variant describ follow section convent write vector row vector exampl mean push vector linear layer mean multipli weight matrix right x w xw transform architectur nativ process numer data text must translat text token token integ repres charact short segment charact input side input text pars token sequenc similarli output side output token pars back text modul convers text token sequenc token set token vocabulari token size vocabulari size n vocabulari vocabulari face token outsid vocabulari typic special token use written unk unknown commonli use token byte pair encod wordpiec sentencepiec token convert embed vector via lookup tabl equival state multipli represent token embed matrix exampl input token represent embed vector e b e emb token embed vector ad respect posit encod vector see produc sequenc input vector number dimens embed vector call hidden size embed size written emb emb size written model model origin transform paper layer almost revers embed layer wherea embed layer convert token vector layer convert vector probabl distribut token layer layer u n e b e x f x x w b unemb x softmax matrix shape emb n vocabulari emb vocabulari embed matrix matrix w w sometim requir transpos practic call weight tie posit encod vector represent rel posit token within sequenc provid transform model inform word input sequenc shall induc bia toward order input sequenc exampl input sequenc man bite dog process differ dog bite man posit encod defin function type f r r z f r r z posit even integ full posit encod defin origin paper f k f k sin θ co θ k f f θ r k r n k n n free paramet significantli larger biggest k k would input posit encod function origin paper use n function simpler form written complex function type f r c f r c f e r k k f k r n main reason use posit encod function use shift linear transform f δ g f δ f f diag f f δ r r distanc one wish shift allow transform take encod posit find encod posit matrix multipl take linear sum convolut also implement linear transform j c j f δ j j c j g f δ j f j j f j j j diag f j f constant c j j allow transform take encod posit find linear sum encod locat neighbor sum encod posit fed attent mechan would creat attent weight neighbor much like happen convolut neural network languag model author word hypothes would allow model easili learn attend rel posit typic implement oper done real number complex number sinc complex multipl implement real matrix multipl mere notat differ like earlier model origin transform model use architectur encod consist encod layer process input token togeth one layer anoth decod consist decod layer iter process encod output decod output token far purpos encod layer creat contextu represent token represent correspond token mix inform input token via mechan decod layer contain two attent sublay incorpor output encod contextu input token represent mix inform among input token decod token gener far infer time encod decod layer neural network addit process output contain residu connect layer normal step layer contain paramet transform model feedforward network ffn modul transform multilay perceptron f f n x ϕ x w b w b ffn x ϕ activ function origin transform use relu activ number neuron middl layer call intermedi size gpt filter size bert feedforward size bert typic larger embed size exampl seri bert seri intermedi size model time embed size ffn emb ffn emb attent mechan use transform architectur scale attent unit unit transform model learn three weight matric queri weight w q q key weight w k k valu weight w v v modul take three sequenc queri sequenc key sequenc valu sequenc queri sequenc sequenc length ℓ seq queri seq queri entri vector dimens emb queri emb queri similarli key valu sequenc vector x queri queri queri sequenc multipli matrix w q q produc queri vector q x queri w q queri q matrix queri vector queri matrix q x queri w q queri q similarli construct key matrix k x key w k key k valu matrix v x valu w v valu v usual case w q w k w v q k v squar matric mean emb queri queri emb queri queri etc attent weight calcul use queri key vector attent weight j ij token token j j dot product q k j j attent weight divid squar root dimens key vector k k stabil gradient train pass softmax normal weight fact w q q w k k differ matric allow attent token attend token j j q k j j larg necessarili mean token j j attend token q j k j could small output attent unit token weight sum valu vector token weight j ij attent token token attent calcul token express one larg matrix calcul use softmax function use train due comput matrix oper optim quickli comput matrix oper matric q q k k v v defin matric th row vector q k v respect repres attent attent q k v softmax q k k v align attent q k v softmax k align softmax appli row matrix number dimens queri vector queri size queri queri similarli key size key key valu size valu valu output dimens attent head head dimens head head attent mechan requir follow three equal hold ℓ seq key ℓ seq valu queri key valu head seq key seq valu queri key valu head otherwis unconstrain attent head use fashion x queri x key x valu queri key valu attent head use fashion usual x queri x key x valu queri key valu theoret possibl three differ rare case practic one set w q w k w v q k v matric call attent head layer transform model multipl attent head attent head attend token relev token multipl attent head allow model differ definit relev specif queri key project matric w q q w k k involv attent score comput defin relev meanwhil valu project matrix w v v combin part output project matrix w determin attend token influenc inform pass subsequ layer ultim output logit addit scope attent rang token relationship captur attent head expand token pass success layer allow model captur complex depend deeper layer mani transform attent head encod relev relat meaning human exampl attent head attend mostli next word other mainli attend verb direct object comput attent head perform parallel allow fast process output attent layer concaten pass neural network layer concret let multipl attent head index multiheadedattent q k v concat n head attent x w q x w k x w v w multiheadedattent q k v concat head attent q k v matrix x x concaten word embed matric w q w k w v q k v project matric own individu attent head w final project matrix own whole attent head theoret possibl attent head differ head dimens head head rare case practic exampl smallest model mechan follow dimens emb n head head emb head head sinc output project matrix w r r squar matrix transform architectur construct calcul output token iter assum refer calcul first output token step output token shall remain constant ensur properti model similar autoregress model therefor everi time step calcul output access token posit j j j j natur case time step token j j yet calcul behavior may accomplish softmax stage ad mask matrix entri attent link must cut place maskedattent q k v softmax q k k v align maskedattent q k v softmax k align follow matrix commonli use decod modul call causal mask causal causal bmatrix bmatrix word mean token pay attent everi token attent modul thought mask attent modul mask entri zero exampl uncommon use mask matrix xlnet consid mask form p causal p causal p p random permut matrix encod consist embed layer follow multipl encod layer encod layer consist two major compon mechan layer take input sequenc input vector appli mechan produc intermedi sequenc vector appli layer vector individu schemat given input vector h h combin matrix h h h encoderlay h ffn multiheadedattent h h h ffn multiheadedattent h h h align given input vector combin matrix h bmatrix bmatrix encoderlay h bmatrix ffn multiheadedattent h h h ffn multiheadedattent h h h bmatrix align ffn ffn stand network succinctli write encoderlay h ffn multiheadedattent h h h encoderlay h ffn multiheadedattent h h h implicit convent ffn ffn appli row matrix individu encod layer stack first encod layer take sequenc input vector embed layer produc sequenc vector sequenc vector process second encod output final encod layer use decod encod process entir input everi token attend everi token attent need causal mask decod consist embed layer follow multipl decod layer follow layer decod consist three major compon causal mask mechan mechan neural network decod function similar fashion encod addit attent mechan insert instead draw relev inform encod gener encod mechan also call attent like first encod first decod take posit inform embed output sequenc input rather encod transform must use current futur output predict output output sequenc must partial mask prevent revers inform flow allow autoregress text gener decod attent inappropri token attend token yet gener thu modul decod causal mask contrast mechan attend output vector encod comput decod start decod consequ need mask mechan schemat h maskedmultiheadedattent h h h decoderlay h ffn multiheadedattent h h e h e align h maskedmultiheadedattent h h h decoderlay h ffn multiheadedattent h e e align h e e matrix row output vector encod last decod follow final layer produc output probabl vocabulari one token sampl accord probabl decod run produc next token etc autoregress gener output text encod layer contain sublay feedforward network decod layer contain sublay causal mask feedforward network final point detail residu connect layer normal layernorm ln conceptu unnecessari necessari numer stabil converg similarli feedforward network modul appli individu vector layernorm also appli individu vector two common convent use convent convent output sublay l e r n r x u b l e r x layernorm sublay x u b l e r x sublay x function implement sublay convent output sublay x u b l e r l e r n r x sublay layernorm x origin transform use convent difficult train requir care hyperparamet tune learn rate start small gradual increas convent propos sever time found easier train requir lead faster converg follow pseudocod standard transform adapt transform architectur modular allow variat sever common variat describ transform appli encod map input text sequenc vector repres input text usual use text embed represent learn downstream applic bert less often use current found significantli better train transform take encod transform liter sinc without encod mechan noth attend thu decod layer transform compos two sublay causal mask feedforward network usual use text gener instruct follow model gpt seri chinchilla seri transform gener origin transform sublay per encod layer sublay per decod layer etc might minor architectur improv altern activ function chang locat normal etc also usual use text gener instruct follow model seri prefixlm prefix languag model architectur prefix mask differ causal mask specif mask form figur prefixlm causal prefixlm bmatrix causal bmatrix first column correspond prefix subsequ column correspond autoregress gener text base prefix resembl model less sparsiti model rare use though cite theoret possibl benchmark comparison also mix model exampl googl translat replac previou model model argument run much faster run autoregress origin transform use relu activ function activ function develop llama seri use swiglu bert use gelu altern activ function often use combin gate linear unit feedforward modul normal use transform differ layernorm one exampl rmsnorm use llama seri exampl includ capsulenorm scalenorm fixnorm transform may use posit encod method sinusoid origin transform paper report use learn posit encod find superior sinusoid one later found causal mask provid enough signal transform decod learn implicitli perform absolut posit encod without posit encod modul rope rotari posit embed best explain consid list vector x x x x x x pick angl θ rope encod rope x x co θ sin θ sin θ co θ x x x co θ x sin θ x co θ x sin θ rope pmatrix pmatrix pmatrix pmatrix pmatrix pmatrix equival write vector complex number z x x rope encod multipl angl rope z e θ z rope list n vector rope encod defin sequenc angl θ θ n n rope encod appli pair coordin benefit rope two vector depend rel locat rope x rope n rope x k rope n k rope x rope n rope x rope integ k k alibi attent linear bias replac posit encod origin transform instead addit posit encod directli plug attent mechan specif alibi attent mechan attent q k v softmax q k k b v align attent q k v softmax k align real number scalar b b linear bia matrix defin b pmatrix pmatrix word b j j j idea linear bia matrix soften mask repres full attent paid repres attent paid linear bia matrix increas attent paid one direct decreas attent paid direct alibi allow pretrain short context window finetun longer context window sinc directli plug attent mechan combin posit encod plug bottom entir network sinusoid encod origin transform well rope mani other locat rel posit encod similar alibi gener attent q k v softmax q k k b v align attent q k v softmax k align b b toeplitz matrix b j b j j j whenev j j contrast origin sinusoid posit encod absolut posit encod transform model implement standard deep learn framework tensorflow pytorch transform librari produc hug face suppli architectur pretrain model flashattent algorithm implement transform attent mechan effici gpu perform matrix multipl block block fit within cach gpu care manag block minim data copi gpu cach data movement slow improv version develop cater rise demand languag model capabl handl longer context length offer enhanc work partit parallel enabl achiev gpu speed increas origin flashattent key advanc includ reduct flop improv parallel sequenc length dimens better work partit gpu warp ad support head dimens attent mqa attent gqa benchmark reveal faster flashattent faster standard attent implement pytorch futur develop includ optim new hardwar like gpu new data type like attent chang multihead attent mechan wherea normal multiheadedattent q k v concat n head attent x w q x w k x w v w multiheadedattent q k v concat head attent q k v attent one w k w v k v thu multiqueryattent q k v concat n head attent x w q x w k x w v w multiqueryattent q k v concat head attent q k v neutral effect model qualiti train speed increas infer speed gener attent gqa partit attent head group share pair mqa gqa one group standard multihead atteniton gqa maxim number group autoregress transform use infer gener text queri vector differ step key valu vector alway kv cach method save comput key valu vector attent block recomput new token pagedattent appli memori page kv cach transform use prompt custom support agent key valu vector comput prompt save disk save comput signific model use mani short interact onlin chatbot transform use larg languag model autoregress sequenc gener gener stream text one token time howev set decod languag model mean spare comput power avail specul decod use spare comput power comput sever token parallel similarli specul execut cpu futur token comput concurr specul valu previou token later discard turn specul incorrect specif consid transform model like context window size gener entir context window autoregress greedi decod must run time time gener token x x x howev educ guess valu token could verifi parallel one run model check x inde token largest output specul decod smaller model simpl heurist use gener specul token subsequ verifi larger model exampl suppos small model gener four specul token x x x x x x x x token run larger model x x x x accept run larg model alreadi gener new token x replac x x x x complet discard process repeat start token token gener decod similar idea appli except specul token accept reject stochast way guarante final output distribut specul decod use train architectur expens especi long input mani method develop attempt address issu long rang arena standard benchmark compar behavior transform architectur long input standard attent graph either causal scale n n n number token sequenc reform reduc comput load n n ln n n use hash revers layer spars attent use attent graph grow slower n exampl bigbird use random network grow n n ordinari transform requir memori size quadrat size context window transform reduc linear depend still retain advantag transform link key valu random featur attent use fourier random featur φ x co w x sin w x co w x sin w x x w w independ sampl normal distribut n σ n choic paramet satisfi e φ x φ e x σ e x e x σ e e x σ φ x e σ φ e x σ φ x e σ φ x e x x consequ attent one queri written attent q k v softmax q k k v φ q e k σ φ k v φ q e k σ φ k attent q k v softmax k q q σ k k similarli multipl queri multihead attent approxim comput linear time comput matrix φ k v first multipli queri essenc manag obtain precis version attent q k v softmax q k k v q k v k attent q k v softmax k q k perform use random featur attent w w first independ sampl normal distribut n σ n process transform also modal input output beyond text usual find way token modal multimod model either train scratch finetun studi found transform pretrain natur languag finetun paramet becom competit lstm varieti logic visual task demonstr transfer learn llava model compos languag model vision model connect linear layer linear layer finetun vision transform adapt transform comput vision break input imag seri patch turn vector treat like token standard transform conform later whisper follow pattern speech recognit first turn speech signal spectrogram treat like imag broken seri patch turn vector treat like token standard transform perceiv variant transform design multimod imag gener notabl architectur parti phenaki muse unlik later model diffus model instead use transform autoregress gener text follow token represent imag convert variat autoencod imag parti transform encod process text prompt decod gener token represent imag muse transform train predict mask imag token unmask imag token gener input token mask predict includ next iter token predict phenaki model bidirect mask transform condit text token gener token decod video transform great success natur languag process nlp mani larg languag model albertagpt claud bert xlnet roberta chatgpt demonstr abil transform perform wide varieti subtask relat practic applic includ beyond tradit nlp transform architectur success applic
Vision transformer,https://en.wikipedia.org/wiki/Vision_transformer,"A vision transformer (ViT) is a transformer designed for computer vision.[1] A ViT decomposes an input image into a series of patches (rather than text into tokens), serializes each patch into a vector, and maps it to a smaller dimension with a single matrix multiplication. These vector embeddings are then processed by a transformer encoder as if they were token embeddings.
 ViTs were designed as alternatives to convolutional neural networks (CNNs) in computer vision applications. They have different inductive biases, training stability, and data efficiency.[2] Compared to CNNs, ViTs are less data efficient, but have higher capacity. Some of the largest modern computer vision models are ViTs, such as one with 22B parameters.[3][4] In 2024, a 113 billion-parameter ViT model was proposed (the largest ViT to date) for weather and climate prediction, and trained on the Frontier supercomputer with a throughput of 1.6 exaFLOPs.[5]
 Subsequent to its publication, many variants were proposed, with hybrid architectures with both features of ViTs and CNNs. ViTs have found application in image recognition, image segmentation, and autonomous driving.[6][7]
 Transformers were introduced in Attention Is All You Need (2017),[8] and have found widespread use in natural language processing. A 2019 paper[9] applied ideas from the Transformer to computer vision. Specifically, they started with a ResNet, a standard convolutional neural network used for computer vision, and replaced all convolutional kernels by the self-attention mechanism found in a Transformer. It resulted in superior performance. However, it is not a Vision Transformer.
 In 2020, an encoder-only Transformer was adapted for computer vision, yielding the ViT, which reached state of the art in image classification, overcoming the previous dominance of CNN.[1] The masked autoencoder (2022) extended ViT to work with unsupervised training. The vision transformer and the masked autoencoder, in turn, stimulated new developments in convolutional neural networks.[10][11]
 Subsequently, there was cross-fertilization between the previous CNN approach and the ViT approach.
 In 2021, some important variants of the Vision Transformers were proposed. These variants are mainly intended to be more efficient, more accurate or better suited to a specific domain. Two studies [12][13] improved efficiency and robustness of ViT by adding a CNN as a preprocessor. The Swin Transformer[14] achieved state-of-the-art results on some object detection datasets such as COCO, by using convolution-like sliding windows of attention mechanism, and the pyramid process in classical computer vision.
 The basic architecture, used by the original 2020 paper,[1] is as follows. In summary, it is a BERT-like encoder-only Transformer.
 The input image is of type 





R


H
×
W
×
C




{\displaystyle \mathbb {R} ^{H\times W\times C}}

, where 



H
,
W
,
C


{\displaystyle H,W,C}

 are height, width, channel (RGB). It is then split into square-shaped patches of type 





R


P
×
P
×
C




{\displaystyle \mathbb {R} ^{P\times P\times C}}

.
 For each patch, the patch is pushed through a linear operator, to obtain a vector (""patch embedding""). The position of the patch is also transformed into a vector by ""position encoding"". The two vectors are added, then pushed through several Transformer encoders.
 The attention mechanism in a ViT repeatedly transforms representation vectors of image patches, incorporating more and more semantic relations between image patches in an image. This is analogous to how in natural language processing, as representation vectors flow through a transformer, they incorporate more and more semantic relations between words, from syntax to semantics.
 The above architecture turns an image into a sequence of vector representations. To use these for downstream applications, an additional head needs to be trained to interpret them.
 For example, to use it for classification, one can add a shallow MLP on top of it that outputs a probability distribution over classes. The original paper uses a linear-GeLU-linear-softmax network.[1]
 
The original ViT was an encoder-only Transformer supervise-trained to predict the image label from the patches of the image. As in the case of BERT, it uses a special token <CLS> in the input side, and the corresponding output vector is used as the only input of the final output MLP head. The special token is an architectural hack to allow the model to compress all information relevant for predicting the image label into one vector. Transformers found their initial applications in natural language processing tasks, as demonstrated by language models such as BERT and GPT-3. By contrast the typical image processing system uses a convolutional neural network (CNN). Well-known projects include Xception, ResNet, EfficientNet,[15] DenseNet,[16] and Inception.[17]
 Transformers measure the relationships between pairs of input tokens (words in the case of text strings), termed attention. The cost is quadratic in the number of tokens. For images, the basic unit of analysis is the pixel. However, computing relationships for every pixel pair in a typical image is prohibitive in terms of memory and computation. Instead, ViT computes relationships among pixels in various small sections of the image (e.g., 16x16 pixels), at a drastically reduced cost. The sections (with positional embeddings) are placed in a sequence. The embeddings are learnable vectors. Each section is arranged into a linear sequence and multiplied by the embedding matrix. The result, with the position embedding is fed to the transformer.[17]
 After the ViT processes an image, it produces some embedding vectors. These must be converted to a single class probability prediction by some kind of network. In the original ViT and Masked Autoencoder, they used a dummy [CLS] token , in emulation of the BERT language model. The output at [CLS] is the classification token, which is then processed by a LayerNorm-feedforward-softmax module into a probability distribution.
 Global average pooling (GAP) does not use the dummy token, but simply takes the average of all output tokens as the classification token. It was mentioned in the original ViT as being equally good.[1]
 Multihead attention pooling (MAP) applies a multiheaded attention block to pooling. Specifically, it takes as input a list of vectors 




x

1


,

x

2


,
…
,

x

n




{\displaystyle x_{1},x_{2},\dots ,x_{n}}

, which might be thought of as the output vectors of a layer of a ViT. The output from MAP is 




M
u
l
t
i
h
e
a
d
e
d
A
t
t
e
n
t
i
o
n

(
Q
,
V
,
V
)


{\displaystyle \mathrm {MultiheadedAttention} (Q,V,V)}

, where 



q


{\displaystyle q}

 is a trainable query vector, and 



V


{\displaystyle V}

 is the matrix with rows being 




x

1


,

x

2


,
…
,

x

n




{\displaystyle x_{1},x_{2},\dots ,x_{n}}

.[18] This was first proposed in the Set Transformer architecture.[19]
 Later papers demonstrated that GAP and MAP both perform better than BERT-like pooling.[18][20] A variant of MAP was proposed as class attention, which applies MAP, then feedforward, then MAP again.[21]
 Re-attention was proposed to allow training deep ViT. It changes the multiheaded attention module.[22]
 The Masked Autoencoder[23] took inspiration from denoising autoencoders and context encoders.[24] It has two ViTs put end-to-end. The first one (""encoder"") takes in image patches with positional encoding, and outputs vectors representing each patch. The second one (called ""decoder"", even though it is still an encoder-only Transformer) takes in vectors with positional encoding and outputs image patches again. During training, both the encoder and the decoder ViTs are used. During inference, only the encoder ViT is used.
 During training, each image is cut into patches, and with their positional embeddings added. Of these, only 25% of the patches are selected. The encoder ViT processes the selected patches. No mask tokens are used. Then, mask tokens are added back in, and positional embeddings added again. These are processed by the decoder ViT, which outputs a reconstruction of the full image. The loss is the total mean-squared loss in pixel-space for all masked patches (reconstruction loss is not computed for non-masked patches). 
 A similar architecture was BERT ViT (BEiT), published concurrently.[25]
 Like the Masked Autoencoder, the DINO (self-distillation with no labels) method is a way to train a ViT by self-supervision.[26] DINO is a form of teacher-student self-distillation. In DINO, the student is the model itself, and the teacher is an exponential average of the student's past states. The method is similar to previous works like momentum contrast[27] and bootstrap your own latent (BYOL).[28]
 The loss function used in DINO is the cross-entropy loss between the output of the teacher network (




f


θ

t

′





{\displaystyle f_{\theta '_{t}}}

) and the output of the student network (




f


θ

t






{\displaystyle f_{\theta _{t}}}

). The teacher network is an exponentially decaying average of the student network's past parameters: 




θ

t

′

=
α

θ

t


+
α
(
1
−
α
)

θ

t
−
1


+
⋯


{\displaystyle \theta '_{t}=\alpha \theta _{t}+\alpha (1-\alpha )\theta _{t-1}+\cdots }

. The inputs to the networks are two different crops of the same image, represented as 



T
(
x
)


{\displaystyle T(x)}

 and 




T
′

(
x
)


{\displaystyle T'(x)}

, where 



x


{\displaystyle x}

 is the original image. The loss function is written as



L
(

f


θ

t

′



(
T
(
x
)
)
,

f


θ

t




(

T
′

(
x
)
)
)


{\displaystyle L(f_{\theta '_{t}}(T(x)),f_{\theta _{t}}(T'(x)))}

One issue is that the network can ""collapse"" by always outputting the same value (



y


{\displaystyle y}

), regardless of the input. To prevent this collapse, DINO employs two strategies:
 In January 2024, Meta AI Research released an updated version called DINOv2[29] with improvements in architecture, loss function, and optimization technique. It was trained on a larger and more diverse dataset. The features learned by DINOv2 were more transferable, meaning it had better performance in downstream tasks.
 The Swin Transformer (""Shifted windows"")[14] took inspiration from standard CNNs:
 It is improved by Swin Transformer V2,[30] which modifies upon the ViT by a different attention mechanism[14]: Figure 1 :
 The TimeSformer[31] was designed for video understanding tasks, and it applied a factorized self-attention, similar to the factorized convolution kernels found in the Inception CNN architecture.[32] Schematically, it divides a video into frames, and each frame into a square grid of patches (same as ViT). Let each patch coordinate be denoted by 



x
,
y
,
t


{\displaystyle x,y,t}

, denoting horizontal, vertical, and time. 
 The TimeSformer also considered other attention layer designs, such as the ""height attention layer"" where the requirement is 




x
′

=
x
,

t
′

=
t


{\displaystyle x'=x,t'=t}

. However, they found empirically that the best design interleaves one space attention layer and one time attention layer.
 In ViT-VQGAN,[33] there are two ViT encoders and a discriminator. One encodes 8x8 patches of an image into a list of vectors, one for each patch. The vectors can only come from a discrete set of ""codebook"", as in vector quantization. Another encodes the quantized vectors back to image patches. The training objective attempts to make the reconstruction image (the output image) faithful to the input image. The discriminator (usually a convolutional network, but other networks are allowed) attempts to decide if an image is an original real image, or a reconstructed image by the ViT.
 The idea is essentially the same as vector quantized variational autoencoder (VQVAE) plus generative adversarial network (GAN).
 After such a ViT-VQGAN is trained, it can be used to code an arbitrary image into a list of symbols, and code an arbitrary list of symbols into an image. The list of symbols can be used to train into a standard autoregressive transformer (like GPT), for autoregressively generating an image. Further, one can take a list of caption-image pairs, convert the images into strings of symbols, and train a standard GPT-style transformer. Then at test time, one can just give an image caption, and have it autoregressively generate the image. This is the structure of Google Parti.[34]
 Other examples include the visual transformer,[35] CoAtNet,[36] CvT,[37] the data-efficient ViT (DeiT),[38] etc.
 In the Transformer in Transformer architecture, each layer applies a vision Transformer layer on each image patch embedding, add back the resulting tokens to the embedding, then applies another vision Transformer layer.[39]
 Typically, ViT uses patch sizes larger than standard CNN kernels (3x3 to 7x7). ViT is more sensitive to the choice of the optimizer, hyperparameters, and network depth. Preprocessing with a layer of smaller-size, overlapping (stride < size) convolutional filters helps with performance and stability.[13]
 This different behavior seems to derive from the different inductive biases they possess. 
 CNN applies the same set of filters for processing the entire image. This allows them to be more data efficient and less sensitive to local perturbations.[2] ViT applies self-attention, allowing them to easily capture long-range relationships between patches. They also require more data to train, but they can ingest more training data compared to CNN, which might not improve after training on a large enough training dataset. ViT also appears more robust to input image distortions such as adversarial patches or permutations.[40]
 ViT have been used in many Computer Vision tasks with excellent results and in some cases even state-of-the-art. Image Classification, Object Detection, Video Deepfake Detection,[41] Image segmentation,[42] Anomaly detection, Image Synthesis, Cluster analysis, Autonomous Driving.[6][7]
 ViT had been used for image generation as backbones for GAN[43] and for diffusion models (diffusion transformer, or DiT).[44]
 DINO[26] has been demonstrated to learn useful representations for clustering images and exploring morphological profiles on biological datasets, such as images generated with the Cell Painting assay.[45]
",vision transform vit transform design comput vision vit decompos input imag seri patch rather text token serial patch vector map smaller dimens singl matrix multipl vector embed process transform encod token embed vit design altern convolut neural network cnn comput vision applic differ induct bias train stabil data effici compar cnn vit less data effici higher capac largest modern comput vision model vit one paramet vit model propos largest vit date weather climat predict train frontier supercomput throughput exaflop subsequ public mani variant propos hybrid architectur featur vit cnn vit found applic imag recognit imag segment autonom drive transform introduc attent need found widespread use natur languag process paper appli idea transform comput vision specif start resnet standard convolut neural network use comput vision replac convolut kernel mechan found transform result superior perform howev vision transform transform adapt comput vision yield vit reach state art imag classif overcom previou domin cnn mask autoencod extend vit work unsupervis train vision transform mask autoencod turn stimul new develop convolut neural network subsequ previou cnn approach vit approach import variant vision transform propos variant mainli intend effici accur better suit specif domain two studi improv effici robust vit ad cnn preprocessor swin transform achiev result object detect dataset coco use slide window attent mechan pyramid process classic comput vision basic architectur use origin paper follow summari transform input imag type r h w c r c h w c h w c height width channel rgb split patch type r p p c r c patch patch push linear oper obtain vector patch embed posit patch also transform vector posit encod two vector ad push sever transform encod attent mechan vit repeatedli transform represent vector imag patch incorpor semant relat imag patch imag analog natur languag process represent vector flow transform incorpor semant relat word syntax semant architectur turn imag sequenc vector represent use downstream applic addit head need train interpret exampl use classif one add shallow mlp top output probabl distribut class origin paper use network origin vit transform predict imag label patch imag case bert use special token cl input side correspond output vector use input final output mlp head special token architectur hack allow model compress inform relev predict imag label one vector transform found initi applic natur languag process task demonstr languag model bert contrast typic imag process system use convolut neural network cnn project includ xception resnet efficientnet densenet incept transform measur relationship pair input token word case text string term attent cost quadrat number token imag basic unit analysi pixel howev comput relationship everi pixel pair typic imag prohibit term memori comput instead vit comput relationship among pixel variou small section imag pixel drastic reduc cost section posit embed place sequenc embed learnabl vector section arrang linear sequenc multipli embed matrix result posit embed fed transform vit process imag produc embed vector must convert singl class probabl predict kind network origin vit mask autoencod use dummi cl token emul bert languag model output cl classif token process modul probabl distribut global averag pool gap use dummi token simpli take averag output token classif token mention origin vit equal good multihead attent pool map appli multihead attent block pool specif take input list vector x x x n n might thought output vector layer vit output map u l h e e e n n q v v multiheadedattent q v v q q trainabl queri vector v v matrix row x x x n n first propos set transform architectur later paper demonstr gap map perform better pool variant map propos class attent appli map feedforward map propos allow train deep vit chang multihead attent modul mask autoencod took inspir denois autoencod context encod two vit put first one encod take imag patch posit encod output vector repres patch second one call decod even though still transform take vector posit encod output imag patch train encod decod vit use infer encod vit use train imag cut patch posit embed ad patch select encod vit process select patch mask token use mask token ad back posit embed ad process decod vit output reconstruct full imag loss total loss mask patch reconstruct loss comput patch similar architectur bert vit beit publish concurr like mask autoencod dino label method way train vit dino form dino student model teacher exponenti averag student past state method similar previou work like momentum contrast bootstrap latent byol loss function use dino loss output teacher network f θ output student network f θ teacher network exponenti decay averag student network past paramet θ α θ α α θ input network two differ crop imag repres x x x x x x origin imag loss function written l f θ x f θ x l x x one issu network collaps alway output valu regardless input prevent collaps dino employ two strategi januari meta ai research releas updat version call improv architectur loss function optim techniqu train larger divers dataset featur learn transfer mean better perform downstream task swin transform shift window took inspir standard cnn improv swin transform modifi upon vit differ attent mechan figur timesform design video understand task appli factor similar factor convolut kernel found incept cnn architectur schemat divid video frame frame squar grid patch vit let patch coordin denot x x denot horizont vertic time timesform also consid attent layer design height attent layer requir x x howev found empir best design interleav one space attent layer one time attent layer two vit encod discrimin one encod patch imag list vector one patch vector come discret set codebook vector quantiz anoth encod quantiz vector back imag patch train object attempt make reconstruct imag output imag faith input imag discrimin usual convolut network network allow attempt decid imag origin real imag reconstruct imag vit idea essenti vector quantiz variat autoencod vqvae plu gener adversari network gan train use code arbitrari imag list symbol code arbitrari list symbol imag list symbol use train standard autoregress transform like gpt autoregress gener imag one take list pair convert imag string symbol train standard transform test time one give imag caption autoregress gener imag structur googl parti exampl includ visual transform coatnet cvt vit deit etc transform transform architectur layer appli vision transform layer imag patch embed add back result token embed appli anoth vision transform layer typic vit use patch size larger standard cnn kernel vit sensit choic optim hyperparamet network depth preprocess layer overlap stride size convolut filter help perform stabil differ behavior seem deriv differ induct bias possess cnn appli set filter process entir imag allow data effici less sensit local perturb vit appli allow easili captur relationship patch also requir data train ingest train data compar cnn might improv train larg enough train dataset vit also appear robust input imag distort adversari patch permut vit use mani comput vision task excel result case even imag classif object detect video deepfak detect imag segment anomali detect imag synthesi cluster analysi autonom drive vit use imag gener backbon gan diffus model diffus transform dit dino demonstr learn use represent cluster imag explor morpholog profil biolog dataset imag gener cell paint assay
Mamba (deep learning architecture),https://en.wikipedia.org/wiki/Mamba_(deep_learning_architecture),"Mamba[a] is a deep learning architecture focused on sequence modeling. It was developed by researchers from Carnegie Mellon University and Princeton University to address some limitations of transformer models, especially in processing long sequences. It is based on the Structured State Space sequence (S4) model.[2][3][4]
 To enable handling long data sequences, Mamba incorporates the Structured State Space sequence model (S4).[2] S4 can effectively and efficiently model long dependencies by combining continuous-time, recurrent, and convolutional models. These enable it to handle irregularly sampled data, unbounded context, and remain computationally efficient during training and inferencing.[5]
 Mamba introduces significant enhancements to S4, particularly in its treatment of time-variant operations. It adopts a unique selection mechanism that adapts structured state space model (SSM) parameters based on the input.[6][2] This enables Mamba to selectively focus on relevant information within sequences, effectively filtering out less pertinent data. The model transitions from a time-invariant to a time-varying framework, which impacts both computation and efficiency.[2][7]
 Mamba employs a hardware-aware algorithm that exploits GPUs, by using kernel fusion, parallel scan, and recomputation.[2] The implementation avoids materializing expanded states in memory-intensive layers, thereby improving performance and memory usage. The result is significantly more efficient in processing long sequences compared to transformers.[2][7]
 Additionally, Mamba simplifies its architecture by integrating the SSM design with MLP blocks, resulting in a homogeneous and streamlined structure, furthering the model's capability for general sequence modeling across data types that include language, audio, and genomics, while maintaining efficiency in both training and inference.[2]
 Operating on byte-sized tokens, transformers scale poorly as every token must ""attend"" to every other token leading to O(n2) scaling laws, as a result, Transformers opt to use subword tokenization to reduce the number of tokens in text, however, this leads to very large vocabulary tables and word embeddings.
 This research investigates a novel approach to language modeling, MambaByte, which departs from the standard token-based methods. Unlike traditional models that rely on breaking text into discrete units, MambaByte directly processes raw byte sequences.  This eliminates the need for tokenization, potentially offering several advantages:[8]
 Subword tokenisation introduces a number of quirks in LLMs, such as failure modes where LLMs can't spell words, reverse certain words, handle rare tokens, which are not present in byte-level tokenisation.[9]
 MoE Mamba represents a pioneering integration of the Mixture of Experts (MoE) technique with the Mamba architecture, enhancing the efficiency and scalability of State Space Models (SSMs) in language modeling. This model leverages the strengths of both MoE and SSMs, achieving significant gains in training efficiency—requiring 2.2 times fewer training steps than its predecessor, Mamba, while maintaining competitive performance. MoE Mamba showcases improved efficiency and effectiveness by combining selective state space modeling with expert-based processing, offering a promising avenue for future research in scaling SSMs to handle tens of billions of parameters. The model's design involves alternating Mamba and MoE layers, allowing it to efficiently integrate the entire sequence context and apply the most relevant expert for each token.[10][11]
 Vision Mamba (Vim) integrates SSMs with visual data processing, employing bidirectional Mamba blocks for visual sequence encoding. This method reduces the computational demands typically associated with self-attention in visual tasks. Tested on ImageNet classification, COCO object detection, and ADE20k semantic segmentation, Vim showcases enhanced performance and efficiency and is capable of handling high-resolution images with lower computational resources. This positions Vim as a scalable model for future advancements in visual representation learning.[12]
 Jamba is a novel architecture built on a hybrid transformer and mamba SSM architecture developed by AI21 Labs with 52 billion parameters, making it the largest Mamba-variant created so far. It has a context window of 256k tokens.[13]
 Mamba LLM represents a significant potential shift in large language model architecture, offering faster, more efficient, and scalable models[citation needed].
 Applications include language translation, content generation, long-form text analysis, audio, and speech processing[citation needed].
",mamba deep learn architectur focus sequenc model develop research carnegi mellon univers princeton univers address limit transform model especi process long sequenc base structur state space sequenc model enabl handl long data sequenc mamba incorpor structur state space sequenc model effect effici model long depend combin recurr convolut model enabl handl irregularli sampl data unbound context remain comput effici train inferenc mamba introduc signific enhanc particularli treatment oper adopt uniqu select mechan adapt structur state space model ssm paramet base input enabl mamba select focu relev inform within sequenc effect filter less pertin data model transit framework impact comput effici mamba employ algorithm exploit gpu use kernel fusion parallel scan recomput implement avoid materi expand state layer therebi improv perform memori usag result significantli effici process long sequenc compar transform addit mamba simplifi architectur integr ssm design mlp block result homogen streamlin structur further model capabl gener sequenc model across data type includ languag audio genom maintain effici train infer oper token transform scale poorli everi token must attend everi token lead scale law result transform opt use subword token reduc number token text howev lead larg vocabulari tabl word embed research investig novel approach languag model mambabyt depart standard method unlik tradit model reli break text discret unit mambabyt directli process raw byte sequenc elimin need token potenti offer sever advantag subword tokenis introduc number quirk llm failur mode llm ca spell word revers certain word handl rare token present tokenis moe mamba repres pioneer integr mixtur expert moe techniqu mamba architectur enhanc effici scalabl state space model ssm languag model model leverag strength moe ssm achiev signific gain train time fewer train step predecessor mamba maintain competit perform moe mamba showcas improv effici effect combin select state space model process offer promis avenu futur research scale ssm handl ten billion paramet model design involv altern mamba moe layer allow effici integr entir sequenc context appli relev expert token vision mamba vim integr ssm visual data process employ bidirect mamba block visual sequenc encod method reduc comput demand typic associ visual task test imagenet classif coco object detect semant segment vim showcas enhanc perform effici capabl handl imag lower comput resourc posit vim scalabl model futur advanc visual represent learn jamba novel architectur built hybrid transform mamba ssm architectur develop lab billion paramet make largest creat far context window token mamba llm repres signific potenti shift larg languag model architectur offer faster effici scalabl model citat need applic includ languag translat content gener text analysi audio speech process citat need
Spiking neural network,https://en.wikipedia.org/wiki/Spiking_neural_network,"Spiking neural networks (SNNs) are artificial neural networks (ANN) that more closely mimic natural neural networks.[1] These models leverage timing of discrete spikes as the main information carrier.[2]
 In addition to neuronal and synaptic state, SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather transmit information only when a membrane potential—an intrinsic quality of the neuron related to its membrane electrical charge—reaches a specific value, called the threshold. When the membrane potential reaches the threshold, the neuron fires, and generates a signal that travels to other neurons which, in turn, increase or decrease their potentials in response to this signal. A neuron model that fires at the moment of threshold crossing is also called a spiking neuron model.[3]
 Although it was previously believed that the brain encoded information through spike rates, which can be considered as the analogue variable output of a traditional ANN,[4] research in the field of neurobiology has indicated that high speed processing cannot solely be performed through a rate based scheme. For example humans can perform an image recognition task at rate requiring no more than 10ms of processing time per neuron through the successive layers (going from the retina to the temporal lobe). This time window is too short for a rate based encoding. The precise spike timings in a small set of spiking neurons also has a higher information coding capacity compared with a rate based approach.[5]
 The most prominent spiking neuron model is the leaky integrate-and-fire model.[6] In the integrate-and-fire model, the momentary activation level (modeled as a differential equation) is normally considered to be the neuron's state, with incoming spikes pushing this value higher or lower, until the state eventually either decays or—if the firing threshold is reached—the neuron fires. After firing, the state variable is reset to a lower value.
 Various decoding methods exist for interpreting the outgoing spike train as a real-value number, relying on either the frequency of spikes (rate-code), the time-to-first-spike after stimulation, or the interval between spikes.
 Many multi-layer artificial neural networks are fully connected, receiving input from every neuron in the previous layer and signalling every neuron in the subsequent layer. Although these networks have achieved breakthroughs in many fields, they are biologically inaccurate and do not mimic the operation mechanism of neurons in the brain of a living thing.[citation needed]
 The biologically inspired Hodgkin–Huxley model of a spiking neuron was proposed  in 1952. This model describes how action potentials are initiated and propagated. Communication between neurons, which requires the exchange of chemical neurotransmitters in the synaptic gap, is described in various models, such as the integrate-and-fire model, FitzHugh–Nagumo model (1961–1962), and Hindmarsh–Rose model (1984). The leaky integrate-and-fire model (or a derivative) is commonly used as it is easier to compute than the Hodgkin–Huxley model.[7]
 While the notion of an artificial spiking neural network became very popular only during the first quarter of the twenty-first century, [8][9][10] there are a number of studies between 1980 and 1995 that supported the concept and in which the first models of this type of artificial neural networks appeared to simulate non-algorithmic intelligent information processing systems.[11][12][13] However, the very notion of the spiking neural network as a mathematical model had already been worked on in the early 1970s.[14]
 Information in the brain is represented as action potentials (neuron spikes), which may be grouped into spike trains or even coordinated waves of brain activity. A fundamental question of neuroscience is to determine whether neurons communicate by a rate or temporal code.[15] Temporal coding suggests that a single spiking neuron can replace hundreds of hidden units on a sigmoidal neural net.[1]
 An SNN computes in the continuous rather than the discrete domain. The idea is that neurons may not test for activation in every iteration of propagation (as is the case in a typical multilayer perceptron network), but only when their membrane potentials reach a certain value. When a neuron is activated, it produces a signal that is passed to connected neurons, raising or lowering their membrane potential.
 In a spiking neural network, a neuron's current state is defined as its membrane potential (possibly modeled as a differential equation).[16] An input pulse causes the membrane potential to rise for a period of time and then gradually decline. Encoding schemes have been constructed to interpret these pulse sequences as a number, taking into account both pulse frequency and pulse interval. A neural network model based on pulse generation time can be established.[17] Using the exact time of pulse occurrence, a neural network can employ more information and offer better computing properties.[18]
 The SNN approach produces a continuous output instead of the binary output of traditional artificial neural networks (ANNs). Pulse trains are not easily interpretable, hence the need for encoding schemes as above. However, a pulse train representation may be more suited for processing spatiotemporal data (or continual real-world sensory data classification).[19] SNNs consider space by connecting neurons only to nearby neurons so that they process input blocks separately (similar to CNN using filters). They consider time by encoding information as pulse trains so as not to lose information in a binary encoding. This avoids the additional complexity of a recurrent neural network (RNN). It turns out that impulse neurons are more powerful computational units than traditional artificial neurons.[20]
 SNNs are theoretically more powerful than so called ""second-generation networks"" defined in[20] as ""[ANNs] based on computational units that apply activation function with a continuous set of possible output values to a weighted sum (or polynomial) of the inputs; however, SNN training issues and hardware requirements limit their use. Although unsupervised biologically inspired learning methods are available such as Hebbian learning and STDP, no effective supervised training method is suitable for SNNs that can provide better performance than second-generation networks.[20] Spike-based activation of SNNs is not differentiable thus making it hard to develop gradient descent based training methods to perform error backpropagation.
 SNNs have much larger computational costs for simulating realistic neural models than traditional ANNs.[21]
 Pulse-coupled neural networks (PCNN) are often confused with SNNs. A PCNN can be seen as a kind of SNN.
 Currently there are a few challenges when using SNNs that researchers are actively working on. The first challenge concerns the nondifferentiability of the spiking nonlinearity. The expressions for both the forward- and backward-learning methods contain the derivative of the neural activation function which is non-differentiable because neuron's output is either 1 when it spikes, and 0 otherwise. This all-or-nothing behavior of the binary spiking nonlinearity stops gradients from “flowing” and makes LIF neurons unsuitable for gradient-based optimization. The second challenge concerns the implementation of the optimization algorithm itself. Standard BP can be expensive in terms of computation, memory, and communication and may be poorly suited to the constraints dictated by the hardware that implements it (e.g., a computer, brain, or neuromorphic device).[22] Regarding the first challenge there are several approaches to resolving it. A few of them are:
 In the development of SNNs, incorporating additional neuron dynamics like Spike Frequency Adaptation (SFA) into neuron models marks a notable advance, enhancing both efficiency and computational power.[6][23] These neurons stand in between biological complexity and computational complexity.[24] Originating from biological insights, SFA offers significant computational benefits by reducing power usage through efficient coding,[25] especially in cases of repetitive or intense stimuli. This adaptation improves signal clarity against background noise and introduces an elementary short-term memory at the neuron level, which in turn, refines the accuracy and efficiency of information processing.[26] Recently, this phenomenon was mostly achieved using compartmental neuron models. The simpler versions are of neuron models with adaptive thresholds, an indirect way of achieving SFA. It equips SNNs with improved learning capabilities, even with constrained synaptic plasticity, and elevates computational efficiency.[27][28] This feature lessens the demand on network layers by decreasing the need for spike processing, thus cutting down on computational load and memory access time—essential aspects of neural computation. Moreover, SNNs utilizing neurons capable of SFA achieve levels of accuracy that rival those of conventional artificial neural networks, including those based on long short-term memory models,[29][30] while also requiring fewer neurons for comparable computational tasks. This efficiency not only streamlines the computational workflow but also conserves space and energy, offering a pragmatic step forward in the practical application of SNNs for complex computing tasks while maintaining a commitment to technical integrity.High-performance deep spiking neural networks with 0.3 spikes per neuron
 SNNs can in principle be applied to the same applications as traditional ANNs.[31] In addition, SNNs can model the central nervous system of biological organisms, such as an insect seeking food without prior knowledge of the environment.[32] Due to their relative realism, they can be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function, recordings of this circuit can be compared to the output of the corresponding SNN, evaluating the plausibility of the hypothesis. However, there is a lack of effective training mechanisms for SNNs, which can be inhibitory for some applications, including computer vision tasks.
 As of 2019 SNNs lag behind ANNs in terms of accuracy, but the gap is decreasing, and has vanished on some tasks.[33]
 When using SNNs for image based data, the images need to be converted into binary spike trains.[34] Types of encodings include:[35]
 A diverse range of application software can simulate SNNs. This software can be classified according to its uses:
  These simulate complex neural models with a high level of detail and accuracy. Large networks usually require lengthy processing. Candidates include:[36]
 Future neuromorphic architectures[39] will comprise billions of such nanosynapses, which require a clear understanding of the physical mechanisms responsible for plasticity. Experimental systems based on ferroelectric tunnel junctions have been used to show that STDP can be harnessed from heterogeneous polarization switching. Through combined scanning probe imaging, electrical transport and atomic-scale molecular dynamics, conductance variations can be modelled by nucleation-dominated reversal of domains. Simulations show that arrays of ferroelectric nanosynapses can autonomously learn to recognize patterns in a predictable way, opening the path towards unsupervised learning.[40]
 Classification capabilities of spiking networks trained according to unsupervised learning methods[45] have been tested on the common benchmark datasets, such as, Iris, Wisconsin Breast Cancer or Statlog Landsat dataset.[46][47] Various approaches to information encoding and network design have been used. For example, a 2-layer feedforward network for data clustering and classification. Based on the idea proposed in Hopfield (1995) the authors implemented models of local receptive fields combining the properties of radial basis functions (RBF) and spiking neurons to convert input signals (classified data) having a floating-point representation into a spiking representation.[48][49]
",spike neural network snn artifici neural network ann close mimic natur neural network model leverag time discret spike main inform carrier addit neuron synapt state snn incorpor concept time oper model idea neuron snn transmit inform propag cycl happen typic perceptron network rather transmit inform membran intrins qualiti neuron relat membran electr specif valu call threshold membran potenti reach threshold neuron fire gener signal travel neuron turn increas decreas potenti respons signal neuron model fire moment threshold cross also call spike neuron model although previous believ brain encod inform spike rate consid analogu variabl output tradit ann research field neurobiolog indic high speed process sole perform rate base scheme exampl human perform imag recognit task rate requir process time per neuron success layer go retina tempor lobe time window short rate base encod precis spike time small set spike neuron also higher inform code capac compar rate base approach promin spike neuron model leaki model model momentari activ level model differenti equat normal consid neuron state incom spike push valu higher lower state eventu either decay fire threshold neuron fire fire state variabl reset lower valu variou decod method exist interpret outgo spike train number reli either frequenc spike stimul interv spike mani artifici neural network fulli connect receiv input everi neuron previou layer signal everi neuron subsequ layer although network achiev breakthrough mani field biolog inaccur mimic oper mechan neuron brain live thing citat need biolog inspir model spike neuron propos model describ action potenti initi propag commun neuron requir exchang chemic neurotransmitt synapt gap describ variou model model model model leaki model deriv commonli use easier comput model notion artifici spike neural network becam popular first quarter centuri number studi support concept first model type artifici neural network appear simul intellig inform process system howev notion spike neural network mathemat model alreadi work earli inform brain repres action potenti neuron spike may group spike train even coordin wave brain activ fundament question neurosci determin whether neuron commun rate tempor code tempor code suggest singl spike neuron replac hundr hidden unit sigmoid neural net snn comput continu rather discret domain idea neuron may test activ everi iter propag case typic multilay perceptron network membran potenti reach certain valu neuron activ produc signal pass connect neuron rais lower membran potenti spike neural network neuron current state defin membran potenti possibl model differenti equat input puls caus membran potenti rise period time gradual declin encod scheme construct interpret puls sequenc number take account puls frequenc puls interv neural network model base puls gener time establish use exact time puls occurr neural network employ inform offer better comput properti snn approach produc continu output instead binari output tradit artifici neural network ann puls train easili interpret henc need encod scheme howev puls train represent may suit process spatiotempor data continu sensori data classif snn consid space connect neuron nearbi neuron process input block separ similar cnn use filter consid time encod inform puls train lose inform binari encod avoid addit complex recurr neural network rnn turn impuls neuron power comput unit tradit artifici neuron snn theoret power call network defin ann base comput unit appli activ function continu set possibl output valu weight sum polynomi input howev snn train issu hardwar requir limit use although unsupervis biolog inspir learn method avail hebbian learn stdp effect supervis train method suitabl snn provid better perform network activ snn differenti thu make hard develop gradient descent base train method perform error backpropag snn much larger comput cost simul realist neural model tradit ann neural network pcnn often confus snn pcnn seen kind snn current challeng use snn research activ work first challeng concern nondifferenti spike nonlinear express method contain deriv neural activ function neuron output either spike otherwis behavior binari spike nonlinear stop gradient flow make lif neuron unsuit optim second challeng concern implement optim algorithm standard bp expens term comput memori commun may poorli suit constraint dictat hardwar implement comput brain neuromorph devic regard first challeng sever approach resolv develop snn incorpor addit neuron dynam like spike frequenc adapt sfa neuron model mark notabl advanc enhanc effici comput power neuron stand biolog complex comput complex origin biolog insight sfa offer signific comput benefit reduc power usag effici code especi case repetit intens stimuli adapt improv signal clariti background nois introduc elementari memori neuron level turn refin accuraci effici inform process recent phenomenon mostli achiev use compartment neuron model simpler version neuron model adapt threshold indirect way achiev sfa equip snn improv learn capabl even constrain synapt plastic elev comput effici featur lessen demand network layer decreas need spike process thu cut comput load memori access aspect neural comput moreov snn util neuron capabl sfa achiev level accuraci rival convent artifici neural network includ base long memori model also requir fewer neuron compar comput task effici streamlin comput workflow also conserv space energi offer pragmat step forward practic applic snn complex comput task maintain commit technic deep spike neural network spike per neuron snn principl appli applic tradit ann addit snn model central nervou system biolog organ insect seek food without prior knowledg environ due rel realism use studi oper biolog neural circuit start hypothesi topolog biolog neuron circuit function record circuit compar output correspond snn evalu plausibl hypothesi howev lack effect train mechan snn inhibitori applic includ comput vision task snn lag behind ann term accuraci gap decreas vanish task use snn imag base data imag need convert binari spike train type encod includ divers rang applic softwar simul snn softwar classifi accord use simul complex neural model high level detail accuraci larg network usual requir lengthi process candid includ futur neuromorph architectur compris billion nanosynaps requir clear understand physic mechan respons plastic experiment system base ferroelectr tunnel junction use show stdp har heterogen polar switch combin scan probe imag electr transport molecular dynam conduct variat model revers domain simul show array ferroelectr nanosynaps autonom learn recogn pattern predict way open path toward unsupervis learn classif capabl spike network train accord unsupervis learn method test common benchmark dataset iri wisconsin breast cancer statlog landsat dataset variou approach inform encod network design use exampl feedforward network data cluster classif base idea propos hopfield author implement model local recept field combin properti radial basi function rbf spike neuron convert input signal classifi data represent spike represent
Memtransistor,https://en.wikipedia.org/wiki/Memtransistor,"The memtransistor (a blend word from Memory Transfer Resistor) is an experimental multi-terminal passive electronic component that might be used in the construction of artificial neural networks.[1] It is a combination of the memristor and transistor technology.[2] This technology is different from the 1T-1R approach since the devices are merged into one single entity. Multiple memristors can be embedded with a single transistor, enabling it to more accurately model a neuron with its multiple synaptic connections. A neural network produced from these would provide hardware-based artificial intelligence with a good foundation.[1][3]
 These types of devices would allow for a synapse model that could realise a learning rule, by which the synaptic efficacy is altered by voltages applied to the terminals of the device.  An example of such a learning rule is spike-timing-dependant-plasticty by which the weight of the synapse, in this case the conductivity, could be modulated based on the timing of pre and post synaptic spikes arriving at each terminal. The advantage of this approach over two terminal memristive devices is that read and write protocols have the possibility to occur simultaneously and distinctly.
 Researchers at Northwestern University have fabricated a seven-terminal device fabricated on molybdenum disulfide (MoS2). One terminal controls the current between the other six.[4] It has been shown that the I_D / V_D characteristics of the transistor can be modified even after fabrication. Subsequently, designs which would originally require multiple (selectable) transistors can be implemented with a single configurable transistor.
",memtransistor blend word memori transfer resistor experiment passiv electron compon might use construct artifici neural network combin memristor transistor technolog technolog differ approach sinc devic merg one singl entiti multipl memristor embed singl transistor enabl accur model neuron multipl synapt connect neural network produc would provid artifici intellig good foundat type devic would allow synaps model could realis learn rule synapt efficaci alter voltag appli termin devic exampl learn rule weight synaps case conduct could modul base time pre post synapt spike arriv termin advantag approach two termin memrist devic read write protocol possibl occur simultan distinctli research northwestern univers fabric devic fabric molybdenum disulfid one termin control current six shown characterist transistor modifi even fabric subsequ design would origin requir multipl select transistor implement singl configur transistor
Electrochemical RAM,https://en.wikipedia.org/wiki/Electrochemical_RAM,"Electrochemical Random-Access Memory (ECRAM) is a type of non-volatile memory (NVM) with multiple levels per cell (MLC) designed for deep learning analog acceleration.[1][2][3] An ECRAM cell is a three-terminal device composed of a conductive channel, an insulating electrolyte, an ionic reservoir, and metal contacts. The resistance of the channel is modulated by ionic exchange at the interface between the channel and the electrolyte upon application of an electric field. The charge-transfer process allows both for state retention in the absence of applied power, and for programming of multiple distinct levels, both differentiating ECRAM operation from that of a field-effect transistor (FET). The write operation is deterministic and can result in symmetrical potentiation and depression, making ECRAM arrays attractive for acting as artificial synaptic weights in physical implementations of artificial neural networks (ANN). The technological challenges include open circuit potential (OCP) and semiconductor foundry compatibility associated with energy materials. Universities, government laboratories, and corporate research teams have contributed to the development of ECRAM for analog computing. Notably, Sandia National Laboratories designed a lithium-based cell inspired by solid-state battery materials,[4] Stanford University built an organic proton-based cell,[5] and International Business Machines (IBM) demonstrated in-memory selector-free parallel programming for a logistic regression task in an array of metal-oxide ECRAM designed for insertion in the back end of line (BEOL).[6] In 2022, researchers at Massachusetts Institute of Technology built an inorganic, CMOS-compatible protonic technology that achieved near-ideal modulation characteristics using nanosecond fast pulses [7]
 Stress to the gate, relative to channel electrodes, can be applied in the form of fixed current or bias, driving ions toward - or away from - the electrolyte/channel interface where charge transfer occurs with free carriers. Upon insertion in the channel, the ionic charge is neutralized and the atomic species intercalate or bind to the conductive host matrix, in some cases yielding strain and localized phase transformation. Such reversible processes are equivalent to anodic/cathodic reactions in battery cells or electrochromic devices. Although in ECRAM, the programming of the memory element is defined not as a change in capacity or opacity, but by a change of channel conductivity associated with atomic species being inserted or removed as a result of the stress signal.
 The read operation is decoupled from the write operation thanks to the presence of three electrodes, therefore limiting read disturb. A small bias is applied between the channel electrodes, with the resulting read current being proportional to the channel conductivity, hence sensing the programmed state of the device.
 The programming speed of ECRAM cells is not limited by the bulk diffusion of ions. They indeed only need to cross the interface plane between the electrolyte and the channel to induce a change in conductivity. Nanosecond write pulses can indeed trigger programming.[8] Trade-offs between gate capacitance, electronic conductivity, etc., can yield settling transients, limiting the maximum read-write frequency.[9]
 ECRAM arrays are integrated in a pseudo-crossbar layout, the gate access line being common to all devices in a row or column. If a change in electrochemical potential, the driving force of a battery, occurs upon ionic exchange between channel and gate electrode, an open circuit potential (OCP) exists at the gate contact and will differ device to device depending on the programmed state. To prevent cross-talk between cells sharing a gate line, an access device to isolate each one is added in series with the memory element.[10] Suppressing OCP in the ECRAM design, minimizes the cell size/complexity, allowing for selector-free parallel read/programming of device arrays.[6]
 Non-volatile memory (NVM) can be leveraged for in-memory compute, thereby reducing the frequency of data transfer between storage and processing units. This can ultimately improve compute time and energy efficiency over hierarchical system architectures by eliminating the Von Neumann bottleneck. Hence, when using multi-level cells (MLC) at the nodes of cross-bar arrays, one can perform analog operations on time or voltage encoded data such as vector (row input signal) × matrix (memory array) multiply. Following Kirchhoff's and Ohm's laws, the resulting vector is then obtained by integrating the current collected at each column. For ECRAM cells, an additional line is added at each row to write the cells during programming cycles, thereby yielding a pseudo-crossbar architecture. In the field of artificial intelligence (AI), deep neural networks (DNN) are used for classification and learning tasks, relying on a large number of matrix-multiply operations. Therefore, analog compute with NVM technology for such tasks are extremely attractive. ECRAM cells are uniquely positioned for use in analog deep learning accelerators due to their inherent deterministic and symmetric programming nature when compared to other devices such as resistive RAM (ReRAM or RRAM) and phase-change memory (PCM).
 Physical implementation of artificial neural networks (ANN) must perform at iso-accuracy when benchmarked against floating point precision weights in software. This sets the boundary for device properties needed for analog deep learning accelerators. In the design of their resisistive processing unit (RPU), IBM Research has published such requirements,[11] a subset of which is listed here. Algorithm and hardware co-design can relax them somewhat but not without other trade-offs.[12]
 NVM use as synaptic weights in lieu of storage implies significantly different requirements when it comes to target resistance range, number of levels, and  programming speed and symmetry. Because the in-memory computation occurs in parallel through the array, many devices are addressed concurrently and therefore need to have a high average resistance to limit energy dissipation. To perform high-accuracy computation and be resilient to noise, the NVM cell needs a large number of distinct states. The programming time needs only to be fast between levels, not from the highest to the lowest resistance states. During each programming cycle (back-propagation), weight updates can be negative or positive, and the up/down traces therefore need symmetry to allow learning algorithms to converge. All NVM technologies do struggle with these targets. ECRAM individual cells can meet such stringent metrics,[6] but also need to demonstrate high-density array yield and stochasticity.
 As reported in a 2019 publication in Science, by Elliot J. Fuller, Alec A. Talin, et al. from Sandia National Laboratories, in collaboration with Stanford University, and the University of Massachusetts Amherst:[10]
 Using co-planar organic multilevel cells, isolated by conductive bridge memory (CBM) devices, the team demonstrates parallel programming and addressing in up to 3×3 arrays. In particular a 2-layer neural network is mapped to the array by transferring the weights necessary to perform an inference task resulting in a XOR operation on the binary input vector.
 Individual cells are shown to have the following properties (not all achieved in the same device configuration); speed = 1 MHz read-write cycles, number of states > 50 (tunable), resistance range = 50-100 nS (tunable), endurance > 108 write ops, size = 50×50 μm2.
 As reported in a 2019 proceeding of the IEEE International Electron Device Meeting (IEDM), by Seyoung Kim, John Rozen, et al. from IBM Research:[6]
 Using metal-oxide ECRAM cells, selector-free, the team demonstrates parallel programming and addressing in 2×2 arrays. In particular, a logistic regression task is performed in-memory with 1,000 2×1 vectors as training set. 2D curve fit is achieved in a dozen epochs.
 Individual cells are shown to have the following properties (not all achieved in the same device configuration); speed = 10 ns write pulses, number of states > 1,000 (tunable), resistance range = 0-50 μS (tunable), endurance > 107 write ops, size < 1×1 μm2.
 Various institutions have demonstrated ECRAM cells with vastly different materials, layouts, and performances. 
An example set for discrete cells is listed in the table.
 Based on lithium ions, Li-ECRAM devices have demonstrated repeatable and controlled switching by applying known materials from battery technology to the memory design.[4][13][14] Consequently, such cells can exhibit an OCP which varies over several volts, depending on the programmed state.
 Based on hydrogen ions, H-ECRAM devices have proven fast, necessitating small driving forces to induce programming.[5][15][16] High diffusion coefficients in various materials can be accompanied by lack of retention within the memory cell, impacting endurance. Most H-ECRAM designs use liquid and/or organic electrolytes. In a 2022 study, researchers at Massachusetts Institute of Technology demonstrated a CMOS-compatible technology based on phosphosilicate glass electrolyte that achieved ultrafast modulation characteristics with high energy efficiency.[7] The same year researchers at the Royal Institute of Technology KTH showed ECRAMS based on hydrogen intercalation into the 2D material MXene, marking the first demonstration of high speed 2D ECRAMs. [18]
 Metal-oxide based ECRAM, are inspired from OxRam materials and high-k/metal gate technology used in commercial semiconductor offerings. MO-ECRAM do enable negligible OCP and sub-μs write operations.[6]
 For advanced semiconductor memory or compute applications, a technology needs to be compatible with very large scale integration (VLSI). This puts constraints on materials used, and the techniques employed to fabricate functional devices. The implications for ECRAM are described here.
 A semiconductor foundry can handle several technologies and has strict rules when it comes to materials being introduced in its expensive toolset to avoid cross-contamination and loss of device yield. In particular, metallic mobile ions, if present in active areas, can induce device drift and affect reliability. There are several other considerations for the foundries; including safety, cost, volume, etc. Hence, lithium ion-based Li-ECRAM faces unique challenges beyond the presence of OCP.
 Memory arrays require logic periphery to operate and interface with the rest of the compute system. Such periphery is based on field-effect transistors (FETs) built on the surface of silicon wafer substrates with a high thermal budget at the front end of line (FEOL). Memory cells can be inserted between upper metal levels at back end of line (BEOL) but will still need to remain unaffected by temperatures up to ~400 °C used in subsequent steps. Together with high density patterning challenges, these restrictions make organic devices unsuitable for such integration. The ECRAMs based on 2D MXene materials [18] have shown the potential to be unaffected by 400 °C heating, but additional development is needed for the integration of ion conductors.
 One way to introduce novel memory materials can be to use heterogeneous integration (HI) where the device array is fabricated independently from the logic controls and then bonded to the FET-containing chip to enable its use as high bandwidth memory (HBM). However, the cost and complexity associated with such scheme negatively affects the value proposition for displacing existing memory technologies.
",electrochem memori ecram type memori nvm multipl level per cell mlc design deep learn analog acceler ecram cell devic compos conduct channel insul electrolyt ionic reservoir metal contact resist channel modul ionic exchang interfac channel electrolyt upon applic electr field process allow state retent absenc appli power program multipl distinct level differenti ecram oper transistor fet write oper determinist result symmetr potenti depress make ecram array attract act artifici synapt weight physic implement artifici neural network ann technolog challeng includ open circuit potenti ocp semiconductor foundri compat associ energi materi univers govern laboratori corpor research team contribut develop ecram analog comput notabl sandia nation laboratori design cell inspir batteri materi stanford univers built organ cell intern busi machin ibm demonstr parallel program logist regress task array ecram design insert back end line beol research massachusett institut technolog built inorgan proton technolog achiev modul characterist use nanosecond fast puls stress gate rel channel electrod appli form fix current bia drive ion toward away interfac charg transfer occur free carrier upon insert channel ionic charg neutral atom speci intercal bind conduct host matrix case yield strain local phase transform revers process equival reaction batteri cell electrochrom devic although ecram program memori element defin chang capac opac chang channel conduct associ atom speci insert remov result stress signal read oper decoupl write oper thank presenc three electrod therefor limit read disturb small bia appli channel electrod result read current proport channel conduct henc sens program state devic program speed ecram cell limit bulk diffus ion inde need cross interfac plane electrolyt channel induc chang conduct nanosecond write puls inde trigger program gate capacit electron conduct yield settl transient limit maximum frequenc ecram array integr layout gate access line common devic row column chang electrochem potenti drive forc batteri occur upon ionic exchang channel gate electrod open circuit potenti ocp exist gate contact differ devic devic depend program state prevent cell share gate line access devic isol one ad seri memori element suppress ocp ecram design minim cell allow parallel devic array memori nvm leverag comput therebi reduc frequenc data transfer storag process unit ultim improv comput time energi effici hierarch system architectur elimin von neumann bottleneck henc use cell mlc node array one perform analog oper time voltag encod data vector row input signal matrix memori array multipli follow kirchhoff ohm law result vector obtain integr current collect column ecram cell addit line ad row write cell program cycl therebi yield architectur field artifici intellig ai deep neural network dnn use classif learn task reli larg number oper therefor analog comput nvm technolog task extrem attract ecram cell uniqu posit use analog deep learn acceler due inher determinist symmetr program natur compar devic resist ram reram rram memori pcm physic implement artifici neural network ann must perform benchmark float point precis weight softwar set boundari devic properti need analog deep learn acceler design resisist process unit rpu ibm research publish requir subset list algorithm hardwar relax somewhat without nvm use synapt weight lieu storag impli significantli differ requir come target resist rang number level program speed symmetri comput occur parallel array mani devic address concurr therefor need high averag resist limit energi dissip perform comput resili nois nvm cell need larg number distinct state program time need fast level highest lowest resist state program cycl weight updat neg posit trace therefor need symmetri allow learn algorithm converg nvm technolog struggl target ecram individu cell meet stringent metric also need demonstr array yield stochast report public scienc elliot fuller alec talin et al sandia nation laboratori collabor stanford univers univers massachusett amherst use organ multilevel cell isol conduct bridg memori cbm devic team demonstr parallel program address array particular neural network map array transfer weight necessari perform infer task result xor oper binari input vector individu cell shown follow properti achiev devic configur speed mhz cycl number state tunabl resist rang ns tunabl endur write op size report proceed ieee intern electron devic meet iedm seyoung kim john rozen et al ibm research use ecram cell team demonstr parallel program address array particular logist regress task perform vector train set curv fit achiev dozen epoch individu cell shown follow properti achiev devic configur speed ns write puls number state tunabl resist rang μs tunabl endur write op size variou institut demonstr ecram cell vastli differ materi layout perform exampl set discret cell list tabl base lithium ion devic demonstr repeat control switch appli known materi batteri technolog memori design consequ cell exhibit ocp vari sever volt depend program state base hydrogen ion devic proven fast necessit small drive forc induc program high diffus coeffici variou materi accompani lack retent within memori cell impact endur design use liquid organ electrolyt studi research massachusett institut technolog demonstr technolog base phosphosil glass electrolyt achiev ultrafast modul characterist high energi effici year research royal institut technolog kth show ecram base hydrogen intercal materi mxene mark first demonstr high speed ecram base ecram inspir oxram materi gate technolog use commerci semiconductor offer enabl neglig ocp write oper advanc semiconductor memori comput applic technolog need compat larg scale integr vlsi put constraint materi use techniqu employ fabric function devic implic ecram describ semiconductor foundri handl sever technolog strict rule come materi introduc expens toolset avoid loss devic yield particular metal mobil ion present activ area induc devic drift affect reliabl sever consider foundri includ safeti cost volum etc henc lithium face uniqu challeng beyond presenc ocp memori array requir logic peripheri oper interfac rest comput system peripheri base transistor fet built surfac silicon wafer substrat high thermal budget front end line feol memori cell insert upper metal level back end line beol still need remain unaffect temperatur use subsequ step togeth high densiti pattern challeng restrict make organ devic unsuit integr ecram base mxene materi shown potenti unaffect heat addit develop need integr ion conductor one way introduc novel memori materi use heterogen integr hi devic array fabric independ logic control bond chip enabl use high bandwidth memori hbm howev cost complex associ scheme neg affect valu proposit displac exist memori technolog
Q-learning,https://en.wikipedia.org/wiki/Q-learning,"Q-learning is a model-free reinforcement learning algorithm that teaches an agent to assign values to each action it might take, conditioned on the agent being in a particular state. It does not require a model of the environment (hence ""model-free""), and it can handle problems with stochastic transitions and rewards without requiring adaptations.[1]
 For any finite Markov decision process, Q-learning finds an optimal policy in the sense of maximizing the expected value of the total reward over any and all successive steps, starting from the current state.[2] Q-learning can identify an optimal action-selection policy for any given finite Markov decision process, given infinite exploration time and a partly random policy.[2] ""Q"" refers to the function that the algorithm computes: the expected reward—that is, the quality—of an action taken in a given state.[3]
 Reinforcement learning involves an agent, a set of states 





S




{\displaystyle {\mathcal {S}}}

, and a set 





A




{\displaystyle {\mathcal {A}}}

 of actions per state. By performing an action 



a
∈


A




{\displaystyle a\in {\mathcal {A}}}

, the agent transitions from state to state. Executing an action in a specific state provides the agent with a reward (a numerical score).
 The goal of the agent is to maximize its total reward. It does this by adding the maximum reward attainable from future states to the reward for achieving its current state, effectively influencing the current action by the potential future reward. This potential reward is a weighted sum of expected values of the rewards of all future steps starting from the current state.[1]
 As an example, consider the process of boarding a train, in which the reward is measured by the negative of the total time spent boarding (alternatively, the cost of boarding the train is equal to the boarding time). One strategy is to enter the train door as soon as they open, minimizing the initial wait time for yourself. If the train is crowded, however, then you will have a slow entry after the initial action of entering the door as people are fighting you to depart the train as you attempt to board. The total boarding time, or cost, is then:
 On the next day, by random chance (exploration), you decide to wait and let other people depart first. This initially results in a longer wait time. However, less time is spent fighting the departing passengers. Overall, this path has a higher reward than that of the previous day, since the total boarding time is now:
 Through exploration, despite the initial (patient) action resulting in a larger cost (or negative reward) than in the forceful strategy, the overall cost is lower, thus revealing a more rewarding strategy.
 After 



Δ
t


{\displaystyle \Delta t}

 steps into the future the agent will decide some next step. The weight for this step is calculated as 




γ

Δ
t




{\displaystyle \gamma ^{\Delta t}}

, where 



γ


{\displaystyle \gamma }

 (the discount factor) is a number between 0 and 1 (



0
≤
γ
≤
1


{\displaystyle 0\leq \gamma \leq 1}

). Assuming 



γ
<
1


{\displaystyle \gamma <1}

, it has the effect of valuing rewards received earlier higher than those received later (reflecting the value of a ""good start""). 



γ


{\displaystyle \gamma }

 may also be interpreted as the probability to succeed (or survive) at every step 



Δ
t


{\displaystyle \Delta t}

.
 The algorithm, therefore, has a function that calculates the quality of a state–action combination:
 Before learning begins, ⁠



Q


{\displaystyle Q}

⁠ is initialized to a possibly arbitrary fixed value (chosen by the programmer). Then, at each time 



t


{\displaystyle t}

 the agent selects an action 




A

t




{\displaystyle A_{t}}

, observes a reward 




R

t
+
1




{\displaystyle R_{t+1}}

, enters a new state 




S

t
+
1




{\displaystyle S_{t+1}}

 (that may depend on both the previous state 




S

t




{\displaystyle S_{t}}

 and the selected action), and 



Q


{\displaystyle Q}

 is updated. The core of the algorithm is a Bellman equation as a simple value iteration update, using the weighted average of the current value and the new information:[4]
 where 




R

t
+
1




{\displaystyle R_{t+1}}

 is the reward received when moving from the state 




S

t




{\displaystyle S_{t}}

 to the state 




S

t
+
1




{\displaystyle S_{t+1}}

, and 



α


{\displaystyle \alpha }

 is the learning rate 



(
0
<
α
≤
1
)


{\displaystyle (0<\alpha \leq 1)}

.
 Note that 




Q

n
e
w


(

S

t


,

A

t


)


{\displaystyle Q^{new}(S_{t},A_{t})}

 is the sum of three factors:
 An episode of the algorithm ends when state 




S

t
+
1




{\displaystyle S_{t+1}}

 is a final or terminal state. However, Q-learning can also learn in non-episodic tasks (as a result of the property of convergent infinite series). If the discount factor is lower than 1, the action values are finite even if the problem can contain infinite loops.
 For all final states 




s

f




{\displaystyle s_{f}}

, 



Q
(

s

f


,
a
)


{\displaystyle Q(s_{f},a)}

 is never updated, but is set to the reward value 



r


{\displaystyle r}

 observed for state 




s

f




{\displaystyle s_{f}}

. In most cases, 



Q
(

s

f


,
a
)


{\displaystyle Q(s_{f},a)}

 can be taken to equal zero.
 The learning rate or step size determines to what extent newly acquired information overrides old information. A factor of 0 makes the agent learn nothing (exclusively exploiting prior knowledge), while a factor of 1 makes the agent consider only the most recent information (ignoring prior knowledge to explore possibilities). In fully deterministic environments, a learning rate of 




α

t


=
1


{\displaystyle \alpha _{t}=1}

 is optimal. When the problem is stochastic, the algorithm converges under some technical conditions on the learning rate that require it to decrease to zero. In practice, often a constant learning rate is used, such as 




α

t


=
0.1


{\displaystyle \alpha _{t}=0.1}

 for all 



t


{\displaystyle t}

.[5]
 The discount factor ⁠



γ


{\displaystyle \gamma }

⁠ determines the importance of future rewards. A factor of 0 will make the agent ""myopic"" (or short-sighted) by only considering current rewards, i.e. 




r

t




{\displaystyle r_{t}}

 (in the update rule above), while a factor approaching 1 will make it strive for a long-term high reward. If the discount factor meets or exceeds 1, the action values may diverge. For ⁠



γ
=
1


{\displaystyle \gamma =1}

⁠, without a terminal state, or if the agent never reaches one, all environment histories become infinitely long, and utilities with additive, undiscounted rewards generally become infinite.[6] Even with a discount factor only slightly lower than 1, Q-function learning leads to propagation of errors and instabilities when the value function is approximated with an artificial neural network.[7] In that case, starting with a lower discount factor and increasing it towards its final value accelerates learning.[8]
 Since Q-learning is an iterative algorithm, it implicitly assumes an initial condition before the first update occurs. High initial values, also known as ""optimistic initial conditions"",[9] can encourage exploration: no matter what action is selected, the update rule will cause it to have lower values than the other alternative, thus increasing their choice probability. The first reward 



r


{\displaystyle r}

 can be used to reset the initial conditions.[10] According to this idea, the first time an action is taken the reward is used to set the value of 



Q


{\displaystyle Q}

. This allows immediate learning in case of fixed deterministic rewards. A model that incorporates reset of initial conditions (RIC) is expected to predict participants' behavior better than a model that assumes any arbitrary initial condition (AIC).[10] RIC seems to be consistent with human behaviour in repeated binary choice experiments.[10]
 Q-learning at its simplest stores data in tables. This approach falters with increasing numbers of states/actions since the likelihood of the agent visiting a particular state and performing a particular action is increasingly small.
 Q-learning can be combined with function approximation.[11] This makes it possible to apply the algorithm to larger problems, even when the state space is continuous.
 One solution is to use an (adapted) artificial neural network as a function approximator.[12] Another possibility is to integrate Fuzzy Rule Interpolation (FRI) and use sparse fuzzy rule-bases[13] instead of discrete Q-tables or ANNs, which has the advantage of being a human-readable knowledge representation form. Function approximation may speed up learning in finite problems, due to the fact that the algorithm can generalize earlier experiences to previously unseen states.
 Another technique to decrease the state/action space quantizes possible values. Consider the example of learning to balance a stick on a finger. To describe a state at a certain point in time involves the position of the finger in space, its velocity, the angle of the stick and the angular velocity of the stick. This yields a four-element vector that describes one state, i.e. a snapshot of one state encoded into four values. The problem is that infinitely many possible states are present. To shrink the possible space of valid actions multiple values can be assigned to a bucket. The exact distance of the finger from its starting position (-Infinity to Infinity) is not known, but rather whether it is far away or not (Near, Far).[14]
 Q-learning was introduced by Chris Watkins in 1989.[15] A convergence proof was presented by Watkins and Peter Dayan in 1992.[16]
 Watkins was addressing “Learning from delayed rewards”, the title of his PhD thesis. Eight years earlier in 1981 the same problem, under the name of “Delayed reinforcement learning”, was solved by Bozinovski's Crossbar Adaptive Array (CAA).[17][18] The memory matrix 



W
=
‖
w
(
a
,
s
)
‖


{\displaystyle W=\|w(a,s)\|}

 was the same as the eight years later Q-table of Q-learning. The architecture introduced the term “state evaluation” in reinforcement learning. The crossbar learning algorithm, written in mathematical pseudocode in the paper, in each iteration performs the following computation:
 The term “secondary reinforcement” is borrowed from animal learning theory, to model state values via backpropagation: the state value ⁠



v
(

s
′

)


{\displaystyle v(s')}

⁠ of the consequence situation is backpropagated to the previously encountered situations. CAA computes state values vertically and actions horizontally (the ""crossbar""). Demonstration graphs showing delayed reinforcement learning contained states (desirable, undesirable, and neutral states), which were computed by the state evaluation function. This learning system was a forerunner of the Q-learning algorithm.[19]
 In 2014, Google DeepMind patented[20] an application of Q-learning to deep learning, titled ""deep reinforcement learning"" or ""deep Q-learning"" that can play Atari 2600 games at expert human levels.
 The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy of the agent and the data distribution, and the correlations between Q and the target values. The method can be used for stochastic search in various domains and applications.[1][21]
 The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed.[3] This removes correlations in the observation sequence and smooths changes in the data distribution. Iterative updates adjust Q towards target values that are only periodically updated, further reducing correlations with the target.[22]
 Because the future maximum approximated action value in Q-learning is evaluated using the same Q function as in current action selection policy, in noisy environments Q-learning can sometimes overestimate the action values, slowing the learning. A variant called Double Q-learning was proposed to correct this. Double Q-learning[23] is an off-policy reinforcement learning algorithm, where a different policy is used for value evaluation than what is used to select the next action.
 In practice, two separate value functions  




Q

A




{\displaystyle Q^{A}}

 and 




Q

B




{\displaystyle Q^{B}}

 are trained in a mutually symmetric fashion using separate experiences. The double Q-learning update step is then as follows:
 Now the estimated value of the discounted future is evaluated using a different policy, which solves the overestimation issue.
 This algorithm was later modified in 2015 and combined with deep learning,[24] as in the DQN algorithm, resulting in Double DQN, which outperforms the original DQN algorithm.[25]
 Delayed Q-learning is an alternative implementation of the online Q-learning algorithm, with probably approximately correct (PAC) learning.[26]
 Greedy GQ is a variant of Q-learning to use in combination with (linear) function approximation.[27] The advantage of Greedy GQ is that convergence is guaranteed even when function approximation is used to estimate the action values.
 Distributional Q-learning is a variant of Q-learning which seeks to model the distribution of returns rather than the expected return of each action. It has been observed to facilitate estimate by deep neural networks and can enable alternative control methods, such as risk-sensitive control.[28]
 Q-learning has been proposed in the multi-agent setting (see Section 4.1.2 in [29]). One approach consists in pretending the environment is passive.[30] Littman proposes the minimax Q learning algorithm.[31]
 The standard Q-learning algorithm (using a 



Q


{\displaystyle Q}

 table) applies only to discrete action and state spaces. Discretization of these values leads to inefficient learning, largely due to the curse of dimensionality. However, there are adaptations of Q-learning that attempt to solve this problem such as Wire-fitted Neural Network Q-Learning.[32]
",reinforc learn algorithm teach agent assign valu action might take condit agent particular state requir model environ henc handl problem stochast transit reward without requir adapt finit markov decis process find optim polici sens maxim expect valu total reward success step start current state identifi optim polici given finit markov decis process given infinit explor time partli random polici q refer function algorithm comput expect action taken given state reinforc learn involv agent set state set action per state perform action agent transit state state execut action specif state provid agent reward numer score goal agent maxim total reward ad maximum reward attain futur state reward achiev current state effect influenc current action potenti futur reward potenti reward weight sum expect valu reward futur step start current state exampl consid process board train reward measur neg total time spent board altern cost board train equal board time one strategi enter train door soon open minim initi wait time train crowd howev slow entri initi action enter door peopl fight depart train attempt board total board time cost next day random chanc explor decid wait let peopl depart first initi result longer wait time howev less time spent fight depart passeng overal path higher reward previou day sinc total board time explor despit initi patient action result larger cost neg reward forc strategi overal cost lower thu reveal reward strategi δ step futur agent decid next step weight step calcul γ δ γ discount factor number γ assum γ effect valu reward receiv earlier higher receiv later reflect valu good start γ may also interpret probabl succeed surviv everi step δ algorithm therefor function calcul qualiti combin learn begin q q initi possibl arbitrari fix valu chosen programm time agent select action observ reward r enter new state may depend previou state select action q q updat core algorithm bellman equat simpl valu iter updat use weight averag current valu new inform r reward receiv move state state α learn rate α note q n e w new sum three factor episod algorithm end state final termin state howev also learn task result properti converg infinit seri discount factor lower action valu finit even problem contain infinit loop final state f f q f q f never updat set reward valu r r observ state f f case q f q f taken equal zero learn rate step size determin extent newli acquir inform overrid old inform factor make agent learn noth exclus exploit prior knowledg factor make agent consid recent inform ignor prior knowledg explor possibl fulli determinist environ learn rate α optim problem stochast algorithm converg technic condit learn rate requir decreas zero practic often constant learn rate use α discount factor γ determin import futur reward factor make agent myopic consid current reward r updat rule factor approach make strive high reward discount factor meet exce action valu may diverg γ without termin state agent never reach one environ histori becom infinit long util addit undiscount reward gener becom infinit even discount factor slightli lower learn lead propag error instabl valu function approxim artifici neural network case start lower discount factor increas toward final valu acceler learn sinc iter algorithm implicitli assum initi condit first updat occur high initi valu also known optimist initi condit encourag explor matter action select updat rule caus lower valu altern thu increas choic probabl first reward r r use reset initi condit accord idea first time action taken reward use set valu q q allow immedi learn case fix determinist reward model incorpor reset initi condit ric expect predict particip behavior better model assum arbitrari initi condit aic ric seem consist human behaviour repeat binari choic experi simplest store data tabl approach falter increas number sinc likelihood agent visit particular state perform particular action increasingli small combin function approxim make possibl appli algorithm larger problem even state space continu one solut use adapt artifici neural network function approxim anoth possibl integr fuzzi rule interpol fri use spars fuzzi instead discret ann advantag knowledg represent form function approxim may speed learn finit problem due fact algorithm gener earlier experi previous unseen state anoth techniqu decreas space quantiz possibl valu consid exampl learn balanc stick finger describ state certain point time involv posit finger space veloc angl stick angular veloc stick yield vector describ one state snapshot one state encod four valu problem infinit mani possibl state present shrink possibl space valid action multipl valu assign bucket exact distanc finger start posit infin known rather whether far away near far introduc chri watkin converg proof present watkin peter dayan watkin address learn delay reward titl phd thesi eight year earlier problem name delay reinforc learn solv bozinovski crossbar adapt array caa memori matrix w w eight year later architectur introduc term state evalu reinforc learn crossbar learn algorithm written mathemat pseudocod paper iter perform follow comput term secondari reinforc borrow anim learn theori model state valu via backpropag state valu v v consequ situat backpropag previous encount situat caa comput state valu vertic action horizont crossbar demonstr graph show delay reinforc learn contain state desir undesir neutral state comput state evalu function learn system forerunn algorithm googl deepmind patent applic deep learn titl deep reinforc learn deep play atari game expert human level deepmind system use deep convolut neural network layer tile convolut filter mimic effect recept field reinforc learn unstabl diverg nonlinear function approxim neural network use repres instabl come correl present sequenc observ fact small updat q may significantli chang polici agent data distribut correl q target valu method use stochast search variou domain applic techniqu use experi replay biolog inspir mechan use random sampl prior action instead recent action proceed remov correl observ sequenc smooth chang data distribut iter updat adjust q toward target valu period updat reduc correl target futur maximum approxim action valu evalu use q function current action select polici noisi environ sometim overestim action valu slow learn variant call doubl propos correct doubl reinforc learn algorithm differ polici use valu evalu use select next action practic two separ valu function q q b b train mutual symmetr fashion use separ experi doubl updat step follow estim valu discount futur evalu use differ polici solv overestim issu algorithm later modifi combin deep learn dqn algorithm result doubl dqn outperform origin dqn algorithm delay altern implement onlin algorithm probabl approxim correct pac learn greedi gq variant use combin linear function approxim advantag greedi gq converg guarante even function approxim use estim action valu distribut variant seek model distribut return rather expect return action observ facilit estim deep neural network enabl altern control method control propos set see section one approach consist pretend environ passiv littman propos minimax q learn algorithm standard algorithm use q q tabl appli discret action state space discret valu lead ineffici learn larg due curs dimension howev adapt attempt solv problem neural network
Extraterrestrial life,https://en.wikipedia.org/wiki/Extraterrestrial_life,"
 Extraterrestrial life, or alien life (colloquially, alien), is life that originates from another world rather than on Earth. No extraterrestrial life has yet been scientifically conclusively detected. Such life might range from simple forms such as prokaryotes to intelligent beings, possibly bringing forth civilizations that might be far more, or far less, advanced than humans.[1][2][3] The Drake equation speculates about the existence of sapient life elsewhere in the universe. The science of extraterrestrial life is known as astrobiology.
 Speculation about the possibility of inhabited worlds beyond Earth dates back to antiquity. Early Christian writers discussed the idea of a ""plurality of worlds"" as proposed by earlier thinkers such as Democritus; Augustine references Epicurus's idea of innumerable worlds ""throughout the boundless immensity of space"" in The City of God.[4]
 Pre-modern writers typically assumed extraterrestrial ""worlds"" are inhabited by living beings. William Vorilong, in the 15th century, acknowledged the possibility Jesus could have visited extraterrestrial worlds to redeem their inhabitants.[5] Nicholas of Cusa wrote in 1440 that Earth is ""a brilliant star"" like other celestial objects visible in space; which would appear similar to the Sun, from an exterior perspective, due to a layer of ""fiery brightness"" in the outer layer of the atmosphere. He theorised all extraterrestrial bodies could be inhabited by men, plants, and animals, including the Sun.[6] Descartes wrote that there was no means to prove the stars were not inhabited by ""intelligent creatures"", but their existence was a matter of speculation.[7]
 When considering the atmospheric composition and ecosystems hosted by extraterrestrial bodies, extraterrestrial life can seem more speculation than reality, due to the harsh conditions and disparate chemical composition of the atmospheres,[8] when compared to the life-abundant Earth. However, there are many extreme and chemically harsh ecosystems on Earth that do support forms of life and are often hypothesized to be the origin of life on Earth. Hydrothermal vents,[9] acidic hot springs,[10] and volcanic lakes[11] are examples of life forming under difficult circumstances, provide parallels to the extreme environments on other planets and support the possibility of extraterrestrial life.
 Since the mid-20th century, active research has taken place to look for signs of extraterrestrial life, encompassing searches for current and historic extraterrestrial life, and a narrower search for extraterrestrial intelligent life. Depending on the category of search, methods range from analysis of telescope and specimen data[12] to radios used to detect and transmit communications.[13]
 The concept of extraterrestrial life, and particularly extraterrestrial intelligence, has had a major cultural impact, especially extraterrestrials in fiction. Science fiction has communicated scientific ideas, imagined a range of possibilities, and influenced public interest in and perspectives on extraterrestrial life. One shared space is the debate over the wisdom of attempting communication with extraterrestrial intelligence. Some encourage aggressive methods to try to contact intelligent extraterrestrial life. Others – citing the tendency of technologically advanced human societies to enslave or destroy less advanced societies – argue it may be dangerous to actively draw attention to Earth.[14][15]
 Initially, after the Big Bang the universe was too hot to allow life. 15 million years later, it cooled to temperate levels, but the elements that make up living things did not exist yet. The only freely available elements at that point were hydrogen and helium. Carbon and oxygen (and later, water) would not appear until 50 million years later, created through stellar fusion. At that point, the difficulty for life to appear was not the temperature, but the scarcity of free heavy elements.[16] Planetary systems emerged, and the first organic compounds may have formed in the protoplanetary disk of dust grains that would eventually create rocky planets like Earth. Although Earth was in a molten state after its birth and may have burned any organics that fell in it, it would have been more receptive once it cooled down.[17] Once the right conditions on Earth were met, life started by a chemical process known as abiogenesis. Alternatively, life may have formed less frequently, then spread – by meteoroids, for example – between habitable planets in a process called panspermia.[18][19]
 During most of its stellar evolution stars combine hydrogen nuclei to make helium nuclei by stellar fusion, and the comparatively lighter weight of helium allows the star to release the extra energy. The process continues until the star uses all of its available fuel, with the speed of consumption being related to the size of the star. During its last stages, stars start combining helium nuclei to form carbon nuclei. The higher-sized stars can further combine carbon nuclei to create oxygen and silicon, oxygen into neon and sulfur, and so on until iron. In the end, the star blows much of its content back into the stellar medium, where it would join clouds that would eventually become new generations of stars and planets. Many of those materials are the raw components of life on Earth. As this process takes place in all the universe, said materials are ubiquitous in the cosmos and not a rarity from the Solar System.[20]
 Earth is a planet in the Solar System, a planetary system formed by a star at the center, the Sun, and the objects that orbit it: other planets, moons, asteroids, and comets. The sun is part of the Milky Way, a galaxy. The Milky Way is part of the Local Group, a galaxy group that is in turn part of the Laniakea Supercluster. The universe is composed of all similar structures in existence.[21] The immense distances between celestial objects is a difficulty for the study of extraterrestrial life. So far, humans have only set foot on the Moon and sent robotic probes to other planets and moons in the Solar System. Although probes can withstand conditions that may be lethal to humans, the distances cause time delays: the New Horizons took nine years after launch to reach Pluto.[22] No probe has ever reached extrasolar planetary systems. The Voyager 2 has left the Solar System at a speed of 50,000 kilometers per hour, if it headed towards the Alpha Centauri system, the closest one to Earth at 4.4 light years, it would reach it in 100,000 years. Under current technology such systems can only be studied by telescopes, which have limitations.[22] It is estimated that dark matter has a larger amount of combined matter than stars and gas clouds, but as it plays no role on the stellar evolution of stars and planets, it is usually not taken into account by astrobiology.[23]
 There is an area around a star, the circumstellar habitable zone or ""Goldilocks zone"", where water may be at the right temperature to exist in liquid form at a planetary surface. This area is neither too close to the star, where water would become steam, nor too far away, where water would be frozen as ice. However, although useful as an approximation, planetary habitability is complex and defined by several factors. Being in the habitable zone is not enough for a planet to be habitable, not even to actually have such liquid water. Venus is located in the habitable zone of the Solar System but does not have liquid water because of the conditions of its atmosphere. Jovian planets or gas giants are not considered habitable even if they orbit close enough to their stars as hot Jupiters, due to crushing atmospheric pressures.[24] The actual distances for the habitable zones vary according to the type of star, and even the solar activity of each specific star influences the local habitability. The type of star also defines the time the habitable zone will exist, as its presence and limits will change along with the stars stellar evolution.[25]
 The Big Bang took place 14 billion years ago, the Solar System was formed 4 and a half billion years ago, and the first hominids appeared 60 million years ago. Life on other planets may have started, evolved, given birth to extraterrestrial intelligences, and perhaps even faced a planetary extinction event millions or even billions of years ago. The brief times of existence of Earth's species, when considered from a cosmic perspective, may suggest that extraterrestrial life may be equally fleeting under such a scale.[26]
 Life on Earth is quite ubiquitous across the planet and has adapted over time to almost all the available environments in it, extremophiles and the deep biosphere thrive at even the most hostile ones. As a result, it is inferred that life in other celestial bodies may be equally adaptive. However, the origin of life is unrelated to its ease of adaptation, and may have stricter requirements. A celestial body may not have any life on it, even if it was habitable.[27]
 It is unclear if life and intelligent life are ubiquitous in the cosmos or rare. The hypothesis of ubiquitous extraterrestrial life relies on three main ideas. The first one, the size of the universe allows for plenty of planets to have a similar habitability to Earth, and the age of the universe gives enough time for a long process analog to the history of Earth to happen there. The second is that the chemical elements that make up life, such as carbon and water, are ubiquitous in the universe. The third is that the physical laws are universal, which means that the forces that would facilitate or prevent the existence of life would be the same ones as on Earth.[28] According to this argument, made by scientists such as Carl Sagan and Stephen Hawking, it would be improbable for life not to exist somewhere else other than Earth.[29][30] This argument is embodied in the Copernican principle, which states that Earth does not occupy a unique position in the Universe, and the mediocrity principle, which states that there is nothing special about life on Earth.[31]
 Other authors consider instead that life in the cosmos, or at least multicellular life, may be actually rare. The Rare Earth hypothesis maintains that life on Earth is possible because of a series of factors that range from the location in the galaxy and the configuration of the Solar System to local characteristics of the planet, and that it is unlikely that all such requirements are simultaneously met by another planet. The proponents of this hypothesis consider that very little evidence suggests the existence of extraterrestrial life, and that at this point it is just a desired result and not a reasonable scientific explanation for any gathered data.[32][33]
 In 1961, astronomer and astrophysicist Frank Drake devised the Drake equation as a way to stimulate scientific dialogue at a meeting on the search for extraterrestrial intelligence (SETI).[34][better source needed] The Drake equation is a probabilistic argument used to estimate the number of active, communicative extraterrestrial civilisations in the Milky Way galaxy. The Drake equation is:
 where:
 and
 Drake's proposed estimates are as follows, but numbers on the right side of the equation are agreed as speculative and open to substitution:
 



10,000
=
5
⋅
0.5
⋅
2
⋅
1
⋅
0.2
⋅
1
⋅
10,000


{\displaystyle 10{,}000=5\cdot 0.5\cdot 2\cdot 1\cdot 0.2\cdot 1\cdot 10{,}000}

[35][better source needed]
 The Drake equation has proved controversial since, although it is written as a math equation, none of its values were known at the time. Although some values may eventually be measured, others are based on social sciences and are not knowable by their very nature.[36] This does not allow one to make noteworthy conclusions from the equation.[37]
 Based on observations from the Hubble Space Telescope, there are nearly 2 trillion galaxies in the observable universe.[38] It is estimated that at least ten per cent of all Sun-like stars have a system of planets,[39] i.e. there are 6.25×1018 stars with planets orbiting them in the observable universe. Even if it is assumed that only one out of a billion of these stars has planets supporting life, there would be some 6.25 billion life-supporting planetary systems in the observable universe. A 2013 study based on results from the Kepler spacecraft estimated that the Milky Way contains at least as many planets as it does stars, resulting in 100–400 billion exoplanets.[40][41] The Nebular hypothesis that explains the formation of the Solar System and other planetary systems would suggest that those can have several configurations, and not all of them may have rocky planets within the habitable zone.[42]
 The apparent contradiction between high estimates of the probability of the existence of extraterrestrial civilisations and the lack of evidence for such civilisations is known as the Fermi paradox.[43] Dennis W. Sciama claimed that life's existence in the universe depends on various fundamental constants. Zhi-Wei Wang and Samuel L. Braunstein suggest that a random universe capable of supporting life is likely to be just barely able to do so, giving a potential explanation to the Fermi paradox.[44]
 If extraterrestrial life exists, it could range from simple microorganisms and multicellular organisms similar to animals or plants, to complex alien intelligences akin to humans. When scientists talk about extraterrestrial life, they consider all those types. Although it is possible that extraterrestrial life may have other configurations, scientists use the hierarchy of lifeforms from Earth for simplicity, as it is the only one known to exist.[45]
 The first basic requirement for life is an environment with non-equilibrium thermodynamics, which means that the thermodynamic equilibrium must be broken by a source of energy. The traditional sources of energy in the cosmos are the stars, such as for life on Earth, which depends on the energy of the sun. However, there are other alternative energy sources, such as volcanoes, plate tectonics, and hydrothermal vents. There are ecosystems on Earth in deep areas of the ocean that do not receive sunlight, and take energy from black smokers instead.[46] Magnetic fields and radioactivity have also been proposed as sources of energy, although they would be less efficient ones.[47]
 Life on Earth requires water in a liquid state as a solvent in which biochemical reactions take place. It is highly unlikely that an abiogenesis process can start within a gaseous or solid medium: the atom speeds, either too fast or too slow, make it difficult for specific ones to meet and start chemical reactions. A liquid medium also allows the transport of nutrients and substances required for metabolism.[48] Sufficient quantities of carbon and other elements, along with water, might enable the formation of living organisms on terrestrial planets with a chemical make-up and temperature range similar to that of Earth.[49][50] Life based on ammonia rather than water has been suggested as an alternative, though this solvent appears less suitable than water. It is also conceivable that there are forms of life whose solvent is a liquid hydrocarbon, such as methane, ethane or propane.[51]
 Another unknown aspect of potential extraterrestrial life would be the chemical elements that would compose it. Life on Earth is largely composed of carbon, but there could be other hypothetical types of biochemistry. A replacement for carbon would need to be able to create complex molecules, store information required for evolution, and be freely available in the medium. To create DNA, RNA, or a close analog, such an element should be able to bind its atoms with many others, creating complex and stable molecules. It should be able to create at least three covalent bonds: two for making long strings and at least a third to add new links and allow for diverse information. Only nine elements meet this requirement: boron, nitrogen, phosphorus, arsenic, antimony (three bonds), carbon, silicon, germanium and tin (four bonds). As for abundance, carbon, nitrogen, and silicon are the most abundant ones in the universe, far more than the others. On Earth's crust the most abundant of those elements is silicon, in the Hydrosphere it is carbon and in the atmosphere, it is carbon and nitrogen. Silicon, however, has disadvantages over carbon. The molecules formed with silicon atoms are less stable, and more vulnerable to acids, oxygen, and light. An ecosystem of silicon-based lifeforms would require very low temperatures, high atmospheric pressure, an atmosphere devoid of oxygen, and a solvent other than water. The low temperatures required would add an extra problem, the difficulty to kickstart a process of abiogenesis to create life in the first place.[52] Norman Horowitz, head of the Jet Propulsion Laboratory bioscience section for the Mariner and Viking missions from 1965 to 1976 considered that the great versatility of the carbon atom makes it the element most likely to provide solutions, even exotic solutions, to the problems of survival of life on other planets.[53]  However, he also considered that the conditions found on Mars were incompatible with carbon based life. 
 Even if extraterrestrial life is based on carbon and uses water as a solvent, like Earth life, it may still have a radically different biochemistry.  Life is generally considered to be a product of natural selection.  It has been proposed that to undergo natural selection a living entity must have the capacity to replicate itself, the capacity to avoid damage/decay, and the capacity to acquire and process resources in support of the first two capacities.[54] Life on Earth started with an RNA world and later evolved to its current form, where some of the RNA tasks were transferred to DNA and proteins. Extraterrestrial life may still be stuck using RNA, or evolve into other configurations. It is unclear if our biochemistry is the most efficient one that could be generated, or which elements would follow a similar pattern.[55] However, it is likely that, even if cells had a different composition to those from Earth, they would still have a cell membrane. Life on Earth jumped from prokaryotes to eukaryotes and from unicellular organisms to multicellular organisms through evolution. So far no alternative process to achieve such a result has been conceived, even if hypothetical. Evolution requires life to be divided into individual organisms, and no alternative organisation has been satisfactorily proposed either. At the basic level, membranes define the limit of a cell, between it and its environment, while remaining partially open to exchange energy and resources with it.[56]
 The evolution from simple cells to eukaryotes, and from them to multicellular lifeforms, is not guaranteed. The Cambrian explosion took place thousands of millions of years after the origin of life, and its causes are not fully known yet. On the other hand, the jump to multicellularity took place several times, which suggests that it could be a case of convergent evolution, and so likely to take place on other planets as well. Palaeontologist Simon Conway Morris considers that convergent evolution would lead to kingdoms similar to our plants and animals, and that many features are likely to develop in alien animals as well, such as bilateral symmetry, limbs, digestive systems and heads with sensory organs.[57] Scientists from the University of Oxford analysed it from the perspective of evolutionary theory and wrote in a study in the International Journal of Astrobiology that aliens may be similar to humans.[58] The planetary context would also have an influence: a planet with higher gravity would have smaller animals, and other types of stars can lead to non-green photosynthesizers. The amount of energy available would also affect biodiversity, as an ecosystem sustained by black smokers or hydrothermal vents would have less energy available than those sustained by a star's light and heat, and so its lifeforms would not grow beyond a certain complexity.[57] There is also research in assessing the capacity of life for developing intelligence. It has been suggested that this capacity arises with the number of potential niches a planet contains, and that the complexity of life itself is reflected in the information density of planetary environments, which in turn can be computed from its niches.[59]
 It is common knowledge that the conditions on other planets in the solar system, in addition to the many galaxies outside of the Milky Way galaxy, are very harsh and seem to be too extreme to harbor any life.[60] The environmental conditions on these planets can have intense UV radiation paired with extreme temperatures, lack of water,[61] and much more that can lead to conditions that don't seem to favor the creation or maintenance of extraterrestrial life. However, there has been much historical evidence that some of the earliest and most basic forms of life on Earth originated in some extreme environments[62] that seem unlikely to have harbored life at least at one point in Earth's history. Fossil evidence as well as many historical theories backed up by years of research and studies have marked environments like hydrothermal vents or acidic hot springs as some of the first places that life could have originated on Earth.[63] These environments can be considered extreme when compared to the typical ecosystems that the majority of life on Earth now inhabit, as hydrothermal vents are scorching hot due to the magma escaping from the Earth's mantle and meeting the much colder oceanic water. Even in today's world, there can be a diverse population of bacteria found inhabiting the area surrounding these hydrothermal vents[64] which can suggest that some form of life can be supported even in the harshest of environments like the other planets in the solar system.
 The aspects of these harsh environments that make them ideal for the origin of life on Earth, as well as the possibility of creation of life on other planets, is the chemical reactions forming spontaneously. For example, the hydrothermal vents found on the ocean floor are known to support many chemosynthetic processes[9] which allow organisms to utilize energy through reduced chemical compounds that fix carbon.[64] In return, these reactions will allow for organisms to live in relatively low oxygenated environments while maintaining enough energy to support themselves. The early Earth environment was reducing[65] and therefore, these carbon fixing compounds were necessary for the survival and possible origin of life on Earth. With the little amount of information that scientists have found regarding the atmosphere on other planets in the Milky Way galaxy and beyond, the atmospheres are most likely reducing or with very low oxygen levels,[66] especially when compared with Earth's atmosphere. If there were the necessary elements and ions on these planets, the same carbon fixing, reduced chemical compounds occurring around hydrothermal vents could also occur on these planets' surfaces and possibly result in the origin of extraterrestrial life.
 The Solar System has a wide variety of planets, dwarf planets, and moons, and each one is studied for its potential to host life. Each one has its own specific conditions that may benefit or harm life. So far, the only lifeforms found are those from Earth. No extraterrestrial intelligence other than humans exists or has ever existed within the Solar System.[67] Astrobiologist Mary Voytek points out that it would be unlikely to find large ecosystems, as they would have already been detected by now.[24]
 The inner Solar System is likely devoid of life. However, Venus is still of interest to astrobiologists, as it is a terrestrial planet that was likely similar to Earth in its early stages and developed in a different way. There is a greenhouse effect, the surface is the hottest in the Solar System, sulfuric acid clouds, all surface liquid water is lost, and it has a thick carbon-dioxide atmosphere with huge pressure.[68] Comparing both helps to understand the precise differences that lead to beneficial or harmful conditions for life. And despite the conditions against life on Venus, there are suspicions that microbial life-forms may still survive in high-altitude clouds.[24]
 Mars is a cold and almost airless desert, inhospitable to life. However, recent studies revealed that water on Mars used to be quite abundant, forming rivers, lakes, and perhaps even oceans. Mars may have been habitable back then, and life on Mars may have been possible. But when the planetary core ceased to generate a magnetic field, solar winds removed the atmosphere and the planet became vulnerable to solar radiation. Ancient life-forms may still have left fossilised remains, and microbes may still survive deep underground.[24]
 As mentioned, the gas giants and ice giants are unlikely to contain life. The most distant solar system bodies, found in the Kuiper Belt and outwards, are locked in permanent deep-freeze, but cannot be ruled out completely.[24]
 Although the giant planets themselves are highly unlikely to have life, there is much hope to find it on moons orbiting these planets. Europa, from the Jovian system, has a subsurface ocean below a thick layer of ice. Ganymede and Callisto also have subsurface oceans, but life is less likely in them because water is sandwiched between layers of solid ice. Europa would have contact between the ocean and the rocky surface, which helps the chemical reactions. It may be difficult to dig so deep in order to study those oceans, though. Enceladus, a tiny moon of Saturn with another subsurface ocean, may not need to be dug, as it releases water to space in eruption columns. The space probe Cassini flew inside one of these, but could not make a full study because NASA did not expect this phenomenon and did not equip the probe to study ocean water. Still, Cassini detected complex organic molecules, salts, evidence of hydrothermal activity, hydrogen, and methane.[24]
 Titan is the only celestial body in the Solar System besides Earth that has liquid bodies on the surface. It has rivers, lakes, and rain of hydrocarbons, methane, and ethane, and even a cycle similar to Earth's water cycle. This special context encourages speculations about lifeforms with different biochemistry, but the cold temperatures would make such chemistry take place at a very slow pace. Water is rock-solid on the surface, but Titan does have a subsurface water ocean like several other moons. However, it is of such a great depth that it would be very difficult to access it for study.[24]
 The science that searches and studies life in the universe, both on Earth and elsewhere, is called astrobiology. With the study of Earth's life, the only known form of life, astrobiology seeks to study how life starts and evolves and the requirements for its continuous existence. This helps to determine what to look for when searching for life in other celestial bodies. This is a complex area of study, and uses the combined perspectives of several scientific disciplines, such as astronomy, biology, chemistry, geology, oceanography, and atmospheric sciences.[69]
 The scientific search for extraterrestrial life is being carried out both directly and indirectly. As of September 2017[update], 3,667 exoplanets in 2,747 systems have been identified, and other planets and moons in the Solar System hold the potential for hosting primitive life such as microorganisms. As of 8 February 2021, an updated status of studies considering the possible detection of lifeforms on Venus (via phosphine) and Mars (via methane) was reported.[70]
 Scientists search for biosignatures within the Solar System by studying planetary surfaces and examining meteorites. Some claim to have identified evidence that microbial life has existed on Mars.[73][74][75][76] In 1996, a controversial report stated that structures resembling nanobacteria were discovered in a meteorite, ALH84001, formed of rock ejected from Mars.[73][74] Although all the unusual properties of the meteorite were eventually explained as the result of inorganic processes, the controversy over its discovery laid the groundwork for the development of astrobiology.[73]
 An experiment on the two Viking Mars landers reported gas emissions from heated Martian soil samples that some scientists argue are consistent with the presence of living microorganisms.[77] Lack of corroborating evidence from other experiments on the same samples suggests that a non-biological reaction is a more likely hypothesis.[77][78][79][80]
 In February 2005 NASA scientists reported they may have found some evidence of extraterrestrial life on Mars.[81] The two scientists, Carol Stoker and Larry Lemke of NASA's Ames Research Center, based their claim on methane signatures found in Mars's atmosphere resembling the methane production of some forms of primitive life on Earth, as well as on their own study of primitive life near the Rio Tinto river in Spain. NASA officials soon distanced NASA from the scientists' claims, and Stoker herself backed off from her initial assertions.[82]
 In November 2011, NASA launched the Mars Science Laboratory that landed the Curiosity rover on Mars. It is designed to assess the past and present habitability on Mars using a variety of scientific instruments. The rover landed on Mars at Gale Crater in August 2012.[83][84]
 A group of scientists at Cornell University started a catalog of microorganisms, with the way each one reacts to sunlight. The goal is to help with the search for similar organisms in exoplanets, as the starlight reflected by planets rich in such organisms would have a specific spectrum, unlike that of starlight reflected from lifeless planets. If Earth was studied from afar with this system, it would reveal a shade of green, as a result of the abundance of plants with photosynthesis.[85]
 In August 2011, NASA studied meteorites found on Antarctica, finding adenine, guanine, hypoxanthine and xanthine. Adenine and guanine are components of DNA, and the others are used in other biological processes. The studies ruled out pollution of the meteorites on Earth, as those components would not be freely available the way they were found in the samples. This discovery suggests that several organic molecules that serve as building blocks of life may be generated within asteroids and comets.[86][87] In October 2011, scientists reported that cosmic dust contains complex organic compounds (""amorphous organic solids with a mixed aromatic-aliphatic structure"") that could be created naturally, and rapidly, by stars.[88][89][90] It is still unclear if those compounds played a role in the creation of life on Earth, but Sun Kwok, of the University of Hong Kong, thinks so. ""If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life.""[88]
 In August 2012, and in a world first, astronomers at Copenhagen University reported the detection of a specific sugar molecule, glycolaldehyde, in a distant star system. The molecule was found around the protostellar binary IRAS 16293-2422, which is located 400 light years from Earth.[91] Glycolaldehyde is needed to form ribonucleic acid, or RNA, which is similar in function to DNA. This finding suggests that complex organic molecules may form in stellar systems prior to the formation of planets, eventually arriving on young planets early in their formation.[92]
 In December 2023, astronomers reported the first time discovery, in the plumes of Enceladus, moon of the planet Saturn, of hydrogen cyanide, a possible chemical essential for life[93] as we know it, as well as other organic molecules, some of which are yet to be better identified and understood. According to the researchers, ""these [newly discovered] compounds could potentially support extant microbial communities or drive complex organic synthesis leading to the origin of life.""[94][95]
 Although most searches are focused on the biology of extraterrestrial life, an extraterrestrial intelligence capable enough to develop a civilization may be detectable by other means as well. Technology may generate technosignatures, effects on the native planet that may not be caused by natural causes. There are three main types of techno-signatures considered: interstellar communications, effects on the atmosphere, and planetary-sized structures such as Dyson spheres.[96]
 Organizations such as the SETI Institute search the cosmos for potential forms of communication. They started with radio waves, and now search for laser pulses as well. The challenge for this search is that there are natural sources of such signals as well, such as gamma-ray bursts and supernovae, and the difference between a natural signal and an artificial one would be in its specific patterns. Astronomers intend to use artificial intelligence for this, as it can manage large amounts of data and is devoid of biases and preconceptions.[96] Besides, even if there is an advanced extraterrestrial civilization, there is no guarantee that it is transmitting radio communications in the direction of Earth. The length of time required for a signal to travel across space means that a potential answer may arrive decades or centuries after the initial message.[97]
 The atmosphere of Earth is rich in nitrogen dioxide as a result of air pollution, which can be detectable. The natural abundance of carbon, which is also relatively reactive, makes it likely to be a basic component of the development of a potential extraterrestrial technological civilization, as it is on Earth. Fossil fuels may likely be generated and used on such worlds as well. The abundance of chlorofluorocarbons in the atmosphere can also be a clear technosignature, considering their role in ozone depletion. Light pollution may be another technosignature, as multiple lights on the night side of a rocky planet can be a sign of advanced technological development. However, modern telescopes are not strong enough to study exoplanets with the required level of detail to perceive it.[96]
 The Kardashev scale proposes that a civilization may eventually start consuming energy directly from its local star. This would require giant structures built next to it, called Dyson spheres. Those speculative structures would cause an excess infrared radiation, that telescopes may notice. The infrared radiation is typical of young stars, surrounded by dusty protoplanetary disks that will eventually form planets. An older star such as the Sun would have no natural reason to have excess infrared radiation.[96] The presence of heavy elements in a star's light-spectrum is another potential biosignature; such elements would (in theory) be found if the star were being used as an incinerator/repository for nuclear waste products.[98]
 Some astronomers search for extrasolar planets that may be conducive to life, narrowing the search to terrestrial planets within the habitable zones of their stars.[99][100] Since 1992, over four thousand exoplanets have been discovered (7,026 planets in 4,949 planetary systems including 1007 multiple planetary systems as of 24 July 2024).[101]
 The extrasolar planets so far discovered range in size from that of terrestrial planets similar to Earth's size to that of gas giants larger than Jupiter.[101] The number of observed exoplanets is expected to increase greatly in the coming years.[102][better source needed] The Kepler space telescope has also detected a few thousand[103][104] candidate planets,[105][106] of which about 11% may be false positives.[107]
 There is at least one planet on average per star.[108] About 1 in 5 Sun-like stars[a] have an ""Earth-sized""[b] planet in the habitable zone,[c] with the nearest expected to be within 12 light-years distance from Earth.[109][110] Assuming 200 billion stars in the Milky Way,[d] that would be 11 billion potentially habitable Earth-sized planets in the Milky Way, rising to 40 billion if red dwarfs are included.[111] The rogue planets in the Milky Way possibly number in the trillions.[112]
 The nearest known exoplanet is Proxima Centauri b, located 4.2 light-years (1.3 pc) from Earth in the southern constellation of Centaurus.[113]
 As of March 2014[update], the least massive exoplanet known is PSR B1257+12 A, which is about twice the mass of the Moon. The most massive planet listed on the NASA Exoplanet Archive is DENIS-P J082303.1−491201 b,[114][115] about 29 times the mass of Jupiter, although according to most definitions of a planet, it is too massive to be a planet and may be a brown dwarf instead. Almost all of the planets detected so far are within the Milky Way, but there have also been a few possible detections of extragalactic planets. The study of planetary habitability also considers a wide range of other factors in determining the suitability of a planet for hosting life.[12]
 One sign that a planet probably already contains life is the presence of an atmosphere with significant amounts of oxygen, since that gas is highly reactive and generally would not last long without constant replenishment. This replenishment occurs on Earth through photosynthetic organisms. One way to analyse the atmosphere of an exoplanet is through spectrography when it transits its star, though this might only be feasible with dim stars like white dwarfs.[116]
 The modern concept of extraterrestrial life is based on assumptions that were not commonplace during the early days of astronomy. The first explanations for the celestial objects seen in the night sky were based on mythology. Scholars from Ancient Greece were the first to consider that the universe is inherently understandable and rejected explanations based on supernatural incomprehensible forces, such as the myth of the Sun being pulled across the sky in the chariot of Apollo. They had not developed the scientific method yet and based their ideas on pure thought and speculation, but they developed precursor ideas to it, such as that explanations had to be discarded if they contradict observable facts. The discussions of those Greek scholars established many of the pillars that would eventually lead to the idea of extraterrestrial life, such as Earth being round and not flat. The cosmos was first structured in a geocentric model that considered that the sun and all other celestial bodies revolve around Earth. However, they did not consider them as worlds. In Greek understanding, the world was composed by both Earth and the celestial objects with noticeable movements. Anaximander thought that the cosmos was made from apeiron, a substance that created the world, and that the world would eventually return to the cosmos.
 Eventually two groups emerged, the atomists that thought that matter at both Earth and the cosmos was equally made of small atoms of the classical elements (earth, water, fire and air), and the Aristotelians who thought that those elements were exclusive of Earth and that the cosmos was made of a fifth one, the aether. Atomist Epicurus thought that the processes that created the world, its animals and plants should have created other worlds elsewhere, along with their own animals and plants. Aristotle thought instead that all the earth element naturally fell towards the center of the universe, and that would made it impossible for other planets to exist elsewhere. Under that reasoning, Earth was not only in the center, it was also the only planet in the universe.[117]
 Cosmic pluralism, the plurality of worlds, or simply pluralism, describes the philosophical belief in numerous ""worlds"" in addition to Earth, which might harbor extraterrestrial life. The earliest recorded assertion of extraterrestrial human life is found in ancient scriptures of Jainism. There are multiple ""worlds"" mentioned in Jain scriptures that support human life. These include, among others, Bharat Kshetra, Mahavideh Kshetra, Airavat Kshetra, and Hari kshetra.[118][119][120] Medieval Muslim writers like Fakhr al-Din al-Razi and Muhammad al-Baqir supported cosmic pluralism on the basis of the Qur'an.[121] Chaucer's poem The House of Fame engaged in medieval thought experiments that postulated the plurality of worlds.[122] However, those ideas about other worlds were different from the current knowledge about the structure of the universe, and did not postulate the existence of planetary systems other than the Solar System. When those authors talk about other worlds, they talk about places located at the center of their own systems, and with their own stellar vaults and cosmos surrounding them.[123]
 The Greek ideas and the disputes between atomists and Aristotelians outlived the fall of the Greek empire. The Great Library of Alexandria compiled information about it, part of which was translated by Islamic scholars and thus survived the end of the Library. Baghdad combined the knowledge of the Greeks, the Indians, the Chinese and its own scholars, and the knowledge expanded through the Byzantine Empire. From there it eventually returned to Europe by the time of the Middle Ages. However, as the Greek atomist doctrine held that the world was created by random movements of atoms, with no need for a creator deity, it became associated with atheism, and the dispute intertwined with religious ones.[124] Still, the Church did not react to those topics in a homogeneous way, and there were stricter and more permissive views within the church itself.[125]
 The first known mention of the term 'panspermia' was in the writings of the 5th-century BC Greek philosopher Anaxagoras. He proposed the idea that life exists everywhere.[126]
 By the time of the late Middle Ages there were many known inaccuracies in the geocentric model, but it was kept in use because naked eye observations provided limited data. Nicolaus Copernicus started the Copernican Revolution by proposing that the planets revolve around the sun rather than Earth. His proposal had little acceptance at first because, as he kept the assumption that orbits were perfect circles, his model led to as many inaccuracies as the geocentric one. Tycho Brahe improved the available data with naked-eye observatories, which worked with highly complex sextants and quadrants. Tycho could not make sense of his observations, but Johannes Kepler did: orbits were not perfect circles, but ellipses. This knowledge benefited the Copernican model, which worked now almost perfectly. The invention of the telescope a short time later, perfected by Galileo Galilei, clarified the final doubts, and the paradigm shift was completed.[127] Under this new understanding, the notion of extraterrestrial life became feasible: if Earth is but just a planet orbiting around a star, there may be planets similar to Earth elsewhere. The astronomical study of distant bodies also proved that physical laws are the same elsewhere in the universe as on Earth, with nothing making the planet truly special.[128]
 The new ideas were met with resistance from the Catholic church. Galileo was tried for the heliocentric model, which was considered heretical, and forced to recant it.[129] The best-known early-modern proponent of ideas of extraterrestrial life was the Italian philosopher Giordano Bruno, who argued in the 16th century for an infinite universe in which every star is surrounded by its own planetary system. Bruno wrote that other worlds ""have no less virtue nor a nature different to that of our earth"" and, like Earth, ""contain animals and inhabitants"".[130] Bruno's belief in the plurality of worlds was one of the charges leveled against him by the Venetian Holy Inquisition, which trialed and executed him.[131]
 The heliocentric model was further strengthened by the postulation of the theory of gravity by Sir Isaac Newton. This theory provided the mathematics that explains the motions of all things in the universe, including planetary orbits. By this point, the geocentric model was definitely discarded. By this time, the use of the scientific method had become a standard, and new discoveries were expected to provide evidence and rigorous mathematical explanations. Science also took a deeper interest in the mechanics of natural phenomena, trying to explain not just the way nature works but also the reasons for working that way.[132]
 There was very little actual discussion about extraterrestrial life before this point, as the Aristotlean ideas remained influential while geocentrism was still accepted. When it was finally proved wrong, it not only meant that Earth was not the center of the universe, but also that the lights seen in the sky were not just lights, but physical objects. The notion that life may exist in them as well soon became an ongoing topic of discussion, although one with no practical ways to investigate.[133]
 The possibility of extraterrestrials remained a widespread speculation as scientific discovery accelerated. William Herschel, the discoverer of Uranus, was one of many 18th–19th-century astronomers who believed that the Solar System is populated by alien life. Other scholars of the period who championed ""cosmic pluralism"" included Immanuel Kant and Benjamin Franklin. At the height of the Enlightenment, even the Sun and Moon were considered candidates for extraterrestrial inhabitants.[134][135]
 Speculation about life on Mars increased in the late 19th century, following telescopic observation of apparent Martian canals – which soon, however, turned out to be optical illusions.[136] Despite this, in 1895, American astronomer Percival Lowell published his book Mars, followed by Mars and its Canals in 1906, proposing that the canals were the work of a long-gone civilisation.[137]
 Spectroscopic analysis of Mars's atmosphere began in earnest in 1894, when U.S. astronomer William Wallace Campbell showed that neither water nor oxygen was present in the Martian atmosphere.[138] By 1909 better telescopes and the best perihelic opposition of Mars since 1877 conclusively put an end to the canal hypothesis.[139]
 As a consequence of the belief in the spontaneous generation there was little thought about the conditions of each celestial body: it was simply assumed that life would thrive anywhere. This theory was disproved by Louis Pasteur in the 19th century. Popular belief in thriving alien civilisations elsewhere in the solar system still remained strong until Mariner 4 and Mariner 9 provided close images of Mars, which debunked forever the idea of the existence of Martians and decreased the previous expectations of finding alien life in general.[140] The end of the spontaneous generation belief forced to investigate the origin of life. Although abiogenesis is the more accepted theory, a number of authors reclaimed the term ""panspermia"" and proposed that life was brought to Earth from elsewhere.[126] Some of those authors are Jöns Jacob Berzelius (1834),[141] Kelvin (1871),[142] Hermann von Helmholtz (1879)[143] and, somewhat later, by Svante Arrhenius (1903).[144]
 The science fiction genre, although not so named during the time, developed during the late 19th century. The expansion of the genre of extraterrestrials in fiction influenced the popular perception over the real-life topic, making people eager to jump to conclusions about the discovery of aliens. Science marched at a slower pace, some discoveries fueled expectations and others dashed excessive hopes. For example, with the advent of telescopes, most structures seen on the Moon or Mars were immediately attributed to Selenites or Martians, and later ones (such as more powerful telescopes) revealed that all such discoveries were natural features.[131] A famous case is the Cydonia region of Mars, first imaged by the Viking 1 orbiter. The low-resolution photos showed a rock formation that resembled a human face, but later spacecraft took photos in higher detail that showed that there was nothing special about the site.[145]
 The search and study of extraterrestrial life became a science of its own, astrobiology. Also known as exobiology, this discipline is studied by the NASA, the ESA, the INAF, and others. Astrobiology studies life from Earth as well, but with a cosmic perspective. For example, abiogenesis is of interest to astrobiology, not because of the origin of life on Earth, but for the chances of a similar process taking place in other celestial bodies. Many aspects of life, from its definition to its chemistry, are analyzed as either likely to be similar in all forms of life across the cosmos or only native to Earth.[146] Astrobiology, however, remains constrained by the current lack of extraterrestrial life-forms to study, as all life on Earth comes from the same ancestor, and it is hard to infer general characteristics from a group with a single example to analyse.[147]
 The 20th century came with great technological advances, speculations about future hypothetical technologies, and an increased basic knowledge of science by the general population thanks to science divulgation through the mass media. The public interest in extraterrestrial life and the lack of discoveries by mainstream science led to the emergence of pseudosciences that provided affirmative, if questionable, answers to the existence of aliens. Ufology claims that many unidentified flying objects (UFOs) would be spaceships from alien species, and ancient astronauts hypothesis claim that aliens would have visited Earth in antiquity and prehistoric times but people would have failed to understand it by then.[148] Most UFOs or UFO sightings[149] can be readily explained as sightings of Earth-based aircraft (including top-secret aircraft), known astronomical objects or weather phenomenons, or as hoaxes.[150]
 Looking beyond the pseudosciences, Lewis White Beck strove to elevate the level of public discourse on the topic of extraterrestrial life by tracing the evolution of philosophical thought over the centuries from ancient times into the modern era. His review of the contributions made by Lucretius, Plutarch, Aristotle, Copernicus, Immanuel Kant, John Wilkins, Charles Darwin and Karl Marx demonstrated that even in modern times, humanity could be profoundly influenced in its search for extraterrestrial life by subtle and comforting archetypal ideas which are largely derived from firmly held religious, philosophical and existential belief systems. On a positive note, however,  Beck further argued that even if the search for extraterrestrial life proves to be unsuccessful, the endeavor itself could have beneficial consequences by assisting humanity in its attempt to actualize superior ways of living here on Earth.[151]
 By the 21st century, it was accepted that multicellular life in the Solar System can only exist on Earth, but the interest in extraterrestrial life increased regardless. This is a result of the advances in several sciences. The knowledge of planetary habitability allows to consider on scientific terms the likelihood of finding life at each specific celestial body, as it is known which features are beneficial and harmful for life. Astronomy and telescopes also improved to the point exoplanets can be confirmed and even studied, increasing the number of search places. Life may still exist elsewhere in the Solar System in unicellular form, but the advances in spacecraft allow to send robots to study samples in situ, with tools of growing complexity and reliability. Although no extraterrestrial life has been found and life may still be just a rarity from Earth, there are scientific reasons to suspect that it can exist elsewhere, and technological advances that may detect it if it does.[152]
 Many scientists are optimistic about the chances of finding alien life. In the words of SETI's Frank Drake, ""All we know for sure is that the sky is not littered with powerful microwave transmitters"".[153] Drake noted that it is entirely possible that advanced technology results in communication being carried out in some way other than conventional radio transmission. At the same time, the data returned by space probes, and giant strides in detection methods, have allowed science to begin delineating habitability criteria on other worlds, and to confirm that at least other planets are plentiful, though aliens remain a question mark. The Wow! signal, detected in 1977 by a SETI project, remains a subject of speculative debate.[154]
 On the other hand, other scientists are pessimistic. Jacques Monod wrote that ""Man knows at last that he is alone in the indifferent immensity of the universe, whence which he has emerged by chance"".[155] In 2000, geologist and paleontologist Peter Ward and astrobiologist Donald Brownlee published a book entitled Rare Earth: Why Complex Life is Uncommon in the Universe.[156][better source needed] In it, they discussed the Rare Earth hypothesis, in which they claim that Earth-like life is rare in the universe, whereas microbial life is common. Ward and Brownlee are open to the idea of evolution on other planets that is not based on essential Earth-like characteristics such as DNA and carbon.
 As for the possible risks, theoretical physicist Stephen Hawking warned in 2010 that humans should not try to contact alien life forms. He warned that aliens might pillage Earth for resources. ""If aliens visit us, the outcome would be much as when Columbus landed in America, which didn't turn out well for the Native Americans"", he said.[157] Jared Diamond had earlier expressed similar concerns.[158] On 20 July 2015, Hawking and Russian billionaire Yuri Milner, along with the SETI Institute, announced a well-funded effort, called the Breakthrough Initiatives, to expand efforts to search for extraterrestrial life. The group contracted the services of the 100-meter Robert C. Byrd Green Bank Telescope in West Virginia in the United States and the 64-meter Parkes Telescope in New South Wales, Australia.[159] On 13 February 2015, scientists (including Geoffrey Marcy, Seth Shostak, Frank Drake and David Brin) at a convention of the American Association for the Advancement of Science, discussed Active SETI and whether transmitting a message to possible intelligent extraterrestrials in the Cosmos was a good idea;[160][161] one result was a statement, signed by many, that a ""worldwide scientific, political and humanitarian discussion must occur before any message is sent"".[162]
 The 1967 Outer Space Treaty and the 1979 Moon Agreement define rules of planetary protection against potentially hazardous extraterrestrial life. COSPAR also provides guidelines for planetary protection.[163] A committee of the United Nations Office for Outer Space Affairs had in 1977 discussed for a year strategies for interacting with extraterrestrial life or intelligence. The discussion ended without any conclusions. As of 2010, the UN lacks response mechanisms for the case of an extraterrestrial contact.[164]
 One of the NASA divisions is the Office of Safety and Mission Assurance (OSMA), also known as the Planetary Protection Office. A part of its mission is to ""rigorously preclude backward contamination of Earth by extraterrestrial life.""[165]
 In 2016, the Chinese Government released a white paper detailing its space program. According to the document, one of the research objectives of the program is the search for extraterrestrial life.[166] It is also one of the objectives of the Chinese Five-hundred-meter Aperture Spherical Telescope (FAST) program.[167]
 In 2020, Dmitry Rogozin, the head of the Russian space agency, said the search for extraterrestrial life is one of the main goals of deep space research. 
He also acknowledged the possibility of existence of primitive life on other planets of the Solar System.[168]
 The French space agency has an office for the study of ""non-identified aero spatial phenomena"".[169][170] The agency is maintaining a publicly accessible database of such phenomena, with over 1600 detailed entries. According to the head of the office, the vast majority of entries have a mundane explanation; but for 25% of entries, their extraterrestrial origin can neither be confirmed nor denied.[169]
 In 2020, chairman of the Israel Space Agency Isaac Ben-Israel stated that the probability of detecting life in outer space is ""quite large"". But he disagrees with his former colleague Haim Eshed who stated that there are contacts between an advanced alien civilisation and some of Earth's governments.[171]
 Although the idea of extraterrestrial peoples became feasible once astronomy developed enough to understand the nature of planets, they were not thought of as being any different from humans. Having no scientific explanation for the origin of mankind and its relation to other species, there was no reason to expect them to be any other way. This was changed by the 1859 book On the Origin of Species by Charles Darwin, which proposed the theory of evolution. Now with the notion that evolution on other planets may take other directions, science fiction authors created bizarre aliens, clearly distinct from humans. A usual way to do that was to add body features from other animals, such as insects or octopuses. Costuming and special effects feasibility alongside budget considerations forced films and TV series to tone down the fantasy, but these limitations lessened since the 1990s with the advent of computer-generated imagery (CGI), and later on as CGI became more effective and less expensive.[172]
 Real-life events sometimes captivate people's imagination and this influences the works of fiction. For example, during the Barney and Betty Hill incident, the first recorded claim of an alien abduction, the couple reported that they were abducted and experimented on by aliens with oversized heads, big eyes, pale grey skin, and small noses, a description that eventually became the grey alien archetype once used in works of fiction.[172]
",extraterrestri life alien life colloqui alien life origin anoth world rather earth extraterrestri life yet scientif conclus detect life might rang simpl form prokaryot intellig be possibl bring forth civil might far far less advanc human drake equat specul exist sapient life elsewher univers scienc extraterrestri life known astrobiolog specul possibl inhabit world beyond earth date back antiqu earli christian writer discuss idea plural world propos earlier thinker democritu augustin refer epicuru idea innumer world throughout boundless immens space citi god writer typic assum extraterrestri world inhabit live be william vorilong centuri acknowledg possibl jesu could visit extraterrestri world redeem inhabit nichola cusa wrote earth brilliant star like celesti object visibl space would appear similar sun exterior perspect due layer fieri bright outer layer atmospher theoris extraterrestri bodi could inhabit men plant anim includ sun descart wrote mean prove star inhabit intellig creatur exist matter specul consid atmospher composit ecosystem host extraterrestri bodi extraterrestri life seem specul realiti due harsh condit dispar chemic composit atmospher compar earth howev mani extrem chemic harsh ecosystem earth support form life often hypothes origin life earth hydrotherm vent acid hot spring volcan lake exampl life form difficult circumst provid parallel extrem environ planet support possibl extraterrestri life sinc centuri activ research taken place look sign extraterrestri life encompass search current histor extraterrestri life narrow search extraterrestri intellig life depend categori search method rang analysi telescop specimen data radio use detect transmit commun concept extraterrestri life particularli extraterrestri intellig major cultur impact especi extraterrestri fiction scienc fiction commun scientif idea imagin rang possibl influenc public interest perspect extraterrestri life one share space debat wisdom attempt commun extraterrestri intellig encourag aggress method tri contact intellig extraterrestri life other cite tendenc technolog advanc human societi enslav destroy less advanc societi argu may danger activ draw attent earth initi big bang univers hot allow life million year later cool temper level element make live thing exist yet freeli avail element point hydrogen helium carbon oxygen later water would appear million year later creat stellar fusion point difficulti life appear temperatur scarciti free heavi element planetari system emerg first organ compound may form protoplanetari disk dust grain would eventu creat rocki planet like earth although earth molten state birth may burn organ fell would recept cool right condit earth met life start chemic process known abiogenesi altern life may form less frequent spread meteoroid exampl habit planet process call panspermia stellar evolut star combin hydrogen nuclei make helium nuclei stellar fusion compar lighter weight helium allow star releas extra energi process continu star use avail fuel speed consumpt relat size star last stage star start combin helium nuclei form carbon nuclei star combin carbon nuclei creat oxygen silicon oxygen neon sulfur iron end star blow much content back stellar medium would join cloud would eventu becom new gener star planet mani materi raw compon life earth process take place univers said materi ubiquit cosmo rariti solar system earth planet solar system planetari system form star center sun object orbit planet moon asteroid comet sun part milki way galaxi milki way part local group galaxi group turn part laniakea superclust univers compos similar structur exist immens distanc celesti object difficulti studi extraterrestri life far human set foot moon sent robot probe planet moon solar system although probe withstand condit may lethal human distanc caus time delay new horizon took nine year launch reach pluto probe ever reach extrasolar planetari system voyag left solar system speed kilomet per hour head toward alpha centauri system closest one earth light year would reach year current technolog system studi telescop limit estim dark matter larger amount combin matter star ga cloud play role stellar evolut star planet usual taken account astrobiolog area around star circumstellar habit zone goldilock zone water may right temperatur exist liquid form planetari surfac area neither close star water would becom steam far away water would frozen ice howev although use approxim planetari habit complex defin sever factor habit zone enough planet habit even actual liquid water venu locat habit zone solar system liquid water condit atmospher jovian planet ga giant consid habit even orbit close enough star hot jupit due crush atmospher pressur actual distanc habit zone vari accord type star even solar activ specif star influenc local habit type star also defin time habit zone exist presenc limit chang along star stellar evolut big bang took place billion year ago solar system form half billion year ago first hominid appear million year ago life planet may start evolv given birth extraterrestri intellig perhap even face planetari extinct event million even billion year ago brief time exist earth speci consid cosmic perspect may suggest extraterrestri life may equal fleet scale life earth quit ubiquit across planet adapt time almost avail environ extremophil deep biospher thrive even hostil one result infer life celesti bodi may equal adapt howev origin life unrel eas adapt may stricter requir celesti bodi may life even habit unclear life intellig life ubiquit cosmo rare hypothesi ubiquit extraterrestri life reli three main idea first one size univers allow plenti planet similar habit earth age univers give enough time long process analog histori earth happen second chemic element make life carbon water ubiquit univers third physic law univers mean forc would facilit prevent exist life would one earth accord argument made scientist carl sagan stephen hawk would improb life exist somewher els earth argument embodi copernican principl state earth occupi uniqu posit univers mediocr principl state noth special life earth author consid instead life cosmo least multicellular life may actual rare rare earth hypothesi maintain life earth possibl seri factor rang locat galaxi configur solar system local characterist planet unlik requir simultan met anoth planet propon hypothesi consid littl evid suggest exist extraterrestri life point desir result reason scientif explan gather data astronom astrophysicist frank drake devis drake equat way stimul scientif dialogu meet search extraterrestri intellig seti better sourc need drake equat probabilist argument use estim number activ commun extraterrestri civilis milki way galaxi drake equat drake propos estim follow number right side equat agre specul open substitut better sourc need drake equat prove controversi sinc although written math equat none valu known time although valu may eventu measur other base social scienc knowabl natur allow one make noteworthi conclus equat base observ hubbl space telescop nearli trillion galaxi observ univers estim least ten per cent star system planet star planet orbit observ univers even assum one billion star planet support life would billion planetari system observ univers studi base result kepler spacecraft estim milki way contain least mani planet star result billion exoplanet nebular hypothesi explain format solar system planetari system would suggest sever configur may rocki planet within habit zone appar contradict high estim probabl exist extraterrestri civilis lack evid civilis known fermi paradox denni sciama claim life exist univers depend variou fundament constant wang samuel braunstein suggest random univers capabl support life like bare abl give potenti explan fermi paradox extraterrestri life exist could rang simpl microorgan multicellular organ similar anim plant complex alien intellig akin human scientist talk extraterrestri life consid type although possibl extraterrestri life may configur scientist use hierarchi lifeform earth simplic one known exist first basic requir life environ thermodynam mean thermodynam equilibrium must broken sourc energi tradit sourc energi cosmo star life earth depend energi sun howev altern energi sourc volcano plate tecton hydrotherm vent ecosystem earth deep area ocean receiv sunlight take energi black smoker instead magnet field radioact also propos sourc energi although would less effici one life earth requir water liquid state solvent biochem reaction take place highli unlik abiogenesi process start within gaseou solid medium atom speed either fast slow make difficult specif one meet start chemic reaction liquid medium also allow transport nutrient substanc requir metabol suffici quantiti carbon element along water might enabl format live organ terrestri planet chemic temperatur rang similar earth life base ammonia rather water suggest altern though solvent appear less suitabl water also conceiv form life whose solvent liquid hydrocarbon methan ethan propan anoth unknown aspect potenti extraterrestri life would chemic element would compos life earth larg compos carbon could hypothet type biochemistri replac carbon would need abl creat complex molecul store inform requir evolut freeli avail medium creat dna rna close analog element abl bind atom mani other creat complex stabl molecul abl creat least three coval bond two make long string least third add new link allow divers inform nine element meet requir boron nitrogen phosphoru arsen antimoni three bond carbon silicon germanium tin four bond abund carbon nitrogen silicon abund one univers far other earth crust abund element silicon hydrospher carbon atmospher carbon nitrogen silicon howev disadvantag carbon molecul form silicon atom less stabl vulner acid oxygen light ecosystem lifeform would requir low temperatur high atmospher pressur atmospher devoid oxygen solvent water low temperatur requir would add extra problem difficulti kickstart process abiogenesi creat life first place norman horowitz head jet propuls laboratori bioscienc section marin vike mission consid great versatil carbon atom make element like provid solut even exot solut problem surviv life planet howev also consid condit found mar incompat carbon base life even extraterrestri life base carbon use water solvent like earth life may still radic differ biochemistri life gener consid product natur select propos undergo natur select live entiti must capac replic capac avoid capac acquir process resourc support first two capac life earth start rna world later evolv current form rna task transfer dna protein extraterrestri life may still stuck use rna evolv configur unclear biochemistri effici one could gener element would follow similar pattern howev like even cell differ composit earth would still cell membran life earth jump prokaryot eukaryot unicellular organ multicellular organ evolut far altern process achiev result conceiv even hypothet evolut requir life divid individu organ altern organis satisfactorili propos either basic level membran defin limit cell environ remain partial open exchang energi resourc evolut simpl cell eukaryot multicellular lifeform guarante cambrian explos took place thousand million year origin life caus fulli known yet hand jump multicellular took place sever time suggest could case converg evolut like take place planet well palaeontologist simon conway morri consid converg evolut would lead kingdom similar plant anim mani featur like develop alien anim well bilater symmetri limb digest system head sensori organ scientist univers oxford analys perspect evolutionari theori wrote studi intern journal astrobiolog alien may similar human planetari context would also influenc planet higher graviti would smaller anim type star lead photosynthes amount energi avail would also affect biodivers ecosystem sustain black smoker hydrotherm vent would less energi avail sustain star light heat lifeform would grow beyond certain complex also research assess capac life develop intellig suggest capac aris number potenti nich planet contain complex life reflect inform densiti planetari environ turn comput nich common knowledg condit planet solar system addit mani galaxi outsid milki way galaxi harsh seem extrem harbor life environment condit planet intens uv radiat pair extrem temperatur lack water much lead condit seem favor creation mainten extraterrestri life howev much histor evid earliest basic form life earth origin extrem environ seem unlik harbor life least one point earth histori fossil evid well mani histor theori back year research studi mark environ like hydrotherm vent acid hot spring first place life could origin earth environ consid extrem compar typic ecosystem major life earth inhabit hydrotherm vent scorch hot due magma escap earth mantl meet much colder ocean water even today world divers popul bacteria found inhabit area surround hydrotherm vent suggest form life support even harshest environ like planet solar system aspect harsh environ make ideal origin life earth well possibl creation life planet chemic reaction form spontan exampl hydrotherm vent found ocean floor known support mani chemosynthet process allow organ util energi reduc chemic compound fix carbon return reaction allow organ live rel low oxygen environ maintain enough energi support earli earth environ reduc therefor carbon fix compound necessari surviv possibl origin life earth littl amount inform scientist found regard atmospher planet milki way galaxi beyond atmospher like reduc low oxygen level especi compar earth atmospher necessari element ion planet carbon fix reduc chemic compound occur around hydrotherm vent could also occur planet surfac possibl result origin extraterrestri life solar system wide varieti planet dwarf planet moon one studi potenti host life one specif condit may benefit harm life far lifeform found earth extraterrestri intellig human exist ever exist within solar system astrobiologist mari voytek point would unlik find larg ecosystem would alreadi detect inner solar system like devoid life howev venu still interest astrobiologist terrestri planet like similar earth earli stage develop differ way greenhous effect surfac hottest solar system sulfur acid cloud surfac liquid water lost thick atmospher huge pressur compar help understand precis differ lead benefici harm condit life despit condit life venu suspicion microbi may still surviv cloud mar cold almost airless desert inhospit life howev recent studi reveal water mar use quit abund form river lake perhap even ocean mar may habit back life mar may possibl planetari core ceas gener magnet field solar wind remov atmospher planet becam vulner solar radiat ancient may still left fossilis remain microb may still surviv deep underground mention ga giant ice giant unlik contain life distant solar system bodi found kuiper belt outward lock perman rule complet although giant planet highli unlik life much hope find moon orbit planet europa jovian system subsurfac ocean thick layer ice ganymed callisto also subsurfac ocean life less like water sandwich layer solid ice europa would contact ocean rocki surfac help chemic reaction may difficult dig deep order studi ocean though enceladu tini moon saturn anoth subsurfac ocean may need dug releas water space erupt column space probe cassini flew insid one could make full studi nasa expect phenomenon equip probe studi ocean water still cassini detect complex organ molecul salt evid hydrotherm activ hydrogen methan titan celesti bodi solar system besid earth liquid bodi surfac river lake rain hydrocarbon methan ethan even cycl similar earth water cycl special context encourag specul lifeform differ biochemistri cold temperatur would make chemistri take place slow pace water surfac titan subsurfac water ocean like sever moon howev great depth would difficult access studi scienc search studi life univers earth elsewher call astrobiolog studi earth life known form life astrobiolog seek studi life start evolv requir continu exist help determin look search life celesti bodi complex area studi use combin perspect sever scientif disciplin astronomi biolog chemistri geolog oceanographi atmospher scienc scientif search extraterrestri life carri directli indirectli septemb updat exoplanet system identifi planet moon solar system hold potenti host primit life microorgan februari updat statu studi consid possibl detect lifeform venu via phosphin mar via methan report scientist search biosignatur within solar system studi planetari surfac examin meteorit claim identifi evid microbi life exist mar controversi report state structur resembl nanobacteria discov meteorit form rock eject mar although unusu properti meteorit eventu explain result inorgan process controversi discoveri laid groundwork develop astrobiolog experi two vike mar lander report ga emiss heat martian soil sampl scientist argu consist presenc live microorgan lack corrobor evid experi sampl suggest reaction like hypothesi februari nasa scientist report may found evid extraterrestri life mar two scientist carol stoker larri lemk nasa ame research center base claim methan signatur found mar atmospher resembl methan product form primit life earth well studi primit life near rio tinto river spain nasa offici soon distanc nasa scientist claim stoker back initi assert novemb nasa launch mar scienc laboratori land curios rover mar design assess past present habit mar use varieti scientif instrument rover land mar gale crater august group scientist cornel univers start catalog microorgan way one react sunlight goal help search similar organ exoplanet starlight reflect planet rich organ would specif spectrum unlik starlight reflect lifeless planet earth studi afar system would reveal shade green result abund plant photosynthesi august nasa studi meteorit found antarctica find adenin guanin hypoxanthin xanthin adenin guanin compon dna other use biolog process studi rule pollut meteorit earth compon would freeli avail way found sampl discoveri suggest sever organ molecul serv build block life may gener within asteroid comet octob scientist report cosmic dust contain complex organ compound amorph organ solid mix structur could creat natur rapidli star still unclear compound play role creation life earth sun kwok univers hong kong think case life earth may easier time get start organ serv basic ingredi life august world first astronom copenhagen univers report detect specif sugar molecul glycolaldehyd distant star system molecul found around protostellar binari ira locat light year earth glycolaldehyd need form ribonucl acid rna similar function dna find suggest complex organ molecul may form stellar system prior format planet eventu arriv young planet earli format decemb astronom report first time discoveri plume enceladu moon planet saturn hydrogen cyanid possibl chemic essenti life know well organ molecul yet better identifi understood accord research newli discov compound could potenti support extant microbi commun drive complex organ synthesi lead origin life although search focus biolog extraterrestri life extraterrestri intellig capabl enough develop civil may detect mean well technolog may gener technosignatur effect nativ planet may caus natur caus three main type consid interstellar commun effect atmospher structur dyson sphere organ seti institut search cosmo potenti form commun start radio wave search laser puls well challeng search natur sourc signal well burst supernova differ natur signal artifici one would specif pattern astronom intend use artifici intellig manag larg amount data devoid bias preconcept besid even advanc extraterrestri civil guarante transmit radio commun direct earth length time requir signal travel across space mean potenti answer may arriv decad centuri initi messag atmospher earth rich nitrogen dioxid result air pollut detect natur abund carbon also rel reactiv make like basic compon develop potenti extraterrestri technolog civil earth fossil fuel may like gener use world well abund chlorofluorocarbon atmospher also clear technosignatur consid role ozon deplet light pollut may anoth technosignatur multipl light night side rocki planet sign advanc technolog develop howev modern telescop strong enough studi exoplanet requir level detail perceiv kardashev scale propos civil may eventu start consum energi directli local star would requir giant structur built next call dyson sphere specul structur would caus excess infrar radiat telescop may notic infrar radiat typic young star surround dusti protoplanetari disk eventu form planet older star sun would natur reason excess infrar radiat presenc heavi element star anoth potenti biosignatur element would theori found star use nuclear wast product astronom search extrasolar planet may conduc life narrow search terrestri planet within habit zone star sinc four thousand exoplanet discov planet planetari system includ multipl planetari system juli extrasolar planet far discov rang size terrestri planet similar earth size ga giant larger jupit number observ exoplanet expect increas greatli come year better sourc need kepler space telescop also detect thousand candid planet may fals posit least one planet averag per star star b planet habit zone c nearest expect within distanc earth assum billion star milki way would billion potenti habit planet milki way rise billion red dwarf includ rogu planet milki way possibl number trillion nearest known exoplanet proxima centauri b locat pc earth southern constel centauru march updat least massiv exoplanet known psr twice mass moon massiv planet list nasa exoplanet archiv b time mass jupit although accord definit planet massiv planet may brown dwarf instead almost planet detect far within milki way also possibl detect extragalact planet studi planetari habit also consid wide rang factor determin suitabl planet host life one sign planet probabl alreadi contain life presenc atmospher signific amount oxygen sinc ga highli reactiv gener would last long without constant replenish replenish occur earth photosynthet organ one way analys atmospher exoplanet spectrographi transit star though might feasibl dim star like white dwarf modern concept extraterrestri life base assumpt commonplac earli day astronomi first explan celesti object seen night sky base mytholog scholar ancient greec first consid univers inher understand reject explan base supernatur incomprehens forc myth sun pull across sky chariot apollo develop scientif method yet base idea pure thought specul develop precursor idea explan discard contradict observ fact discuss greek scholar establish mani pillar would eventu lead idea extraterrestri life earth round flat cosmo first structur geocentr model consid sun celesti bodi revolv around earth howev consid world greek understand world compos earth celesti object notic movement anaximand thought cosmo made apeiron substanc creat world world would eventu return cosmo eventu two group emerg atomist thought matter earth cosmo equal made small atom classic element earth water fire air aristotelian thought element exclus earth cosmo made fifth one aether atomist epicuru thought process creat world anim plant creat world elsewher along anim plant aristotl thought instead earth element natur fell toward center univers would made imposs planet exist elsewher reason earth center also planet univers cosmic plural plural world simpli plural describ philosoph belief numer world addit earth might harbor extraterrestri life earliest record assert extraterrestri human life found ancient scriptur jainism multipl world mention jain scriptur support human life includ among other bharat kshetra mahavideh kshetra airavat kshetra hari kshetra mediev muslim writer like fakhr muhammad support cosmic plural basi chaucer poem hous fame engag mediev thought experi postul plural world howev idea world differ current knowledg structur univers postul exist planetari system solar system author talk world talk place locat center system stellar vault cosmo surround greek idea disput atomist aristotelian outliv fall greek empir great librari alexandria compil inform part translat islam scholar thu surviv end librari baghdad combin knowledg greek indian chines scholar knowledg expand byzantin empir eventu return europ time middl age howev greek atomist doctrin held world creat random movement atom need creator deiti becam associ atheism disput intertwin religi one still church react topic homogen way stricter permiss view within church first known mention term write bc greek philosoph anaxagora propos idea life exist everywher time late middl age mani known inaccuraci geocentr model kept use nake eye observ provid limit data nicolau copernicu start copernican revolut propos planet revolv around sun rather earth propos littl accept first kept assumpt orbit perfect circl model led mani inaccuraci geocentr one tycho brahe improv avail data observatori work highli complex sextant quadrant tycho could make sens observ johann kepler orbit perfect circl ellips knowledg benefit copernican model work almost perfectli invent telescop short time later perfect galileo galilei clarifi final doubt paradigm shift complet new understand notion extraterrestri life becam feasibl earth planet orbit around star may planet similar earth elsewher astronom studi distant bodi also prove physic law elsewher univers earth noth make planet truli special new idea met resist cathol church galileo tri heliocentr model consid heret forc recant propon idea extraterrestri life italian philosoph giordano bruno argu centuri infinit univers everi star surround planetari system bruno wrote world less virtu natur differ earth like earth contain anim inhabit bruno belief plural world one charg level venetian holi inquisit trial execut heliocentr model strengthen postul theori graviti sir isaac newton theori provid mathemat explain motion thing univers includ planetari orbit point geocentr model definit discard time use scientif method becom standard new discoveri expect provid evid rigor mathemat explan scienc also took deeper interest mechan natur phenomena tri explain way natur work also reason work way littl actual discuss extraterrestri life point aristotlean idea remain influenti geocentr still accept final prove wrong meant earth center univers also light seen sky light physic object notion life may exist well soon becam ongo topic discuss although one practic way investig possibl extraterrestri remain widespread specul scientif discoveri acceler william herschel discover uranu one mani astronom believ solar system popul alien life scholar period champion cosmic plural includ immanuel kant benjamin franklin height enlighten even sun moon consid candid extraterrestri inhabit specul life mar increas late centuri follow telescop observ appar martian canal soon howev turn optic illus despit american astronom perciv lowel publish book mar follow mar canal propos canal work civilis spectroscop analysi mar atmospher began earnest astronom william wallac campbel show neither water oxygen present martian atmospher better telescop best perihel opposit mar sinc conclus put end canal hypothesi consequ belief spontan gener littl thought condit celesti bodi simpli assum life would thrive anywher theori disprov loui pasteur centuri popular belief thrive alien civilis elsewher solar system still remain strong marin marin provid close imag mar debunk forev idea exist martian decreas previou expect find alien life gener end spontan gener belief forc investig origin life although abiogenesi accept theori number author reclaim term panspermia propos life brought earth elsewher author jön jacob berzeliu kelvin hermann von helmholtz somewhat later svant arrheniu scienc fiction genr although name time develop late centuri expans genr extraterrestri fiction influenc popular percept topic make peopl eager jump conclus discoveri alien scienc march slower pace discoveri fuel expect other dash excess hope exampl advent telescop structur seen moon mar immedi attribut selenit martian later one power telescop reveal discoveri natur featur famou case cydonia region mar first imag vike orbit photo show rock format resembl human face later spacecraft took photo higher detail show noth special site search studi extraterrestri life becam scienc astrobiolog also known exobiolog disciplin studi nasa esa inaf other astrobiolog studi life earth well cosmic perspect exampl abiogenesi interest astrobiolog origin life earth chanc similar process take place celesti bodi mani aspect life definit chemistri analyz either like similar form life across cosmo nativ earth astrobiolog howev remain constrain current lack extraterrestri studi life earth come ancestor hard infer gener characterist group singl exampl analys centuri came great technolog advanc specul futur hypothet technolog increas basic knowledg scienc gener popul thank scienc divulg mass media public interest extraterrestri life lack discoveri mainstream scienc led emerg pseudosci provid affirm question answer exist alien ufolog claim mani unidentifi fli object ufo would spaceship alien speci ancient astronaut hypothesi claim alien would visit earth antiqu prehistor time peopl would fail understand ufo ufo sight readili explain sight aircraft includ aircraft known astronom object weather phenomenon hoax look beyond pseudosci lewi white beck strove elev level public discours topic extraterrestri life trace evolut philosoph thought centuri ancient time modern era review contribut made lucretiu plutarch aristotl copernicu immanuel kant john wilkin charl darwin karl marx demonstr even modern time human could profoundli influenc search extraterrestri life subtl comfort archetyp idea larg deriv firmli held religi philosoph existenti belief system posit note howev beck argu even search extraterrestri life prove unsuccess endeavor could benefici consequ assist human attempt actual superior way live earth centuri accept multicellular life solar system exist earth interest extraterrestri life increas regardless result advanc sever scienc knowledg planetari habit allow consid scientif term likelihood find life specif celesti bodi known featur benefici harm life astronomi telescop also improv point exoplanet confirm even studi increas number search place life may still exist elsewher solar system unicellular form advanc spacecraft allow send robot studi sampl situ tool grow complex reliabl although extraterrestri life found life may still rariti earth scientif reason suspect exist elsewher technolog advanc may detect mani scientist optimist chanc find alien life word seti frank drake know sure sky litter power microwav transmitt drake note entir possibl advanc technolog result commun carri way convent radio transmiss time data return space probe giant stride detect method allow scienc begin delin habit criteria world confirm least planet plenti though alien remain question mark wow signal detect seti project remain subject specul debat hand scientist pessimist jacqu monod wrote man know last alon indiffer immens univers whenc emerg chanc geologist paleontologist peter ward astrobiologist donald brownle publish book entitl rare earth complex life uncommon univers better sourc need discuss rare earth hypothesi claim life rare univers wherea microbi life common ward brownle open idea evolut planet base essenti characterist dna carbon possibl risk theoret physicist stephen hawk warn human tri contact alien life form warn alien might pillag earth resourc alien visit us outcom would much columbu land america turn well nativ american said jare diamond earlier express similar concern juli hawk russian billionair yuri milner along seti institut announc effort call breakthrough initi expand effort search extraterrestri life group contract servic robert byrd green bank telescop west virginia unit state park telescop new south wale australia februari scientist includ geoffrey marci seth shostak frank drake david brin convent american associ advanc scienc discuss activ seti whether transmit messag possibl intellig extraterrestri cosmo good idea one result statement sign mani worldwid scientif polit humanitarian discuss must occur messag sent outer space treati moon agreement defin rule planetari protect potenti hazard extraterrestri life cospar also provid guidelin planetari protect committe unit nation offic outer space affair discuss year strategi interact extraterrestri life intellig discuss end without conclus un lack respons mechan case extraterrestri contact one nasa divis offic safeti mission assur osma also known planetari protect offic part mission rigor preclud backward contamin earth extraterrestri life chines govern releas white paper detail space program accord document one research object program search extraterrestri life also one object chines apertur spheric telescop fast program dmitri rogozin head russian space agenc said search extraterrestri life one main goal deep space research also acknowledg possibl exist primit life planet solar system french space agenc offic studi aero spatial phenomena agenc maintain publicli access databas phenomena detail entri accord head offic vast major entri mundan explan entri extraterrestri origin neither confirm deni chairman israel space agenc isaac state probabl detect life outer space quit larg disagre former colleagu haim esh state contact advanc alien civilis earth govern although idea extraterrestri peopl becam feasibl astronomi develop enough understand natur planet thought differ human scientif explan origin mankind relat speci reason expect way chang book origin speci charl darwin propos theori evolut notion evolut planet may take direct scienc fiction author creat bizarr alien clearli distinct human usual way add bodi featur anim insect octopus costum special effect feasibl alongsid budget consider forc film tv seri tone fantasi limit lessen sinc advent imageri cgi later cgi becam effect less expens event sometim captiv peopl imagin influenc work fiction exampl barney betti hill incid first record claim alien abduct coupl report abduct experi alien overs head big eye pale grey skin small nose descript eventu becam grey alien archetyp use work fiction
Extraterrestrial intelligence,https://en.wikipedia.org/wiki/Extraterrestrial_intelligence,"Extraterrestrial intelligence (ETI) refers to hypothetical intelligent extraterrestrial life. No such life has ever been verifiably observed to exist.[1] The question of whether other inhabited worlds might exist has been debated since ancient times.[2] The modern form of the concept emerged when the Copernican Revolution demonstrated that the Earth was a planet revolving around the Sun, and other planets were, conversely, other worlds.[3] The question of whether other inhabited planets or moons exist was a natural consequence of this new understanding. It has become one of the most speculative questions in science and is a central theme of science fiction and popular culture.[4]
 An alternative name for it is ""Extraterrestrial Technological Instantiations"" (ETI). The term was coined to avoid the use of terms such as ""civilizations"" ""species"" and ""intelligence"", as those may prove to be ambiguous and open to interpretation, or simply inapplicable in its local context.[5]
 Intelligence is, along with the more precise concept of sapience, used to describe extraterrestrial life with similar cognitive abilities as humans. Another interchangeable term is sophoncy, being wise or wiser, first coined by Karen Anderson and published in the 1966 works by her husband Poul Anderson.
 Sentience, like consciousness, is a concept sometimes mistakenly used to refer to the concept of intelligence and sapience, since it does not exclude forms of life that are non-sapient (or more broadly non-intelligent or non-conscious).[6]
 The term extraterrestrial civilization frames a more particular case of extraterrestrial intelligence. It is the possible long-term result of intelligent and specifically sapient extraterrestrial life.
 The Copernican principle is generalized to the relativistic concept that humans are not privileged observers of the universe.[7]  Many prominent scientists, including Stephen Hawking[8] have proposed that the sheer scale of the universe makes it improbable for intelligent life not to have emerged elsewhere.  However, Fermi's Paradox highlights the apparent contradiction between high estimates of the probability of the existence of extraterrestrial civilization and humanity's lack of contact with, or evidence for, such civilizations.[9]
 So far, there is no observation of extraterrestrial life, including intelligent extraterrestrial life.[10]
 The Kardashev scale is a speculative method of measuring a civilization's level of technological advancement, based on the amount of energy a civilization is able to utilize.[11]
 The Drake equation is a probabilistic framework used to estimate the number of active, communicative extraterrestrial civilizations in the Milky Way galaxy.[12]
 There has been a search for signals from extraterrestrial intelligence for several decades, with no significant results.[13] Active SETI (Active Search for Extra-Terrestrial Intelligence) is the attempt to send messages to intelligent extraterrestrial life. Active SETI messages are usually sent in the form of radio signals. Physical messages like that of the Pioneer plaque may also be considered an active SETI message.
 Communication with extraterrestrial intelligence (CETI) is a branch of the search for extraterrestrial intelligence that focuses on composing and deciphering messages that could theoretically be understood by another technological civilization. The best-known CETI experiment was the 1974 Arecibo message composed by Frank Drake and Carl Sagan. There are multiple independent organizations and individuals engaged in CETI research.
 The U.S. government's position, in line with that of most relevant experts, is that ""chances of contact with an extraterrestrial intelligence are extremely small, given the distances involved.""[14][15] This line of thinking has led some to conclude that first contact might be made with extraterrestrial artificial intelligence, rather than with biological beings.[16][17][18]
 The Wow! signal remains the best candidate for an extraterrestrial radio signal ever detected, though the fact that no similar signal has ever been observed again makes attribution of the signal to any cause difficult if not impossible.[19]
 On 14 June 2022 astronomers working with China's FAST telescope reported the possibility of having detected artificial (presumably alien) signals, but cautions that further studies are required to determine if some kind of natural radio interference may be the source.[20] On 18 June 2022 Dan Werthimer, chief scientist for several SETI-related projects, reportedly noted that “These signals are from radio interference; they are due to radio pollution from earthlings, not from E.T.”[21]
 The potential changes from extraterrestrial contact could vary greatly in magnitude and type, based on the extraterrestrial civilization's level of technological advancement, degree of benevolence or malevolence, and level of mutual comprehension between itself and humanity.[22] Some theories suggest that an extraterrestrial civilization could be advanced enough to dispense with biology, living instead inside of advanced computers.[22] The medium through which humanity is contacted, be it electromagnetic radiation, direct physical interaction, extraterrestrial artefact, or otherwise, may also influence the results of contact. Incorporating these factors, various systems have been created to assess the implications of extraterrestrial contact.
 The implications of extraterrestrial contact, particularly with a technologically superior civilization, have often been likened to the meeting of two vastly different human cultures on Earth, a historical precedent being the Columbian Exchange. Such meetings have generally led to the destruction of the civilization receiving contact (as opposed to the ""contactor"", which initiates contact), and therefore destruction of human civilization is a possible outcome.[23] However, the absence of any such contact to date means such conjecture is largely speculative.
 The extraterrestrial hypothesis is the idea that some UFOs are vehicles containing or sent by extraterrestrial beings (usually called aliens in this context).[13] As an explanation for UFOs, ETI is sometimes contrasted with EDI (extradimensional intelligence), for example by J. Allen Hynek.[24] In 2023, House lawmakers held a hearing  to examine how the executive branch handles reports of UFOs.
 The theories and reception of the probability of intelligent life has been a recurring cultural element, particularly of popular culture since the prospect and achievement of spaceflight.
New Mexico has even declared in 2003 the 14th of February as the Extraterrestrial Culture Day.[25]
",extraterrestri intellig eti refer hypothet intellig extraterrestri life life ever verifi observ exist question whether inhabit world might exist debat sinc ancient time modern form concept emerg copernican revolut demonstr earth planet revolv around sun planet convers world question whether inhabit planet moon exist natur consequ new understand becom one specul question scienc central theme scienc fiction popular cultur altern name extraterrestri technolog instanti eti term coin avoid use term civil speci intellig may prove ambigu open interpret simpli inapplic local context intellig along precis concept sapienc use describ extraterrestri life similar cognit abil human anoth interchang term sophonci wise wiser first coin karen anderson publish work husband poul anderson sentienc like conscious concept sometim mistakenli use refer concept intellig sapienc sinc exclud form life broadli term extraterrestri civil frame particular case extraterrestri intellig possibl result intellig specif sapient extraterrestri life copernican principl gener relativist concept human privileg observ univers mani promin scientist includ stephen hawk propos sheer scale univers make improb intellig life emerg elsewher howev fermi paradox highlight appar contradict high estim probabl exist extraterrestri civil human lack contact evid civil far observ extraterrestri life includ intellig extraterrestri life kardashev scale specul method measur civil level technolog advanc base amount energi civil abl util drake equat probabilist framework use estim number activ commun extraterrestri civil milki way galaxi search signal extraterrestri intellig sever decad signific result activ seti activ search intellig attempt send messag intellig extraterrestri life activ seti messag usual sent form radio signal physic messag like pioneer plaqu may also consid activ seti messag commun extraterrestri intellig ceti branch search extraterrestri intellig focus compos deciph messag could theoret understood anoth technolog civil ceti experi arecibo messag compos frank drake carl sagan multipl independ organ individu engag ceti research govern posit line relev expert chanc contact extraterrestri intellig extrem small given distanc involv line think led conclud first contact might made extraterrestri artifici intellig rather biolog be wow signal remain best candid extraterrestri radio signal ever detect though fact similar signal ever observ make attribut signal caus difficult imposs june astronom work china fast telescop report possibl detect artifici presum alien signal caution studi requir determin kind natur radio interfer may sourc june dan werthim chief scientist sever project reportedli note signal radio interfer due radio pollut earthl potenti chang extraterrestri contact could vari greatli magnitud type base extraterrestri civil level technolog advanc degre benevol malevol level mutual comprehens human theori suggest extraterrestri civil could advanc enough dispens biolog live instead insid advanc comput medium human contact electromagnet radiat direct physic interact extraterrestri artefact otherwis may also influenc result contact incorpor factor variou system creat assess implic extraterrestri contact implic extraterrestri contact particularli technolog superior civil often liken meet two vastli differ human cultur earth histor preced columbian exchang meet gener led destruct civil receiv contact oppos contactor initi contact therefor destruct human civil possibl outcom howev absenc contact date mean conjectur larg specul extraterrestri hypothesi idea ufo vehicl contain sent extraterrestri be usual call alien context explan ufo eti sometim contrast edi extradimension intellig exampl allen hynek hous lawmak held hear examin execut branch handl report ufo theori recept probabl intellig life recur cultur element particularli popular cultur sinc prospect achiev spaceflight new mexico even declar februari extraterrestri cultur day
Life,https://en.wikipedia.org/wiki/Life,"
 Life on Earth:
 Life is a quality that distinguishes matter that has biological processes, such as signaling and self-sustaining processes, from matter that does not. It is defined descriptively by the capacity for homeostasis, organisation, metabolism, growth, adaptation, response to stimuli, and reproduction. All life over time eventually reaches a state of death, and none is immortal. Many philosophical definitions of living systems have been proposed, such as self-organizing systems. Viruses in particular make definition difficult as they replicate only in host cells. Life exists all over the Earth in air, water, and soil, with many ecosystems forming the biosphere. Some of these are harsh environments occupied only by extremophiles.
 Life has been studied since ancient times, with theories such as Empedocles's materialism asserting that it was composed of four eternal elements, and Aristotle's hylomorphism asserting that living things have souls and embody both form and matter. Life originated at least 3.5 billion years ago, resulting in a universal common ancestor. This evolved into all the species that exist now, by way of many extinct species, some of which have left traces as fossils. Attempts to classify living things, too, began with Aristotle. Modern classification began with Carl Linnaeus's system of binomial nomenclature in the 1740s.
 Living things are composed of biochemical molecules, formed mainly from a few core chemical elements. All living things contain two types of large molecule, proteins and nucleic acids, the latter usually both DNA and RNA: these carry the information needed by each species, including the instructions to make each type of protein. The proteins, in turn, serve as the machinery which carries out the many chemical processes of life. The cell is the structural and functional unit of life. Smaller organisms, including prokaryotes (bacteria and archaea), consist of small single cells. Larger organisms, mainly eukaryotes, can consist of single cells or may be multicellular with more complex structure. Life is only known to exist on Earth but extraterrestrial life is thought probable. Artificial life is being simulated and explored by scientists and engineers.
 
 The definition of life has long been a challenge for scientists and philosophers.[2][3][4] This is partially because life is a process, not a substance.[5][6][7] This is complicated by a lack of knowledge of the characteristics of living entities, if any, that may have developed outside Earth.[8][9] Philosophical definitions of life have also been put forward, with similar difficulties on how to distinguish living things from the non-living.[10] Legal definitions of life have been debated, though these generally focus on the decision to declare a human dead, and the legal ramifications of this decision.[11] At least 123 definitions of life have been compiled.[12]
 Since there is no consensus for a definition of life, most current definitions in biology are descriptive. Life is considered a characteristic of something that preserves, furthers or reinforces its existence in the given environment. This implies all or most of the following traits:[4][13][14][15][16][17]
 From a physics perspective, an organism is a thermodynamic system with an organised molecular structure that can reproduce itself and evolve as survival dictates.[21][22] Thermodynamically, life has been described as an open system which makes use of gradients in its surroundings to create imperfect copies of itself.[23] Another way of putting this is to define life as ""a self-sustained chemical system capable of undergoing Darwinian evolution"", a definition adopted by a NASA committee attempting to define life for the purposes of exobiology, based on a suggestion by Carl Sagan.[24][25] This definition, however, has been widely criticised because according to it, a single sexually reproducing individual is not alive as it is incapable of evolving on its own.[26]
 Others take a living systems theory viewpoint that does not necessarily depend on molecular chemistry. One systemic definition of life is that living things are self-organizing and autopoietic (self-producing). Variations of this include Stuart Kauffman's definition as an autonomous agent or a multi-agent system capable of reproducing itself, and of completing at least one thermodynamic work cycle.[27] This definition is extended by the evolution of novel functions over time.[28]
 Death is the termination of all vital functions or life processes in an organism or cell.[29][30]
One of the challenges in defining death is in distinguishing it from life. Death would seem to refer to either the moment life ends, or when the state that follows life begins.[30] However, determining when death has occurred is difficult, as cessation of life functions is often not simultaneous across organ systems.[31] Such determination, therefore, requires drawing conceptual lines between life and death. This is problematic because there is little consensus over how to define life. The nature of death has for millennia been a central concern of the world's religious traditions and of philosophical inquiry. Many religions maintain faith in either a kind of afterlife or reincarnation for the soul, or resurrection of the body at a later date.[32]
 Whether or not viruses should be considered as alive is controversial.[33][34] They are most often considered as just gene coding replicators rather than forms of life.[35] They have been described as ""organisms at the edge of life""[36] because they possess genes, evolve by natural selection,[37][38] and replicate by making multiple copies of themselves through self-assembly. However, viruses do not metabolise and they require a host cell to make new products. Virus self-assembly within host cells has implications for the study of the origin of life, as it may support the hypothesis that life could have started as self-assembling organic molecules.[39][40]
 Some of the earliest theories of life were materialist, holding that all that exists is matter, and that life is merely a complex form or arrangement of matter. Empedocles (430 BC) argued that everything in the universe is made up of a combination of four eternal ""elements"" or ""roots of all"": earth, water, air, and fire. All change is explained by the arrangement and rearrangement of these four elements. The various forms of life are caused by an appropriate mixture of elements.[41]
Democritus (460 BC) was an atomist; he thought that the essential characteristic of life was having a soul (psyche), and that the soul, like everything else, was composed of fiery atoms. He elaborated on fire because of the apparent connection between life and heat, and because fire moves.[42]
Plato, in contrast, held that the world was organised by permanent forms, reflected imperfectly in matter; forms provided direction or intelligence, explaining the regularities observed in the world.[43] The mechanistic materialism that originated in ancient Greece was revived and revised by the French philosopher René Descartes (1596–1650), who held that animals and humans were assemblages of parts that together functioned as a machine. This idea was developed further by Julien Offray de La Mettrie (1709–1750) in his book L'Homme Machine.[44] In the 19th century the advances in cell theory in biological science encouraged this view. The evolutionary theory of Charles Darwin (1859) is a mechanistic explanation for the origin of species by means of natural selection.[45] At the beginning of the 20th century Stéphane Leduc (1853–1939) promoted the idea that biological processes could be understood in terms of physics and chemistry, and that their growth resembled that of inorganic crystals immersed in solutions of sodium silicate. His ideas, set out in his book La biologie synthétique,[46] were widely dismissed during his lifetime, but has incurred a resurgence of interest in the work of Russell, Barge and colleagues.[47]
 Hylomorphism is a theory first expressed by the Greek philosopher Aristotle (322 BC). The application of hylomorphism to biology was important to Aristotle, and biology is extensively covered in his extant writings. In this view, everything in the material universe has both matter and form, and the form of a living thing is its soul (Greek psyche, Latin anima). There are three kinds of souls: the vegetative soul of plants, which causes them to grow and decay and nourish themselves, but does not cause motion and sensation; the animal soul, which causes animals to move and feel; and the rational soul, which is the source of consciousness and reasoning, which (Aristotle believed) is found only in man.[48] Each higher soul has all of the attributes of the lower ones. Aristotle believed that while matter can exist without form, form cannot exist without matter, and that therefore the soul cannot exist without the body.[49]
 This account is consistent with teleological explanations of life, which account for phenomena in terms of purpose or goal-directedness. Thus, the whiteness of the polar bear's coat is explained by its purpose of camouflage. The direction of causality (from the future to the past) is in contradiction with the scientific evidence for natural selection, which explains the consequence in terms of a prior cause. Biological features are explained not by looking at future optimal results, but by looking at the past evolutionary history of a species, which led to the natural selection of the features in question.[50]
 Spontaneous generation was the belief that living organisms can form without descent from similar organisms. Typically, the idea was that certain forms such as fleas could arise from inanimate matter such as dust or the supposed seasonal generation of mice and insects from mud or garbage.[51]
 The theory of spontaneous generation was proposed by Aristotle,[52] who compiled and expanded the work of prior natural philosophers and the various ancient explanations of the appearance of organisms; it was considered the best explanation for two millennia. It was decisively dispelled by the experiments of Louis Pasteur in 1859, who expanded upon the investigations of predecessors such as Francesco Redi.[53][54] Disproof of the traditional ideas of spontaneous generation is no longer controversial among biologists.[55][56][57]
 Vitalism is the belief that there is a non-material life-principle. This originated with Georg Ernst Stahl (17th century), and remained popular until the middle of the 19th century. It appealed to philosophers such as Henri Bergson, Friedrich Nietzsche, and Wilhelm Dilthey,[58] anatomists like Xavier Bichat, and chemists like Justus von Liebig.[59] Vitalism included the idea that there was a fundamental difference between organic and inorganic material, and the belief that organic material can only be derived from living things. This was disproved in 1828, when Friedrich Wöhler prepared urea from inorganic materials.[60] This Wöhler synthesis is considered the starting point of modern organic chemistry. It is of historical significance because for the first time an organic compound was produced in inorganic reactions.[59]
 During the 1850s Hermann von Helmholtz, anticipated by Julius Robert von Mayer, demonstrated that no energy is lost in muscle movement, suggesting that there were no ""vital forces"" necessary to move a muscle.[61] These results led to the abandonment of scientific interest in vitalistic theories, especially after Eduard Buchner's demonstration that alcoholic fermentation could occur in cell-free extracts of yeast.[62] Nonetheless, belief still exists in pseudoscientific theories such as homoeopathy, which interprets diseases and sickness as caused by disturbances in a hypothetical vital force or life force.[63]
 The age of Earth is about 4.54 billion years.[64] Life on Earth has existed for at least 3.5 billion years,[65][66][67][68] with the oldest physical traces of life dating back 3.7 billion years.[69][70] Estimates from molecular clocks, as summarised in the TimeTree public database, place the origin of life around 4.0 billion years ago.[71] Hypotheses on the origin of life attempt to explain the formation of a universal common ancestor from simple organic molecules via pre-cellular life to protocells and metabolism.[72] In 2016, a set of 355 genes from the last universal common ancestor was tentatively identified.[73]
 The biosphere is postulated to have developed, from the origin of life onwards, at least some 3.5 billion years ago.[74] The earliest evidence for life on Earth includes biogenic graphite found in 3.7 billion-year-old metasedimentary rocks from Western Greenland[69] and microbial mat fossils found in 3.48 billion-year-old sandstone from Western Australia.[70] More recently, in 2015, ""remains of biotic life"" were found in 4.1 billion-year-old rocks in Western Australia.[65] In 2017, putative fossilised microorganisms (or microfossils) were announced to have been discovered in hydrothermal vent precipitates in the Nuvvuagittuq Belt of Quebec, Canada that were as old as 4.28 billion years, the oldest record of life on Earth, suggesting ""an almost instantaneous emergence of life"" after ocean formation 4.4 billion years ago, and not long after the formation of the Earth 4.54 billion years ago.[75]
 Evolution is the change in heritable characteristics of biological populations over successive generations. It results in the appearance of new species and often the disappearance of old ones.[76][77] Evolution occurs when evolutionary processes such as natural selection (including sexual selection) and genetic drift act on genetic variation, resulting in certain characteristics increasing or decreasing in frequency within a population over successive generations.[78] The process of evolution has given rise to biodiversity at every level of biological organisation.[79][80]
 Fossils are the preserved remains or traces of organisms from the remote past. The totality of fossils, both discovered and undiscovered, and their placement in layers (strata) of sedimentary rock is known as the fossil record. A preserved specimen is called a fossil if it is older than the arbitrary date of 10,000 years ago.[81] Hence, fossils range in age from the youngest at the start of the Holocene Epoch to the oldest from the Archaean Eon, up to 3.4 billion years old.[82][83]
 Extinction is the process by which a species dies out.[84] The moment of extinction is the death of the last individual of that species. Because a species' potential range may be very large, determining this moment is difficult, and is usually done retrospectively after a period of apparent absence. Species become extinct when they are no longer able to survive in changing habitat or against superior competition. Over 99% of all the species that have ever lived are now extinct.[85][86][87][88] Mass extinctions may have accelerated evolution by providing opportunities for new groups of organisms to diversify.[89]
 The diversity of life on Earth is a result of the dynamic interplay between genetic opportunity, metabolic capability, environmental challenges,[90] and symbiosis.[91][92][93] For most of its existence, Earth's habitable environment has been dominated by microorganisms and subjected to their metabolism and evolution. As a consequence of these microbial activities, the physical-chemical environment on Earth has been changing on a geologic time scale, thereby affecting the path of evolution of subsequent life.[90] For example, the release of molecular oxygen by cyanobacteria as a by-product of photosynthesis induced global changes in the Earth's environment. Because oxygen was toxic to most life on Earth at the time, this posed novel evolutionary challenges, and ultimately resulted in the formation of Earth's major animal and plant species. This interplay between organisms and their environment is an inherent feature of living systems.[90]
 
The biosphere is the global sum of all ecosystems. It can also be termed as the zone of life on Earth, a closed system (apart from solar and cosmic radiation and heat from the interior of the Earth), and largely self-regulating.[95] Organisms exist in every part of the biosphere, including soil, hot springs, inside rocks at least 19 km (12 mi) deep underground, the deepest parts of the ocean, and at least 64 km (40 mi) high in the atmosphere.[96][97][98] For example, spores of Aspergillus niger have been detected in the mesosphere at an altitude of 48 to 77 km.[99] Under test conditions, life forms have been observed to survive in the vacuum of space.[100][101] Life forms thrive in the deep Mariana Trench,[102] and inside rocks up to 580 m (1,900 ft; 0.36 mi) below the sea floor under 2,590 m (8,500 ft; 1.61 mi) of ocean off the coast of the northwestern United States,[103][104] and 2,400 m (7,900 ft; 1.5 mi) beneath the seabed off Japan.[105] In 2014, life forms were found living 800 m (2,600 ft; 0.50 mi) below the ice of Antarctica.[106][107] Expeditions of the International Ocean Discovery Program found unicellular life in 120 °C sediment 1.2 km below seafloor in the Nankai Trough subduction zone.[108] According to one researcher, ""You can find microbes everywhere—they're extremely adaptable to conditions, and survive wherever they are.""[103] The inert components of an ecosystem are the physical and chemical factors necessary for life—energy (sunlight or chemical energy), water, heat, atmosphere, gravity, nutrients, and ultraviolet solar radiation protection.[109] In most ecosystems, the conditions vary during the day and from one season to the next. To live in most ecosystems, then, organisms must be able to survive a range of conditions, called the ""range of tolerance"".[110] Outside that are the ""zones of physiological stress"", where the survival and reproduction are possible but not optimal. Beyond these zones are the ""zones of intolerance"", where survival and reproduction of that organism is unlikely or impossible. Organisms that have a wide range of tolerance are more widely distributed than organisms with a narrow range of tolerance.[110]
 To survive, some microorganisms have evolved to withstand freezing, complete desiccation, starvation, high levels of radiation exposure, and other physical or chemical challenges. These extremophile microorganisms may survive exposure to such conditions for long periods.[90][111] They excel at exploiting uncommon sources of energy. Characterization of the structure and metabolic diversity of microbial communities in such extreme environments is ongoing.[112]
 The first classification of organisms was made by the Greek philosopher Aristotle (384–322 BC), who grouped living things as either plants or animals, based mainly on their ability to move. He distinguished animals with blood from animals without blood, which can be compared with the concepts of vertebrates and invertebrates respectively, and divided the blooded animals into five groups: viviparous quadrupeds (mammals), oviparous quadrupeds (reptiles and amphibians), birds, fishes and whales. The bloodless animals were divided into five groups: cephalopods, crustaceans, insects (which included the spiders, scorpions, and centipedes), shelled animals (such as most molluscs and echinoderms), and ""zoophytes"" (animals that resemble plants). This theory remained dominant for more than a thousand years.[113]
 In the late 1740s, Carl Linnaeus introduced his system of binomial nomenclature for the classification of species. Linnaeus attempted to improve the composition and reduce the length of the previously used many-worded names by abolishing unnecessary rhetoric, introducing new descriptive terms and precisely defining their meaning.[114]
 The fungi were originally treated as plants. For a short period Linnaeus had classified them in the taxon Vermes in Animalia, but later placed them back in Plantae. Herbert Copeland classified the Fungi in his Protoctista, including them with single-celled organisms and thus partially avoiding the problem but acknowledging their special status.[115] The problem was eventually solved by Whittaker, when he gave them their own kingdom in his five-kingdom system. Evolutionary history shows that the fungi are more closely related to animals than to plants.[116]
 As advances in microscopy enabled detailed study of cells and microorganisms, new groups of life were revealed, and the fields of cell biology and microbiology were created. These new organisms were originally described separately in protozoa as animals and protophyta/thallophyta as plants, but were united by Ernst Haeckel in the kingdom Protista; later, the prokaryotes were split off in the kingdom Monera, which would eventually be divided into two separate groups, the Bacteria and the Archaea. This led to the six-kingdom system and eventually to the current three-domain system, which is based on evolutionary relationships.[117] However, the classification of eukaryotes, especially of protists, is still controversial.[118]
 As microbiology developed, viruses, which are non-cellular, were discovered. Whether these are considered alive has been a matter of debate; viruses lack characteristics of life such as cell membranes, metabolism and the ability to grow or respond to their environments. Viruses have been classed into ""species"" based on their genetics, but many aspects of such a classification remain controversial.[119]
 The original Linnaean system has been modified many times, for example as follows:
 The attempt to organise the Eukaryotes into a small number of kingdoms has been challenged. The Protozoa do not form a clade or natural grouping,[127] and nor do the Chromista (Chromalveolata).[128]
 The ability to sequence large numbers of complete genomes has allowed biologists to take a metagenomic view of the phylogeny of the whole tree of life. This has led to the realisation that the majority of living things are bacteria, and that all have a common origin.[117][129]
 All life forms require certain core chemical elements for their biochemical functioning. These include carbon, hydrogen, nitrogen, oxygen, phosphorus, and sulfur—the elemental macronutrients for all organisms.[130] Together these make up nucleic acids, proteins and lipids, the bulk of living matter. Five of these six elements comprise the chemical components of DNA, the exception being sulfur. The latter is a component of the amino acids cysteine and methionine. The most abundant of these elements in organisms is carbon, which has the desirable attribute of forming multiple, stable covalent bonds. This allows carbon-based (organic) molecules to form the immense variety of chemical arrangements described in organic chemistry.[131]
Alternative hypothetical types of biochemistry have been proposed that eliminate one or more of these elements, swap out an element for one not on the list, or change required chiralities or other chemical properties.[132][133]
 Deoxyribonucleic acid or DNA is a molecule that carries most of the genetic instructions used in the growth, development, functioning and reproduction of all known living organisms and many viruses. DNA and RNA are nucleic acids; alongside proteins and complex carbohydrates, they are one of the three major types of macromolecule that are essential for all known forms of life. Most DNA molecules consist of two biopolymer strands coiled around each other to form a double helix. The two DNA strands are known as polynucleotides since they are composed of simpler units called nucleotides.[134] Each nucleotide is composed of a nitrogen-containing nucleobase—either cytosine (C), guanine (G), adenine (A), or thymine (T)—as well as a sugar called deoxyribose and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. According to base pairing rules (A with T, and C with G), hydrogen bonds bind the nitrogenous bases of the two separate polynucleotide strands to make double-stranded DNA. This has the key property that each strand contains all the information needed to recreate the other strand, enabling the information to be preserved during reproduction and cell division.[135] Within cells, DNA is organised into long structures called chromosomes. During cell division these chromosomes are duplicated in the process of DNA replication, providing each cell its own complete set of chromosomes. Eukaryotes store most of their DNA inside the cell nucleus.[136]
 Cells are the basic unit of structure in every living thing, and all cells arise from pre-existing cells by division.[137][138] Cell theory was formulated by Henri Dutrochet, Theodor Schwann, Rudolf Virchow and others during the early nineteenth century, and subsequently became widely accepted.[139] The activity of an organism depends on the total activity of its cells, with energy flow occurring within and between them. Cells contain hereditary information that is carried forward as a genetic code during cell division.[140]
 There are two primary types of cells, reflecting their evolutionary origins. Prokaryote cells lack a nucleus and other membrane-bound organelles, although they have circular DNA and ribosomes. Bacteria and Archaea are two domains of prokaryotes. The other primary type is the eukaryote cell, which has a distinct nucleus bound by a nuclear membrane and membrane-bound organelles, including mitochondria, chloroplasts, lysosomes, rough and smooth endoplasmic reticulum, and vacuoles. In addition, their DNA is organised into chromosomes. All species of large complex organisms are eukaryotes, including animals, plants and fungi, though with a wide diversity of protist microorganisms.[141] The conventional model is that eukaryotes evolved from prokaryotes, with the main organelles of the eukaryotes forming through endosymbiosis between bacteria and the progenitor eukaryotic cell.[142]
 The molecular mechanisms of cell biology are based on proteins. Most of these are synthesised by the ribosomes through an enzyme-catalyzed process called protein biosynthesis. A sequence of amino acids is assembled and joined based upon gene expression of the cell's nucleic acid.[143] In eukaryotic cells, these proteins may then be transported and processed through the Golgi apparatus in preparation for dispatch to their destination.[144]
 Cells reproduce through a process of cell division in which the parent cell divides into two or more daughter cells. For prokaryotes, cell division occurs through a process of fission in which the DNA is replicated, then the two copies are attached to parts of the cell membrane. In eukaryotes, a more complex process of mitosis is followed. However, the result is the same; the resulting cell copies are identical to each other and to the original cell (except for mutations), and both are capable of further division following an interphase period.[145]
 Multicellular organisms may have first evolved through the formation of colonies of identical cells. These cells can form group organisms through cell adhesion. The individual members of a colony are capable of surviving on their own, whereas the members of a true multi-cellular organism have developed specialisations, making them dependent on the remainder of the organism for survival. Such organisms are formed clonally or from a single germ cell that is capable of forming the various specialised cells that form the adult organism. This specialisation allows multicellular organisms to exploit resources more efficiently than single cells.[146] About 800 million years ago, a minor genetic change in a single molecule, the enzyme GK-PID, may have allowed organisms to go from a single cell organism to one of many cells.[147]
 Cells have evolved methods to perceive and respond to their microenvironment, thereby enhancing their adaptability. Cell signalling coordinates cellular activities, and hence governs the basic functions of multicellular organisms. Signaling between cells can occur through direct cell contact using juxtacrine signalling, or indirectly through the exchange of agents as in the endocrine system. In more complex organisms, coordination of activities can occur through a dedicated nervous system.[148]
 Though life is confirmed only on Earth, many think that extraterrestrial life is not only plausible, but probable or inevitable,[149][150] possibly resulting in a biophysical cosmology instead of a mere physical cosmology.[151] Other planets and moons in the Solar System and other planetary systems are being examined for evidence of having once supported simple life, and projects such as SETI are trying to detect radio transmissions from possible alien civilisations. Other locations within the Solar System that may host microbial life include the subsurface of Mars, the upper atmosphere of Venus,[152] and subsurface oceans on some of the moons of the giant planets.[153][154]
 Investigation of the tenacity and versatility of life on Earth,[111] as well as an understanding of the molecular systems that some organisms utilise to survive such extremes, is important for the search for extraterrestrial life.[90] For example, lichen could survive for a month in a simulated Martian environment.[155][156]
 Beyond the Solar System, the region around another main-sequence star that could support Earth-like life on an Earth-like planet is known as the habitable zone. The inner and outer radii of this zone vary with the luminosity of the star, as does the time interval during which the zone survives. Stars more massive than the Sun have a larger habitable zone, but remain on the Sun-like ""main sequence"" of stellar evolution for a shorter time interval. Small red dwarfs have the opposite problem, with a smaller habitable zone that is subject to higher levels of magnetic activity and the effects of tidal locking from close orbits. Hence, stars in the intermediate mass range such as the Sun may have a greater likelihood for Earth-like life to develop.[157] The location of the star within a galaxy may also affect the likelihood of life forming. Stars in regions with a greater abundance of heavier elements that can form planets, in combination with a low rate of potentially habitat-damaging supernova events, are predicted to have a higher probability of hosting planets with complex life.[158] The variables of the Drake equation are used to discuss the conditions in planetary systems where civilisation is most likely to exist, within wide bounds of uncertainty.[159] A ""Confidence of Life Detection"" scale (CoLD) for reporting evidence of life beyond Earth has been proposed.[160][161]
 Artificial life is the simulation of any aspect of life, as through computers, robotics, or biochemistry.[162] Synthetic biology is a new area of biotechnology that combines science and biological engineering. The common goal is the design and construction of new biological functions and systems not found in nature. Synthetic biology includes the broad redefinition and expansion of biotechnology, with the ultimate goals of being able to design and build engineered biological systems that process information, manipulate chemicals, fabricate materials and structures, produce energy, provide food, and maintain and enhance human health and the environment.[163]
",life earth life qualiti distinguish matter biolog process signal process matter defin descript capac homeostasi organis metabol growth adapt respons stimuli reproduct life time eventu reach state death none immort mani philosoph definit live system propos system virus particular make definit difficult replic host cell life exist earth air water soil mani ecosystem form biospher harsh environ occupi extremophil life studi sinc ancient time theori empedocl materi assert compos four etern element aristotl hylomorph assert live thing soul embodi form matter life origin least billion year ago result univers common ancestor evolv speci exist way mani extinct speci left trace fossil attempt classifi live thing began aristotl modern classif began carl linnaeu system binomi nomenclatur live thing compos biochem molecul form mainli core chemic element live thing contain two type larg molecul protein nucleic acid latter usual dna rna carri inform need speci includ instruct make type protein protein turn serv machineri carri mani chemic process life cell structur function unit life smaller organ includ prokaryot bacteria archaea consist small singl cell larger organ mainli eukaryot consist singl cell may multicellular complex structur life known exist earth extraterrestri life thought probabl artifici life simul explor scientist engin definit life long challeng scientist philosoph partial life process substanc complic lack knowledg characterist live entiti may develop outsid earth philosoph definit life also put forward similar difficulti distinguish live thing legal definit life debat though gener focu decis declar human dead legal ramif decis least definit life compil sinc consensu definit life current definit biolog descript life consid characterist someth preserv further reinforc exist given environ impli follow trait physic perspect organ thermodynam system organis molecular structur reproduc evolv surviv dictat thermodynam life describ open system make use gradient surround creat imperfect copi anoth way put defin life chemic system capabl undergo darwinian evolut definit adopt nasa committe attempt defin life purpos exobiolog base suggest carl sagan definit howev wide criticis accord singl sexual reproduc individu aliv incap evolv other take live system theori viewpoint necessarili depend molecular chemistri one system definit life live thing autopoiet variat includ stuart kauffman definit autonom agent system capabl reproduc complet least one thermodynam work cycl definit extend evolut novel function time death termin vital function life process organ cell one challeng defin death distinguish life death would seem refer either moment life end state follow life begin howev determin death occur difficult cessat life function often simultan across organ system determin therefor requir draw conceptu line life death problemat littl consensu defin life natur death millennia central concern world religi tradit philosoph inquiri mani religion maintain faith either kind afterlif reincarn soul resurrect bodi later date whether virus consid aliv controversi often consid gene code replic rather form life describ organ edg life possess gene evolv natur select replic make multipl copi howev virus metabolis requir host cell make new product viru within host cell implic studi origin life may support hypothesi life could start organ molecul earliest theori life materialist hold exist matter life mere complex form arrang matter empedocl bc argu everyth univers made combin four etern element root earth water air fire chang explain arrang rearrang four element variou form life caus appropri mixtur element democritu bc atomist thought essenti characterist life soul psych soul like everyth els compos fieri atom elabor fire appar connect life heat fire move plato contrast held world organis perman form reflect imperfectli matter form provid direct intellig explain regular observ world mechanist materi origin ancient greec reviv revis french philosoph rené descart held anim human assemblag part togeth function machin idea develop julien offray de la mettri book machin centuri advanc cell theori biolog scienc encourag view evolutionari theori charl darwin mechanist explan origin speci mean natur select begin centuri stéphane leduc promot idea biolog process could understood term physic chemistri growth resembl inorgan crystal immers solut sodium silic idea set book la biologi synthétiqu wide dismiss lifetim incur resurg interest work russel barg colleagu hylomorph theori first express greek philosoph aristotl bc applic hylomorph biolog import aristotl biolog extens cover extant write view everyth materi univers matter form form live thing soul greek psych latin anima three kind soul veget soul plant caus grow decay nourish caus motion sensat anim soul caus anim move feel ration soul sourc conscious reason aristotl believ found man higher soul attribut lower one aristotl believ matter exist without form form exist without matter therefor soul exist without bodi account consist teleolog explan life account phenomena term purpos thu white polar bear coat explain purpos camouflag direct causal futur past contradict scientif evid natur select explain consequ term prior caus biolog featur explain look futur optim result look past evolutionari histori speci led natur select featur question spontan gener belief live organ form without descent similar organ typic idea certain form flea could aris inanim matter dust suppos season gener mice insect mud garbag theori spontan gener propos aristotl compil expand work prior natur philosoph variou ancient explan appear organ consid best explan two millennia decis dispel experi loui pasteur expand upon investig predecessor francesco redi disproof tradit idea spontan gener longer controversi among biologist vital belief origin georg ernst stahl centuri remain popular middl centuri appeal philosoph henri bergson friedrich nietzsch wilhelm dilthey anatomist like xavier bichat chemist like justu von liebig vital includ idea fundament differ organ inorgan materi belief organ materi deriv live thing disprov friedrich wöhler prepar urea inorgan materi wöhler synthesi consid start point modern organ chemistri histor signific first time organ compound produc inorgan reaction hermann von helmholtz anticip juliu robert von mayer demonstr energi lost muscl movement suggest vital forc necessari move muscl result led abandon scientif interest vitalist theori especi eduard buchner demonstr alcohol ferment could occur extract yeast nonetheless belief still exist pseudoscientif theori homoeopathi interpret diseas sick caus disturb hypothet vital forc life forc age earth billion year life earth exist least billion year oldest physic trace life date back billion year estim molecular clock summaris timetre public databas place origin life around billion year ago hypothes origin life attempt explain format univers common ancestor simpl organ molecul via life protocel metabol set gene last univers common ancestor tent identifi biospher postul develop origin life onward least billion year ago earliest evid life earth includ biogen graphit found metasedimentari rock western greenland microbi mat fossil found sandston western australia recent remain biotic life found rock western australia put fossilis microorgan microfossil announc discov hydrotherm vent precipit nuvvuagittuq belt quebec canada old billion year oldest record life earth suggest almost instantan emerg life ocean format billion year ago long format earth billion year ago evolut chang herit characterist biolog popul success gener result appear new speci often disappear old one evolut occur evolutionari process natur select includ sexual select genet drift act genet variat result certain characterist increas decreas frequenc within popul success gener process evolut given rise biodivers everi level biolog organis fossil preserv remain trace organ remot past total fossil discov undiscov placement layer strata sedimentari rock known fossil record preserv specimen call fossil older arbitrari date year ago henc fossil rang age youngest start holocen epoch oldest archaean eon billion year old extinct process speci die moment extinct death last individu speci speci potenti rang may larg determin moment difficult usual done retrospect period appar absenc speci becom extinct longer abl surviv chang habitat superior competit speci ever live extinct mass extinct may acceler evolut provid opportun new group organ diversifi divers life earth result dynam interplay genet opportun metabol capabl environment challeng symbiosi exist earth habit environ domin microorgan subject metabol evolut consequ microbi activ environ earth chang geolog time scale therebi affect path evolut subsequ life exampl releas molecular oxygen cyanobacteria photosynthesi induc global chang earth environ oxygen toxic life earth time pose novel evolutionari challeng ultim result format earth major anim plant speci interplay organ environ inher featur live system biospher global sum ecosystem also term zone life earth close system apart solar cosmic radiat heat interior earth larg organ exist everi part biospher includ soil hot spring insid rock least km mi deep underground deepest part ocean least km mi high atmospher exampl spore aspergillu niger detect mesospher altitud km test condit life form observ surviv vacuum space life form thrive deep mariana trench insid rock ft mi sea floor ft mi ocean coast northwestern unit state ft mi beneath seab japan life form found live ft mi ice antarctica expedit intern ocean discoveri program found unicellular life sediment km seafloor nankai trough subduct zone accord one research find microb extrem adapt condit surviv wherev inert compon ecosystem physic chemic factor necessari sunlight chemic energi water heat atmospher graviti nutrient ultraviolet solar radiat protect ecosystem condit vari day one season next live ecosystem organ must abl surviv rang condit call rang toler outsid zone physiolog stress surviv reproduct possibl optim beyond zone zone intoler surviv reproduct organ unlik imposs organ wide rang toler wide distribut organ narrow rang toler surviv microorgan evolv withstand freez complet desicc starvat high level radiat exposur physic chemic challeng extremophil microorgan may surviv exposur condit long period excel exploit uncommon sourc energi character structur metabol divers microbi commun extrem environ ongo first classif organ made greek philosoph aristotl bc group live thing either plant anim base mainli abil move distinguish anim blood anim without blood compar concept vertebr invertebr respect divid blood anim five group vivipar quadrup mammal ovipar quadrup reptil amphibian bird fish whale bloodless anim divid five group cephalopod crustacean insect includ spider scorpion centiped shell anim mollusc echinoderm zoophyt anim resembl plant theori remain domin thousand year late carl linnaeu introduc system binomi nomenclatur classif speci linnaeu attempt improv composit reduc length previous use name abolish unnecessari rhetor introduc new descript term precis defin mean fungi origin treat plant short period linnaeu classifi taxon verm animalia later place back planta herbert copeland classifi fungi protoctista includ organ thu partial avoid problem acknowledg special statu problem eventu solv whittak gave kingdom system evolutionari histori show fungi close relat anim plant advanc microscopi enabl detail studi cell microorgan new group life reveal field cell biolog microbiolog creat new organ origin describ separ protozoa anim plant unit ernst haeckel kingdom protista later prokaryot split kingdom monera would eventu divid two separ group bacteria archaea led system eventu current system base evolutionari relationship howev classif eukaryot especi protist still controversi microbiolog develop virus discov whether consid aliv matter debat virus lack characterist life cell membran metabol abil grow respond environ virus class speci base genet mani aspect classif remain controversi origin linnaean system modifi mani time exampl follow attempt organis eukaryot small number kingdom challeng protozoa form clade natur group chromista chromalveolata abil sequenc larg number complet genom allow biologist take metagenom view phylogeni whole tree life led realis major live thing bacteria common origin life form requir certain core chemic element biochem function includ carbon hydrogen nitrogen oxygen phosphoru element macronutri organ togeth make nucleic acid protein lipid bulk live matter five six element compris chemic compon dna except sulfur latter compon amino acid cystein methionin abund element organ carbon desir attribut form multipl stabl coval bond allow organ molecul form immens varieti chemic arrang describ organ chemistri altern hypothet type biochemistri propos elimin one element swap element one list chang requir chiral chemic properti deoxyribonucl acid dna molecul carri genet instruct use growth develop function reproduct known live organ mani virus dna rna nucleic acid alongsid protein complex carbohydr one three major type macromolecul essenti known form life dna molecul consist two biopolym strand coil around form doubl helix two dna strand known polynucleotid sinc compos simpler unit call nucleotid nucleotid compos cytosin c guanin g adenin thymin well sugar call deoxyribos phosphat group nucleotid join one anoth chain coval bond sugar one nucleotid phosphat next result altern backbon accord base pair rule c g hydrogen bond bind nitrogen base two separ polynucleotid strand make dna key properti strand contain inform need recreat strand enabl inform preserv reproduct cell divis within cell dna organis long structur call chromosom cell divis chromosom duplic process dna replic provid cell complet set chromosom eukaryot store dna insid cell nucleu cell basic unit structur everi live thing cell aris cell divis cell theori formul henri dutrochet theodor schwann rudolf virchow other earli nineteenth centuri subsequ becam wide accept activ organ depend total activ cell energi flow occur within cell contain hereditari inform carri forward genet code cell divis two primari type cell reflect evolutionari origin prokaryot cell lack nucleu organel although circular dna ribosom bacteria archaea two domain prokaryot primari type eukaryot cell distinct nucleu bound nuclear membran organel includ mitochondria chloroplast lysosom rough smooth endoplasm reticulum vacuol addit dna organis chromosom speci larg complex organ eukaryot includ anim plant fungi though wide divers protist microorgan convent model eukaryot evolv prokaryot main organel eukaryot form endosymbiosi bacteria progenitor eukaryot cell molecular mechan cell biolog base protein synthesis ribosom process call protein biosynthesi sequenc amino acid assembl join base upon gene express cell nucleic acid eukaryot cell protein may transport process golgi apparatu prepar dispatch destin cell reproduc process cell divis parent cell divid two daughter cell prokaryot cell divis occur process fission dna replic two copi attach part cell membran eukaryot complex process mitosi follow howev result result cell copi ident origin cell except mutat capabl divis follow interphas period multicellular organ may first evolv format coloni ident cell cell form group organ cell adhes individu member coloni capabl surviv wherea member true organ develop specialis make depend remaind organ surviv organ form clonal singl germ cell capabl form variou specialis cell form adult organ specialis allow multicellular organ exploit resourc effici singl cell million year ago minor genet chang singl molecul enzym may allow organ go singl cell organ one mani cell cell evolv method perceiv respond microenviron therebi enhanc adapt cell signal coordin cellular activ henc govern basic function multicellular organ signal cell occur direct cell contact use juxtacrin signal indirectli exchang agent endocrin system complex organ coordin activ occur dedic nervou system though life confirm earth mani think extraterrestri life plausibl probabl inevit possibl result biophys cosmolog instead mere physic cosmolog planet moon solar system planetari system examin evid support simpl life project seti tri detect radio transmiss possibl alien civilis locat within solar system may host microbi life includ subsurfac mar upper atmospher venu subsurfac ocean moon giant planet investig tenac versatil life earth well understand molecular system organ utilis surviv extrem import search extraterrestri life exampl lichen could surviv month simul martian environ beyond solar system region around anoth star could support life planet known habit zone inner outer radii zone vari luminos star time interv zone surviv star massiv sun larger habit zone remain main sequenc stellar evolut shorter time interv small red dwarf opposit problem smaller habit zone subject higher level magnet activ effect tidal lock close orbit henc star intermedi mass rang sun may greater likelihood life develop locat star within galaxi may also affect likelihood life form star region greater abund heavier element form planet combin low rate potenti supernova event predict higher probabl host planet complex life variabl drake equat use discuss condit planetari system civilis like exist within wide bound uncertainti confid life detect scale cold report evid life beyond earth propos artifici life simul aspect life comput robot biochemistri synthet biolog new area biotechnolog combin scienc biolog engin common goal design construct new biolog function system found natur synthet biolog includ broad redefinit expans biotechnolog ultim goal abl design build engin biolog system process inform manipul chemic fabric materi structur produc energi provid food maintain enhanc human health environ
Abiogenesis,https://en.wikipedia.org/wiki/Abiogenesis,"
 Abiogenesis is the natural process by which life arises from non-living matter, such as simple organic compounds. The prevailing scientific hypothesis is that the transition from non-living to living entities on Earth was not a single event, but a process of increasing complexity involving the formation of a habitable planet, the prebiotic synthesis of organic molecules, molecular self-replication, self-assembly, autocatalysis, and the emergence of cell membranes. The transition from non-life to life has never been observed experimentally, but many proposals have been made for different stages of the process.
 The study of abiogenesis aims to determine how pre-life chemical reactions gave rise to life under conditions strikingly different from those on Earth today. It primarily uses tools from biology and chemistry, with more recent approaches attempting a synthesis of many sciences. Life functions through the specialized chemistry of carbon and water, and builds largely upon four key families of chemicals: lipids for cell membranes, carbohydrates such as sugars, amino acids for protein metabolism, and nucleic acid DNA and RNA for the mechanisms of heredity. Any successful theory of abiogenesis must explain the origins and interactions of these classes of molecules.
 Many approaches to abiogenesis investigate how self-replicating molecules, or their components, came into existence. Researchers generally think that current life descends from an RNA world, although other self-replicating and self-catalyzing molecules may have preceded RNA. Other approaches (""metabolism-first"" hypotheses) focus on understanding how catalysis in chemical systems on the early Earth might have provided the precursor molecules necessary for self-replication. The classic 1952 Miller–Urey experiment demonstrated that most amino acids, the chemical constituents of proteins, can be synthesized from inorganic compounds under conditions intended to replicate those of the early Earth. External sources of energy may have triggered these reactions, including lightning, radiation, atmospheric entries of micro-meteorites and implosion of bubbles in sea and ocean waves. 
 While the last universal common ancestor of all modern organisms (LUCA) is thought to have been quite different from the origin of life, investigations into LUCA can guide research into early universal characteristics. A genomics approach has sought to characterise LUCA by identifying the genes shared by Archaea and Bacteria, members of the two major branches of life (with Eukaryotes included in the archaean branch in the two-domain system). It appears there are 60 proteins common to all life and 355 prokaryotic genes that trace to LUCA; their functions imply that the LUCA was anaerobic with the Wood–Ljungdahl pathway, deriving energy by chemiosmosis, and maintaining its hereditary material with DNA, the genetic code, and ribosomes. Although the LUCA lived over 4 billion years ago (4 Gya), researchers believe it was far from the first form of life. Earlier cells might have had a leaky membrane and been powered by a naturally occurring proton gradient near a deep-sea white smoker hydrothermal vent.
 Earth remains the only place in the universe known to harbor life. Geochemical and fossil evidence from the Earth informs most studies of abiogenesis. The Earth was formed at 4.54 Gya, and the earliest evidence of life on Earth dates from at least 3.8 Gya from Western Australia. Some studies have suggested that fossil micro-organisms may have lived within hydrothermal vent precipitates dated 3.77 to 4.28 Gya from Quebec, soon after ocean formation 4.4 Gya during the Hadean.
 Life consists of reproduction with (heritable) variations.[3] NASA defines life as ""a self-sustaining chemical system capable of Darwinian [i.e., biological] evolution.""[4] Such a system is complex; the last universal common ancestor (LUCA), presumably a single-celled organism which lived some 4 billion years ago, already had hundreds of genes encoded in the DNA genetic code that is universal today. That in turn implies a suite of cellular machinery including messenger RNA, transfer RNA, and ribosomes to translate the code into proteins. Those proteins included enzymes to operate its anaerobic respiration via the Wood–Ljungdahl metabolic pathway, and a DNA polymerase to replicate its genetic material.[5][6]
 The challenge for abiogenesis (origin of life)[7][8][9] researchers is to explain how such a complex and tightly interlinked system could develop by evolutionary steps, as at first sight all its parts are necessary to enable it to function. For example, a cell, whether the LUCA or in a modern organism, copies its DNA with the DNA polymerase enzyme, which is in turn produced by translating the DNA polymerase gene in the DNA. Neither the enzyme nor the DNA can be produced without the other.[10] The evolutionary process could have involved molecular self-replication, self-assembly such as of cell membranes, and autocatalysis via RNA ribozymes.[5][6][11] Nonetheless, the transition of non-life to life has never been observed experimentally, nor has there been a satisfactory chemical explanation.[12]
 The preconditions to the development of a living cell like the LUCA are clear enough, though disputed in their details: a habitable world is formed with a supply of minerals and liquid water. Prebiotic synthesis creates a range of simple organic compounds, which are assembled into polymers such as proteins and RNA. On the other side, the process after the LUCA is readily understood: biological evolution caused the development of a wide range of species with varied forms and biochemical capabilities. However, the derivation of living things such as LUCA from simple components is far from understood.[1]
 Although Earth remains the only place where life is known,[13][14] the science of astrobiology seeks evidence of life on other planets. The 2015 NASA strategy on the origin of life aimed to solve the puzzle by identifying interactions, intermediary structures and functions, energy sources, and environmental factors that contributed to the diversity, selection, and replication of evolvable macromolecular systems,[2] and mapping the chemical landscape of potential primordial informational polymers. The advent of polymers that could replicate, store genetic information, and exhibit properties subject to selection was, it suggested, most likely a critical step in the emergence of prebiotic chemical evolution.[2] Those polymers derived, in turn, from simple organic compounds such as nucleobases, amino acids, and sugars that could have been formed by reactions in the environment.[15][8][16][17] A successful theory of the origin of life must explain how all these chemicals came into being.[18]
 One ancient view of the origin of life, from Aristotle until the 19th century, is of spontaneous generation.[19] This theory held that ""lower"" animals such as insects were generated by decaying organic substances, and that life arose by chance.[20][21] This was questioned from the 17th century, in works like Thomas Browne's Pseudodoxia Epidemica.[22][23] In 1665, Robert Hooke published the first drawings of a microorganism. In 1676, Antonie van Leeuwenhoek drew and described microorganisms, probably protozoa and bacteria.[24] Van Leeuwenhoek disagreed with spontaneous generation, and by the 1680s convinced himself, using experiments ranging from sealed and open meat incubation and the close study of insect reproduction, that the theory was incorrect.[25] In 1668 Francesco Redi showed that no maggots appeared in meat when flies were prevented from laying eggs.[26] By the middle of the 19th century, spontaneous generation was considered disproven.[27][28]
 Another ancient idea dating back to Anaxagoras in the 5th century BC is panspermia,[29] the idea that life exists throughout the universe, distributed by meteoroids, asteroids, comets[30] and planetoids.[31] It does not attempt to explain how life originated in itself, but shifts the origin of life on Earth to another heavenly body. The advantage is that life is not required to have formed on each planet it occurs on, but rather in a more limited set of locations, or even a single location, and then spread about the galaxy to other star systems via cometary or meteorite impact.[32] Panspermia did not get much scientific support because it was largely used to deflect the need of an answer instead of explaining observable phenomena. Although the interest in panspermia grew when the study of meteorites found traces of organic materials in them, it is currently accepted that life started locally on Earth.[33]
 The idea that life originated from non-living matter in slow stages appeared in Herbert Spencer's 1864–1867 book Principles of Biology, and in William Turner Thiselton-Dyer's 1879 paper ""On spontaneous generation and evolution"". On 1 February 1871 Charles Darwin wrote about these publications to Joseph Hooker, and set out his own speculation, suggesting that the original spark of life may have begun in a ""warm little pond, with all sorts of ammonia and phosphoric salts, light, heat, electricity, &c., present, that a proteine compound was chemically formed ready to undergo still more complex changes."" Darwin went on to explain that ""at the present day such matter would be instantly devoured or absorbed, which would not have been the case before living creatures were formed.""[34][35][36]
 Alexander Oparin in 1924 and J. B. S. Haldane in 1929 proposed that the first molecules constituting the earliest cells slowly self-organized from a primordial soup, and this theory is called the Oparin–Haldane hypothesis.[37][38] Haldane suggested that the Earth's prebiotic oceans consisted of a ""hot dilute soup"" in which organic compounds could have formed.[21][39] J. D. Bernal showed that such mechanisms could form most of the necessary molecules for life from inorganic precursors.[40] In 1967, he suggested three ""stages"": the origin of biological monomers; the origin of biological polymers; and the evolution from molecules to cells.[41][42]
 In 1952, Stanley Miller and Harold Urey carried out a chemical experiment to demonstrate how organic molecules could have formed spontaneously from inorganic precursors under prebiotic conditions like those posited by the Oparin–Haldane hypothesis. It used a highly reducing (lacking oxygen) mixture of gases—methane, ammonia, and hydrogen, as well as water vapor—to form simple organic monomers such as amino acids.[43][44] Bernal said of the Miller–Urey experiment that ""it is not enough to explain the formation of such molecules, what is necessary, is a physical-chemical explanation of the origins of these molecules that suggests the presence of suitable sources and sinks for free energy.""[45] However, current scientific consensus describes the primitive atmosphere as weakly reducing or neutral,[46][47] diminishing the amount and variety of amino acids that could be produced. The addition of iron and carbonate minerals, present in early oceans, however, produces a diverse array of amino acids.[46] Later work has focused on two other potential reducing environments: outer space and deep-sea hydrothermal vents.[48][49][50]
 Soon after the Big Bang, which occurred roughly 14 Gya, the only chemical elements present in the universe were hydrogen, helium, and lithium, the three lightest atoms in the periodic table. These elements gradually accreted and began orbiting in disks of gas and dust. Gravitational accretion of material at the hot and dense centers of these protoplanetary disks formed stars by the fusion of hydrogen.[51] Early stars were massive and short-lived, producing all the heavier elements through stellar nucleosynthesis. Element formation through stellar nucleosynthesis proceeds to its most stable element Iron-56. Heavier elements were formed during supernovae at the end of a stars lifecycle. Carbon, currently the fourth most abundant chemical element in the universe (after hydrogen, helium, and oxygen), was formed mainly in white dwarf stars, particularly those bigger than twice the mass of the sun.[52] As these stars reached the end of their lifecycles, they ejected these heavier elements, among them carbon and oxygen, throughout the universe. These heavier elements allowed for the formation of new objects, including rocky planets and other bodies.[53] According to the nebular hypothesis, the formation and evolution of the Solar System began 4.6 Gya with the gravitational collapse of a small part of a giant molecular cloud. Most of the collapsing mass collected in the center, forming the Sun, while the rest flattened into a protoplanetary disk out of which the planets, moons, asteroids, and other small Solar System bodies formed.[54]
 The age of the Earth is 4.54 Gya as found by radiometric dating of calcium-aluminium-rich inclusions in carbonaceous chrondrite meteorites, the oldest material in the Solar System.[55][56] The Hadean Earth (from its formation until 4 Gya) was at first inhospitable to any living organisms. During its formation, the Earth lost a significant part of its initial mass, and consequentially lacked the gravity to hold molecular hydrogen and the bulk of the original inert gases.[57] Soon after initial accretion of Earth at 4.48 Ga, its collision with Theia, a hypothesised impactor, is thought to have created the ejected debris that would eventually form the Moon.[58] This impact would have removed the Earth's primary atmosphere, leaving behind clouds of viscous silicates and carbon dioxide. This unstable atmosphere was short-lived and condensed shortly after to form the bulk silicate Earth, leaving behind an atmosphere largely consisting of water vapor, nitrogen, and carbon dioxide, with smaller amounts of carbon monoxide, hydrogen, and sulfur compounds.[59][60] The solution of carbon dioxide in water is thought to have made the seas slightly acidic, with a pH of about 5.5.[61]
 Condensation to form liquid oceans is theorised to have occurred as early as the Moon-forming impact.[62][63]  This scenario has found support from the dating of 4.404 Gya zircon crystals with high δ18O values from metamorphosed quartzite of Mount Narryer in Western Australia.[64][65] The Hadean atmosphere has been characterized as a ""gigantic, productive outdoor chemical laboratory,"" similar to volcanic gases today which still support some abiotic chemistry. Despite the likely increased volcanism from early plate tectonics, the Earth may have been a predominantly water world between 4.4 and 4.3 Gya. It is debated whether or not crust was exposed above this ocean due to uncertainties of what early plate tectonics looked like. For early life to have developed, it is generally thought that a land setting is required, so this question is essential to determining when in Earth's history life evolved.[66] The post-Moon-forming impact Earth likely existed with little if any continental crust, a turbulent atmosphere, and a hydrosphere subject to intense ultraviolet light from a T Tauri stage Sun, from cosmic radiation, and from continued asteroid and comet impacts.[67] Despite all this, niche environments likely existed conducive to life on Earth in the Late-Hadean to Early-Archaean.
 The Late Heavy Bombardment hypothesis posits that a period of intense impact occurred at ~3.9 Gya during the Hadean.[68][69] A cataclysmic impact event would have had the potential to sterilise all life on Earth by volatilising liquid oceans and blocking the Sun needed for photosynthesising primary producers, pushing back the earliest possible emergence of life to after Late Heavy Bombardment.[70] Recent research questions both the intensity of the Late Heavy Bombardment as well as its potential for sterilisation. Uncertainties as to whether Late Heavy Bombardment was one giant impact or a period of greater impact rates greatly changed the implication of its destructive power.[71][72] The 3.9 Ga date arises from dating of Apollo mission sample returns collected mostly near the Imbrium Basin, biasing the age of recorded impacts.[73] Impact modelling of the lunar surface reveals that rather than a cataclysmic event at 3.9 Ga, multiple small-scale, short-lived periods of bombardment likely occurred.[74] Terrestrial data backs this idea by showing multiple periods of ejecta in the rock record both before and after the 3.9 Ga marker, suggesting that the early Earth was subject to continuous impacts that would not have had as great an impact on extinction as previously thought.[75] If the Late Heavy Bombardment did not take place, this allows for the emergence of life to have taken place far before 3.9 Ga.
 If life evolved in the ocean at depths of more than ten meters, it would have been shielded both from late impacts and the then high levels of ultraviolet radiation from the sun. Geothermically heated oceanic crust could have yielded far more organic compounds through deep hydrothermal vents than the Miller–Urey experiments indicated.[76] The available energy is maximized at 100–150 °C, the temperatures at which hyperthermophilic bacteria and thermoacidophilic archaea live.[77]
 The exact timing at which life emerged on Earth is unknown. Minimum age estimates are based on evidence from the geologic rock record. The earliest physical evidence of life so far found consists of microbialites in the Nuvvuagittuq Greenstone Belt of Northern Quebec, in banded iron formation rocks at least 3.77 and possibly as old as 4.32 Gya. The micro-organisms lived within hydrothermal vent precipitates, soon after the 4.4 Gya formation of oceans during the Hadean. The microbes resembled modern hydrothermal vent bacteria, supporting the view that abiogenesis began in such an environment.[78]
 Biogenic graphite has been found in 3.7 Gya metasedimentary rocks from southwestern Greenland[79] and in microbial mat fossils from 3.49 Gya cherts in the Pilbara region of Western Australia.[80] Evidence of early life in rocks from Akilia Island, near the Isua supracrustal belt in southwestern Greenland, dating to 3.7 Gya, have shown biogenic carbon isotopes.[81] In other parts of the Isua supracrustal belt, graphite inclusions trapped within garnet crystals are connected to the other elements of life: oxygen, nitrogen, and possibly phosphorus in the form of phosphate, providing further evidence for life 3.7 Gya.[82] In the Pilbara region of Western Australia, compelling evidence of early life was found in pyrite-bearing sandstone in a fossilized beach, with rounded tubular cells that oxidized sulfur by photosynthesis in the absence of oxygen.[83][84] Carbon isotope ratios on graphite inclusions from the Jack Hills zircons suggest that life could have existed on Earth from 4.1 Gya.[85]
 The Pilbara region of Western Australia contains the Dresser Formation with rocks 3.48 Gya, including layered structures called stromatolites. Their modern counterparts are created by photosynthetic micro-organisms including cyanobacteria.[86] These lie within undeformed hydrothermal-sedimentary strata; their texture indicates a biogenic origin. Parts of the Dresser formation preserve hot springs on land, but other regions seem to have been shallow seas.[87] A molecular clock analysis suggests the LUCA emerged prior to the Late Heavy Bombardment (3.9 Gya).[88]
 All chemical elements except for hydrogen and helium derive from stellar nucleosynthesis. The basic chemical ingredients of life – the carbon-hydrogen molecule (CH), the carbon-hydrogen positive ion (CH+) and the carbon ion (C+) – were produced by ultraviolet light from stars.[89] Complex molecules, including organic molecules, form naturally both in space and on planets.[90] Organic molecules on the early Earth could have had either terrestrial origins, with organic molecule synthesis driven by impact shocks or by other energy sources, such as ultraviolet light, redox coupling, or electrical discharges; or extraterrestrial origins (pseudo-panspermia), with organic molecules formed in interstellar dust clouds raining down on to the planet.[91][92]
 An organic compound is a chemical whose molecules contain carbon. Carbon is abundant in the Sun, stars, comets, and in the atmospheres of most planets.[93] Organic compounds are relatively common in space, formed by ""factories of complex molecular synthesis"" which occur in molecular clouds and circumstellar envelopes, and chemically evolve after reactions are initiated mostly by ionizing radiation.[90][94][95] Purine and pyrimidine nucleobases including guanine, adenine, cytosine, uracil, and thymine have been found in meteorites. These could have provided the materials for DNA and RNA to form on the early Earth.[96] The amino acid glycine was found in material ejected from comet Wild 2; it had earlier been detected in meteorites.[97] Comets are encrusted with dark material, thought to be a tar-like organic substance formed from simple carbon compounds under ionizing radiation. A rain of material from comets could have brought such complex organic molecules to Earth.[98][99][60] It is estimated that during the Late Heavy Bombardment, meteorites may have delivered up to five million tons of organic prebiotic elements to Earth per year.[60]
 Polycyclic aromatic hydrocarbons (PAH) are the most common and abundant polyatomic molecules in the observable universe, and are a major store of carbon.[93][100][101][102] They seem to have formed shortly after the Big Bang,[103][101][102] and are associated with new stars and exoplanets.[93] They are a likely constituent of Earth's primordial sea.[103][101][102] PAHs have been detected in nebulae,[104] and in the interstellar medium, in comets, and in meteorites.[93]
 The PAH world hypothesis posits PAHs as precursors to the RNA world.[105] A star, HH 46-IR, resembling the sun early in its life, is surrounded by a disk of material which contains molecules including cyanide compounds, hydrocarbons, and carbon monoxide. PAHs in the interstellar medium can be transformed through hydrogenation, oxygenation, and hydroxylation to more complex organic compounds used in living cells.[106]
 Organic compounds introduced on Earth by interstellar dust particles can help to form complex molecules, thanks to their peculiar surface-catalytic activities.[107][108] Studies of the 12C/13C isotopic ratios of organic compounds in the Murchison meteorite suggest that the RNA component uracil and related molecules, including xanthine, were formed extraterrestrially.[109] NASA studies of meteorites suggest that all four DNA nucleobases (adenine, guanine and related organic molecules) have been formed in outer space.[107][110][111] The cosmic dust permeating the universe contains complex organics (""amorphous organic solids with a mixed aromatic–aliphatic structure"") that could be created rapidly by stars.[112] Glycolaldehyde, a sugar molecule and RNA precursor, has been detected in regions of space including around protostars and on meteorites.[113][114]
 As early as the 1860s, experiments demonstrated that biologically relevant molecules can be produced from interaction of simple carbon sources with abundant inorganic catalysts. The spontaneous formation of complex polymers from abiotically generated monomers under the conditions posited by the ""soup"" theory is not straightforward. Besides the necessary basic organic monomers, compounds that would have prohibited the formation of polymers were also formed in high concentration during the Miller–Urey and Joan Oró experiments.[115] Biology uses essentially 20 amino acids for its coded protein enzymes, representing a very small subset of the structurally possible products. Since life tends to use whatever is available, an explanation is needed for why the set used is so small.[116] Formamide is attractive as a medium that potentially provided a source of amino acid derivatives from simple aldehyde and nitrile feedstocks.[117]
 Alexander Butlerov showed in 1861 that the formose reaction created sugars including tetroses, pentoses, and hexoses when formaldehyde is heated under basic conditions with divalent metal ions like calcium. R. Breslow proposed that the reaction was autocatalytic in 1959.[118]
 Nucleobases, such as guanine and adenine, can be synthesized from simple carbon and nitrogen sources, such as hydrogen cyanide (HCN) and ammonia.[119] Formamide produces all four ribonucleotides when warmed with terrestrial minerals. Formamide is ubiquitous in the Universe, produced by the reaction of water and HCN. It can be concentrated by the evaporation of water.[120][121] HCN is poisonous only to aerobic organisms (eukaryotes and aerobic bacteria), which did not yet exist. It can play roles in other chemical processes such as the synthesis of the amino acid glycine.[60]
 DNA and RNA components including uracil, cytosine and thymine can be synthesized under outer space conditions, using starting chemicals such as pyrimidine found in meteorites. Pyrimidine may have been formed in red giant stars or in interstellar dust and gas clouds.[122] All four RNA-bases may be synthesized from formamide in high-energy density events like extraterrestrial impacts.[123]
 Other pathways for synthesizing bases from inorganic materials have been reported.[124] Freezing temperatures are advantageous for the synthesis of purines, due to the concentrating effect for key precursors such as hydrogen cyanide.[125] However, while adenine and guanine require freezing conditions for synthesis, cytosine and uracil may require boiling temperatures.[126] Seven amino acids and eleven types of nucleobases formed in ice when ammonia and cyanide were left in a freezer for 25 years.[127][128] S-triazines (alternative nucleobases), pyrimidines including cytosine and uracil, and adenine can be synthesized by subjecting a urea solution to freeze-thaw cycles under a reductive atmosphere, with spark discharges as an energy source.[129] The explanation given for the unusual speed of these reactions at such a low temperature is eutectic freezing, which crowds impurities in microscopic pockets of liquid within the ice, causing the molecules to collide more often.[130]
 Prebiotic peptide synthesis is proposed to have occurred through a number of possible routes.  Some center on high temperature/concentration conditions in which condensation becomes energetically favorable, while others focus on the availability of plausible prebiotic condensing agents.[131][further explanation needed]
 Experimental evidence for the formation of peptides in uniquely concentrated environments is bolstered by work suggesting that wet-dry cycles and the presence of specific salts can greatly increase spontaneous condensation of glycine into poly-glycine chains.[132] Other work suggests that while mineral surfaces, such as those of pyrite, calcite, and rutile catalyze peptide condensation, they also catalyze their hydrolysis. The authors suggest that additional chemical activation or coupling would be necessary to produce peptides at sufficient concentrations.  Thus, mineral surface catalysis, while important, is not sufficient alone for peptide synthesis.[133]
 Many prebiotically plausible condensing/activating agents have been identified, including the following: cyanamide, dicyanamide, dicyandiamide, diaminomaleonitrile, urea, trimetaphosphate, NaCl, CuCl2, (Ni,Fe)S, CO, carbonyl sulfide (COS), carbon disulfide (CS2), SO2, and diammonium phosphate (DAP).[131]
 An experiment reported in 2024 used a sapphire substrate with a web of thin cracks under a heat flow, similar to the environment of deep-ocean vents, as a mechanism to separate and concentrate prebiotically relevant building blocks from a dilute mixture, purifying their concentration by up to three orders of magnitude. The authors propose this as a plausible model for the origin of complex biopolymers.[134] This presents another physical process that allows for concentrated peptide precursors to combine in the right conditions. A similar role of increasing amino acid concentration has been suggested for clays as well.[135]
 While all of these scenarios involve the condensation of amino acids, the prebiotic synthesis of peptides from simpler molecules such as CO, NH3 and C, skipping the step of amino acid formation, is very efficient.[136][137]  
 The largest unanswered question in evolution is how simple protocells first arose and differed in reproductive contribution to the following generation, thus initiating the evolution of life. The lipid world theory postulates that the first self-replicating object was lipid-like.[138][139] Phospholipids form lipid bilayers in water while under agitation—the same structure as in cell membranes. These molecules were not present on early Earth, but other amphiphilic long-chain molecules also form membranes. These bodies may expand by insertion of additional lipids, and may spontaneously split into two offspring of similar size and composition. Lipid bodies may have provided sheltering envelopes for information storage, allowing the evolution and preservation of polymers like RNA that store information. Only one or two types of amphiphiles have been studied which might have led to the development of vesicles.[140] There is an enormous number of possible arrangements of lipid bilayer membranes, and those with the best reproductive characteristics would have converged toward a hypercycle reaction,[141][142] a positive feedback composed of two mutual catalysts represented by a membrane site and a specific compound trapped in the vesicle. Such site/compound pairs are transmissible to the daughter vesicles leading to the emergence of distinct lineages of vesicles, which would have allowed natural selection.[143]
 A protocell is a self-organized, self-ordered, spherical collection of lipids proposed as a stepping-stone to the origin of life.[140] A functional protocell has (as of 2014) not yet been achieved in a laboratory setting.[144][145][146] Self-assembled vesicles are essential components of primitive cells.[140] The theory of classical irreversible thermodynamics treats self-assembly under a generalized chemical potential within the framework of dissipative systems.[147][148][149] The second law of thermodynamics requires that overall entropy increases, yet life is distinguished by its great degree of organization. Therefore, a boundary is needed to separate ordered life processes from chaotic non-living matter.[150]
 Irene Chen and Jack W. Szostak suggest that elementary protocells can give rise to cellular behaviors including primitive forms of differential reproduction, competition, and energy storage.[145] Competition for membrane molecules would favor stabilized membranes, suggesting a selective advantage for the evolution of cross-linked fatty acids and even the phospholipids of today.[145] Such micro-encapsulation would allow for metabolism within the membrane and the exchange of small molecules, while retaining large biomolecules inside. Such a membrane is needed for a cell to create its own electrochemical gradient to store energy by pumping ions across the membrane.[151][152] Fatty acid vesicles in conditions relevant to alkaline hydrothermal vents can be stabilized by isoprenoids which are synthesized by the formose reaction; the advantages and disadvantages of isoprenoids incorporated within the lipid bilayer in different microenvironments might have led to the divergence of the membranes of archaea and bacteria.[153]
 Laboratory experiments have shown that vesicles can undergo an evolutionary process under pressure cycling conditions.[154] Simulating the systemic environment in tectonic fault zones within the Earth's crust, pressure cycling leads to the periodic formation of vesicles.[155] Under the same conditions, random peptide chains are being formed, which are being continuously selected for their ability to integrate into the vesicle membrane. A further selection of the vesicles for their stability potentially leads to the development of functional peptide structures,[156][157][158] associated with an increase in the survival rate of the vesicles.
 Life requires a loss of entropy, or disorder, as molecules organize themselves into living matter. At the same time, the emergence of life is associated with the formation of structures beyond a certain threshold of complexity.[159] The emergence of life with increasing order and complexity does not contradict the second law of thermodynamics, which states that overall entropy never decreases, since a living organism creates order in some places (e.g. its living body) at the expense of an increase of entropy elsewhere (e.g. heat and waste production).[160][161][162]
 Multiple sources of energy were available for chemical reactions on the early Earth. Heat from geothermal processes is a standard energy source for chemistry. Other examples include sunlight, lightning,[60] atmospheric entries of micro-meteorites,[163] and implosion of bubbles in sea and ocean waves.[164] This has been confirmed by experiments[165][166] and simulations.[167]
Unfavorable reactions can be driven by highly favorable ones, as in the case of iron-sulfur chemistry. For example, this was probably important for carbon fixation.[a] Carbon fixation by reaction of CO2 with H2S via iron-sulfur chemistry is favorable, and occurs at neutral pH and 100 °C. Iron-sulfur surfaces, which are abundant near hydrothermal vents, can drive the production of small amounts of amino acids and other biomolecules.[60]
 In 1961, Peter Mitchell proposed chemiosmosis as a cell's primary system of energy conversion. The mechanism, now ubiquitous in living cells, powers energy conversion in micro-organisms and in the mitochondria of eukaryotes, making it a likely candidate for early life.[168][169] Mitochondria produce adenosine triphosphate (ATP), the energy currency of the cell used to drive cellular processes such as chemical syntheses. The mechanism of ATP synthesis involves a closed membrane in which the ATP synthase enzyme is embedded. The energy required to release strongly bound ATP has its origin in protons that move across the membrane.[170] In modern cells, those proton movements are caused by the pumping of ions across the membrane, maintaining an electrochemical gradient. In the first organisms, the gradient could have been provided by the difference in chemical composition between the flow from a hydrothermal vent and the surrounding seawater,[152] or perhaps meteoric quinones that were conducive to the development of chemiosmotic energy across lipid membranes if at a terrestrial origin.[171]
 The RNA world hypothesis describes an early Earth with self-replicating and catalytic RNA but no DNA or proteins.[172] Many researchers concur that an RNA world must have preceded the DNA-based life that now dominates.[173] However, RNA-based life may not have been the first to exist.[174][175] Another model echoes Darwin's ""warm little pond"" with cycles of wetting and drying.[176]
 RNA is central to the translation process. Small RNAs can catalyze all the chemical groups and information transfers required for life.[175][177] RNA both expresses and maintains genetic information in modern organisms; and the chemical components of RNA are easily synthesized under the conditions that approximated the early Earth, which were very different from those that prevail today. The structure of the ribosome has been called the ""smoking gun"", with a central core of RNA and no amino acid side chains within 18 Å of the active site that catalyzes peptide bond formation.[178][174][179]
 The concept of the RNA world was proposed in 1962 by Alexander Rich,[180] and the term was coined by Walter Gilbert in 1986.[175][181] There were initial difficulties in the explanation of the abiotic synthesis of the nucleotides cytosine and uracil.[182] Subsequent research has shown possible routes of synthesis; for example, formamide produces all four ribonucleotides and other biological molecules when warmed in the presence of various terrestrial minerals.[120][121]
 RNA replicase can function as both code and catalyst for further RNA replication, i.e. it can be autocatalytic. Jack Szostak has shown that certain catalytic RNAs can join smaller RNA sequences together, creating the potential for self-replication. The RNA replication systems, which include two ribozymes that catalyze each other's synthesis, showed a doubling time of the product of about one hour, and were subject to natural selection under the experimental conditions.[183][184][174] If such conditions were present on early Earth, then natural selection would favor the proliferation of such autocatalytic sets, to which further functionalities could be added.[185][186][187] Self-assembly of RNA may occur spontaneously in hydrothermal vents.[188][189][190] A preliminary form of tRNA could have assembled into such a replicator molecule.[191]
 Possible precursors to protein synthesis include the synthesis of short peptide cofactors or the self-catalysing duplication of RNA. It is likely that the ancestral ribosome was composed entirely of RNA, although some roles have since been taken over by proteins. Major remaining questions on this topic include identifying the selective force for the evolution of the ribosome and determining how the genetic code arose.[192]
 Eugene Koonin has argued that ""no compelling scenarios currently exist for the origin of replication and translation, the key processes that together comprise the core of biological systems and the apparent pre-requisite of biological evolution. The RNA World concept might offer the best chance for the resolution of this conundrum but so far cannot adequately account for the emergence of an efficient RNA replicase or the translation system.""[193]
 In line with the RNA world hypothesis, much of modern biology's templated protein biosynthesis is done by RNA molecules—namely tRNAs and the ribosome (consisting of both protein and rRNA components).  The most central reaction of peptide bond synthesis is understood to be carried out by base catalysis by the 23S rRNA domain V.[194] Experimental evidence has demonstrated successful di- and tripeptide synthesis with a system consisting of only aminoacyl phosphate adaptors and RNA guides, which could be a possible stepping stone between an RNA world and modern protein synthesis.[194][195] Aminoacylation ribozymes that can charge tRNAs with their cognate amino acids have also been selected in in vitro experimentation.[196] The authors also extensively mapped fitness landscapes within their selection to find that chance emergence of active sequences was more important that sequence optimization.[196]
 The first proteins would have had to arise without a fully-fledged system of protein biosynthesis. As discussed above, numerous mechanisms for the prebiotic synthesis of polypeptides exist. However, these random sequence peptides would not have likely had biological function. Thus, significant study has gone into exploring how early functional proteins could have arisen from random sequences. First, some evidence on hydrolysis rates shows that abiotically plausible peptides likely contained significant ""nearest-neighbor"" biases.[197] This could have had some effect on early protein sequence diversity. In other work by Anthony Keefe and Jack Szostak, mRNA display selection on a library of 6*1012 80-mers was used to search for sequences with ATP binding activity. They concluded that approximately 1 in 1011 random sequences had ATP binding function.[198] While this is a single example of functional frequency in the random sequence space, the methodology can serve as a powerful simulation tool for understanding early protein evolution.[199]
 Starting with the work of Carl Woese from 1977, genomics studies have placed the last universal common ancestor (LUCA) of all modern life-forms between Bacteria and a clade formed by Archaea and Eukaryota in the phylogenetic tree of life. It lived over 4 Gya.[200][201] A minority of studies have placed the LUCA in Bacteria, proposing that Archaea and Eukaryota are evolutionarily derived from within Eubacteria;[202] Thomas Cavalier-Smith suggested in 2006 that the phenotypically diverse bacterial phylum Chloroflexota contained the LUCA.[203]
 In 2016, a set of 355 genes likely present in the LUCA was identified. A total of 6.1 million prokaryotic genes from Bacteria and Archaea were sequenced, identifying 355 protein clusters from among 286,514 protein clusters that were probably common to the LUCA. The results suggest that the LUCA was anaerobic with a Wood–Ljungdahl (reductive Acetyl-CoA) pathway, nitrogen- and carbon-fixing, thermophilic. Its cofactors suggest dependence upon an environment rich in hydrogen, carbon dioxide, iron, and transition metals. Its genetic material was probably DNA, requiring the 4-nucleotide genetic code, messenger RNA, transfer RNA, and ribosomes to translate the code into proteins such as enzymes. LUCA likely inhabited an anaerobic hydrothermal vent setting in a geochemically active environment. It was evidently already a complex organism, and must have had precursors; it was not the first living thing.[10][204] The physiology of LUCA has been in dispute.[205][206][207] Previous research identified 60 proteins common to all life.[208]
 Leslie Orgel argued that early translation machinery for the genetic code would be susceptible to error catastrophe. Geoffrey Hoffmann however showed that such machinery can be stable in function against ""Orgel's paradox"".[209][210][211] Metabolic reactions that have also been inferred in LUCA are the incomplete reverse Krebs cycle, gluconeogenesis, the pentose phosphate pathway, glycolysis, reductive amination, and transamination.[212][213]
 A variety of geologic and environmental settings have been proposed for an origin of life. These theories are often in competition with one another as there are many differing views of prebiotic compound availability, geophysical setting, and early life characteristics. The first organism on Earth likely looked different from LUCA. Between the first appearance of life and where all modern phylogenies began branching, an unknown amount of time passed, with unknown gene transfers, extinctions, and evolutionary adaptation to various environmental niches.[214] One major shift is believed to be from the RNA world to an RNA-DNA-protein world. Modern phylogenies provide more pertinent genetic evidence about LUCA than about its precursors.[215]
 The most popular hypotheses for settings for the origin of life are deep sea hydrothermal vents and surface bodies of water. Surface waters can be classified into hot springs, moderate temperature lakes and ponds, and cold settings.
 Early micro-fossils may have come from a hot world of gases such as methane, ammonia, carbon dioxide, and hydrogen sulfide, toxic to much current life.[216] Analysis of the tree of life places thermophilic and hyperthermophilic bacteria and archaea closest to the root, suggesting that life may have evolved in a hot environment.[217] The deep sea or alkaline hydrothermal vent theory posits that life began at submarine hydrothermal vents.[218][219] William Martin and Michael Russell have suggested ""that life evolved in structured iron monosulphide precipitates in a seepage site hydrothermal mound at a redox, pH, and temperature gradient between sulphide-rich hydrothermal fluid and iron(II)-containing waters of the Hadean ocean floor. The naturally arising, three-dimensional compartmentation observed within fossilized seepage-site metal sulphide precipitates indicates that these inorganic compartments were the precursors of cell walls and membranes found in free-living prokaryotes. The known capability of FeS and NiS to catalyze the synthesis of the acetyl-methylsulphide from carbon monoxide and methylsulphide, constituents of hydrothermal fluid, indicates that pre-biotic syntheses occurred at the inner surfaces of these metal-sulphide-walled compartments"".[220]
 These form where hydrogen-rich fluids emerge from below the sea floor, as a result of serpentinization of ultra-mafic olivine with seawater and a pH interface with carbon dioxide-rich ocean water. The vents form a sustained chemical energy source derived from redox reactions, in which electron donors (molecular hydrogen) react with electron acceptors (carbon dioxide); see iron–sulfur world theory. These are exothermic reactions.[218][b]
 Russell demonstrated that alkaline vents created an abiogenic proton motive force chemiosmotic gradient,[220] ideal for abiogenesis. Their microscopic compartments ""provide a natural means of concentrating organic molecules,"" composed of iron-sulfur minerals such as mackinawite, endowed these mineral cells with the catalytic properties envisaged by Günter Wächtershäuser.[221] This movement of ions across the membrane depends on a combination of two factors:
 These two gradients taken together can be expressed as an electrochemical gradient, providing energy for abiogenic synthesis. The proton motive force can be described as the measure of the potential energy stored as a combination of proton and voltage gradients across a membrane (differences in proton concentration and electrical potential).[152]
 The surfaces of mineral particles inside deep-ocean hydrothermal vents have catalytic properties similar to those of enzymes and can create simple organic molecules, such as methanol (CH3OH) and formic, acetic, and pyruvic acids out of the dissolved CO2 in the water, if driven by an applied voltage or by reaction with H2 or H2S.[222][223]
 Starting in 1985, researchers proposed that life arose at hydrothermal vents,[224][225] that spontaneous chemistry in the Earth's crust driven by rock–water interactions at disequilibrium thermodynamically underpinned life's origin[226][227] and that the founding lineages of the archaea and bacteria were H2-dependent autotrophs that used CO2 as their terminal acceptor in energy metabolism.[228] In 2016, Martin suggested, based upon this evidence, that the LUCA ""may have depended heavily on the geothermal energy of the vent to survive"".[10] Pores at deep sea hydrothermal vents are suggested to have been occupied by membrane-bound compartments which promoted biochemical reactions.[229][230] Metabolic intermediates in the Krebs cycle, gluconeogenesis, amino acid bio-synthetic pathways, glycolysis, the pentose phosphate pathway, and including sugars like ribose, and lipid precursors can occur non-enzymatically at conditions relevant to deep-sea alkaline hydrothermal vents.[231]
 If the deep marine hydrothermal setting was the site for the origin of life, then abiogenesis could have happened as early as 4.0-4.2 Gya. If life evolved in the ocean at depths of more than ten meters, it would have been shielded both from impacts and the then high levels of ultraviolet radiation from the sun. The available energy in hydrothermal vents is maximized at 100–150 °C, the temperatures at which hyperthermophilic bacteria and thermoacidophilic archaea live.[232][233] Arguments against a hydrothermal origin of life state that hyperthermophily was a result of convergent evolution in bacteria and archaea, and that a mesophilic environment would have been more likely.[234][235] This hypothesis, suggested in 1999 by Galtier, was proposed one year before the discovery of the Lost City Hydrothermal Field, where white-smoker hydrothermal vents average ~45-90 °C.[236] Moderate temperatures and alkaline seawater such as that at Lost City are now the favoured hydrothermal vent setting in contrast to acidic, high temperature (~350 °C) black-smokers.
 Production of prebiotic organic compounds at hydrothermal vents is estimated to be 1x108 kg yr−1.[237] While a large amount of key prebiotic compounds, such as methane, are found at vents, they are in far lower concentrations than estimates of a Miller-Urey Experiment environment. In the case of methane, the production rate at vents is around 2-4 orders of magnitude lower than predicted amounts in a Miller-Urey Experiment surface atmosphere.[237][238]
 Other arguments against an oceanic vent setting for the origin of life include the inability to concentrate prebiotic materials due to strong dilution from seawater. This open-system cycles compounds through minerals that make up vents, leaving little residence time to accumulate.[239] All modern cells rely on phosphates and potassium for nucleotide backbone and protein formation respectively, making it likely that the first life forms also shared these functions. These elements were not available in high quantities in the Archaean oceans as both primarily come from the weathering of continental rocks on land, far from vent settings. Submarine hydrothermal vents are not conducive to condensation reactions needed for polymerisation to form macromolecules.[240][241]
 An older argument was that key polymers were encapsulated in vesicles after condensation, which supposedly would not happen in saltwater because of the high concentrations of ions. However, while it is true that salinity inhibits vesicle formation from low-diversity mixtures of fatty acids,[242] vesicle formation from a broader, more realistic mix of fatty-acid and 1-alkanol species is more resilient.[243][242]
 Surface bodies of water provide environments able to dry out and be rewetted. Continued wet-dry cycles allow the concentration of prebiotic compounds and condensation reactions to polymerise macromolecules. Moreover, lake and ponds on land allow for detrital input from the weathering of continental rocks which contain apatite, the most common source of phosphates needed for nucleotide backbones. The amount of exposed continental crust in the Hadean is unknown, but models of early ocean depths and rates of ocean island and continental crust growth make it plausible that there was exposed land.[244] Another line of evidence for a surface start to life is the requirement for UV for organism function. UV is necessary for the formation of the U+C nucleotide base pair by partial hydrolysis and nucleobase loss.[245] Simultaneously, UV can be harmful and sterilising to life, especially for simple early lifeforms with little ability to repair radiation damage. Radiation levels from a young Sun were likely greater, and, with no ozone layer, harmful shortwave UV rays would reach the surface of Earth. For life to begin, a shielded environment with influx from UV-exposed sources is necessary to both benefit and protect from UV. Shielding under ice, liquid water, mineral surfaces (e.g. clay) or regolith is possible in a range of surface water settings. While deep sea vents may have input from raining down of surface exposed materials, the likelihood of concentration is lessened by the ocean's open system.[246]
 Most branching phylogenies are thermophilic or hyperthermophilic, making it possible that the Last universal common ancestor (LUCA) and preceding lifeforms were similarly thermophilic. Hot springs are formed from the heating of groundwater by geothermal activity. This intersection allows for influxes of material from deep penetrating waters and from surface runoff that transports eroded continental sediments. Interconnected groundwater systems create a mechanism for distribution of life to wider area.[247]
 Mulkidjanian and co-authors argue that marine environments did not provide the ionic balance and composition universally found in cells, or the ions required by essential proteins and ribozymes, especially with respect to high K+/Na+ ratio, Mn2+, Zn2+ and phosphate concentrations. They argue that the only environments that mimic the needed conditions on Earth are hot springs similar to ones at Kamchatka.[248] Mineral deposits in these environments under an anoxic atmosphere would have suitable pH (while current pools in an oxygenated atmosphere would not), contain precipitates of photocatalytic sulfide minerals that absorb harmful ultraviolet radiation, have wet-dry cycles that concentrate substrate solutions to concentrations amenable to spontaneous formation of biopolymers[249][250] created both by chemical reactions in the hydrothermal environment, and by exposure to UV light during transport from vents to adjacent pools that would promote the formation of biomolecules.[251] The hypothesized pre-biotic environments are similar to hydrothermal vents, with additional components that help explain peculiarities of the LUCA.[248][171]
 A phylogenomic and geochemical analysis of proteins plausibly traced to the LUCA shows that the ionic composition of its intracellular fluid is identical to that of hot springs. The LUCA likely was dependent upon synthesized organic matter for its growth.[248] Experiments show that RNA-like polymers can be synthesized in wet-dry cycling and UV light exposure. These polymers were encapsulated in vesicles after condensation.[242] Potential sources of organics at hot springs might have been transport by interplanetary dust particles, extraterrestrial projectiles, or atmospheric or geochemical synthesis. Hot springs could have been abundant in volcanic landmasses during the Hadean.[171]
 A mesophilic start in surface bodies of waters hypothesis has evolved from Darwin's concept of a 'warm little pond' and the Oparin-Haldane hypothesis. Freshwater bodies under temperate climates can accumulate prebiotic materials while providing suitable environmental conditions conducive to simple life forms. The climate during the Archaean is still a highly debated topic, as there is uncertainty about what continents, oceans, and the atmosphere looked like then. Atmospheric reconstructions of the Archaean from geochemical proxies and models state that sufficient greenhouse gases were present to maintain surface temperatures between 0-40 °C. Under this assumption, there is a greater abundance of moderate temperature niches in which life could begin.[252]
 Strong lines of evidence for mesophily from biomolecular studies include Galtier's G+C nucleotide thermometer. G+C are more abundant in thermophiles due to the added stability of an additional hydrogen bond not present between A+T nucleotides. rRNA sequencing on a diverse range of modern lifeforms show that LUCA's reconstructed G+C content was likely representative of moderate temperatures.[235]
 Although most modern phylogenies are thermophilic or hyperthermophilic, it is possible that their widespread diversity today is a product of convergent evolution and horizontal gene transfer rather than an inherited trait from LUCA.[253] The reverse gyrase topoisomerase is found exclusively in thermophiles and hyperthermophiles as it allows for coiling of DNA.[254] The reverse gyrase enzyme requires ATP to function, both of which are complex biomolecules. If an origin of life is hypothesised to involve a simple organism that had not yet evolved a membrane, let alone ATP, this would make the existence of reverse gyrase improbable. Moreover, phylogenetic studies show that reverse gyrase had an archaeal origin, and that it was transferred to bacteria by horizontal gene transfer. This implies that reverse gyrase was not present in the LUCA.[255]
 Cold-start origin of life theories stem from the idea there may have been cold enough regions on the early Earth that large ice cover could be found. Stellar evolution models predict that the Sun's luminosity was ~25% weaker than it is today. Fuelner states that although this significant decrease in solar energy would have formed an icy planet, there is strong evidence for liquid water to be present, possibly driven by a greenhouse effect. This would create an early Earth with both liquid oceans and icy poles.[256]
 Ice melts that form from ice sheets or glaciers melts create freshwater pools, another niche capable of experiencing wet-dry cycles. While these pools that exist on the surface would be exposed to intense UV radiation, bodies of water within and under ice are sufficiently shielded while remaining connected to UV exposed areas through ice cracks. Suggestions of impact melting of ice allow freshwater paired with meteoritic input, a popular vessel for prebiotic components.[257] Near-seawater levels of sodium chloride are found to destabilize fatty acid membrane self-assembly, making freshwater settings appealing for early membranous life.[258]
 Icy environments would trade the faster reaction rates that occur in warm environments for increased stability and accumulation of larger polymers.[259] Experiments simulating Europa-like conditions of ~20 °C have synthesised amino acids and adenine, showing that Miller-Urey type syntheses can still occur at cold temperatures.[260] In an RNA world, the ribozyme would have had even more functions than in a later DNA-RNA-protein-world. For RNA to function, it must be able to fold, a process that is hindered by temperatures above 30 °C. While RNA folding in psychrophilic organisms is slower, the process is more successful as hydrolysis is also slower. Shorter nucleotides would not suffer from higher temperatures.[261][262]
 An alternative geological environment has been proposed by the geologist Ulrich Schreiber and the physical chemist Christian Mayer: the continental crust.[263] Tectonic fault zones could present a stable and well-protected environment for long-term prebiotic evolution. Inside these systems of cracks and cavities, water and carbon dioxide present the bulk solvents. Their phase state would depend on the local temperature and pressure conditions and could vary between liquid, gaseous and supercritical. When forming two separate phases (e.g., liquid water and supercritical carbon dioxide in depths of little more than 1 km), the system provides optimal conditions for phase transfer reactions. Concurrently, the contents of the tectonic fault zones are being supplied by a multitude of inorganic educts (e.g., carbon monoxide, hydrogen, ammonia, hydrogen cyanide, nitrogen, and even phosphate from dissolved apatite) and simple organic molecules formed by hydrothermal chemistry (e.g. amino acids, long-chain amines, fatty acids, long-chain aldehydes).[264][265] Finally, the abundant mineral surfaces provide a rich choice of catalytic activity.
 An especially interesting section of the tectonic fault zones is located at a depth of approximately 1000 m. For the carbon dioxide part of the bulk solvent, it provides temperature and pressure conditions near the phase transition point between the supercritical and the gaseous state. This leads to a natural accumulation zone for lipophilic organic molecules that dissolve well in supercritical CO2, but not in its gaseous state, leading to their local precipitation.[266] Periodic pressure variations such as caused by geyser activity or tidal influences result in periodic phase transitions, keeping the local reaction environment in a constant non-equilibrium state. In presence of amphiphilic compounds (such as the long chain amines and fatty acids mentioned above), subsequent generations of vesicles are being formed[267] that are constantly and efficiently being selected for their stability.[268] The resulting structures could provide hydrothermal vents as well as hot springs with raw material for further development.
 Homochirality is the geometric uniformity of materials composed of chiral (non-mirror-symmetric) units. Living organisms use molecules that have the same chirality (handedness): with almost no exceptions,[270] amino acids are left-handed while nucleotides and sugars are right-handed. Chiral molecules can be synthesized, but in the absence of a chiral source or a chiral catalyst, they are formed in a 50/50 (racemic) mixture of both forms. Known mechanisms for the production of non-racemic mixtures from racemic starting materials include: asymmetric physical laws, such as the electroweak interaction; asymmetric environments, such as those caused by circularly polarized light, quartz crystals, or the Earth's rotation, statistical fluctuations during racemic synthesis,[269] and spontaneous symmetry breaking.[271][272][273]
 Once established, chirality would be selected for.[274] A small bias (enantiomeric excess) in the population can be amplified into a large one by asymmetric autocatalysis, such as in the Soai reaction.[275] In asymmetric autocatalysis, the catalyst is a chiral molecule, which means that a chiral molecule is catalyzing its own production. An initial enantiomeric excess, such as can be produced by polarized light, then allows the more abundant enantiomer to outcompete the other.[276]
 Homochirality may have started in outer space, as on the Murchison meteorite the amino acid L-alanine (left-handed) is more than twice as frequent as its D (right-handed) form, and L-glutamic acid is more than three times as abundant as its D counterpart.[277][278] Amino acids from meteorites show a left-handed bias, whereas sugars show a predominantly right-handed bias: this is the same preference found in living organisms, suggesting an abiogenic origin of these compounds.[279]
 In a 2010 experiment by Robert Root-Bernstein, ""two D-RNA-oligonucleotides having inverse base sequences (D-CGUA and D-AUGC) and their corresponding L-RNA-oligonucleotides (L-CGUA and L-AUGC) were synthesized and their affinity determined for Gly and eleven pairs of L- and D-amino acids"". The results suggest that homochirality, including codon directionality, might have ""emerged as a function of the origin of the genetic code"".[280]
",abiogenesi natur process life aris matter simpl organ compound prevail scientif hypothesi transit live entiti earth singl event process increas complex involv format habit planet prebiot synthesi organ molecul molecular autocatalysi emerg cell membran transit life never observ experiment mani propos made differ stage process studi abiogenesi aim determin chemic reaction gave rise life condit strikingli differ earth today primarili use tool biolog chemistri recent approach attempt synthesi mani scienc life function special chemistri carbon water build larg upon four key famili chemic lipid cell membran carbohydr sugar amino acid protein metabol nucleic acid dna rna mechan hered success theori abiogenesi must explain origin interact class molecul mani approach abiogenesi investig molecul compon came exist research gener think current life descend rna world although molecul may preced rna approach hypothes focu understand catalysi chemic system earli earth might provid precursor molecul necessari classic experi demonstr amino acid chemic constitu protein synthes inorgan compound condit intend replic earli earth extern sourc energi may trigger reaction includ lightn radiat atmospher entri implos bubbl sea ocean wave last univers common ancestor modern organ luca thought quit differ origin life investig luca guid research earli univers characterist genom approach sought characteris luca identifi gene share archaea bacteria member two major branch life eukaryot includ archaean branch system appear protein common life prokaryot gene trace luca function impli luca anaerob pathway deriv energi chemiosmosi maintain hereditari materi dna genet code ribosom although luca live billion year ago gya research believ far first form life earlier cell might leaki membran power natur occur proton gradient near white smoker hydrotherm vent earth remain place univers known harbor life geochem fossil evid earth inform studi abiogenesi earth form gya earliest evid life earth date least gya western australia studi suggest fossil may live within hydrotherm vent precipit date gya quebec soon ocean format gya hadean life consist reproduct herit variat nasa defin life chemic system capabl darwinian biolog evolut system complex last univers common ancestor luca presum organ live billion year ago alreadi hundr gene encod dna genet code univers today turn impli suit cellular machineri includ messeng rna transfer rna ribosom translat code protein protein includ enzym oper anaerob respir via metabol pathway dna polymeras replic genet materi challeng abiogenesi origin life research explain complex tightli interlink system could develop evolutionari step first sight part necessari enabl function exampl cell whether luca modern organ copi dna dna polymeras enzym turn produc translat dna polymeras gene dna neither enzym dna produc without evolutionari process could involv molecular cell membran autocatalysi via rna ribozym nonetheless transit life never observ experiment satisfactori chemic explan precondit develop live cell like luca clear enough though disput detail habit world form suppli miner liquid water prebiot synthesi creat rang simpl organ compound assembl polym protein rna side process luca readili understood biolog evolut caus develop wide rang speci vari form biochem capabl howev deriv live thing luca simpl compon far understood although earth remain place life known scienc astrobiolog seek evid life planet nasa strategi origin life aim solv puzzl identifi interact intermediari structur function energi sourc environment factor contribut divers select replic evolv macromolecular system map chemic landscap potenti primordi inform polym advent polym could replic store genet inform exhibit properti subject select suggest like critic step emerg prebiot chemic evolut polym deriv turn simpl organ compound nucleobas amino acid sugar could form reaction environ success theori origin life must explain chemic came one ancient view origin life aristotl centuri spontan gener theori held lower anim insect gener decay organ substanc life aros chanc question centuri work like thoma brown pseudodoxia epidemica robert hook publish first draw microorgan antoni van leeuwenhoek drew describ microorgan probabl protozoa bacteria van leeuwenhoek disagre spontan gener convinc use experi rang seal open meat incub close studi insect reproduct theori incorrect francesco redi show maggot appear meat fli prevent lay egg middl centuri spontan gener consid disproven anoth ancient idea date back anaxagora centuri bc panspermia idea life exist throughout univers distribut meteoroid asteroid comet planetoid attempt explain life origin shift origin life earth anoth heavenli bodi advantag life requir form planet occur rather limit set locat even singl locat spread galaxi star system via cometari meteorit impact panspermia get much scientif support larg use deflect need answer instead explain observ phenomena although interest panspermia grew studi meteorit found trace organ materi current accept life start local earth idea life origin matter slow stage appear herbert spencer book principl biolog william turner paper spontan gener evolut februari charl darwin wrote public joseph hooker set specul suggest origin spark life may begun warm littl pond sort ammonia phosphor salt light heat electr present protein compound chemic form readi undergo still complex chang darwin went explain present day matter would instantli devour absorb would case live creatur form alexand oparin haldan propos first molecul constitut earliest cell slowli primordi soup theori call hypothesi haldan suggest earth prebiot ocean consist hot dilut soup organ compound could form bernal show mechan could form necessari molecul life inorgan precursor suggest three stage origin biolog monom origin biolog polym evolut molecul cell stanley miller harold urey carri chemic experi demonstr organ molecul could form spontan inorgan precursor prebiot condit like posit hypothesi use highli reduc lack oxygen mixtur ammonia hydrogen well water form simpl organ monom amino acid bernal said experi enough explain format molecul necessari explan origin molecul suggest presenc suitabl sourc sink free energi howev current scientif consensu describ primit atmospher weakli reduc neutral diminish amount varieti amino acid could produc addit iron carbon miner present earli ocean howev produc divers array amino acid later work focus two potenti reduc environ outer space hydrotherm vent soon big bang occur roughli gya chemic element present univers hydrogen helium lithium three lightest atom period tabl element gradual accret began orbit disk ga dust gravit accret materi hot dens center protoplanetari disk form star fusion hydrogen earli star massiv produc heavier element stellar nucleosynthesi element format stellar nucleosynthesi proce stabl element heavier element form supernova end star lifecycl carbon current fourth abund chemic element univers hydrogen helium oxygen form mainli white dwarf star particularli bigger twice mass sun star reach end lifecycl eject heavier element among carbon oxygen throughout univers heavier element allow format new object includ rocki planet bodi accord nebular hypothesi format evolut solar system began gya gravit collaps small part giant molecular cloud collaps mass collect center form sun rest flatten protoplanetari disk planet moon asteroid small solar system bodi form age earth gya found radiometr date inclus carbonac chrondrit meteorit oldest materi solar system hadean earth format gya first inhospit live organ format earth lost signific part initi mass consequenti lack graviti hold molecular hydrogen bulk origin inert gase soon initi accret earth ga collis theia hypothesis impactor thought creat eject debri would eventu form moon impact would remov earth primari atmospher leav behind cloud viscou silic carbon dioxid unstabl atmospher condens shortli form bulk silic earth leav behind atmospher larg consist water vapor nitrogen carbon dioxid smaller amount carbon monoxid hydrogen sulfur compound solut carbon dioxid water thought made sea slightli acid ph condens form liquid ocean theoris occur earli impact scenario found support date gya zircon crystal high valu metamorphos quartzit mount narryer western australia hadean atmospher character gigant product outdoor chemic laboratori similar volcan gase today still support abiot chemistri despit like increas volcan earli plate tecton earth may predominantli water world gya debat whether crust expos ocean due uncertainti earli plate tecton look like earli life develop gener thought land set requir question essenti determin earth histori life evolv impact earth like exist littl continent crust turbul atmospher hydrospher subject intens ultraviolet light tauri stage sun cosmic radiat continu asteroid comet impact despit nich environ like exist conduc life earth late heavi bombard hypothesi posit period intens impact occur gya hadean cataclysm impact event would potenti sterilis life earth volatilis liquid ocean block sun need photosynthesis primari produc push back earliest possibl emerg life late heavi bombard recent research question intens late heavi bombard well potenti sterilis uncertainti whether late heavi bombard one giant impact period greater impact rate greatli chang implic destruct power ga date aris date apollo mission sampl return collect mostli near imbrium basin bias age record impact impact model lunar surfac reveal rather cataclysm event ga multipl period bombard like occur terrestri data back idea show multipl period ejecta rock record ga marker suggest earli earth subject continu impact would great impact extinct previous thought late heavi bombard take place allow emerg life taken place far life evolv ocean depth ten meter would shield late impact high level ultraviolet radiat sun geotherm heat ocean crust could yield far organ compound deep hydrotherm vent experi indic avail energi maxim temperatur hyperthermophil bacteria thermoacidophil archaea live exact time life emerg earth unknown minimum age estim base evid geolog rock record earliest physic evid life far found consist microbialit nuvvuagittuq greenston belt northern quebec band iron format rock least possibl old gya live within hydrotherm vent precipit soon gya format ocean hadean microb resembl modern hydrotherm vent bacteria support view abiogenesi began environ biogen graphit found gya metasedimentari rock southwestern greenland microbi mat fossil gya chert pilbara region western australia evid earli life rock akilia island near isua supracrust belt southwestern greenland date gya shown biogen carbon isotop part isua supracrust belt graphit inclus trap within garnet crystal connect element life oxygen nitrogen possibl phosphoru form phosphat provid evid life gya pilbara region western australia compel evid earli life found sandston fossil beach round tubular cell oxid sulfur photosynthesi absenc oxygen carbon isotop ratio graphit inclus jack hill zircon suggest life could exist earth gya pilbara region western australia contain dresser format rock gya includ layer structur call stromatolit modern counterpart creat photosynthet includ cyanobacteria lie within undeform strata textur indic biogen origin part dresser format preserv hot spring land region seem shallow sea molecular clock analysi suggest luca emerg prior late heavi bombard gya chemic element except hydrogen helium deriv stellar nucleosynthesi basic chemic ingredi life molecul ch posit ion carbon ion produc ultraviolet light star complex molecul includ organ molecul form natur space planet organ molecul earli earth could either terrestri origin organ molecul synthesi driven impact shock energi sourc ultraviolet light redox coupl electr discharg extraterrestri origin organ molecul form interstellar dust cloud rain planet organ compound chemic whose molecul contain carbon carbon abund sun star comet atmospher planet organ compound rel common space form factori complex molecular synthesi occur molecular cloud circumstellar envelop chemic evolv reaction initi mostli ioniz radiat purin pyrimidin nucleobas includ guanin adenin cytosin uracil thymin found meteorit could provid materi dna rna form earli earth amino acid glycin found materi eject comet wild earlier detect meteorit comet encrust dark materi thought organ substanc form simpl carbon compound ioniz radiat rain materi comet could brought complex organ molecul earth estim late heavi bombard meteorit may deliv five million ton organ prebiot element earth per year polycycl aromat hydrocarbon pah common abund polyatom molecul observ univers major store carbon seem form shortli big bang associ new star exoplanet like constitu earth primordi sea pah detect nebula interstellar medium comet meteorit pah world hypothesi posit pah precursor rna world star hh resembl sun earli life surround disk materi contain molecul includ cyanid compound hydrocarbon carbon monoxid pah interstellar medium transform hydrogen oxygen hydroxyl complex organ compound use live cell organ compound introduc earth interstellar dust particl help form complex molecul thank peculiar activ studi isotop ratio organ compound murchison meteorit suggest rna compon uracil relat molecul includ xanthin form extraterrestri nasa studi meteorit suggest four dna nucleobas adenin guanin relat organ molecul form outer space cosmic dust permeat univers contain complex organ amorph organ solid mix structur could creat rapidli star glycolaldehyd sugar molecul rna precursor detect region space includ around protostar meteorit earli experi demonstr biolog relev molecul produc interact simpl carbon sourc abund inorgan catalyst spontan format complex polym abiot gener monom condit posit soup theori straightforward besid necessari basic organ monom compound would prohibit format polym also form high concentr joan oró experi biolog use essenti amino acid code protein enzym repres small subset structur possibl product sinc life tend use whatev avail explan need set use small formamid attract medium potenti provid sourc amino acid deriv simpl aldehyd nitril feedstock alexand butlerov show formos reaction creat sugar includ tetros pentos hexos formaldehyd heat basic condit dival metal ion like calcium breslow propos reaction autocatalyt nucleobas guanin adenin synthes simpl carbon nitrogen sourc hydrogen cyanid hcn ammonia formamid produc four ribonucleotid warm terrestri miner formamid ubiquit univers produc reaction water hcn concentr evapor water hcn poison aerob organ eukaryot aerob bacteria yet exist play role chemic process synthesi amino acid glycin dna rna compon includ uracil cytosin thymin synthes outer space condit use start chemic pyrimidin found meteorit pyrimidin may form red giant star interstellar dust ga cloud four may synthes formamid densiti event like extraterrestri impact pathway synthes base inorgan materi report freez temperatur advantag synthesi purin due concentr effect key precursor hydrogen cyanid howev adenin guanin requir freez condit synthesi cytosin uracil may requir boil temperatur seven amino acid eleven type nucleobas form ice ammonia cyanid left freezer year altern nucleobas pyrimidin includ cytosin uracil adenin synthes subject urea solut cycl reduct atmospher spark discharg energi sourc explan given unusu speed reaction low temperatur eutect freez crowd impur microscop pocket liquid within ice caus molecul collid often prebiot peptid synthesi propos occur number possibl rout center high condit condens becom energet favor other focu avail plausibl prebiot condens agent explan need experiment evid format peptid uniqu concentr environ bolster work suggest cycl presenc specif salt greatli increas spontan condens glycin chain work suggest miner surfac pyrit calcit rutil catalyz peptid condens also catalyz hydrolysi author suggest addit chemic activ coupl would necessari produc peptid suffici concentr thu miner surfac catalysi import suffici alon peptid synthesi mani prebiot plausibl agent identifi includ follow cyanamid dicyanamid dicyandiamid diaminomaleonitril urea trimetaphosph nacl ni fe co carbonyl sulfid co carbon disulfid diammonium phosphat dap experi report use sapphir substrat web thin crack heat flow similar environ vent mechan separ concentr prebiot relev build block dilut mixtur purifi concentr three order magnitud author propos plausibl model origin complex biopolym present anoth physic process allow concentr peptid precursor combin right condit similar role increas amino acid concentr suggest clay well scenario involv condens amino acid prebiot synthesi peptid simpler molecul co c skip step amino acid format effici largest unansw question evolut simpl protocel first aros differ reproduct contribut follow gener thu initi evolut life lipid world theori postul first object phospholipid form lipid bilay water structur cell membran molecul present earli earth amphiphil molecul also form membran bodi may expand insert addit lipid may spontan split two offspr similar size composit lipid bodi may provid shelter envelop inform storag allow evolut preserv polym like rna store inform one two type amphiphil studi might led develop vesicl enorm number possibl arrang lipid bilay membran best reproduct characterist would converg toward hypercycl reaction posit feedback compos two mutual catalyst repres membran site specif compound trap vesicl pair transmiss daughter vesicl lead emerg distinct lineag vesicl would allow natur select protocel spheric collect lipid propos origin life function protocel yet achiev laboratori set vesicl essenti compon primit cell theori classic irrevers thermodynam treat gener chemic potenti within framework dissip system second law thermodynam requir overal entropi increas yet life distinguish great degre organ therefor boundari need separ order life process chaotic matter iren chen jack szostak suggest elementari protocel give rise cellular behavior includ primit form differenti reproduct competit energi storag competit membran molecul would favor stabil membran suggest select advantag evolut fatti acid even phospholipid today would allow metabol within membran exchang small molecul retain larg biomolecul insid membran need cell creat electrochem gradient store energi pump ion across membran fatti acid vesicl condit relev alkalin hydrotherm vent stabil isoprenoid synthes formos reaction advantag disadvantag isoprenoid incorpor within lipid bilay differ microenviron might led diverg membran archaea bacteria laboratori experi shown vesicl undergo evolutionari process pressur cycl condit simul system environ tecton fault zone within earth crust pressur cycl lead period format vesicl condit random peptid chain form continu select abil integr vesicl membran select vesicl stabil potenti lead develop function peptid structur associ increas surviv rate vesicl life requir loss entropi disord molecul organ live matter time emerg life associ format structur beyond certain threshold complex emerg life increas order complex contradict second law thermodynam state overal entropi never decreas sinc live organ creat order place live bodi expens increas entropi elsewher heat wast product multipl sourc energi avail chemic reaction earli earth heat geotherm process standard energi sourc chemistri exampl includ sunlight lightn atmospher entri implos bubbl sea ocean wave confirm experi simul unfavor reaction driven highli favor one case chemistri exampl probabl import carbon fixat carbon fixat reaction via chemistri favor occur neutral ph surfac abund near hydrotherm vent drive product small amount amino acid biomolecul peter mitchel propos chemiosmosi cell primari system energi convers mechan ubiquit live cell power energi convers mitochondria eukaryot make like candid earli life mitochondria produc adenosin triphosph atp energi currenc cell use drive cellular process chemic synthes mechan atp synthesi involv close membran atp synthas enzym embed energi requir releas strongli bound atp origin proton move across membran modern cell proton movement caus pump ion across membran maintain electrochem gradient first organ gradient could provid differ chemic composit flow hydrotherm vent surround seawat perhap meteor quinon conduc develop chemiosmot energi across lipid membran terrestri origin rna world hypothesi describ earli earth catalyt rna dna protein mani research concur rna world must preced life domin howev life may first exist anoth model echo darwin warm littl pond cycl wet dri rna central translat process small rna catalyz chemic group inform transfer requir life rna express maintain genet inform modern organ chemic compon rna easili synthes condit approxim earli earth differ prevail today structur ribosom call smoke gun central core rna amino acid side chain within å activ site catalyz peptid bond format concept rna world propos alexand rich term coin walter gilbert initi difficulti explan abiot synthesi nucleotid cytosin uracil subsequ research shown possibl rout synthesi exampl formamid produc four ribonucleotid biolog molecul warm presenc variou terrestri miner rna replicas function code catalyst rna replic autocatalyt jack szostak shown certain catalyt rna join smaller rna sequenc togeth creat potenti rna replic system includ two ribozym catalyz synthesi show doubl time product one hour subject natur select experiment condit condit present earli earth natur select would favor prolifer autocatalyt set function could ad rna may occur spontan hydrotherm vent preliminari form trna could assembl replic molecul possibl precursor protein synthesi includ synthesi short peptid cofactor duplic rna like ancestr ribosom compos entir rna although role sinc taken protein major remain question topic includ identifi select forc evolut ribosom determin genet code aros eugen koonin argu compel scenario current exist origin replic translat key process togeth compris core biolog system appar biolog evolut rna world concept might offer best chanc resolut conundrum far adequ account emerg effici rna replicas translat system line rna world hypothesi much modern biolog templat protein biosynthesi done rna trna ribosom consist protein rrna compon central reaction peptid bond synthesi understood carri base catalysi rrna domain experiment evid demonstr success tripeptid synthesi system consist aminoacyl phosphat adaptor rna guid could possibl step stone rna world modern protein synthesi aminoacyl ribozym charg trna cognat amino acid also select vitro experiment author also extens map fit landscap within select find chanc emerg activ sequenc import sequenc optim first protein would aris without system protein biosynthesi discuss numer mechan prebiot synthesi polypeptid exist howev random sequenc peptid would like biolog function thu signific studi gone explor earli function protein could arisen random sequenc first evid hydrolysi rate show abiot plausibl peptid like contain signific bias could effect earli protein sequenc divers work anthoni keef jack szostak mrna display select librari use search sequenc atp bind activ conclud approxim random sequenc atp bind function singl exampl function frequenc random sequenc space methodolog serv power simul tool understand earli protein evolut start work carl woes genom studi place last univers common ancestor luca modern bacteria clade form archaea eukaryota phylogenet tree life live gya minor studi place luca bacteria propos archaea eukaryota evolutionarili deriv within eubacteria thoma suggest phenotyp divers bacteri phylum chloroflexota contain luca set gene like present luca identifi total million prokaryot gene bacteria archaea sequenc identifi protein cluster among protein cluster probabl common luca result suggest luca anaerob reduct pathway thermophil cofactor suggest depend upon environ rich hydrogen carbon dioxid iron transit metal genet materi probabl dna requir genet code messeng rna transfer rna ribosom translat code protein enzym luca like inhabit anaerob hydrotherm vent set geochem activ environ evid alreadi complex organ must precursor first live thing physiolog luca disput previou research identifi protein common life lesli orgel argu earli translat machineri genet code would suscept error catastroph geoffrey hoffmann howev show machineri stabl function orgel paradox metabol reaction also infer luca incomplet revers kreb cycl gluconeogenesi pentos phosphat pathway glycolysi reduct amin transamin varieti geolog environment set propos origin life theori often competit one anoth mani differ view prebiot compound avail geophys set earli life characterist first organ earth like look differ luca first appear life modern phylogeni began branch unknown amount time pass unknown gene transfer extinct evolutionari adapt variou environment nich one major shift believ rna world world modern phylogeni provid pertin genet evid luca precursor popular hypothes set origin life deep sea hydrotherm vent surfac bodi water surfac water classifi hot spring moder temperatur lake pond cold set earli may come hot world gase methan ammonia carbon dioxid hydrogen sulfid toxic much current life analysi tree life place thermophil hyperthermophil bacteria archaea closest root suggest life may evolv hot environ deep sea alkalin hydrotherm vent theori posit life began submarin hydrotherm vent william martin michael russel suggest life evolv structur iron monosulphid precipit seepag site hydrotherm mound redox ph temperatur gradient hydrotherm fluid iron ii water hadean ocean floor natur aris compartment observ within fossil metal sulphid precipit indic inorgan compart precursor cell wall membran found prokaryot known capabl fe ni catalyz synthesi carbon monoxid methylsulphid constitu hydrotherm fluid indic synthes occur inner surfac compart form fluid emerg sea floor result serpentin olivin seawat ph interfac carbon ocean water vent form sustain chemic energi sourc deriv redox reaction electron donor molecular hydrogen react electron acceptor carbon dioxid see world theori exotherm reaction b russel demonstr alkalin vent creat abiogen proton motiv forc chemiosmot gradient ideal abiogenesi microscop compart provid natur mean concentr organ molecul compos miner mackinawit endow miner cell catalyt properti envisag günter wächtershäus movement ion across membran depend combin two factor two gradient taken togeth express electrochem gradient provid energi abiogen synthesi proton motiv forc describ measur potenti energi store combin proton voltag gradient across membran differ proton concentr electr potenti surfac miner particl insid hydrotherm vent catalyt properti similar enzym creat simpl organ molecul methanol formic acet pyruv acid dissolv water driven appli voltag reaction start research propos life aros hydrotherm vent spontan chemistri earth crust driven interact disequilibrium thermodynam underpin life origin found lineag archaea bacteria autotroph use termin acceptor energi metabol martin suggest base upon evid luca may depend heavili geotherm energi vent surviv pore deep sea hydrotherm vent suggest occupi compart promot biochem reaction metabol intermedi kreb cycl gluconeogenesi amino acid pathway glycolysi pentos phosphat pathway includ sugar like ribos lipid precursor occur condit relev alkalin hydrotherm vent deep marin hydrotherm set site origin life abiogenesi could happen earli gya life evolv ocean depth ten meter would shield impact high level ultraviolet radiat sun avail energi hydrotherm vent maxim temperatur hyperthermophil bacteria thermoacidophil archaea live argument hydrotherm origin life state hyperthermophili result converg evolut bacteria archaea mesophil environ would like hypothesi suggest galtier propos one year discoveri lost citi hydrotherm field hydrotherm vent averag moder temperatur alkalin seawat lost citi favour hydrotherm vent set contrast acid high temperatur product prebiot organ compound hydrotherm vent estim kg larg amount key prebiot compound methan found vent far lower concentr estim experi environ case methan product rate vent around order magnitud lower predict amount experi surfac atmospher argument ocean vent set origin life includ inabl concentr prebiot materi due strong dilut seawat cycl compound miner make vent leav littl resid time accumul modern cell reli phosphat potassium nucleotid backbon protein format respect make like first life form also share function element avail high quantiti archaean ocean primarili come weather continent rock land far vent set submarin hydrotherm vent conduc condens reaction need polymeris form macromolecul older argument key polym encapsul vesicl condens supposedli would happen saltwat high concentr ion howev true salin inhibit vesicl format mixtur fatti acid vesicl format broader realist mix speci resili surfac bodi water provid environ abl dri rewet continu cycl allow concentr prebiot compound condens reaction polymeris macromolecul moreov lake pond land allow detrit input weather continent rock contain apatit common sourc phosphat need nucleotid backbon amount expos continent crust hadean unknown model earli ocean depth rate ocean island continent crust growth make plausibl expos land anoth line evid surfac start life requir uv organ function uv necessari format nucleotid base pair partial hydrolysi nucleobas loss simultan uv harm sterilis life especi simpl earli lifeform littl abil repair radiat damag radiat level young sun like greater ozon layer harm shortwav uv ray would reach surfac earth life begin shield environ influx sourc necessari benefit protect uv shield ice liquid water miner surfac clay regolith possibl rang surfac water set deep sea vent may input rain surfac expos materi likelihood concentr lessen ocean open system branch phylogeni thermophil hyperthermophil make possibl last univers common ancestor luca preced lifeform similarli thermophil hot spring form heat groundwat geotherm activ intersect allow influx materi deep penetr water surfac runoff transport erod continent sediment interconnect groundwat system creat mechan distribut life wider area mulkidjanian argu marin environ provid ionic balanc composit univers found cell ion requir essenti protein ribozym especi respect high ratio phosphat concentr argu environ mimic need condit earth hot spring similar one kamchatka miner deposit environ anox atmospher would suitabl ph current pool oxygen atmospher would contain precipit photocatalyt sulfid miner absorb harm ultraviolet radiat cycl concentr substrat solut concentr amen spontan format biopolym creat chemic reaction hydrotherm environ exposur uv light transport vent adjac pool would promot format biomolecul hypothes environ similar hydrotherm vent addit compon help explain peculiar luca phylogenom geochem analysi protein plausibl trace luca show ionic composit intracellular fluid ident hot spring luca like depend upon synthes organ matter growth experi show polym synthes cycl uv light exposur polym encapsul vesicl condens potenti sourc organ hot spring might transport interplanetari dust particl extraterrestri projectil atmospher geochem synthesi hot spring could abund volcan landmass hadean mesophil start surfac bodi water hypothesi evolv darwin concept littl pond hypothesi freshwat bodi temper climat accumul prebiot materi provid suitabl environment condit conduc simpl life form climat archaean still highli debat topic uncertainti contin ocean atmospher look like atmospher reconstruct archaean geochem proxi model state suffici greenhous gase present maintain surfac temperatur assumpt greater abund moder temperatur nich life could begin strong line evid mesophili biomolecular studi includ galtier nucleotid thermomet abund thermophil due ad stabil addit hydrogen bond present nucleotid rrna sequenc divers rang modern lifeform show luca reconstruct content like repres moder temperatur although modern phylogeni thermophil hyperthermophil possibl widespread divers today product converg evolut horizont gene transfer rather inherit trait luca revers gyras topoisomeras found exclus thermophil hyperthermophil allow coil dna revers gyras enzym requir atp function complex biomolecul origin life hypothesis involv simpl organ yet evolv membran let alon atp would make exist revers gyras improb moreov phylogenet studi show revers gyras archaeal origin transfer bacteria horizont gene transfer impli revers gyras present luca origin life theori stem idea may cold enough region earli earth larg ice cover could found stellar evolut model predict sun luminos weaker today fuelner state although signific decreas solar energi would form ici planet strong evid liquid water present possibl driven greenhous effect would creat earli earth liquid ocean ici pole ice melt form ice sheet glacier melt creat freshwat pool anoth nich capabl experienc cycl pool exist surfac would expos intens uv radiat bodi water within ice suffici shield remain connect uv expos area ice crack suggest impact melt ice allow freshwat pair meteorit input popular vessel prebiot compon level sodium chlorid found destabil fatti acid membran make freshwat set appeal earli membran life ici environ would trade faster reaction rate occur warm environ increas stabil accumul larger polym experi simul condit synthesis amino acid adenin show type synthes still occur cold temperatur rna world ribozym would even function later rna function must abl fold process hinder temperatur rna fold psychrophil organ slower process success hydrolysi also slower shorter nucleotid would suffer higher temperatur altern geolog environ propos geologist ulrich schreiber physic chemist christian mayer continent crust tecton fault zone could present stabl environ prebiot evolut insid system crack caviti water carbon dioxid present bulk solvent phase state would depend local temperatur pressur condit could vari liquid gaseou supercrit form two separ phase liquid water supercrit carbon dioxid depth littl km system provid optim condit phase transfer reaction concurr content tecton fault zone suppli multitud inorgan educt carbon monoxid hydrogen ammonia hydrogen cyanid nitrogen even phosphat dissolv apatit simpl organ molecul form hydrotherm chemistri amino acid amin fatti acid aldehyd final abund miner surfac provid rich choic catalyt activ especi interest section tecton fault zone locat depth approxim carbon dioxid part bulk solvent provid temperatur pressur condit near phase transit point supercrit gaseou state lead natur accumul zone lipophil organ molecul dissolv well supercrit gaseou state lead local precipit period pressur variat caus geyser activ tidal influenc result period phase transit keep local reaction environ constant state presenc amphiphil compound long chain amin fatti acid mention subsequ gener vesicl form constantli effici select stabil result structur could provid hydrotherm vent well hot spring raw materi develop homochir geometr uniform materi compos chiral unit live organ use molecul chiral handed almost except amino acid nucleotid sugar chiral molecul synthes absenc chiral sourc chiral catalyst form racem mixtur form known mechan product mixtur racem start materi includ asymmetr physic law electroweak interact asymmetr environ caus circularli polar light quartz crystal earth rotat statist fluctuat racem synthesi spontan symmetri break establish chiral would select small bia enantiomer excess popul amplifi larg one asymmetr autocatalysi soai reaction asymmetr autocatalysi catalyst chiral molecul mean chiral molecul catalyz product initi enantiomer excess produc polar light allow abund enantiom outcompet homochir may start outer space murchison meteorit amino acid twice frequent form acid three time abund counterpart amino acid meteorit show bia wherea sugar show predominantli bia prefer found live organ suggest abiogen origin compound experi robert two invers base sequenc correspond synthes affin determin gli eleven pair acid result suggest homochir includ codon direction might emerg function origin genet code
Exoplanet,https://en.wikipedia.org/wiki/Exoplanet,"
 An exoplanet or extrasolar planet is a planet outside the Solar System. The first possible evidence of an exoplanet was noted in 1917 but was not then recognized as such. The first confirmed detection of an exoplanet was in 1992 around a pulsar, and the first detection around a main-sequence star was in 1995. A different planet, first detected in 1988, was confirmed in 2003. As of 19 December 2024, there are 5,811 confirmed exoplanets in 4,340 planetary systems, with 973 systems having more than one planet.[3][4] In collaboration with ground-based and other space-based observatories the James Webb Space Telescope (JWST) is expected to give more insight into exoplanet traits, such as their composition, environmental conditions, and potential for life.[5]
 There are many methods of detecting exoplanets. Transit photometry and Doppler spectroscopy have found the most, but these methods suffer from a clear observational bias favoring the detection of planets near the star; thus, 85% of the exoplanets detected are inside the tidal locking zone.[6] In several cases, multiple planets have been observed around a star.[7] About 1 in 5 Sun-like stars[a] are estimated to have an ""Earth-sized""[b] planet in the habitable zone.[c][8][9] Assuming there are 200 billion stars in the Milky Way,[d] it can be hypothesized that there are 11 billion potentially habitable Earth-sized planets in the Milky Way, rising to 40 billion if planets orbiting the numerous red dwarfs are included.[10]
 The least massive exoplanet known is Draugr (also known as PSR B1257+12 A or PSR B1257+12 b), which is about twice the mass of the Moon. The most massive exoplanet listed on the NASA Exoplanet Archive is HR 2562 b,[11][12][13] about 30 times the mass of Jupiter. However, according to some definitions of a planet (based on the nuclear fusion of deuterium[14]), it is too massive to be a planet and might be a brown dwarf. Known orbital times for exoplanets vary from less than an hour (for those closest to their star) to thousands of years. Some exoplanets are so far away from the star that it is difficult to tell whether they are gravitationally bound to it.
 Almost all planets detected so far are within the Milky Way. However, there is evidence that extragalactic planets, exoplanets located in other galaxies, may exist.[15][16] The nearest exoplanets are located 4.2 light-years (1.3 parsecs) from Earth and orbit Proxima Centauri, the closest star to the Sun.[17]
 The discovery of exoplanets has intensified interest in the search for extraterrestrial life. There is special interest in planets that orbit in a star's habitable zone (sometimes called ""goldilocks zone""), where it is possible for liquid water, a prerequisite for life as we know it, to exist on the surface. However, the study of planetary habitability also considers a wide range of other factors in determining the suitability of a planet for hosting life.[18]
 Rogue planets are those that do not orbit any higher mass host. Such objects are considered a separate category of planetary-mass objects, especially if they are gas giants, often counted as sub-brown dwarfs.[19] The rogue planets in the Milky Way possibly number in the billions or more.[20][21]
 The official definition of the term planet used by the International Astronomical Union (IAU) only covers the Solar System and thus does not apply to exoplanets.[22][23] The IAU Working Group on Extrasolar Planets issued a position statement containing a working definition of ""planet"" in 2001 and which was modified in 2003.[24] An exoplanet was defined by the following criteria:
 This working definition was amended by the IAU's Commission F2: Exoplanets and the Solar System in August 2018.[25][26] The official working definition of an exoplanet is now as follows:
 The IAU's working definition is not always used. One alternate suggestion is that planets should be distinguished from brown dwarfs on the basis of their formation. It is widely thought that giant planets form through core accretion, which may sometimes produce planets with masses above the deuterium fusion threshold;[27][28][14] massive planets of that sort may have already been observed.[29] Brown dwarfs form like stars from the direct gravitational collapse of clouds of gas, and this formation mechanism also produces objects that are below the 13 MJup limit and can be as low as 1 MJup.[30] Objects in this mass range that orbit their stars with wide separations of hundreds or thousands of Astronomical Units (AU) and have large star/object mass ratios likely formed as brown dwarfs; their atmospheres would likely have a composition more similar to their host star than accretion-formed planets, which would contain increased abundances of heavier elements. Most directly imaged planets as of April 2014 are massive and have wide orbits so probably represent the low-mass end of a brown dwarf formation.[31] One study suggests that objects above 10 MJup formed through gravitational instability and should not be thought of as planets.[32]
 Also, the 13-Jupiter-mass cutoff does not have a precise physical significance. Deuterium fusion can occur in some objects with a mass below that cutoff.[14] The amount of deuterium fused depends to some extent on the composition of the object.[33] As of 2011, the Extrasolar Planets Encyclopaedia included objects up to 25 Jupiter masses, saying, ""The fact that there is no special feature around 13 MJup in the observed mass spectrum reinforces the choice to forget this mass limit"".[34] As of 2016, this limit was increased to 60 Jupiter masses[35] based on a study of mass–density relationships.[36] The Exoplanet Data Explorer includes objects up to 24 Jupiter masses with the advisory: ""The 13 Jupiter-mass distinction by the IAU Working Group is physically unmotivated for planets with rocky cores, and observationally problematic due to the sin i ambiguity.""[37] The NASA Exoplanet Archive includes objects with a mass (or minimum mass) equal to or less than 30 Jupiter masses.[38] Another criterion for separating planets and brown dwarfs, rather than deuterium fusion, formation process or location, is whether the core pressure is dominated by Coulomb pressure or electron degeneracy pressure with the dividing line at around 5 Jupiter masses.[39][40]
 The convention for naming exoplanets is an extension of the system used for designating multiple-star systems as adopted by the International Astronomical Union (IAU). For exoplanets orbiting a single star, the IAU designation is formed by taking the designated or proper name of its parent star, and adding a lower case letter.[42] Letters are given in order of each planet's discovery around the parent star, so that the first planet discovered in a system is designated ""b"" (the parent star is considered ""a"") and later planets are given subsequent letters. If several planets in the same system are discovered at the same time, the closest one to the star gets the next letter, followed by the other planets in order of orbital size. A provisional IAU-sanctioned standard exists to accommodate the designation of circumbinary planets. A limited number of exoplanets have IAU-sanctioned proper names. Other naming systems exist.
 For centuries scientists, philosophers, and science fiction writers suspected that extrasolar planets existed, but there was no way of knowing whether they were real in fact, how common they were, or how similar they might be to the planets of the Solar System. Various detection claims made in the nineteenth century were rejected by astronomers.
 The first evidence of a possible exoplanet, orbiting Van Maanen 2, was noted in 1917, but was not recognized as such. The astronomer Walter Sydney Adams, who later became director of the Mount Wilson Observatory, produced a spectrum of the star using Mount Wilson's 60-inch telescope. He interpreted the spectrum to be of an F-type main-sequence star, but it is now thought that such a spectrum could be caused by the residue of a nearby exoplanet that had been pulverized by the gravity of the star, the resulting dust then falling onto the star.[43]
 The first suspected scientific detection of an exoplanet occurred in 1988. Shortly afterwards, the first confirmation[44] of detection came in 1992 when Aleksander Wolszczan announced the discovery of several terrestrial-mass planets orbiting the pulsar PSR B1257+12.[45] The first confirmation of an exoplanet orbiting a main-sequence star was made in 1995, when a giant planet was found in a four-day orbit around the nearby star 51 Pegasi. Some exoplanets have been imaged directly by telescopes, but the vast majority have been detected through indirect methods, such as the transit method and the radial-velocity method. In February 2018, researchers using the Chandra X-ray Observatory, combined with a planet detection technique called microlensing, found evidence of planets in a distant galaxy, stating, ""Some of these exoplanets are as (relatively) small as the moon, while others are as massive as Jupiter. Unlike Earth, most of the exoplanets are not tightly bound to stars, so they're actually wandering through space or loosely orbiting between stars. We can estimate that the number of planets in this [faraway] galaxy is more than a trillion.""[46]
 This space we declare to be infinite... In it are an infinity of worlds of the same kind as our own. In the sixteenth century, the Italian philosopher Giordano Bruno, an early supporter of the Copernican theory that Earth and other planets orbit the Sun (heliocentrism), put forward the view that fixed stars are similar to the Sun and are likewise accompanied by planets.
 In the eighteenth century, the same possibility was mentioned by Isaac Newton in the ""General Scholium"" that concludes his Principia. Making a comparison to the Sun's planets, he wrote ""And if the fixed stars are the centres of similar systems, they will all be constructed according to a similar design and subject to the dominion of One.""[48]
 In 1938, D.Belorizky demonstrated that it was realistic to search for exo-Jupiters by using transit photometry.[49]
 In 1952, more than 40 years before the first hot Jupiter was discovered, Otto Struve wrote that there is no compelling reason that planets could not be much closer to their parent star than is the case in the Solar System, and proposed that Doppler spectroscopy and the transit method could detect super-Jupiters in short orbits.[50]
 Claims of exoplanet detections have been made since the nineteenth century. Some of the earliest involve the binary star 70 Ophiuchi. In 1855, William Stephen Jacob at the East India Company's Madras Observatory reported that orbital anomalies made it ""highly probable"" that there was a ""planetary body"" in this system.[51] In the 1890s, Thomas J. J. See of the University of Chicago and the United States Naval Observatory stated that the orbital anomalies proved the existence of a dark body in the 70 Ophiuchi system with a 36-year period around one of the stars.[52] However, Forest Ray Moulton published a paper proving that a three-body system with those orbital parameters would be highly unstable.[53]
 During the 1950s and 1960s, Peter van de Kamp of Swarthmore College made another prominent series of detection claims, this time for planets orbiting Barnard's Star.[54] Astronomers now generally regard all early reports of detection as erroneous.[55]
 In 1991, Andrew Lyne, M. Bailes and S. L. Shemar claimed to have discovered a pulsar planet in orbit around PSR 1829-10, using pulsar timing variations.[56] The claim briefly received intense attention, but Lyne and his team soon retracted it.[57]
 As of 24 July 2024, a total of 5,811 confirmed exoplanets are listed in the NASA Exoplanet Archive, including a few that were confirmations of controversial claims from the late 1980s.[58] The first published discovery to receive subsequent confirmation was made in 1988 by the Canadian astronomers Bruce Campbell, G. A. H. Walker, and Stephenson Yang of the University of Victoria and the University of British Columbia.[59] Although they were cautious about claiming a planetary detection, their radial-velocity observations suggested that a planet orbits the star Gamma Cephei. Partly because the observations were at the very limits of instrumental capabilities at the time, astronomers remained skeptical for several years about this and other similar observations. It was thought some of the apparent planets might instead have been brown dwarfs, objects intermediate in mass between planets and stars. In 1990, additional observations were published that supported the existence of the planet orbiting Gamma Cephei,[60] but subsequent work in 1992 again raised serious doubts.[61] Finally, in 2003, improved techniques allowed the planet's existence to be confirmed.[62]
 On 9 January 1992, radio astronomers Aleksander Wolszczan and Dale Frail announced the discovery of two planets orbiting the pulsar PSR 1257+12.[45] This discovery was confirmed, and is generally considered to be the first definitive detection of exoplanets. Follow-up observations solidified these results, and confirmation of a third planet in 1994 revived the topic in the popular press.[63] These pulsar planets are thought to have formed from the unusual remnants of the supernova that produced the pulsar, in a second round of planet formation, or else to be the remaining rocky cores of gas giants that somehow survived the supernova and then decayed into their current orbits. As pulsars are aggressive stars, it was considered unlikely at the time that a planet may be able to be formed in their orbit.[64]
 In the early 1990s, a group of astronomers led by Donald Backer, who were studying what they thought was a binary pulsar (PSR B1620−26 b), determined that a third object was needed to explain the observed Doppler shifts. Within a few years, the gravitational effects of the planet on the orbit of the pulsar and white dwarf had been measured, giving an estimate of the mass of the third object that was too small for it to be a star. The conclusion that the third object was a planet was announced by Stephen Thorsett and his collaborators in 1993.[65]
 On 6 October 1995, Michel Mayor and Didier Queloz of the University of Geneva announced the first definitive detection of an exoplanet orbiting a main-sequence star, nearby G-type star 51 Pegasi.[66][67][68] This discovery, made at the Observatoire de Haute-Provence, ushered in the modern era of exoplanetary discovery, and was recognized by a share of the 2019 Nobel Prize in Physics. Technological advances, most notably in high-resolution spectroscopy, led to the rapid detection of many new exoplanets: astronomers could detect exoplanets indirectly by measuring their gravitational influence on the motion of their host stars. More extrasolar planets were later detected by observing the variation in a star's apparent luminosity as an orbiting planet transited in front of it.[66]
 Initially, the most known exoplanets were massive planets that orbited very close to their parent stars. Astronomers were surprised by these ""hot Jupiters"", because theories of planetary formation had indicated that giant planets should only form at large distances from stars. But eventually more planets of other sorts were found, and it is now clear that hot Jupiters make up the minority of exoplanets.[66] In 1999, Upsilon Andromedae became the first main-sequence star known to have multiple planets.[69] Kepler-16 contains the first discovered planet that orbits a binary main-sequence star system.[70]
 On 26 February 2014, NASA announced the discovery of 715 newly verified exoplanets around 305 stars by the Kepler Space Telescope. These exoplanets were checked using a statistical technique called ""verification by multiplicity"".[71][72][73] Before these results, most confirmed planets were gas giants comparable in size to Jupiter or larger because they were more easily detected, but the Kepler planets are mostly between the size of Neptune and the size of Earth.[71]
 On 23 July 2015, NASA announced Kepler-452b, a near-Earth-size planet orbiting the habitable zone of a G2-type star.[74]
 On 6 September 2018, NASA discovered an exoplanet about 145 light years away from Earth in the constellation Virgo.[75] This exoplanet, Wolf 503b, is twice the size of Earth and was discovered orbiting a type of star known as an ""Orange Dwarf"". Wolf 503b completes one orbit in as few as six days because it is very close to the star. Wolf 503b is the only exoplanet that large that can be found near the so-called small planet radius gap. The gap, sometimes called the Fulton gap,[75][76] is the observation that it is unusual to find exoplanets with sizes between 1.5 and 2 times the radius of the Earth.[77]
 In January 2020, scientists announced the discovery of TOI 700 d, the first Earth-sized planet in the habitable zone detected by TESS.[78]
 As of January 2020, NASA's Kepler and TESS missions had identified 4374 planetary candidates yet to be confirmed,[79] several of them being nearly Earth-sized and located in the habitable zone, some around Sun-like stars.[80][81][82]
 In September 2020, astronomers reported evidence, for the first time, of an extragalactic planet, M51-ULS-1b, detected by eclipsing a bright X-ray source (XRS), in the Whirlpool Galaxy (M51a).[85][86]
 Also in September 2020, astronomers using microlensing techniques reported the detection, for the first time, of an Earth-mass rogue planet unbounded by any star, and free floating in the Milky Way galaxy.[87][88]
 Planets are extremely faint compared to their parent stars. For example, a Sun-like star is about a billion times brighter than the reflected light from any exoplanet orbiting it. It is difficult to detect such a faint light source, and furthermore, the parent star causes a glare that tends to wash it out. It is necessary to block the light from the parent star to reduce the glare while leaving the light from the planet detectable; doing so is a major technical challenge which requires extreme optothermal stability.[89] All exoplanets that have been directly imaged are both large (more massive than Jupiter) and widely separated from their parent stars.
 Specially designed direct-imaging instruments such as Gemini Planet Imager, VLT-SPHERE, and SCExAO will image dozens of gas giants, but the vast majority of known extrasolar planets have only been detected through indirect methods.
 Planets may form within a few to tens (or more) of millions of years of their star forming.[104][105]
The planets of the Solar System can only be observed in their current state, but observations of different planetary systems of varying ages allows us to observe planets at different stages of evolution. Available observations range from young proto-planetary disks where planets are still forming[106] to planetary systems of over 10 Gyr old.[107] When planets form in a gaseous protoplanetary disk,[108] they accrete hydrogen/helium envelopes.[109][110] These envelopes cool and contract over time and, depending on the mass of the planet, some or all of the hydrogen/helium is eventually lost to space.[108] This means that even terrestrial planets may start off with large radii if they form early enough.[111][112][113] An example is Kepler-51b which has only about twice the mass of Earth but is almost the size of Saturn, which is a hundred times the mass of Earth. Kepler-51b is quite young at a few hundred million years old.[114]
 There is at least one planet on average per star.[7]
About 1 in 5 Sun-like stars[a] have an ""Earth-sized""[b] planet in the habitable zone.[116]
 Most known exoplanets orbit stars roughly similar to the Sun, i.e. main-sequence stars of spectral categories F, G, or K. Lower-mass stars (red dwarfs, of spectral category M) are less likely to have planets massive enough to be detected by the radial-velocity method.[117][118] Despite this, several tens of planets around red dwarfs have been discovered by the Kepler space telescope, which uses the transit method to detect smaller planets.
 Using data from Kepler, a correlation has been found between the metallicity of a star and the probability that the star hosts a giant planet, similar to the size of Jupiter. Stars with higher metallicity are more likely to have planets, especially giant planets, than stars with lower metallicity.[119]
 Some planets orbit one member of a binary star system,[120] and several circumbinary planets have been discovered which orbit both members of a binary star. A few planets in triple star systems are known[121] and one in the quadruple system Kepler-64.
 In 2013, the color of an exoplanet was determined for the first time. The best-fit albedo measurements of HD 189733b suggest that it is deep dark blue.[122][123] Later that same year, the colors of several other exoplanets were determined, including GJ 504 b which visually has a magenta color,[124] and Kappa Andromedae b, which if seen up close would appear reddish in color.[125] Helium planets are expected to be white or grey in appearance.[126]
 The apparent brightness (apparent magnitude) of a planet depends on how far away the observer is, how reflective the planet is (albedo), and how much light the planet receives from its star, which depends on how far the planet is from the star and how bright the star is. So, a planet with a low albedo that is close to its star can appear brighter than a planet with a high albedo that is far from the star.[127]
 The darkest known planet in terms of geometric albedo is TrES-2b, a hot Jupiter that reflects less than 1% of the light from its star, making it less reflective than coal or black acrylic paint. Hot Jupiters are expected to be quite dark due to sodium and potassium in their atmospheres, but it is not known why TrES-2b is so dark—it could be due to an unknown chemical compound.[128][129][130]
 For gas giants, geometric albedo generally decreases with increasing metallicity or atmospheric temperature unless there are clouds to modify this effect. Increased cloud-column depth increases the albedo at optical wavelengths, but decreases it at some infrared wavelengths. Optical albedo increases with age, because older planets have higher cloud-column depths. Optical albedo decreases with increasing mass, because higher-mass giant planets have higher surface gravities, which produces lower cloud-column depths. Also, elliptical orbits can cause major fluctuations in atmospheric composition, which can have a significant effect.[131]
 There is more thermal emission than reflection at some near-infrared wavelengths for massive and/or young gas giants. So, although optical brightness is fully phase-dependent, this is not always the case in the near infrared.[131]
 Temperatures of gas giants reduce over time and with distance from their stars. Lowering the temperature increases optical albedo even without clouds. At a sufficiently low temperature, water clouds form, which further increase optical albedo. At even lower temperatures, ammonia clouds form, resulting in the highest albedos at most optical and near-infrared wavelengths.[131]
 In 2014, a magnetic field around HD 209458 b was inferred from the way hydrogen was evaporating from the planet. It is the first (indirect) detection of a magnetic field on an exoplanet. The magnetic field is estimated to be about one-tenth as strong as Jupiter's.[132][133]
 The magnetic fields of exoplanets are thought to be detectable by their auroral radio emissions with sensitive low-frequency radio telescopes such as LOFAR, although they have yet to be found.[134][135] The radio emissions could measure the rotation rate of the interior of an exoplanet, and may yield a more accurate way to measure exoplanet rotation than by examining the motion of clouds.[136] However, the most sensitive radio search for auroral emissions, thus far, from nine exoplanets with Arecibo also did not result in any discoveries.[137]
 Earth's magnetic field results from its flowing liquid metallic core, but on massive super-Earths with high pressure, different compounds may form which do not match those created under terrestrial conditions. Compounds may form with greater viscosities and high melting temperatures, which could prevent the interiors from separating into different layers and so result in undifferentiated coreless mantles. Forms of magnesium oxide such as MgSi3O12 could be a liquid metal at the pressures and temperatures found in super-Earths and could generate a magnetic field in the mantles of super-Earths.[138][139]
 Hot Jupiters have been observed to have a larger radius than expected. This could be caused by the interaction between the stellar wind and the planet's magnetosphere creating an electric current through the planet that heats it up (Joule heating) causing it to expand. The more magnetically active a star is, the greater the stellar wind and the larger the electric current leading to more heating and expansion of the planet. This theory matches the observation that stellar activity is correlated with inflated planetary radii.[140]
 In August 2018, scientists announced the transformation of gaseous deuterium into a liquid metallic hydrogen form. This may help researchers better understand giant gas planets, such as Jupiter, Saturn and related exoplanets, since such planets are thought to contain a lot of liquid metallic hydrogen, which may be responsible for their observed powerful magnetic fields.[141][142]
 Although scientists previously announced that the magnetic fields of close-in exoplanets may cause increased stellar flares and starspots on their host stars, in 2019 this claim was demonstrated to be false in the HD 189733 system. The failure to detect ""star-planet interactions"" in the well-studied HD 189733 system calls other related claims of the effect into question.[143] A later search for radio emissions from eight exoplanets that orbit within 0.1 astronomical units of their host stars, conducted by the Arecibo radio telescope also failed to find signs of these magnetic star-planet interactions.[144]
 In 2019, the strength of the surface magnetic fields of 4 hot Jupiters were estimated and ranged between 20 and 120 gauss compared to Jupiter's surface magnetic field of 4.3 gauss.[145][146]
 In 2007, two independent teams of researchers came to opposing conclusions about the likelihood of plate tectonics on larger super-Earths[147][148] with one team saying that plate tectonics would be episodic or stagnant[149] and the other team saying that plate tectonics is very likely on super-Earths even if the planet is dry.[150]
 If super-Earths have more than 80 times as much water as Earth, then they become ocean planets with all land completely submerged. However, if there is less water than this limit, then the deep water cycle will move enough water between the oceans and mantle to allow continents to exist.[151][152]
 Large surface temperature variations on 55 Cancri e have been attributed to possible volcanic activity releasing large clouds of dust which blanket the planet and block thermal emissions.[153][154]
 The star 1SWASP J140747.93-394542.6 was occulted by an object that is circled by a ring system much larger than Saturn's rings. However, the mass of the object is not known; it could be a brown dwarf or low-mass star instead of a planet.[155][156]
 The brightness of optical images of Fomalhaut b could be due to starlight reflecting off a circumplanetary ring system with a radius between 20 and 40 times that of Jupiter's radius, about the size of the orbits of the Galilean moons.[157]
 The rings of the Solar System's gas giants are aligned with their planet's equator. However, for exoplanets that orbit close to their star, tidal forces from the star would lead to the outermost rings of a planet being aligned with the planet's orbital plane around the star. A planet's innermost rings would still be aligned with the planet's equator so that if the planet has a tilted rotational axis, then the different alignments between the inner and outer rings would create a warped ring system.[158]
 In December 2013, a candidate exomoon of the rogue planet or red dwarf MOA-2011-BLG-262L was announced.[159] On 3 October 2018, evidence suggesting a large exomoon orbiting Kepler-1625b was reported.[160]
 Atmospheres have been detected around several exoplanets. The first to be observed was HD 209458 b in 2001.[162]
 As of February 2014, more than fifty transiting and five directly imaged exoplanet atmospheres have been observed,[163] resulting in detection of molecular spectral features; observation of day–night temperature gradients; and constraints on vertical atmospheric structure.[164] Also, an atmosphere has been detected on the non-transiting hot Jupiter Tau Boötis b.[165][166]
 In May 2017, glints of light from Earth, seen as twinkling from an orbiting satellite a million miles away, were found to be reflected light from ice crystals in the atmosphere.[167][168] The technology used to determine this may be useful in studying the atmospheres of distant worlds, including those of exoplanets.
 KIC 12557548 b is a small rocky planet, very close to its star, that is evaporating and leaving a trailing tail of cloud and dust like a comet.[169] The dust could be ash erupting from volcanos and escaping due to the small planet's low surface-gravity, or it could be from metals that are vaporized by the high temperatures of being so close to the star with the metal vapor then condensing into dust.[170]
 In June 2015, scientists reported that the atmosphere of GJ 436 b was evaporating, resulting in a giant cloud around the planet and, due to radiation from the host star, a long trailing tail 14 million km (9 million mi) long.[171]
 Tidally locked planets in a 1:1 spin-orbit resonance would have their star always shining directly overhead on one spot, which would be hot with the opposite hemisphere receiving no light and being freezing cold. Such a planet could resemble an eyeball, with the hotspot being the pupil.[172] Planets with an eccentric orbit could be locked in other resonances. 3:2 and 5:2 resonances would result in a double-eyeball pattern with hotspots in both eastern and western hemispheres.[173] Planets with both an eccentric orbit and a tilted axis of rotation would have more complicated insolation patterns.[174]
 Surface features can be distinguished from atmospheric features by comparing emission and reflection spectroscopy with transmission spectroscopy. Mid-infrared spectroscopy of exoplanets may detect rocky surfaces, and near-infrared may identify magma oceans or high-temperature lavas, hydrated silicate surfaces and water ice, giving an unambiguous method to distinguish between rocky and gaseous exoplanets.[175]
 Measuring the intensity of the light it receives from its parent star can estimate the temperature of an exoplanet. For example, the planet OGLE-2005-BLG-390Lb is estimated to have a surface temperature of roughly −220 °C (50 K). However, such estimates may be substantially in error because they depend on the planet's usually unknown albedo, and because factors such as the greenhouse effect may introduce unknown complications. A few planets have had their temperature measured by observing the variation in infrared radiation as the planet moves around in its orbit and is eclipsed by its parent star. For example, the planet HD 189733b has been estimated to have an average temperature of 1,205 K (932 °C) on its dayside and 973 K (700 °C) on its nightside.[177]
 As more planets are discovered, the field of exoplanetology continues to grow into a deeper study of extrasolar worlds, and will ultimately tackle the prospect of life on planets beyond the Solar System.[178] At cosmic distances, life can only be detected if it is developed at a planetary scale and strongly modified the planetary environment, in such a way that the modifications cannot be explained by classical physico-chemical processes (out of equilibrium processes).[178] For example, molecular oxygen (O2) in the atmosphere of Earth is a result of photosynthesis by living plants and many kinds of microorganisms, so it can be used as an indication of life on exoplanets, although small amounts of oxygen could also be produced by non-biological means.[179] Furthermore, a potentially habitable planet must orbit a stable star at a distance within which planetary-mass objects with sufficient atmospheric pressure can support liquid water at their surfaces.[180][181]
 The habitable zone around a star is the region where the temperature is just right to allow liquid water to exist on the surface of a planet; that is, not too close to the star for the water to evaporate and not too far away from the star for the water to freeze. The heat produced by stars varies depending on the size and age of the star, so that the habitable zone can be at different distances for different stars. Also, the atmospheric conditions on the planet influence the planet's ability to retain heat so that the location of the habitable zone is also specific to each type of planet: desert planets (also known as dry planets), with very little water, will have less water vapor in the atmosphere than Earth and so have a reduced greenhouse effect, meaning that a desert planet could maintain oases of water closer to its star than Earth is to the Sun. The lack of water also means there is less ice to reflect heat into space, so the outer edge of desert-planet habitable zones is further out.[182][183] Rocky planets with a thick hydrogen atmosphere could maintain surface water much further out than the Earth–Sun distance.[184] Planets with larger mass have wider habitable zones because gravity reduces the water cloud column depth which reduces the greenhouse effect of water vapor, thus moving the inner edge of the habitable zone closer to the star.[185]
 Planetary rotation rate is one of the major factors determining the circulation of the atmosphere and hence the pattern of clouds: slowly rotating planets create thick clouds that reflect more and so can be habitable much closer to their star. Earth with its current atmosphere would be habitable in Venus's orbit, if it had Venus's slow rotation. If Venus lost its water ocean due to a runaway greenhouse effect, it is likely to have had a higher rotation rate in the past. Alternatively, Venus never had an ocean because water vapor was lost to space during its formation [186] and could have had its slow rotation throughout its history.[187]
 Tidally locked planets (a.k.a. ""eyeball"" planets[188]) can be habitable closer to their star than previously thought due to the effect of clouds: at high stellar flux, strong convection produces thick water clouds near the substellar point that greatly increase the planetary albedo and reduce surface temperatures.[189]
 Planets in the habitable zones of stars with low metallicity are more habitable for complex life on land than high metallicity stars because the stellar spectrum of high metallicity stars is less likely to cause the formation of ozone thus enabling more ultraviolet rays to reach the planet's surface.[190][191]
 Habitable zones have usually been defined in terms of surface temperature, however over half of Earth's biomass is from subsurface microbes,[192] and the temperature increases with depth, so the subsurface can be conducive for microbial life when the surface is frozen and if this is considered, the habitable zone extends much further from the star,[193] even rogue planets could have liquid water at sufficient depths underground.[194] In an earlier era of the universe the temperature of the cosmic microwave background would have allowed any rocky planets that existed to have liquid water on their surface regardless of their distance from a star.[195] Jupiter-like planets might not be habitable, but they could have habitable moons.[196]
 The outer edge of the habitable zone is where planets are completely frozen, but planets well inside the habitable zone can periodically become frozen. If orbital fluctuations or other causes produce cooling, then this creates more ice, but ice reflects sunlight causing even more cooling, creating a feedback loop until the planet is completely or nearly completely frozen. When the surface is frozen, this stops carbon dioxide weathering, resulting in a build-up of carbon dioxide in the atmosphere from volcanic emissions. This creates a greenhouse effect which thaws the planet again. Planets with a large axial tilt[197] are less likely to enter snowball states and can retain liquid water further from their star. Large fluctuations of axial tilt can have even more of a warming effect than a fixed large tilt.[198][199] Paradoxically, planets orbiting cooler stars, such as red dwarfs, are less likely to enter snowball states because the infrared radiation emitted by cooler stars is mostly at wavelengths that are absorbed by ice which heats it up.[200][201]
 If a planet has an eccentric orbit, then tidal heating can provide another source of energy besides stellar radiation. This means that eccentric planets in the radiative habitable zone can be too hot for liquid water. Tides also circularize orbits over time, so there could be planets in the habitable zone with circular orbits that have no water because they used to have eccentric orbits.[202] Eccentric planets further out than the habitable zone would still have frozen surfaces, but the tidal heating could create a subsurface ocean similar to Europa's.[203] In some planetary systems, such as in the Upsilon Andromedae system, the eccentricity of orbits is maintained or even periodically varied by perturbations from other planets in the system. Tidal heating can cause outgassing from the mantle, contributing to the formation and replenishment of an atmosphere.[204]
 A review in 2015 identified exoplanets Kepler-62f, Kepler-186f and Kepler-442b as the best candidates for being potentially habitable.[205] These are at a distance of 1200, 490 and 1,120 light-years away, respectively. Of these, Kepler-186f is in similar size to Earth with its 1.2-Earth-radius measure, and it is located towards the outer edge of the habitable zone around its red dwarf star.
 When looking at the nearest terrestrial exoplanet candidates, Proxima Centauri b is about 4.2 light-years away. Its equilibrium temperature is estimated to be −39 °C (234 K).[206]
 Exoplanets are often members of planetary systems of multiple planets around a star. The planets interact with each other gravitationally and sometimes form resonant systems where the orbital periods of the planets are in integer ratios. The Kepler-223 system contains four planets in an 8:6:4:3 orbital resonance.[210]
 Some hot Jupiters orbit their stars in the opposite direction to their stars' rotation.[211] One proposed explanation is that hot Jupiters tend to form in dense clusters, where perturbations are more common and gravitational capture of planets by neighboring stars is possible.[212]
",exoplanet extrasolar planet planet outsid solar system first possibl evid exoplanet note recogn first confirm detect exoplanet around pulsar first detect around star differ planet first detect confirm decemb confirm exoplanet planetari system system one planet collabor observatori jame webb space telescop jwst expect give insight exoplanet trait composit environment condit potenti life mani method detect exoplanet transit photometri doppler spectroscopi found method suffer clear observ bia favor detect planet near star thu exoplanet detect insid tidal lock zone sever case multipl planet observ around star star estim b planet habit zone c assum billion star milki way hypothes billion potenti habit planet milki way rise billion planet orbit numer red dwarf includ least massiv exoplanet known draugr also known psr psr b twice mass moon massiv exoplanet list nasa exoplanet archiv hr b time mass jupit howev accord definit planet base nuclear fusion deuterium massiv planet might brown dwarf known orbit time exoplanet vari less hour closest star thousand year exoplanet far away star difficult tell whether gravit bound almost planet detect far within milki way howev evid extragalact planet exoplanet locat galaxi may exist nearest exoplanet locat parsec earth orbit proxima centauri closest star sun discoveri exoplanet intensifi interest search extraterrestri life special interest planet orbit star habit zone sometim call goldilock zone possibl liquid water prerequisit life know exist surfac howev studi planetari habit also consid wide rang factor determin suitabl planet host life rogu planet orbit higher mass host object consid separ categori object especi ga giant often count dwarf rogu planet milki way possibl number billion offici definit term planet use intern astronom union iau cover solar system thu appli exoplanet iau work group extrasolar planet issu posit statement contain work definit planet modifi exoplanet defin follow criteria work definit amend iau commiss exoplanet solar system august offici work definit exoplanet follow iau work definit alway use one altern suggest planet distinguish brown dwarf basi format wide thought giant planet form core accret may sometim produc planet mass deuterium fusion threshold massiv planet sort may alreadi observ brown dwarf form like star direct gravit collaps cloud ga format mechan also produc object mjup limit low mjup object mass rang orbit star wide separ hundr thousand astronom unit au larg mass ratio like form brown dwarf atmospher would like composit similar host star planet would contain increas abund heavier element directli imag planet april massiv wide orbit probabl repres end brown dwarf format one studi suggest object mjup form gravit instabl thought planet also cutoff precis physic signific deuterium fusion occur object mass cutoff amount deuterium fuse depend extent composit object extrasolar planet encyclopaedia includ object jupit mass say fact special featur around mjup observ mass spectrum reinforc choic forget mass limit limit increas jupit mass base studi relationship exoplanet data explor includ object jupit mass advisori distinct iau work group physic unmotiv planet rocki core observ problemat due sin ambigu nasa exoplanet archiv includ object mass minimum mass equal less jupit mass anoth criterion separ planet brown dwarf rather deuterium fusion format process locat whether core pressur domin coulomb pressur electron degeneraci pressur divid line around jupit mass convent name exoplanet extens system use design system adopt intern astronom union iau exoplanet orbit singl star iau design form take design proper name parent star ad lower case letter letter given order planet discoveri around parent star first planet discov system design b parent star consid later planet given subsequ letter sever planet system discov time closest one star get next letter follow planet order orbit size provision standard exist accommod design circumbinari planet limit number exoplanet proper name name system exist centuri scientist philosoph scienc fiction writer suspect extrasolar planet exist way know whether real fact common similar might planet solar system variou detect claim made nineteenth centuri reject astronom first evid possibl exoplanet orbit van maanen note recogn astronom walter sydney adam later becam director mount wilson observatori produc spectrum star use mount wilson telescop interpret spectrum star thought spectrum could caus residu nearbi exoplanet pulver graviti star result dust fall onto star first suspect scientif detect exoplanet occur shortli afterward first confirm detect came aleksand wolszczan announc discoveri sever planet orbit pulsar psr first confirm exoplanet orbit star made giant planet found orbit around nearbi star pegasi exoplanet imag directli telescop vast major detect indirect method transit method method februari research use chandra observatori combin planet detect techniqu call microlens found evid planet distant galaxi state exoplanet rel small moon other massiv jupit unlik earth exoplanet tightli bound star actual wander space loos orbit star estim number planet faraway galaxi trillion space declar infinit infin world kind sixteenth centuri italian philosoph giordano bruno earli support copernican theori earth planet orbit sun heliocentr put forward view fix star similar sun likewis accompani planet eighteenth centuri possibl mention isaac newton gener scholium conclud principia make comparison sun planet wrote fix star centr similar system construct accord similar design subject dominion one demonstr realist search use transit photometri year first hot jupit discov otto struve wrote compel reason planet could much closer parent star case solar system propos doppler spectroscopi transit method could detect short orbit claim exoplanet detect made sinc nineteenth centuri earliest involv binari star ophiuchi william stephen jacob east india compani madra observatori report orbit anomali made highli probabl planetari bodi system thoma see univers chicago unit state naval observatori state orbit anomali prove exist dark bodi ophiuchi system period around one star howev forest ray moulton publish paper prove system orbit paramet would highli unstabl peter van de kamp swarthmor colleg made anoth promin seri detect claim time planet orbit barnard star astronom gener regard earli report detect erron andrew lyne bail shemar claim discov pulsar planet orbit around psr use pulsar time variat claim briefli receiv intens attent lyne team soon retract juli total confirm exoplanet list nasa exoplanet archiv includ confirm controversi claim late first publish discoveri receiv subsequ confirm made canadian astronom bruce campbel walker stephenson yang univers victoria univers british columbia although cautiou claim planetari detect observ suggest planet orbit star gamma cephei partli observ limit instrument capabl time astronom remain skeptic sever year similar observ thought appar planet might instead brown dwarf object intermedi mass planet star addit observ publish support exist planet orbit gamma cephei subsequ work rais seriou doubt final improv techniqu allow planet exist confirm januari radio astronom aleksand wolszczan dale frail announc discoveri two planet orbit pulsar psr discoveri confirm gener consid first definit detect exoplanet observ solidifi result confirm third planet reviv topic popular press pulsar planet thought form unusu remnant supernova produc pulsar second round planet format els remain rocki core ga giant somehow surviv supernova decay current orbit pulsar aggress star consid unlik time planet may abl form orbit earli group astronom led donald backer studi thought binari pulsar psr b determin third object need explain observ doppler shift within year gravit effect planet orbit pulsar white dwarf measur give estim mass third object small star conclus third object planet announc stephen thorsett collabor octob michel mayor didier queloz univers geneva announc first definit detect exoplanet orbit star nearbi star pegasi discoveri made observatoir de usher modern era exoplanetari discoveri recogn share nobel prize physic technolog advanc notabl spectroscopi led rapid detect mani new exoplanet astronom could detect exoplanet indirectli measur gravit influenc motion host star extrasolar planet later detect observ variat star appar luminos orbit planet transit front initi known exoplanet massiv planet orbit close parent star astronom surpris hot jupit theori planetari format indic giant planet form larg distanc star eventu planet sort found clear hot jupit make minor exoplanet upsilon andromeda becam first star known multipl planet contain first discov planet orbit binari star system februari nasa announc discoveri newli verifi exoplanet around star kepler space telescop exoplanet check use statist techniqu call verif multipl result confirm planet ga giant compar size jupit larger easili detect kepler planet mostli size neptun size earth juli nasa announc planet orbit habit zone star septemb nasa discov exoplanet light year away earth constel virgo exoplanet wolf twice size earth discov orbit type star known orang dwarf wolf complet one orbit six day close star wolf exoplanet larg found near small planet radiu gap gap sometim call fulton gap observ unusu find exoplanet size time radiu earth januari scientist announc discoveri toi first planet habit zone detect tess januari nasa kepler tess mission identifi planetari candid yet confirm sever nearli locat habit zone around star septemb astronom report evid first time extragalact planet detect eclips bright sourc xr whirlpool galaxi also septemb astronom use microlens techniqu report detect first time rogu planet unbound star free float milki way galaxi planet extrem faint compar parent star exampl star billion time brighter reflect light exoplanet orbit difficult detect faint light sourc furthermor parent star caus glare tend wash necessari block light parent star reduc glare leav light planet detect major technic challeng requir extrem optotherm stabil exoplanet directli imag larg massiv jupit wide separ parent star special design instrument gemini planet imag scexao imag dozen ga giant vast major known extrasolar planet detect indirect method planet may form within ten million year star form planet solar system observ current state observ differ planetari system vari age allow us observ planet differ stage evolut avail observ rang young disk planet still form planetari system gyr old planet form gaseou protoplanetari disk accret envelop envelop cool contract time depend mass planet eventu lost space mean even terrestri planet may start larg radii form earli enough exampl twice mass earth almost size saturn hundr time mass earth quit young hundr million year old least one planet averag per star star b planet habit zone known exoplanet orbit star roughli similar sun star spectral categori f g star red dwarf spectral categori less like planet massiv enough detect method despit sever ten planet around red dwarf discov kepler space telescop use transit method detect smaller planet use data kepler correl found metal star probabl star host giant planet similar size jupit star higher metal like planet especi giant planet star lower metal planet orbit one member binari star system sever circumbinari planet discov orbit member binari star planet tripl star system known one quadrupl system color exoplanet determin first time albedo measur hd suggest deep dark blue later year color sever exoplanet determin includ gj b visual magenta color kappa andromeda b seen close would appear reddish color helium planet expect white grey appear appar bright appar magnitud planet depend far away observ reflect planet albedo much light planet receiv star depend far planet star bright star planet low albedo close star appear brighter planet high albedo far star darkest known planet term geometr albedo hot jupit reflect less light star make less reflect coal black acryl paint hot jupit expect quit dark due sodium potassium atmospher known could due unknown chemic compound ga giant geometr albedo gener decreas increas metal atmospher temperatur unless cloud modifi effect increas depth increas albedo optic wavelength decreas infrar wavelength optic albedo increas age older planet higher depth optic albedo decreas increas mass giant planet higher surfac graviti produc lower depth also ellipt orbit caus major fluctuat atmospher composit signific effect thermal emiss reflect wavelength massiv young ga giant although optic bright fulli alway case near infrar temperatur ga giant reduc time distanc star lower temperatur increas optic albedo even without cloud suffici low temperatur water cloud form increas optic albedo even lower temperatur ammonia cloud form result highest albedo optic wavelength magnet field around hd b infer way hydrogen evapor planet first indirect detect magnet field exoplanet magnet field estim strong jupit magnet field exoplanet thought detect auror radio emiss sensit radio telescop lofar although yet found radio emiss could measur rotat rate interior exoplanet may yield accur way measur exoplanet rotat examin motion cloud howev sensit radio search auror emiss thu far nine exoplanet arecibo also result discoveri earth magnet field result flow liquid metal core massiv high pressur differ compound may form match creat terrestri condit compound may form greater viscos high melt temperatur could prevent interior separ differ layer result undifferenti coreless mantl form magnesium oxid could liquid metal pressur temperatur found could gener magnet field mantl hot jupit observ larger radiu expect could caus interact stellar wind planet magnetospher creat electr current planet heat joul heat caus expand magnet activ star greater stellar wind larger electr current lead heat expans planet theori match observ stellar activ correl inflat planetari radii august scientist announc transform gaseou deuterium liquid metal hydrogen form may help research better understand giant ga planet jupit saturn relat exoplanet sinc planet thought contain lot liquid metal hydrogen may respons observ power magnet field although scientist previous announc magnet field exoplanet may caus increas stellar flare starspot host star claim demonstr fals hd system failur detect interact hd system call relat claim effect question later search radio emiss eight exoplanet orbit within astronom unit host star conduct arecibo radio telescop also fail find sign magnet interact strength surfac magnet field hot jupit estim rang gauss compar jupit surfac magnet field gauss two independ team research came oppos conclus likelihood plate tecton larger one team say plate tecton would episod stagnant team say plate tecton like even planet dri time much water earth becom ocean planet land complet submerg howev less water limit deep water cycl move enough water ocean mantl allow contin exist larg surfac temperatur variat cancri e attribut possibl volcan activ releas larg cloud dust blanket planet block thermal emiss star occult object circl ring system much larger saturn ring howev mass object known could brown dwarf star instead planet bright optic imag fomalhaut b could due starlight reflect circumplanetari ring system radiu time jupit radiu size orbit galilean moon ring solar system ga giant align planet equat howev exoplanet orbit close star tidal forc star would lead outermost ring planet align planet orbit plane around star planet innermost ring would still align planet equat planet tilt rotat axi differ align inner outer ring would creat warp ring system decemb candid exomoon rogu planet red dwarf announc octob evid suggest larg exomoon orbit report atmospher detect around sever exoplanet first observ hd b februari fifti transit five directli imag exoplanet atmospher observ result detect molecular spectral featur observ temperatur gradient constraint vertic atmospher structur also atmospher detect hot jupit tau boöti b may glint light earth seen twinkl orbit satellit million mile away found reflect light ice crystal atmospher technolog use determin may use studi atmospher distant world includ exoplanet kic b small rocki planet close star evapor leav trail tail cloud dust like comet dust could ash erupt volcano escap due small planet low could metal vapor high temperatur close star metal vapor condens dust june scientist report atmospher gj b evapor result giant cloud around planet due radiat host star long trail tail million km million mi long tidal lock planet reson would star alway shine directli overhead one spot would hot opposit hemispher receiv light freez cold planet could resembl eyebal hotspot pupil planet eccentr orbit could lock reson reson would result pattern hotspot eastern western hemispher planet eccentr orbit tilt axi rotat would complic insol pattern surfac featur distinguish atmospher featur compar emiss reflect spectroscopi transmiss spectroscopi spectroscopi exoplanet may detect rocki surfac may identifi magma ocean lava hydrat silic surfac water ice give unambigu method distinguish rocki gaseou exoplanet measur intens light receiv parent star estim temperatur exoplanet exampl planet estim surfac temperatur roughli k howev estim may substanti error depend planet usual unknown albedo factor greenhous effect may introduc unknown complic planet temperatur measur observ variat infrar radiat planet move around orbit eclips parent star exampl planet hd estim averag temperatur k daysid k nightsid planet discov field exoplanetolog continu grow deeper studi extrasolar world ultim tackl prospect life planet beyond solar system cosmic distanc life detect develop planetari scale strongli modifi planetari environ way modif explain classic process equilibrium process exampl molecular oxygen atmospher earth result photosynthesi live plant mani kind microorgan use indic life exoplanet although small amount oxygen could also produc mean furthermor potenti habit planet must orbit stabl star distanc within object suffici atmospher pressur support liquid water surfac habit zone around star region temperatur right allow liquid water exist surfac planet close star water evapor far away star water freez heat produc star vari depend size age star habit zone differ distanc differ star also atmospher condit planet influenc planet abil retain heat locat habit zone also specif type planet desert planet also known dri planet littl water less water vapor atmospher earth reduc greenhous effect mean desert planet could maintain oas water closer star earth sun lack water also mean less ice reflect heat space outer edg habit zone rocki planet thick hydrogen atmospher could maintain surfac water much distanc planet larger mass wider habit zone graviti reduc water cloud column depth reduc greenhous effect water vapor thu move inner edg habit zone closer star planetari rotat rate one major factor determin circul atmospher henc pattern cloud slowli rotat planet creat thick cloud reflect habit much closer star earth current atmospher would habit venu orbit venu slow rotat venu lost water ocean due runaway greenhous effect like higher rotat rate past altern venu never ocean water vapor lost space format could slow rotat throughout histori tidal lock planet eyebal planet habit closer star previous thought due effect cloud high stellar flux strong convect produc thick water cloud near substellar point greatli increas planetari albedo reduc surfac temperatur planet habit zone star low metal habit complex life land high metal star stellar spectrum high metal star less like caus format ozon thu enabl ultraviolet ray reach planet surfac habit zone usual defin term surfac temperatur howev half earth biomass subsurfac microb temperatur increas depth subsurfac conduc microbi life surfac frozen consid habit zone extend much star even rogu planet could liquid water suffici depth underground earlier era univers temperatur cosmic microwav background would allow rocki planet exist liquid water surfac regardless distanc star planet might habit could habit moon outer edg habit zone planet complet frozen planet well insid habit zone period becom frozen orbit fluctuat caus produc cool creat ice ice reflect sunlight caus even cool creat feedback loop planet complet nearli complet frozen surfac frozen stop carbon dioxid weather result carbon dioxid atmospher volcan emiss creat greenhous effect thaw planet planet larg axial tilt less like enter snowbal state retain liquid water star larg fluctuat axial tilt even warm effect fix larg tilt paradox planet orbit cooler star red dwarf less like enter snowbal state infrar radiat emit cooler star mostli wavelength absorb ice heat planet eccentr orbit tidal heat provid anoth sourc energi besid stellar radiat mean eccentr planet radi habit zone hot liquid water tide also circular orbit time could planet habit zone circular orbit water use eccentr orbit eccentr planet habit zone would still frozen surfac tidal heat could creat subsurfac ocean similar europa planetari system upsilon andromeda system eccentr orbit maintain even period vari perturb planet system tidal heat caus outgass mantl contribut format replenish atmospher review identifi exoplanet best candid potenti habit distanc away respect similar size earth measur locat toward outer edg habit zone around red dwarf star look nearest terrestri exoplanet candid proxima centauri b away equilibrium temperatur estim k exoplanet often member planetari system multipl planet around star planet interact gravit sometim form reson system orbit period planet integ ratio system contain four planet orbit reson hot jupit orbit star opposit direct star rotat one propos explan hot jupit tend form dens cluster perturb common gravit captur planet neighbor star possibl
Adolf Hitler,https://en.wikipedia.org/wiki/Adolf_Hitler,"
 Adolf Hitler[a] (20 April 1889 – 30 April 1945) was a German politician who was the dictator of Nazi Germany from 1933 until his suicide in 1945. He rose to power as the leader of the Nazi Party,[c] becoming the chancellor in 1933 and then taking the title of Führer und Reichskanzler in 1934.[d] His invasion of Poland on 1 September 1939 marked the start of the Second World War. He was closely involved in military operations throughout the war and was central to the perpetration of the Holocaust: the genocide of about six million Jews and millions of other victims.
 Hitler was born in Braunau am Inn in Austria-Hungary and was raised near Linz. He lived in Vienna in the first decade of the 1900s before moving to Germany in 1913. He was decorated during his service in the German Army in World War I, receiving the Iron Cross. In 1919, he joined the German Workers' Party (DAP), the precursor of the Nazi Party, and in 1921 was appointed leader of the Nazi Party. In 1923, he attempted to seize governmental power in a failed coup in Munich and was sentenced to five years in prison, serving just over a year of his sentence. While there, he dictated the first volume of his autobiography and political manifesto Mein Kampf (My Struggle). After his early release in 1924, Hitler gained popular support by attacking the Treaty of Versailles and promoting pan-Germanism, antisemitism, and anti-communism with charismatic oratory and Nazi propaganda. He frequently denounced communism as being part of an international Jewish conspiracy.
 By November 1932, the Nazi Party held the most seats in the Reichstag, but not a majority. No political parties were able to form a majority coalition in support of a candidate for chancellor. Former chancellor Franz von Papen and other conservative leaders convinced President Paul von Hindenburg to appoint Hitler as chancellor on 30 January 1933. Shortly thereafter, the Reichstag passed the Enabling Act of 1933, which began the process of transforming the Weimar Republic into Nazi Germany, a one-party dictatorship based on the totalitarian and autocratic ideology of Nazism. Upon Hindenburg's death on 2 August 1934, Hitler succeeded him, becoming simultaneously the head of state and government, with absolute power. Domestically, Hitler implemented numerous racist policies and sought to deport or kill German Jews. His first six years in power resulted in rapid economic recovery from the Great Depression, the abrogation of restrictions imposed on Germany after World War I, and the annexation of territories inhabited by millions of ethnic Germans, which initially gave him significant popular support.
 One of Hitler's key goals was Lebensraum (lit. 'living space') for the German people in Eastern Europe, and his aggressive, expansionist foreign policy is considered the primary cause of World War II in Europe. He directed large-scale rearmament and, on 1 September 1939, invaded Poland, causing Britain and France to declare war on Germany. In June 1941, Hitler ordered an invasion of the Soviet Union. In December 1941, he declared war on the United States. By the end of 1941, German forces and the European Axis powers occupied most of Europe and North Africa. These gains were gradually reversed after 1941, and in 1945 the Allied armies defeated the German army. On 29 April 1945, he married his longtime partner, Eva Braun, in the Führerbunker in Berlin. The couple committed suicide the next day to avoid capture by the Soviet Red Army. In accordance with Hitler's wishes, their corpses were burned.
 The historian and biographer Ian Kershaw described Hitler as ""the embodiment of modern political evil"".[3] Under Hitler's leadership and racist ideology, the Nazi regime was responsible for the genocide of an estimated six million Jews and millions of other victims, whom he and his followers deemed Untermenschen (lit. 'subhumans') or socially undesirable. Hitler and the Nazi regime were also responsible for the deliberate killing of an estimated 19.3 million civilians and prisoners of war. In addition, 28.7 million soldiers and civilians died as a result of military action in the European theatre. The number of civilians killed during World War II was unprecedented in warfare, and the casualties constitute the deadliest conflict in history.
 Hitler's father, Alois Hitler (1837–1903), was the illegitimate child of Maria Schicklgruber.[4] The baptismal register did not show the name of his father, and Alois initially bore his mother's surname, ""Schicklgruber"". In 1842, Johann Georg Hiedler married Alois's mother. Alois was brought up in the family of Hiedler's brother, Johann Nepomuk Hiedler.[5] In 1876, Alois was made legitimate and his baptismal record annotated by a priest to register Johann Georg Hiedler as Alois's father (recorded as ""Georg Hitler"").[6][7] Alois then assumed the surname ""Hitler"",[7] also spelled ""Hiedler"", ""Hüttler"", or ""Huettler"". The name is probably based on the German word Hütte (lit. 'hut'), and has the meaning ""one who lives in a hut"".[8]
 Nazi official Hans Frank suggested that Alois's mother had been employed as a housekeeper by a Jewish family in Graz, and that the family's 19-year-old son Leopold Frankenberger had fathered Alois, a claim that came to be known as the Frankenberger thesis.[9] No Frankenberger was registered in Graz during that period, no record has been produced of Leopold Frankenberger's existence,[10] so historians dismiss the claim that Alois's father was Jewish.[11][12]
 Adolf Hitler was born on 20 April 1889 in Braunau am Inn, a town in Austria-Hungary (present-day Austria), close to the border with the German Empire.[13][14] He was the fourth of six children born to Alois Hitler and his third wife, Klara Pölzl. Three of Hitler's siblings—Gustav, Ida, and Otto—died in infancy.[15] Also living in the household were Alois's children from his second marriage: Alois Jr. (born 1882) and Angela (born 1883).[16] When Hitler was three, the family moved to Passau, Germany.[17] There he acquired the distinctive lower Bavarian dialect, rather than Austrian German, which marked his speech throughout his life.[18][19][20] The family returned to Austria and settled in Leonding in 1894, and in June 1895 Alois retired to Hafeld, near Lambach, where he farmed and kept bees. Hitler attended Volksschule (a state-funded primary school) in nearby Fischlham.[21][22]
 The move to Hafeld coincided with the onset of intense father-son conflicts caused by Hitler's refusal to conform to the strict discipline of his school.[23] Alois tried to browbeat his son into obedience, while Adolf did his best to be the opposite of whatever his father wanted.[24] Alois would also beat his son, although his mother tried to protect him from regular beatings.[25]
 Alois Hitler's farming efforts at Hafeld ended in failure, and in 1897 the family moved to Lambach. The eight-year-old Hitler took singing lessons, sang in the church choir, and even considered becoming a priest.[26] In 1898, the family returned permanently to Leonding. Hitler was deeply affected by the death of his younger brother Edmund in 1900 from measles. Hitler changed from a confident, outgoing, conscientious student to a morose, detached boy who constantly fought with his father and teachers.[27] Paula Hitler recalled how Adolf was a teenage bully who would often slap her.[25]
 Alois had made a successful career in the customs bureau and wanted his son to follow in his footsteps.[28] Hitler later dramatised an episode from this period when his father took him to visit a customs office, depicting it as an event that gave rise to an unforgiving antagonism between father and son, who were both strong-willed.[29][30][31] Ignoring his son's desire to attend a classical high school and become an artist, Alois sent Hitler to the Realschule in Linz in September 1900.[e][32] Hitler rebelled against this decision, and in Mein Kampf states that he intentionally performed poorly in school, hoping that once his father saw ""what little progress I was making at the technical school he would let me devote myself to my dream"".[33]
 Like many Austrian Germans, Hitler began to develop German nationalist ideas from a young age.[34] He expressed loyalty only to Germany, despising the declining Habsburg monarchy and its rule over an ethnically diverse empire.[35][36] Hitler and his friends used the greeting ""Heil"", and sang the ""Deutschlandlied"" instead of the Austrian Imperial anthem.[37] After Alois's sudden death on 3 January 1903, Hitler's performance at school deteriorated and his mother allowed him to leave.[38] He enrolled at the Realschule in Steyr in September 1904, where his behaviour and performance improved.[39] In 1905, after passing a repeat of the final exam, Hitler left the school without any ambitions for further education or clear plans for a career.[40]
 In 1907, Hitler left Linz to live and study fine art in Vienna, financed by orphan's benefits and support from his mother. He applied for admission to the Academy of Fine Arts Vienna but was rejected twice.[41][42] The director suggested Hitler should apply to the School of Architecture, but he lacked the necessary academic credentials because he had not finished secondary school.[43]
 On 21 December 1907, his mother died of breast cancer at the age of 47; Hitler was 18 at the time. In 1909, Hitler ran out of money and was forced to live a bohemian life in homeless shelters and the Meldemannstraße dormitory.[44][45] He earned money as a casual labourer and by painting and selling watercolours of Vienna's sights.[41] During his time in Vienna, he pursued a growing passion for architecture and music, attending ten performances of Lohengrin, his favourite Wagner opera.[46]
 In Vienna, Hitler was first exposed to racist rhetoric.[47] Populists such as mayor Karl Lueger exploited the city's prevalent anti-Semitic sentiment, occasionally also espousing German nationalist notions for political benefit. German nationalism was even more widespread in the Mariahilf district, where Hitler then lived.[48] Georg Ritter von Schönerer became a major influence on Hitler,[49] and he developed an admiration for Martin Luther.[50] Hitler read local newspapers that promoted prejudice and utilised Christian fears of being swamped by an influx of Eastern European Jews[51] as well as pamphlets that published the thoughts of philosophers and theoreticians such as Houston Stewart Chamberlain, Charles Darwin, Friedrich Nietzsche, Gustave Le Bon, and Arthur Schopenhauer.[52] During his life in Vienna, Hitler also developed fervent anti-Slavic sentiments.[53][54]
 The origin and development of Hitler's anti-Semitism remains a matter of debate.[55] His friend August Kubizek claimed that Hitler was a ""confirmed anti-Semite"" before he left Linz.[56] However, historian Brigitte Hamann describes Kubizek's claim as ""problematical"".[57] While Hitler states in Mein Kampf that he first became an anti-Semite in Vienna,[58] Reinhold Hanisch, who helped him sell his paintings, disagrees. Hitler had dealings with Jews while living in Vienna.[59][60][61] Historian Richard J. Evans states that ""historians now generally agree that his notorious, murderous anti-Semitism emerged well after Germany's defeat [in World War I], as a product of the paranoid ""stab-in-the-back"" explanation for the catastrophe"".[62]
 Hitler received the final part of his father's estate in May 1913 and moved to Munich, Germany.[63] When he was conscripted into the Austro-Hungarian Army,[64] he journeyed to Salzburg on 5 February 1914 for medical assessment. After he was deemed unfit for service, he returned to Munich.[65] Hitler later claimed that he did not wish to serve the Habsburg Empire because of the mixture of races in its army and his belief that the collapse of Austria-Hungary was imminent.[66]
 In August 1914, at the outbreak of World War I, Hitler was living in Munich and voluntarily enlisted in the Bavarian Army.[67] According to a 1924 report by the Bavarian authorities, allowing Hitler to serve was most likely an administrative error, because as an Austrian citizen, he should have been returned to Austria.[67] Posted to the Bavarian Reserve Infantry Regiment 16 (1st Company of the List Regiment),[67][68] he served as a dispatch runner on the Western Front in France and Belgium,[69] spending nearly half his time at the regimental headquarters in Fournes-en-Weppes, well behind the front lines.[70][71] In 1914, he was present at the First Battle of Ypres[72] and in that year was decorated for bravery, receiving the Iron Cross, Second Class.[72]
 During his service at headquarters, Hitler pursued his artwork, drawing cartoons and instructions for an army newspaper. During the Battle of the Somme in October 1916, he was wounded in the left thigh when a shell exploded in the dispatch runners' dugout.[72][73] Hitler spent almost two months recovering in hospital at Beelitz, returning to his regiment on 5 March 1917.[74] He was present at the Battle of Arras of 1917 and the Battle of Passchendaele.[72] He received the Black Wound Badge on 18 May 1918.[75] Three months later, in August 1918, on a recommendation by Lieutenant Hugo Gutmann, his Jewish superior, Hitler received the Iron Cross, First Class, a decoration rarely awarded at Hitler's Gefreiter rank.[76][77] On 15 October 1918, he was temporarily blinded in a mustard gas attack and was hospitalised in Pasewalk.[78] While there, Hitler learned of Germany's defeat, and, by his own account, suffered a second bout of blindness after receiving this news.[79]
 Hitler described his role in World War I as ""the greatest of all experiences"", and was praised by his commanding officers for his bravery.[80] His wartime experience reinforced his German patriotism, and he was shocked by Germany's capitulation in November 1918.[81] His displeasure with the collapse of the war effort began to shape his ideology.[82] Like other German nationalists, he believed the Dolchstoßlegende (stab-in-the-back myth), which claimed that the German army, ""undefeated in the field"", had been ""stabbed in the back"" on the home front by civilian leaders, Jews, Marxists, and those who signed the armistice that ended the fighting—later dubbed the ""November criminals"".[83]
 The Treaty of Versailles stipulated that Germany had to relinquish several of its territories and demilitarise the Rhineland. The treaty imposed economic sanctions and levied heavy reparations on the country. Many Germans saw the treaty as an unjust humiliation. They especially objected to Article 231, which they interpreted as declaring Germany responsible for the war.[84] The Versailles Treaty and the economic, social, and political conditions in Germany after the war were later exploited by Hitler for political gain.[85]
 After World War I, Hitler returned to Munich.[86] Without formal education or career prospects, he remained in the Army.[87] In July 1919, he was appointed Verbindungsmann (intelligence agent) of an Aufklärungskommando (reconnaissance unit) of the Reichswehr, assigned to influence other soldiers and to infiltrate the German Workers' Party (DAP). At a DAP meeting on 12 September 1919, Party Chairman Anton Drexler was impressed by Hitler's oratorical skills. He gave him a copy of his pamphlet My Political Awakening, which contained anti-Semitic, nationalist, anti-capitalist, and anti-Marxist ideas.[88] On the orders of his army superiors, Hitler applied to join the party,[89] and within a week was accepted as party member 555 (the party began counting membership at 500 to give the impression they were a much larger party).[90][91]
 Hitler made his earliest known written statement about the Jewish question in a 16 September 1919 letter to Adolf Gemlich (now known as the Gemlich letter). In the letter, Hitler argues that the aim of the government ""must unshakably be the removal of the Jews altogether"".[92] At the DAP, Hitler met Dietrich Eckart, one of the party's founders and a member of the occult Thule Society.[93] Eckart became Hitler's mentor, exchanging ideas with him and introducing him to a wide range of Munich society.[94] To increase its appeal, the DAP changed its name to the Nationalsozialistische Deutsche Arbeiterpartei (National Socialist German Workers' Party (NSDAP), now known as the ""Nazi Party"").[95] Hitler designed the party's banner of a swastika in a white circle on a red background.[96]
 Hitler was discharged from the Army on 31 March 1920 and began working full-time for the party.[97] The party headquarters was in Munich, a centre for anti-government German nationalists determined to eliminate Marxism and undermine the Weimar Republic.[98] In February 1921—already highly effective at crowd manipulation—he spoke to a crowd of over 6,000.[99] To publicise the meeting, two truckloads of party supporters drove around Munich waving swastika flags and distributing leaflets. Hitler soon gained notoriety for his rowdy polemic speeches against the Treaty of Versailles, rival politicians, and especially against Marxists and Jews.[100]
 In June 1921, while Hitler and Eckart were on a fundraising trip to Berlin, a mutiny broke out within the Nazi Party in Munich. Members of its executive committee wanted to merge with the Nuremberg-based German Socialist Party (DSP).[101] Hitler returned to Munich on 11 July and angrily tendered his resignation. The committee members realised that the resignation of their leading public figure and speaker would mean the end of the party.[102] Hitler announced he would rejoin on the condition that he would replace Drexler as party chairman, and that the party headquarters would remain in Munich.[103] The committee agreed, and he rejoined the party on 26 July as member 3,680. Hitler continued to face some opposition within the Nazi Party. Opponents of Hitler in the leadership had Hermann Esser expelled from the party, and they printed 3,000 copies of a pamphlet attacking Hitler as a traitor to the party.[103][f] In the following days, Hitler spoke to several large audiences and defended himself and Esser, to thunderous applause. His strategy proved successful, and at a special party congress on 29 July, he was granted absolute power as party chairman, succeeding Drexler, by a vote of 533 to 1.[104]
 Hitler's vitriolic beer hall speeches began attracting regular audiences. A demagogue,[105] he became adept at using populist themes, including the use of scapegoats, who were blamed for his listeners' economic hardships.[106][107][108] Hitler used personal magnetism and an understanding of crowd psychology to his advantage while engaged in public speaking.[109][110] Historians have noted the hypnotic effect of his rhetoric on large audiences, and of his eyes in small groups.[111] Alfons Heck, a former member of the Hitler Youth, recalled:
 We erupted into a frenzy of nationalistic pride that bordered on hysteria. For minutes on end, we shouted at the top of our lungs, with tears streaming down our faces: Sieg Heil, Sieg Heil, Sieg Heil! From that moment on, I belonged to Adolf Hitler body and soul.[112] Early followers included Rudolf Hess, former air force ace Hermann Göring, and army captain Ernst Röhm. Röhm became head of the Nazis' paramilitary organisation, the Sturmabteilung (SA, ""Stormtroopers""), which protected meetings and attacked political opponents. A critical influence on Hitler's thinking during this period was the Aufbau Vereinigung,[113] a conspiratorial group of White Russian exiles and early Nazis. The group, financed with funds channelled from wealthy industrialists, introduced Hitler to the idea of a Jewish conspiracy, linking international finance with Bolshevism.[114]
 The programme of the Nazi Party was laid out in their 25-point programme on 24 February 1920. This did not represent a coherent ideology, but was a conglomeration of received ideas which had currency in the völkisch Pan-Germanic movement, such as ultranationalism, opposition to the Treaty of Versailles, distrust of capitalism, as well as some socialist ideas. For Hitler, the most important aspect of it was its strong anti-Semitic stance. He also perceived the programme as primarily a basis for propaganda and for attracting people to the party.[115]
 In 1923, Hitler enlisted the help of World War I General Erich Ludendorff for an attempted coup known as the ""Beer Hall Putsch"". The Nazi Party used Italian Fascism as a model for their appearance and policies. Hitler wanted to emulate Benito Mussolini's ""March on Rome"" of 1922 by staging his own coup in Bavaria, to be followed by a challenge to the government in Berlin. Hitler and Ludendorff sought the support of Staatskommissar (State Commissioner) Gustav Ritter von Kahr, Bavaria's de facto ruler. However, Kahr, along with Police Chief Hans Ritter von Seisser and Reichswehr General Otto von Lossow, wanted to install a nationalist dictatorship without Hitler.[116]
 On 8 November 1923, Hitler and the SA stormed a public meeting of 3,000 people organised by Kahr in the Bürgerbräukeller, a beer hall in Munich. Interrupting Kahr's speech, he announced that the national revolution had begun and declared the formation of a new government with Ludendorff.[117] Retiring to a back room, Hitler, with his pistol drawn, demanded and subsequently received the support of Kahr, Seisser, and Lossow.[117] Hitler's forces initially succeeded in occupying the local Reichswehr and police headquarters, but Kahr and his cohorts quickly withdrew their support. Neither the Army nor the state police joined forces with Hitler.[118] The next day, Hitler and his followers marched from the beer hall to the Bavarian War Ministry to overthrow the Bavarian government, but police dispersed them.[119] Sixteen Nazi Party members and four police officers were killed in the failed coup.[120]
 Hitler fled to the home of Ernst Hanfstaengl and by some accounts contemplated suicide.[121] He was depressed but calm when arrested on 11 November 1923 for high treason.[122] His trial before the special People's Court in Munich began in February 1924,[123] and Alfred Rosenberg became temporary leader of the Nazi Party. On 1 April, Hitler was sentenced to five years' imprisonment at Landsberg Prison.[124] There, he received friendly treatment from the guards, and was allowed mail from supporters and regular visits by party comrades. Pardoned by the Bavarian Supreme Court, he was released from jail on 20 December 1924, against the state prosecutor's objections.[125] Including time on remand, Hitler served just over one year in prison.[126]
 While at Landsberg, Hitler dictated most of the first volume of Mein Kampf (lit. 'My Struggle'); originally titled Four and a Half Years of Struggle against Lies, Stupidity, and Cowardice) at first to his chauffeur, Emil Maurice, and then to his deputy, Rudolf Hess.[126][127] The book, dedicated to Thule Society member Dietrich Eckart, was an autobiography and exposition of his ideology. The book laid out Hitler's plans for transforming German society into one based on race. Throughout the book, Jews are equated with ""germs"" and presented as the ""international poisoners"" of society. According to Hitler's ideology, the only solution was their extermination. While Hitler did not describe exactly how this was to be accomplished, his ""inherent genocidal thrust is undeniable"", according to Ian Kershaw.[128]
 Published in two volumes in 1925 and 1926, Mein Kampf sold 228,000 copies between 1925 and 1932. One million copies were sold in 1933, Hitler's first year in office.[129] Shortly before Hitler was eligible for parole, the Bavarian government attempted to have him deported to Austria.[130] The Austrian federal chancellor rejected the request on the specious grounds that his service in the German Army made his Austrian citizenship void.[131] In response, Hitler formally renounced his Austrian citizenship on 7 April 1925.[131]
 At the time of Hitler's release from prison, politics in Germany had become less combative and the economy had improved, limiting Hitler's opportunities for political agitation. As a result of the failed Beer Hall Putsch, the Nazi Party and its affiliated organisations were banned in Bavaria. In a meeting with the Prime Minister of Bavaria, Heinrich Held, on 4 January 1925, Hitler agreed to respect the state's authority and promised that he would seek political power only through the democratic process. The meeting paved the way for the ban on the Nazi Party to be lifted on 16 February.[132]
 However, after an inflammatory speech he gave on 27 February, Hitler was barred from public speaking by the Bavarian authorities, a ban that remained in place until 1927.[133][134] To advance his political ambitions in spite of the ban, Hitler appointed Gregor Strasser, Otto Strasser, and Joseph Goebbels to organise and enlarge the Nazi Party in northern Germany. Gregor Strasser steered a more independent political course, emphasising the socialist elements of the party's programme.[135]
 The stock market in the United States crashed on 24 October 1929. The impact in Germany was dire: millions became unemployed and several major banks collapsed. Hitler and the Nazi Party prepared to take advantage of the emergency to gain support for their party. They promised to repudiate the Versailles Treaty, strengthen the economy, and provide jobs.[136]
 The Great Depression provided a political opportunity for Hitler. Germans were ambivalent about the parliamentary republic, which faced challenges from right- and left-wing extremists. The moderate political parties were increasingly unable to stem the tide of extremism, and the German referendum of 1929 helped to elevate Nazi ideology.[138] The elections of September 1930 resulted in the break-up of a grand coalition and its replacement with a minority cabinet. Its leader, chancellor Heinrich Brüning of the Centre Party, governed through emergency decrees from President Paul von Hindenburg. Governance by decree became the new norm and paved the way for authoritarian forms of government.[139] The Nazi Party rose from obscurity to win 18.3 per cent of the vote and 107 parliamentary seats in the 1930 election, becoming the second-largest party in parliament.[140]
 Hitler made a prominent appearance at the trial of two Reichswehr officers, Lieutenants Richard Scheringer and Hanns Ludin, in late 1930. Both were charged with membership in the Nazi Party, at that time illegal for Reichswehr personnel.[141] The prosecution argued that the Nazi Party was an extremist party, prompting defence lawyer Hans Frank to call on Hitler to testify.[142] On 25 September 1930, Hitler testified that his party would pursue political power solely through democratic elections,[143] which won him many supporters in the officer corps.[144]
 Brüning's austerity measures brought little economic improvement and were extremely unpopular.[145] Hitler exploited this by targeting his political messages specifically at people who had been affected by the inflation of the 1920s and the Depression, such as farmers, war veterans, and the middle class.[146]
 Although Hitler had terminated his Austrian citizenship in 1925, he did not acquire German citizenship for almost seven years. This meant that he was stateless, legally unable to run for public office, and still faced the risk of deportation.[147] On 25 February 1932, the interior minister of Brunswick, Dietrich Klagges, who was a member of the Nazi Party, appointed Hitler as administrator for the state's delegation to the Reichsrat in Berlin, making Hitler a citizen of Brunswick,[148] and thus of Germany.[149]
 Hitler ran against Hindenburg in the 1932 presidential elections. A speech to the Industry Club in Düsseldorf on 27 January 1932 won him support from many of Germany's most powerful industrialists.[150] Hindenburg had support from various nationalist, monarchist, Catholic, and republican parties, and some Social Democrats. Hitler used the campaign slogan ""Hitler über Deutschland"" (""Hitler over Germany""), a reference to his political ambitions and his campaigning by aircraft.[151] He was one of the first politicians to use aircraft travel for campaigning and used it effectively.[152][153] Hitler came in second in both rounds of the election, garnering more than 35 per cent of the vote in the final election. Although he lost to Hindenburg, this election established Hitler as a strong force in German politics.[154]
 The absence of an effective government prompted two influential politicians, Franz von Papen and Alfred Hugenberg, along with several other industrialists and businessmen, to write a letter to Hindenburg. The signers urged Hindenburg to appoint Hitler as leader of a government ""independent from parliamentary parties"", which could turn into a movement that would ""enrapture millions of people"".[155][156]
 Hindenburg reluctantly agreed to appoint Hitler as chancellor after two further parliamentary elections—in July and November 1932—had not resulted in the formation of a majority government. Hitler headed a short-lived coalition government formed by the Nazi Party (which had the most seats in the Reichstag) and Hugenberg's party, the German National People's Party (DNVP). On 30 January 1933, the new cabinet was sworn in during a brief ceremony in Hindenburg's office. The Nazi Party gained three posts: Hitler was named chancellor, Wilhelm Frick Minister of the Interior, and Hermann Göring Minister of the Interior for Prussia.[157] Hitler had insisted on the ministerial positions as a way to gain control over the police in much of Germany.[158]
 As chancellor, Hitler worked against attempts by the Nazi Party's opponents to build a majority government. Because of the political stalemate, he asked Hindenburg to again dissolve the Reichstag, and elections were scheduled for early March. On 27 February 1933, the Reichstag building was set on fire. Göring blamed a communist plot, as Dutch communist Marinus van der Lubbe was found in incriminating circumstances inside the burning building.[159] Until the 1960s, some historians, including William L. Shirer and Alan Bullock, thought the Nazi Party itself was responsible;[160][161] according to Ian Kershaw, writing in 1998, the view of nearly all modern historians is that van der Lubbe set the fire alone.[162][needs update]
 At Hitler's urging, Hindenburg responded by signing the Reichstag Fire Decree of 28 February, drafted by the Nazis, which suspended basic rights and allowed detention without trial. The decree was permitted under Article 48 of the Weimar Constitution, which gave the president the power to take emergency measures to protect public safety and order.[163] Activities of the German Communist Party (KPD) were suppressed, and some 4,000 KPD members were arrested.[164]
 In addition to political campaigning, the Nazi Party engaged in paramilitary violence and the spread of anti-communist propaganda in the days preceding the election. On election day, 6 March 1933, the Nazi Party's share of the vote increased to 43.9 per cent, and the party acquired the largest number of seats in parliament. Hitler's party failed to secure an absolute majority, necessitating another coalition with the DNVP.[165]
 On 21 March 1933, the new Reichstag was constituted with an opening ceremony at the Garrison Church in Potsdam. This ""Day of Potsdam"" was held to demonstrate unity between the Nazi movement and the old Prussian elite and military. Hitler appeared in a morning coat and humbly greeted Hindenburg.[166][167]
 To achieve full political control despite not having an absolute majority in parliament, Hitler's government brought the Ermächtigungsgesetz (Enabling Act) to a vote in the newly elected Reichstag. The Act—officially titled the Gesetz zur Behebung der Not von Volk und Reich (""Law to Remedy the Distress of People and Reich"")—gave Hitler's cabinet the power to enact laws without the consent of the Reichstag for four years. These laws could (with certain exceptions) deviate from the constitution.[168]
 Since it would affect the constitution, the Enabling Act required a two-thirds majority to pass. Leaving nothing to chance, the Nazis used the provisions of the Reichstag Fire Decree to arrest all 81 Communist deputies (in spite of their virulent campaign against the party, the Nazis had allowed the KPD to contest the election)[169] and prevent several Social Democrats from attending.[170]
 On 23 March 1933, the Reichstag assembled at the Kroll Opera House under turbulent circumstances. Ranks of SA men served as guards inside the building, while large groups outside opposing the proposed legislation shouted slogans and threats towards the arriving members of parliament.[171] After Hitler verbally promised Centre party leader Ludwig Kaas that Hindenburg would retain his power of veto, Kaas announced the Centre Party would support the Enabling Act. The Act passed by a vote of 444–94, with all parties except the Social Democrats voting in favour. The Enabling Act, along with the Reichstag Fire Decree, transformed Hitler's government into a de facto legal dictatorship.[172]
 At the risk of appearing to talk nonsense I tell you that the National Socialist movement will go on for 1,000 years! ... Don't forget how people laughed at me 15 years ago when I declared that one day I would govern Germany. They laugh now, just as foolishly, when I declare that I shall remain in power![173] Having achieved full control over the legislative and executive branches of government, Hitler and his allies began to suppress the remaining opposition. The Social Democratic Party was made illegal, and its assets were seized.[174] While many trade union delegates were in Berlin for May Day activities, SA stormtroopers occupied union offices around the country. On 2 May 1933, all trade unions were forced to dissolve, and their leaders were arrested. Some were sent to concentration camps.[175] The German Labour Front was formed as an umbrella organisation to represent all workers, administrators, and company owners, thus reflecting the concept of Nazism in the spirit of Hitler's Volksgemeinschaft (""people's community"").[176]
 By the end of June, the other parties had been intimidated into disbanding. This included the Nazis' nominal coalition partner, the DNVP; with the SA's help, Hitler forced its leader, Hugenberg, to resign on 29 June. On 14 July 1933, the Nazi Party was declared the only legal political party in Germany.[176][174] The demands of the SA for more political and military power caused anxiety among military, industrial, and political leaders. In response, Hitler purged the entire SA leadership in the Night of the Long Knives, which took place from 30 June to 2 July 1934.[177] Hitler targeted Ernst Röhm and other SA leaders who, along with a number of Hitler's political adversaries (such as Gregor Strasser and former chancellor Kurt von Schleicher), were rounded up, arrested, and shot.[178] While the international community and some Germans were shocked by the killings, many in Germany believed Hitler was restoring order.[179]
 Hindenburg died on 2 August 1934. On the previous day, the cabinet had enacted the Law Concerning the Head of State of the German Reich.[2] This law stated that upon Hindenburg's death, the office of president would be abolished, and its powers merged with those of the chancellor. Hitler thus became head of state as well as head of government and was formally named as Führer und Reichskanzler (Leader and Chancellor of the Reich),[1] although Reichskanzler was eventually dropped.[180] With this action, Hitler eliminated the last legal remedy by which he could be removed from office.[181]
 As head of state, Hitler became commander-in-chief of the armed forces. Immediately after Hindenburg's death, at the instigation of the leadership of the Reichswehr, the traditional loyalty oath of soldiers was altered to affirm loyalty to Hitler personally, by name, rather than to the office of commander-in-chief (which was later renamed to supreme commander) or the state.[182] On 19 August, the merger of the presidency with the chancellorship was approved by 88 per cent of the electorate voting in a plebiscite.[183]
 In early 1938, Hitler used blackmail to consolidate his hold over the military by instigating the Blomberg–Fritsch affair. Hitler forced his War Minister, Field Marshal Werner von Blomberg, to resign by using a police dossier that showed that Blomberg's new wife had a record for prostitution.[184][185] Army commander Colonel-General Werner von Fritsch was removed after the Schutzstaffel (SS) produced allegations that he had engaged in a homosexual relationship.[186] Both men had fallen into disfavour because they objected to Hitler's demand to make the Wehrmacht ready for war as early as 1938.[187] Hitler assumed Blomberg's title of Commander-in-Chief, thus taking personal command of the armed forces.[188] He replaced the Ministry of War with the Oberkommando der Wehrmacht (OKW), headed by General Wilhelm Keitel. On the same day, sixteen generals were stripped of their commands and 44 more were transferred; all were suspected of not being sufficiently pro-Nazi.[189] By early February 1938, twelve more generals had been removed.[190]
 Hitler took care to give his dictatorship the appearance of legality. Many of his decrees were explicitly based on the Reichstag Fire Decree and hence on Article 48 of the Weimar Constitution. The Reichstag renewed the Enabling Act twice, each time for a four-year period.[191] While elections to the Reichstag were still held (in 1933, 1936, and 1938), voters were presented with a single list of Nazis and pro-Nazi ""guests"" which received well over 90 per cent of the vote.[192] These sham elections were held in far-from-secret conditions; the Nazis threatened severe reprisals against anyone who did not vote or who voted against.[193]
 In August 1934, Hitler appointed Reichsbank President Hjalmar Schacht as Minister of Economics, and in the following year, as Plenipotentiary for War Economy in charge of preparing the economy for war.[194] Reconstruction and rearmament were financed through Mefo bills, printing money, and seizing the assets of people arrested as enemies of the State, including Jews.[195] The number of unemployed fell from six million in 1932 to fewer than one million in 1936.[196] Hitler oversaw one of the largest infrastructure improvement campaigns in German history, leading to the construction of dams, autobahns, railroads, and other civil works. Wages were slightly lower in the mid to late 1930s compared with wages during the Weimar Republic, while the cost of living increased by 25 per cent.[197] The average work week increased during the shift to a war economy; by 1939, the average German was working between 47 and 50 hours a week.[198]
 Hitler's government sponsored architecture on an immense scale. Albert Speer, instrumental in implementing Hitler's classicist reinterpretation of German culture, was placed in charge of the proposed architectural renovations of Berlin.[199] Despite a threatened multi-nation boycott, Germany hosted the 1936 Olympic Games. Hitler officiated at the opening ceremonies and attended events at both the Winter Games in Garmisch-Partenkirchen and the Summer Games in Berlin.[200]
 In a meeting with German military leaders on 3 February 1933, Hitler spoke of ""conquest for Lebensraum in the East and its ruthless Germanisation"" as his ultimate foreign policy objectives.[201] In March, Prince Bernhard Wilhelm von Bülow, secretary at the Foreign Office (Auswärtiges Amt), issued a statement of major foreign policy aims: Anschluss with Austria, the restoration of Germany's national borders of 1914, rejection of military restrictions under the Treaty of Versailles, the return of the former German colonies in Africa, and a German zone of influence in Eastern Europe. Hitler found Bülow's goals to be too modest.[202] In speeches during this period, he stressed what he termed the peaceful goals of his policies and a willingness to work within international agreements.[203] At the first meeting of his cabinet in 1933, Hitler prioritised military spending over unemployment relief.[204]
 Germany withdrew from the League of Nations and the World Disarmament Conference in October 1933.[205] In January 1935, over 90 per cent of the people of the Saarland, then under League of Nations administration, voted to unite with Germany.[206] That March, Hitler announced an expansion of the Wehrmacht to 600,000 members—six times the number permitted by the Versailles Treaty – including development of an air force (Luftwaffe) and an increase in the size of the navy (Kriegsmarine). Britain, France, Italy, and the League of Nations condemned these violations of the Treaty but did nothing to stop it.[207][208] The Anglo-German Naval Agreement (AGNA) of 18 June allowed German tonnage to increase to 35 per cent of that of the British navy. Hitler called the signing of the AGNA ""the happiest day of his life"", believing that the agreement marked the beginning of the Anglo-German alliance he had predicted in Mein Kampf.[209] France and Italy were not consulted before the signing, directly undermining the League of Nations and setting the Treaty of Versailles on the path towards irrelevance.[210]
 Germany reoccupied the demilitarised zone in the Rhineland in March 1936, in violation of the Versailles Treaty. Hitler also sent troops to Spain to support Francisco Franco during the Spanish Civil War after receiving an appeal for help in July 1936. At the same time, Hitler continued his efforts to create an Anglo-German alliance.[211] In August 1936, in response to a growing economic crisis caused by his rearmament efforts, Hitler ordered Göring to implement a Four Year Plan to prepare Germany for war within the next four years.[212] The plan envisaged an all-out struggle between ""Judeo-Bolshevism"" and German Nazism, which in Hitler's view required a committed effort of rearmament regardless of the economic costs.[213]
 In October 1936, Count Galeazzo Ciano, foreign minister of Mussolini's government, visited Germany, where he signed a Nine-Point Protocol as an expression of rapprochement and had a personal meeting with Hitler. On 1 November, Mussolini declared an ""axis"" between Germany and Italy.[214] On 25 November, Germany signed the Anti-Comintern Pact with Japan. Britain, China, Italy, and Poland were also invited to join the Anti-Comintern Pact, but only Italy signed in 1937. Hitler abandoned his plan of an Anglo-German alliance, blaming ""inadequate"" British leadership.[215] At a meeting in the Reich Chancellery with his foreign ministers and military chiefs that November, Hitler restated his intention of acquiring Lebensraum for the German people. He ordered preparations for war in the East, to begin as early as 1938 and no later than 1943. In the event of his death, the conference minutes, recorded as the Hossbach Memorandum, were to be regarded as his ""political testament"".[216] He felt that a severe decline in living standards in Germany as a result of the economic crisis could only be stopped by military aggression aimed at seizing Austria and Czechoslovakia.[217][218] Hitler urged quick action before Britain and France gained a permanent lead in the arms race.[217] In early 1938, in the wake of the Blomberg–Fritsch affair, Hitler asserted control of the military-foreign policy apparatus, dismissing Neurath as foreign minister and appointing himself as War Minister.[212] From early 1938 onwards, Hitler was carrying out a foreign policy ultimately aimed at war.[219]
 In February 1938, on the advice of his newly appointed foreign minister, the strongly pro-Japanese Joachim von Ribbentrop, Hitler ended the Sino-German alliance with the Republic of China to instead enter into an alliance with the more modern and powerful Empire of Japan. Hitler announced German recognition of Manchukuo, the Japanese puppet state in Manchuria, and renounced German claims to their former colonies in the Pacific held by Japan.[220] Hitler ordered an end to arms shipments to China and recalled all German officers working with the Chinese Army.[220] In retaliation, Chinese General Chiang Kai-shek cancelled all Sino-German economic agreements, depriving the Germans of many Chinese raw materials.[221]
 On 12 March 1938, Hitler announced the unification of Austria with Nazi Germany in the Anschluss.[222][223] Hitler then turned his attention to the ethnic German population of the Sudetenland region of Czechoslovakia.[224] On 28–29 March 1938, Hitler held a series of secret meetings in Berlin with Konrad Henlein of the Sudeten German Party, the largest of the ethnic German parties of the Sudetenland. The men agreed that Henlein would demand increased autonomy for Sudeten Germans from the Czechoslovakian government, thus providing a pretext for German military action against Czechoslovakia. In April 1938 Henlein told the foreign minister of Hungary that ""whatever the Czech government might offer, he would always raise still higher demands ... he wanted to sabotage an understanding by any means because this was the only method to blow up Czechoslovakia quickly"".[225] In private, Hitler considered the Sudeten issue unimportant; his real intention was a war of conquest against Czechoslovakia.[226]
 In April, Hitler ordered the OKW to prepare for Fall Grün (Case Green), the code name for an invasion of Czechoslovakia.[227] As a result of intense French and British diplomatic pressure, on 5 September Czechoslovakian President Edvard Beneš unveiled the ""Fourth Plan"" for constitutional reorganisation of his country, which agreed to most of Henlein's demands for Sudeten autonomy.[228] Henlein's party responded to Beneš' offer by instigating a series of violent clashes with the Czechoslovakian police that led to the declaration of martial law in certain Sudeten districts.[229][230]
 Germany was dependent on imported oil; a confrontation with Britain over the Czechoslovakian dispute could curtail Germany's oil supplies. This forced Hitler to call off Fall Grün, originally planned for 1 October 1938.[231] On 29 September, Hitler, Neville Chamberlain, Édouard Daladier, and Mussolini attended a one-day conference in Munich that led to the Munich Agreement, which handed over the Sudetenland districts to Germany.[232][233]
 Chamberlain was satisfied with the Munich conference, calling the outcome ""peace for our time"", while Hitler was angered about the missed opportunity for war in 1938;[234][235] he expressed his disappointment in a speech on 9 October in Saarbrücken.[236] In Hitler's view, the British-brokered peace, although favourable to the ostensible German demands, was a diplomatic defeat which spurred his intent of limiting British power to pave the way for the eastern expansion of Germany.[237][238] As a result of the summit, Hitler was selected Time magazine's Man of the Year for 1938.[239] In late 1938 and early 1939, the continuing economic crisis caused by rearmament forced Hitler to make major defence cuts.[240] In his ""Export or die"" speech of 30 January 1939, he called for an economic offensive to increase German foreign exchange holdings to pay for raw materials such as high-grade iron needed for military weapons.[240]
 On 14 March 1939, under threat from Hungary, Slovakia declared independence and received protection from Germany.[241] The next day, in violation of the Munich Agreement and possibly as a result of the deepening economic crisis requiring additional assets,[242] Hitler ordered the Wehrmacht to invade the Czech rump state, and from Prague Castle he proclaimed the territory a German protectorate.[243]
 In private discussions in 1939, Hitler declared Britain the main enemy to be defeated and that Poland's obliteration was a necessary prelude for that goal.[244] The eastern flank would be secured and land would be added to Germany's Lebensraum.[245] Offended by the British ""guarantee"" on 31 March 1939 of Polish independence, he said, ""I shall brew them a devil's drink"".[246] In a speech in Wilhelmshaven for the launch of the battleship Tirpitz on 1 April, he threatened to denounce the Anglo-German Naval Agreement if the British continued to guarantee Polish independence, which he perceived as an ""encirclement"" policy.[246] Poland was to either become a German satellite state or it would be neutralised in order to secure the Reich's eastern flank and prevent a possible British blockade.[247]
 Hitler initially favoured the idea of a satellite state, but upon its rejection by the Polish government, he decided to invade and made this the main foreign policy goal of 1939.[248] On 3 April, Hitler ordered the military to prepare for Fall Weiss (""Case White""), the plan for invading Poland on 25 August.[248] In a Reichstag speech on 28 April, he renounced both the Anglo-German Naval Agreement and the German–Polish Non-Aggression Pact.[249] Historians such as William Carr, Gerhard Weinberg, and Ian Kershaw have argued that one reason for Hitler's rush to war was his fear of an early death. He had repeatedly claimed that he must lead Germany into war before he got too old, as his successors might lack his strength of will.[250][251][252] Hitler was concerned that a military attack against Poland could result in a premature war with Britain.[247][253] Hitler's foreign minister and former Ambassador to London, Joachim von Ribbentrop, assured him that neither Britain nor France would honour their commitments to Poland.[254][255] Accordingly, on 22 August 1939 Hitler ordered a military mobilisation against Poland.[256]
 This plan required tacit Soviet support,[257] and the non-aggression pact (the Molotov–Ribbentrop Pact) between Germany and the Soviet Union, led by Joseph Stalin, included a secret agreement to partition Poland between the two countries.[258] Contrary to Ribbentrop's prediction that Britain would sever Anglo-Polish ties, Britain and Poland signed the Anglo-Polish alliance on 25 August 1939. This, along with news from Italy that Mussolini would not honour the Pact of Steel, prompted Hitler to postpone the attack on Poland from 25 August to 1 September.[259] Hitler unsuccessfully tried to manoeuvre the British into neutrality by offering them a non-aggression guarantee on 25 August; he then instructed Ribbentrop to present a last-minute peace plan with an impossibly short time limit in an effort to blame the imminent war on British and Polish inaction.[260][261]
 On 1 September 1939, Germany invaded western Poland under the pretext of having been denied claims to the Free City of Danzig and the right to extraterritorial roads across the Polish Corridor, which Germany had ceded under the Versailles Treaty.[262] In response, Britain and France declared war on Germany on 3 September, surprising Hitler and prompting him to angrily ask Ribbentrop, ""Now what?""[263] France and Britain did not act on their declarations immediately, and on 17 September, Soviet forces invaded eastern Poland.[264]
 The fall of Poland was followed by what contemporary journalists dubbed the ""Phoney War"" or Sitzkrieg (""sitting war""). Hitler instructed the two newly appointed Gauleiters of north-western Poland, Albert Forster of Reichsgau Danzig-West Prussia and Arthur Greiser of Reichsgau Wartheland, to Germanise their areas, with ""no questions asked"" about how this was accomplished.[265] In Forster's area, ethnic Poles merely had to sign forms stating that they had German blood.[266] In contrast, Greiser agreed with Himmler and carried out an ethnic cleansing campaign towards Poles. Greiser soon complained that Forster was allowing thousands of Poles to be accepted as ""racial"" Germans and thus endangered German ""racial purity"".[265] Hitler refrained from getting involved. This inaction has been advanced as an example of the theory of ""working towards the Führer"", in which Hitler issued vague instructions and expected his subordinates to work out policies on their own.[265][267]
 Another dispute pitched one side represented by Heinrich Himmler and Greiser, who championed ethnic cleansing in Poland, against another represented by Göring and Hans Frank (governor-general of occupied Poland), who called for turning Poland into the ""granary"" of the Reich. On 12 February 1940, the dispute was initially settled in favour of the Göring–Frank view, which ended the economically disruptive mass expulsions. On 15 May 1940, Himmler issued a memo entitled ""Some Thoughts on the Treatment of Alien Population in the East"", calling for the expulsion of the entire Jewish population of Europe into Africa and the reduction of the Polish population to a ""leaderless class of labourers"". Hitler called Himmler's memo ""good and correct"", and, ignoring Göring and Frank, implemented the Himmler–Greiser policy in Poland.[268]
 On 9 April, German forces invaded Denmark and Norway. On the same day Hitler proclaimed the birth of the Greater Germanic Reich, his vision of a united empire of Germanic nations of Europe in which the Dutch, Flemish, and Scandinavians were joined into a ""racially pure"" polity under German leadership.[269] In May 1940, Germany attacked France, and conquered Luxembourg, the Netherlands, and Belgium. These victories prompted Mussolini to have Italy join forces with Hitler on 10 June. France and Germany signed an armistice on 22 June.[270] Kershaw notes that Hitler's popularity within Germany—and German support for the war—reached its peak when he returned to Berlin on 6 July from his tour of Paris.[271] Following the unexpected swift victory, Hitler promoted twelve generals to the rank of field marshal during the 1940 Field Marshal Ceremony.[272][273]
 Britain, whose troops were forced to evacuate France by sea from Dunkirk,[274] continued to fight alongside other British dominions in the Battle of the Atlantic. Hitler made peace overtures to the new British leader, Winston Churchill, and upon their rejection he ordered a series of aerial attacks on Royal Air Force airbases and radar stations in southeast England. On 7 September the systematic nightly bombing of London began. The German Luftwaffe failed to defeat the Royal Air Force in what became known as the Battle of Britain.[275] By the end of September, Hitler realised that air superiority for the invasion of Britain (in Operation Sea Lion) could not be achieved, and ordered the operation postponed. The nightly air raids on British cities intensified and continued for months, including London, Plymouth, and Coventry.[276]
 On 27 September 1940, the Tripartite Pact was signed in Berlin by Saburō Kurusu of Imperial Japan, Hitler, and Italian foreign minister Ciano,[277] and later expanded to include Hungary, Romania, and Bulgaria, thus yielding the Axis powers. Hitler's attempt to integrate the Soviet Union into the anti-British bloc failed after inconclusive talks between Hitler and Molotov in Berlin in November, and he ordered preparations for the invasion of the Soviet Union.[278]
 In early 1941, German forces were deployed to North Africa, the Balkans, and the Middle East. In February, German forces arrived in Libya to bolster the Italian presence. In April, Hitler launched the invasion of Yugoslavia, quickly followed by the invasion of Greece.[279] In May, German forces were sent to support Iraqi forces fighting against the British and to invade Crete.[280]
 On 22 June 1941, contravening the Molotov–Ribbentrop Pact of 1939, over three million Axis troops attacked the Soviet Union.[281] This offensive (codenamed Operation Barbarossa) was intended to destroy the Soviet Union and seize its natural resources for subsequent aggression against the Western powers.[282][283] The action was also part of the overall plan to obtain more living space for German people; and Hitler thought a successful invasion would force Britain to negotiate a surrender.[284] The invasion conquered a huge area, including the Baltic republics, Belarus, and West Ukraine. By early August, Axis troops had advanced 500 km (310 miles) and won the Battle of Smolensk. Hitler ordered Army Group Centre to temporarily halt its advance to Moscow and divert its Panzer groups to aid in the encirclement of Leningrad and Kiev.[285] His generals disagreed with this change, having advanced within 400 km (250 miles) of Moscow, and his decision caused a crisis among the military leadership.[286][287] The pause provided the Red Army with an opportunity to mobilise fresh reserves; historian Russel Stolfi considers it to be one of the major factors that caused the failure of the Moscow offensive, which was resumed in October 1941 and ended disastrously in December.[285] During this crisis, Hitler appointed himself as head of the Oberkommando des Heeres.[288]
 On 7 December 1941, Japan attacked the American fleet based at Pearl Harbor, Hawaii. Four days later, Hitler declared war against the United States.[289] On 18 December 1941, Himmler asked Hitler, ""What to do with the Jews of Russia?"", to which Hitler replied, ""als Partisanen auszurotten"" (""exterminate them as partisans"").[290] Israeli historian Yehuda Bauer has commented that the remark is probably as close as historians will ever get to a definitive order from Hitler for the genocide carried out during the Holocaust.[290]
 In late 1942, German forces were defeated in the Second Battle of El Alamein,[291] thwarting Hitler's plans to seize the Suez Canal and the Middle East. Overconfident in his own military expertise following the earlier victories in 1940, Hitler became distrustful of his Army High Command and began to interfere in military and tactical planning, with damaging consequences.[292] In December 1942 and January 1943, Hitler's repeated refusal to allow their withdrawal at the Battle of Stalingrad led to the almost total destruction of the 6th Army. Over 200,000 Axis soldiers were killed and 235,000 were taken prisoner.[293] Thereafter came a decisive strategic defeat at the Battle of Kursk.[294] Hitler's military judgement became increasingly erratic, and Germany's military and economic position deteriorated, as did Hitler's health.[295]
 Following the Allied invasion of Sicily in 1943, Mussolini was removed from power by King Victor Emmanuel III after a vote of no confidence of the Grand Council of Fascism. Marshal Pietro Badoglio, placed in charge of the government, soon surrendered to the Allies.[296] Throughout 1943 and 1944, the Soviet Union steadily forced Hitler's armies into retreat along the Eastern Front. On 6 June 1944, the Western Allied armies landed in northern France in one of the largest amphibious operations in history, Operation Overlord.[297] Many German officers concluded that defeat was inevitable and that continuing under Hitler's leadership would result in the complete destruction of the country.[298]
 Between 1939 and 1945, there were numerous plans to assassinate Hitler, some of which proceeded to significant degrees.[299] The most well-known and significant, the 20 July plot of 1944, came from within Germany and was at least partly driven by the increasing prospect of a German defeat in the war.[300] Part of Operation Valkyrie, the plot involved Claus von Stauffenberg planting a bomb in one of Hitler's headquarters, the Wolf's Lair at Rastenburg. Hitler narrowly survived because staff officer Heinz Brandt moved the briefcase containing the bomb behind a leg of the heavy conference table, which deflected much of the blast. Later, Hitler ordered savage reprisals resulting in the execution of more than 4,900 people.[301] Hitler was put on the United Nations War Crimes Commission's first list of war criminals in December 1944, after determining that Hitler could be held criminally responsible for the acts of the Nazis in occupied countries. By March 1945, at least seven indictments had been filed against him.[302]
 By late 1944, both the Red Army and the Western Allies were advancing into Germany. Recognising the strength and determination of the Red Army, Hitler decided to use his remaining mobile reserves against the American and British armies, which he perceived as far weaker.[303] On 16 December, he launched the Ardennes Offensive to incite disunity among the Western Allies and perhaps convince them to join his fight against the Soviets.[304] After some temporary successes, the offensive failed.[305] With much of Germany in ruins in January 1945, Hitler spoke on the radio: ""However grave as the crisis may be at this moment, it will, despite everything, be mastered by our unalterable will.""[306] Acting on his view that Germany's military failures meant it had forfeited its right to survive as a nation, Hitler ordered the destruction of all German industrial infrastructure before it could fall into Allied hands.[307] Minister for Armaments Albert Speer was entrusted with executing this scorched earth policy, but he secretly disobeyed the order.[307][308] Hitler's hope to negotiate peace with the United States and Britain was encouraged by the death of US President Franklin D. Roosevelt on 12 April 1945, but contrary to his expectations, this caused no rift among the Allies.[304][309]
 On 20 April, his 56th and final birthday, Hitler made his last trip from the Führerbunker to the surface. In the ruined garden of the Reich Chancellery, he awarded Iron Crosses to boy soldiers of the Hitler Youth, who were now fighting the Red Army at the front near Berlin.[310] By 21 April, Georgy Zhukov's 1st Belorussian Front had broken through the defences of General Gotthard Heinrici's Army Group Vistula during the Battle of the Seelow Heights and advanced to the outskirts of Berlin.[311] In denial about the dire situation, Hitler placed his hopes on the undermanned and under-equipped Armeeabteilung Steiner (Army Detachment Steiner), commanded by Felix Steiner. Hitler ordered Steiner to attack the northern flank of the salient, while the German Ninth Army was ordered to attack northward in a pincer attack.[312]
 During a military conference on 22 April, Hitler inquired about Steiner's offensive. He was informed that the attack had not been launched and that the Soviets had entered Berlin. Hitler ordered everyone but Wilhelm Keitel, Alfred Jodl, Hans Krebs, and Wilhelm Burgdorf to leave the room,[313] then launched into a tirade against the perceived treachery and incompetence of his generals, culminating in his declaration—for the first time—that ""everything is lost"".[314] He announced that he would stay in Berlin until the end and then shoot himself.[315]
 By 23 April, the Red Army had surrounded Berlin,[316] and Goebbels made a proclamation urging its citizens to defend the city.[313] That same day, Göring sent a telegram from Berchtesgaden, arguing that as Hitler was isolated in Berlin, Göring should assume leadership of Germany. Göring set a deadline, after which he would consider Hitler incapacitated.[317] Hitler responded by having Göring arrested, and in his last will and testament of 29 April, he removed Göring from all government positions.[318][319] On 28 April, Hitler discovered that Himmler, who had left Berlin on 20 April, was attempting to negotiate a surrender to the Western Allies.[320][321] He considered this treason and ordered Himmler's arrest. He also ordered the execution of Hermann Fegelein, Himmler's SS representative at Hitler's headquarters in Berlin, for desertion.[322]
 After midnight on the night of 28–29 April, Hitler married Eva Braun in a small civil ceremony in the Führerbunker.[323][g] Later that afternoon, Hitler was informed that Mussolini had been executed by the Italian resistance movement on the previous day; this is believed to have increased his determination to avoid capture.[324] On 30 April, Soviet troops were within five hundred metres of the Reich Chancellery when Hitler shot himself in the head and Braun bit into a cyanide capsule.[325][326] In accordance with Hitler's wishes, their corpses were carried outside to the garden behind the Reich Chancellery, where they were placed in a bomb crater, doused with petrol, and set on fire as the Red Army shelling continued.[327][328][329] Grand Admiral Karl Dönitz and Goebbels assumed Hitler's roles as head of state and chancellor respectively.[330] On the evening of 1 May, Goebbels and his wife, Magda, committed suicide in the Reich Chancellery garden, after having poisoned their six children with cyanide.[331]
 Berlin surrendered on 2 May. The remains of the Goebbels family, General Hans Krebs (who had committed suicide that day), and Hitler's dog Blondi were repeatedly buried and exhumed by the Soviets.[332] Hitler's and Braun's remains were alleged to have been moved as well, but this is most likely Soviet disinformation. There is no evidence that any identifiable remains of Hitler or Braun—with the exception of dental bridges—were ever found by them.[333][334][335] While news of Hitler's death spread quickly, a death certificate was not issued until 1956, after a lengthy investigation to collect testimony from 42 witnesses. Hitler's death was entered as an assumption of death based on this testimony.[336]
 If the international Jewish financiers in and outside Europe should succeed in plunging the nations once more into a world war, then the result will not be the Bolshevisation of the earth, and thus the victory of Jewry, but the annihilation of the Jewish race in Europe![337] The Holocaust and Germany's war in the East were based on Hitler's long-standing view that the Jews were the enemy of the German people, and that Lebensraum was needed for Germany's expansion. He focused on Eastern Europe for this expansion, aiming to defeat Poland and the Soviet Union and then removing or killing the Jews and Slavs.[338] The Generalplan Ost (General Plan East) called for deporting the population of occupied Eastern Europe and the Soviet Union to West Siberia, for use as slave labour or to be murdered;[339] the conquered territories were to be colonised by German or ""Germanised"" settlers.[340] The goal was to implement this plan after the conquest of the Soviet Union, but when this failed, Hitler moved the plans forward.[339][341] By January 1942, he had decided that the Jews, Slavs, and other deportees considered undesirable should be killed.[342][h]
 The genocide was organised and executed by Heinrich Himmler and Reinhard Heydrich. The records of the Wannsee Conference, held on 20 January 1942 and led by Heydrich, with fifteen senior Nazi officials participating, provide the clearest evidence of systematic planning for the Holocaust. On 22 February, Hitler was recorded saying, ""we shall regain our health only by eliminating the Jews"".[343] Similarly, at a meeting in July 1941 with leading functionaries of the Eastern territories, Hitler said that the easiest way to quickly pacify the areas would be best achieved by ""shooting everyone who even looks odd"".[344] Although no direct order from Hitler authorising the mass killings has surfaced,[345] his public speeches, orders to his generals, and the diaries of Nazi officials demonstrate that he conceived and authorised the extermination of European Jewry.[346][347] During the war, Hitler repeatedly stated his prophecy of 1939 was being fulfilled, namely, that a world war would bring about the annihilation of the Jewish race.[348] Hitler approved the Einsatzgruppen—killing squads that followed the German army through Poland, the Baltic, and the Soviet Union[349]—and was well informed about their activities.[346][350] By summer 1942, Auschwitz concentration camp was expanded to accommodate large numbers of deportees for murder or enslavement.[351] Scores of other concentration camps and satellite camps were set up throughout Europe, with several camps devoted exclusively to extermination.[352]
 Between 1939 and 1945, the Schutzstaffel (SS), assisted by collaborationist governments and recruits from occupied countries, were responsible for the deaths of at least eleven million non-combatants,[353][339] including the murders of about 6 million Jews (representing two-thirds of the Jewish population of Europe),[354][i] and between 200,000 and 1,500,000 Romani people.[356][354] The victims were killed in concentration and extermination camps and in ghettos, and through mass shootings.[357][358] Many victims of the Holocaust were murdered in gas chambers or shot, while others died of starvation or disease or while working as slave labourers.[357][358] In addition to eliminating Jews, the Nazis planned to reduce the population of the conquered territories by 30 million people through starvation in an action called the Hunger Plan. Food supplies would be diverted to the German army and German civilians. Cities would be razed, and the land allowed to return to forest or resettled by German colonists.[359] Together, the Hunger Plan and Generalplan Ost would have led to the starvation of 80 million people in the Soviet Union.[360] These partially fulfilled plans resulted in additional deaths, bringing the total number of civilians and prisoners of war who died in the democide to an estimated 19.3 million people.[361]
 Hitler's policies resulted in the killing of nearly two million non-Jewish Polish civilians,[362] over three million Soviet prisoners of war,[363] communists and other political opponents, homosexuals, the physically and mentally disabled,[364][365] Jehovah's Witnesses, Adventists, and trade unionists. Hitler never spoke publicly about the killings and seems to have never visited the concentration camps.[366] The Nazis embraced the concept of racial hygiene. On 15 September 1935, Hitler presented two laws—known as the Nuremberg Laws—to the Reichstag. The laws banned sexual relations and marriages between Aryans and Jews and were later extended to include ""Gypsies, Negroes or their bastard offspring"".[367] The laws stripped all non-Aryans of their German citizenship and forbade the employment of non-Jewish women under the age of 45 in Jewish households.[368] Hitler's early eugenic policies targeted children with physical and developmental disabilities in a programme dubbed Action Brandt, and he later authorised a euthanasia programme for adults with serious mental and physical disabilities, now referred to as Aktion T4.[369]
 Hitler ruled the Nazi Party autocratically by asserting the Führerprinzip (leader principle). The principle relied on absolute obedience of all subordinates to their superiors; thus, he viewed the government structure as a pyramid, with himself—the infallible leader—at the apex. Rank in the party was not determined by elections—positions were filled through appointment by those of higher rank, who demanded unquestioning obedience to the will of the leader.[370] Hitler's leadership style was to give contradictory orders to his subordinates and to place them into positions where their duties and responsibilities overlapped with those of others, to have ""the stronger one [do] the job"".[371] In this way, Hitler fostered distrust, competition, and infighting among his subordinates to consolidate and maximise his own power. His cabinet never met after 1938, and he discouraged his ministers from meeting independently.[372][373] Hitler typically did not give written orders; instead, he communicated verbally, or had them conveyed through his close associate Martin Bormann.[374] He entrusted Bormann with his paperwork, appointments, and personal finances; Bormann used his position to control the flow of information and access to Hitler.[375]
 Hitler dominated his country's war effort during World War II to a greater extent than any other national leader. He strengthened his control of the armed forces in 1938, and subsequently made all major decisions regarding Germany's military strategy. His decision to mount a risky series of offensives against Norway, France, and the Low Countries in 1940 against the advice of the military proved successful, though the diplomatic and military strategies he employed in attempts to force the United Kingdom out of the war ended in failure.[376] Hitler deepened his involvement in the war effort by appointing himself commander-in-chief of the Army in December 1941; from this point forward, he personally directed the war against the Soviet Union, while his military commanders facing the Western Allies retained a degree of autonomy.[377] Hitler's leadership became increasingly disconnected from reality as the war turned against Germany, with the military's defensive strategies often hindered by his slow decision-making and frequent directives to hold untenable positions. Nevertheless, he continued to believe that only his leadership could deliver victory.[376] In the final months of the war, Hitler refused to consider peace negotiations, regarding the destruction of Germany as preferable to surrender.[378] The military did not challenge Hitler's dominance of the war effort, and senior officers generally supported and enacted his decisions.[379]
 Hitler created a public image as a celibate man without a domestic life, dedicated entirely to his political mission and the nation.[147][380] He met his lover, Eva Braun, in 1929,[381] and married her on 29 April 1945, one day before they both committed suicide.[382] In September 1931, his half-niece, Geli Raubal, took her own life with Hitler's gun in his Munich apartment. It was rumoured among contemporaries that Geli was in a romantic relationship with him, and her death was a source of deep, lasting pain.[383] Paula Hitler, the younger sister of Hitler and the last living member of his immediate family, died in June 1960.[15]
 Hitler was born to a practising Catholic mother and an anti-clerical father; after leaving home, Hitler never again attended Mass or received the sacraments.[384][385][386] Albert Speer states that Hitler railed against the church to his political associates, and though he never officially left the church, he had no attachment to it.[387] He adds that Hitler felt that in the absence of organised religion, people would turn to mysticism, which he considered regressive.[387] According to Speer, Hitler believed that Japanese religious beliefs or Islam would have been a more suitable religion for Germans than Christianity, with its ""meekness and flabbiness"".[388] Historian John S. Conway states that Hitler was fundamentally opposed to the Christian churches.[389] According to Bullock, Hitler did not believe in God, was anticlerical, and held Christian ethics in contempt because they contravened his preferred view of ""survival of the fittest"".[390] He favoured aspects of Protestantism that suited his own views, and adopted some elements of the Catholic Church's hierarchical organisation, liturgy, and phraseology.[391] In a 1932 speech, Hitler stated that he was not a Catholic, and declared himself a German Christian.[392] In a conversation with Albert Speer, Hitler said, ""Through me the Evangelical Church could become the established church, as in England.""[393]
 Hitler viewed the church as an important politically conservative influence on society,[394] and he adopted a strategic relationship with it that ""suited his immediate political purposes"".[389] In public, Hitler often praised Christian heritage and German Christian culture, though professing a belief in an ""Aryan Jesus"" who fought against the Jews.[395] Any pro-Christian public rhetoric contradicted his private statements, which described Christianity as ""absurdity""[396] and nonsense founded on lies.[397]
 According to a US Office of Strategic Services (OSS) report, ""The Nazi Master Plan"", Hitler planned to destroy the influence of Christian churches within the Reich.[398][399] His eventual goal was the total elimination of Christianity.[400] This goal informed Hitler's movement early on, but he saw it as inexpedient to publicly express this extreme position.[401] According to Bullock, Hitler wanted to wait until after the war before executing this plan.[402] Speer wrote that Hitler had a negative view of Himmler's and Alfred Rosenberg's mystical notions and Himmler's attempt to mythologise the SS. Hitler was more pragmatic, and his ambitions centred on more practical concerns.[403][404]
 Researchers have variously suggested that Hitler suffered from irritable bowel syndrome, skin lesions, irregular heartbeat, coronary sclerosis,[405] Parkinson's disease,[295][406] syphilis,[406] giant-cell arteritis,[407] tinnitus,[408] and monorchism.[409] In a report prepared for the OSS in 1943, Walter Charles Langer of Harvard University described Hitler as a ""neurotic psychopath"".[410] In his 1977 book The Psychopathic God: Adolf Hitler, historian Robert G. L. Waite proposes that Hitler suffered from borderline personality disorder.[411] Historians Henrik Eberle and Hans-Joachim Neumann consider that while he suffered from a number of illnesses including Parkinson's disease, Hitler did not experience pathological delusions and was always fully aware of, and therefore responsible for, his decisions.[412][314]
 Sometime in the 1930s, Hitler adopted a mainly vegetarian diet,[413][414] avoiding all meat and fish from 1942 onwards. At social events, he sometimes gave graphic accounts of the slaughter of animals in an effort to make his guests shun meat.[415] Bormann had a greenhouse constructed near the Berghof (near Berchtesgaden) to ensure a steady supply of fresh fruit and vegetables for Hitler.[416] Hitler stopped drinking alcohol around the time he became vegetarian and thereafter only very occasionally drank beer or wine on social occasions.[417][418] He was a non-smoker for most of his adult life, but smoked heavily in his youth (25 to 40 cigarettes a day); he eventually quit, calling the habit ""a waste of money"".[419] He encouraged his close associates to quit by offering a gold watch to anyone able to break the habit.[420] Hitler began using amphetamine occasionally after 1937 and became addicted to it in late 1942.[421] Speer linked this use of amphetamine to Hitler's increasingly erratic behaviour and inflexible decision-making (for example, rarely allowing military retreats).[422]
 Prescribed 90 medications during the war years by his personal physician, Theodor Morell, Hitler took many pills each day for chronic stomach problems and other ailments.[423] He regularly consumed amphetamine, barbiturates, opiates, and cocaine,[424][425] as well as potassium bromide and atropa belladonna (the latter in the form of Doktor Koster's Antigaspills).[426] He suffered ruptured eardrums as a result of the 20 July plot bomb blast in 1944, and 200 wood splinters had to be removed from his legs.[427] Newsreel footage of Hitler shows tremors in his left hand and a shuffling walk, which began before the war and worsened towards the end of his life.[423] Ernst-Günther Schenck and several other doctors who met Hitler in the last weeks of his life also formed a diagnosis of Parkinson's disease.[428]
 For peace, freedom
and democracy
never again fascism
millions of dead warn [us]
 According to historian Joachim Fest, Hitler's suicide was likened by numerous contemporaries to a ""spell"" being broken.[430] Similarly, Speer commented in Inside the Third Reich on his emotions the day after Hitler's suicide: ""Only now was the spell broken, the magic extinguished.""[431] Public support for Hitler had collapsed by the time of his death, which few Germans mourned; Kershaw argues that most civilians and military personnel were too busy adjusting to the collapse of the country or fleeing from the fighting to take any interest.[432] According to historian John Toland, Nazism ""burst like a bubble"" without its leader.[433]
 Kershaw describes Hitler as ""the embodiment of modern political evil"".[3] ""Never in history has such ruination—physical and moral—been associated with the name of one man"", he adds.[434] Hitler's political programme brought about a world war, leaving behind a devastated and impoverished Eastern and Central Europe. Germany suffered wholesale destruction, characterised as Stunde Null (Zero Hour).[435] Hitler's policies inflicted human suffering on an unprecedented scale;[436] according to R. J. Rummel, the Nazi regime was responsible for the democidal killing of an estimated 19.3 million civilians and prisoners of war.[353] In addition, 28.7 million soldiers and civilians died as a result of military action in the European theatre of World War II.[353] The number of civilians killed during the Second World War was unprecedented in the history of warfare.[437] Historians, philosophers, and politicians often use the word ""evil"" to describe the Nazi regime.[438] Many European countries have criminalised both the promotion of Nazism and Holocaust denial.[439]
 Historian Friedrich Meinecke described Hitler as ""one of the great examples of the singular and incalculable power of personality in historical life"".[440] English historian Hugh Trevor-Roper saw him as ""among the 'terrible simplifiers' of history, the most systematic, the most historical, the most philosophical, and yet the coarsest, cruelest, least magnanimous conqueror the world has ever known"".[441] For the historian John M. Roberts, Hitler's defeat marked the end of a phase of European history dominated by Germany.[442] In its place emerged the Cold War, a global confrontation between the Western Bloc, dominated by the United States and other NATO nations, and the Eastern Bloc, dominated by the Soviet Union.[443] Historian Sebastian Haffner asserted that without Hitler and the displacement of the Jews, the modern nation state of Israel would not exist. He contends that without Hitler, the de-colonisation of former European spheres of influence would have been postponed.[444] Further, Haffner claimed that other than Alexander the Great, Hitler had a more significant impact than any other comparable historical figure, in that he too caused a wide range of worldwide changes in a relatively short time span.[445]
 Hitler exploited documentary films and newsreels to inspire a cult of personality. He was involved and appeared in a series of propaganda films throughout his political career, many made by Leni Riefenstahl, regarded as a pioneer of modern filmmaking.[446] Hitler's propaganda film appearances include:
 Final solution
 Parties
",adolf hitler april april german politician dictat nazi germani suicid rose power leader nazi parti c becom chancellor take titl führer und reichskanzl invas poland septemb mark start second world war close involv militari oper throughout war central perpetr holocaust genocid six million jew million victim hitler born braunau inn rais near linz live vienna first decad move germani decor servic german armi world war receiv iron cross join german worker parti dap precursor nazi parti appoint leader nazi parti attempt seiz government power fail coup munich sentenc five year prison serv year sentenc dictat first volum autobiographi polit manifesto mein kampf struggl earli releas hitler gain popular support attack treati versail promot antisemit charismat oratori nazi propaganda frequent denounc commun part intern jewish conspiraci novemb nazi parti held seat reichstag major polit parti abl form major coalit support candid chancellor former chancellor franz von papen conserv leader convinc presid paul von hindenburg appoint hitler chancellor januari shortli thereaft reichstag pass enabl act began process transform weimar republ nazi germani dictatorship base totalitarian autocrat ideolog nazism upon hindenburg death august hitler succeed becom simultan head state govern absolut power domest hitler implement numer racist polici sought deport kill german jew first six year power result rapid econom recoveri great depress abrog restrict impos germani world war annex territori inhabit million ethnic german initi gave signific popular support one hitler key goal lebensraum lit space german peopl eastern europ aggress expansionist foreign polici consid primari caus world war ii europ direct rearmament septemb invad poland caus britain franc declar war germani june hitler order invas soviet union decemb declar war unit state end german forc european axi power occupi europ north africa gain gradual revers alli armi defeat german armi april marri longtim partner eva braun führerbunk berlin coupl commit suicid next day avoid captur soviet red armi accord hitler wish corps burn historian biograph ian kershaw describ hitler embodi modern polit evil hitler leadership racist ideolog nazi regim respons genocid estim six million jew million victim follow deem untermenschen lit social undesir hitler nazi regim also respons deliber kill estim million civilian prison war addit million soldier civilian die result militari action european theatr number civilian kill world war ii unpreced warfar casualti constitut deadliest conflict histori hitler father aloi hitler illegitim child maria schicklgrub baptism regist show name father aloi initi bore mother surnam schicklgrub johann georg hiedler marri aloi mother aloi brought famili hiedler brother johann nepomuk hiedler aloi made legitim baptism record annot priest regist johann georg hiedler aloi father record georg hitler aloi assum surnam hitler also spell hiedler hüttler huettler name probabl base german word hütte lit mean one live hut nazi offici han frank suggest aloi mother employ housekeep jewish famili graz famili son leopold frankenberg father aloi claim came known frankenberg thesi frankenberg regist graz period record produc leopold frankenberg exist historian dismiss claim aloi father jewish adolf hitler born april braunau inn town austria close border german empir fourth six children born aloi hitler third wife klara pölzl three hitler ida infanc also live household aloi children second marriag aloi born angela born hitler three famili move passau germani acquir distinct lower bavarian dialect rather austrian german mark speech throughout life famili return austria settl leond june aloi retir hafeld near lambach farm kept bee hitler attend volksschul primari school nearbi fischlham move hafeld coincid onset intens conflict caus hitler refus conform strict disciplin school aloi tri browbeat son obedi adolf best opposit whatev father want aloi would also beat son although mother tri protect regular beat aloi hitler farm effort hafeld end failur famili move lambach hitler took sing lesson sang church choir even consid becom priest famili return perman leond hitler deepli affect death younger brother edmund measl hitler chang confid outgo conscienti student moros detach boy constantli fought father teacher paula hitler recal adolf teenag bulli would often slap aloi made success career custom bureau want son follow footstep hitler later dramatis episod period father took visit custom offic depict event gave rise unforgiv antagon father son ignor son desir attend classic high school becom artist aloi sent hitler realschul linz septemb e hitler rebel decis mein kampf state intent perform poorli school hope father saw littl progress make technic school would let devot dream like mani austrian german hitler began develop german nationalist idea young age express loyalti germani despis declin habsburg monarchi rule ethnic divers empir hitler friend use greet heil sang deutschlandli instead austrian imperi anthem aloi sudden death januari hitler perform school deterior mother allow leav enrol realschul steyr septemb behaviour perform improv pass repeat final exam hitler left school without ambit educ clear plan career hitler left linz live studi fine art vienna financ orphan benefit support mother appli admiss academi fine art vienna reject twice director suggest hitler appli school architectur lack necessari academ credenti finish secondari school decemb mother die breast cancer age hitler time hitler ran money forc live bohemian life homeless shelter meldemannstraß dormitori earn money casual labour paint sell watercolour vienna sight time vienna pursu grow passion architectur music attend ten perform lohengrin favourit wagner opera vienna hitler first expos racist rhetor populist mayor karl lueger exploit citi preval sentiment occasion also espous german nationalist notion polit benefit german nation even widespread mariahilf district hitler live georg ritter von schönerer becam major influenc hitler develop admir martin luther hitler read local newspap promot prejudic utilis christian fear swamp influx eastern european jew well pamphlet publish thought philosoph theoretician houston stewart chamberlain charl darwin friedrich nietzsch gustav le bon arthur schopenhau life vienna hitler also develop fervent sentiment origin develop hitler remain matter debat friend august kubizek claim hitler confirm left linz howev historian brigitt hamann describ kubizek claim problemat hitler state mein kampf first becam vienna reinhold hanisch help sell paint disagre hitler deal jew live vienna historian richard evan state historian gener agre notori murder emerg well germani defeat world war product paranoid explan catastroph hitler receiv final part father estat may move munich germani conscript armi journey salzburg februari medic assess deem unfit servic return munich hitler later claim wish serv habsburg empir mixtur race armi belief collaps immin august outbreak world war hitler live munich voluntarili enlist bavarian armi accord report bavarian author allow hitler serv like administr error austrian citizen return austria post bavarian reserv infantri regiment compani list regiment serv dispatch runner western front franc belgium spend nearli half time regiment headquart well behind front line present first battl ypre year decor braveri receiv iron cross second class servic headquart hitler pursu artwork draw cartoon instruct armi newspap battl somm octob wound left thigh shell explod dispatch runner dugout hitler spent almost two month recov hospit beelitz return regiment march present battl arra battl passchendael receiv black wound badg may three month later august recommend lieuten hugo gutmann jewish superior hitler receiv iron cross first class decor rare award hitler gefreit rank octob temporarili blind mustard ga attack hospitalis pasewalk hitler learn germani defeat account suffer second bout blind receiv news hitler describ role world war greatest experi prais command offic braveri wartim experi reinforc german patriot shock germani capitul novemb displeasur collaps war effort began shape ideolog like german nationalist believ dolchstoßlegend myth claim german armi undef field stab back home front civilian leader jew marxist sign armistic end dub novemb crimin treati versail stipul germani relinquish sever territori demilitaris rhineland treati impos econom sanction levi heavi repar countri mani german saw treati unjust humili especi object articl interpret declar germani respons war versail treati econom social polit condit germani war later exploit hitler polit gain world war hitler return munich without formal educ career prospect remain armi juli appoint verbindungsmann intellig agent aufklärungskommando reconnaiss unit reichswehr assign influenc soldier infiltr german worker parti dap dap meet septemb parti chairman anton drexler impress hitler orator skill gave copi pamphlet polit awaken contain nationalist idea order armi superior hitler appli join parti within week accept parti member parti began count membership give impress much larger parti hitler made earliest known written statement jewish question septemb letter adolf gemlich known gemlich letter letter hitler argu aim govern must unshak remov jew altogeth dap hitler met dietrich eckart one parti founder member occult thule societi eckart becam hitler mentor exchang idea introduc wide rang munich societi increas appeal dap chang name nationalsozialistisch deutsch arbeiterpartei nation socialist german worker parti nsdap known nazi parti hitler design parti banner swastika white circl red background hitler discharg armi march began work parti parti headquart munich centr german nationalist determin elimin marxism undermin weimar republ februari highli effect crowd spoke crowd publicis meet two truckload parti support drove around munich wave swastika flag distribut leaflet hitler soon gain notorieti rowdi polem speech treati versail rival politician especi marxist jew june hitler eckart fundrais trip berlin mutini broke within nazi parti munich member execut committe want merg german socialist parti dsp hitler return munich juli angrili tender resign committe member realis resign lead public figur speaker would mean end parti hitler announc would rejoin condit would replac drexler parti chairman parti headquart would remain munich committe agre rejoin parti juli member hitler continu face opposit within nazi parti oppon hitler leadership hermann esser expel parti print copi pamphlet attack hitler traitor parti f follow day hitler spoke sever larg audienc defend esser thunder applaus strategi prove success special parti congress juli grant absolut power parti chairman succeed drexler vote hitler vitriol beer hall speech began attract regular audienc demagogu becam adept use populist theme includ use scapegoat blame listen econom hardship hitler use person magnet understand crowd psycholog advantag engag public speak historian note hypnot effect rhetor larg audienc eye small group alfon heck former member hitler youth recal erupt frenzi nationalist pride border hysteria minut end shout top lung tear stream face sieg heil sieg heil sieg heil moment belong adolf hitler bodi soul earli follow includ rudolf hess former air forc ace hermann göring armi captain ernst röhm röhm becam head nazi paramilitari organis sturmabteilung sa stormtroop protect meet attack polit oppon critic influenc hitler think period aufbau vereinigung conspiratori group white russian exil earli nazi group financ fund channel wealthi industrialist introduc hitler idea jewish conspiraci link intern financ bolshev programm nazi parti laid programm februari repres coher ideolog conglomer receiv idea currenc völkisch movement ultranation opposit treati versail distrust capit well socialist idea hitler import aspect strong stanc also perceiv programm primarili basi propaganda attract peopl parti hitler enlist help world war gener erich ludendorff attempt coup known beer hall putsch nazi parti use italian fascism model appear polici hitler want emul benito mussolini march rome stage coup bavaria follow challeng govern berlin hitler ludendorff sought support staatskommissar state commission gustav ritter von kahr bavaria de facto ruler howev kahr along polic chief han ritter von seisser reichswehr gener otto von lossow want instal nationalist dictatorship without hitler novemb hitler sa storm public meet peopl organis kahr bürgerbräukel beer hall munich interrupt kahr speech announc nation revolut begun declar format new govern ludendorff retir back room hitler pistol drawn demand subsequ receiv support kahr seisser lossow hitler forc initi succeed occupi local reichswehr polic headquart kahr cohort quickli withdrew support neither armi state polic join forc hitler next day hitler follow march beer hall bavarian war ministri overthrow bavarian govern polic dispers sixteen nazi parti member four polic offic kill fail coup hitler fled home ernst hanfstaengl account contempl suicid depress calm arrest novemb high treason trial special peopl court munich began februari alfr rosenberg becam temporari leader nazi parti april hitler sentenc five year imprison landsberg prison receiv friendli treatment guard allow mail support regular visit parti comrad pardon bavarian suprem court releas jail decemb state prosecutor object includ time remand hitler serv one year prison landsberg hitler dictat first volum mein kampf lit struggl origin titl four half year struggl lie stupid cowardic first chauffeur emil mauric deputi rudolf hess book dedic thule societi member dietrich eckart autobiographi exposit ideolog book laid hitler plan transform german societi one base race throughout book jew equat germ present intern poison societi accord hitler ideolog solut extermin hitler describ exactli accomplish inher genocid thrust undeni accord ian kershaw publish two volum mein kampf sold copi one million copi sold hitler first year offic shortli hitler elig parol bavarian govern attempt deport austria austrian feder chancellor reject request speciou ground servic german armi made austrian citizenship void respons hitler formal renounc austrian citizenship april time hitler releas prison polit germani becom less comb economi improv limit hitler opportun polit agit result fail beer hall putsch nazi parti affili organis ban bavaria meet prime minist bavaria heinrich held januari hitler agre respect state author promis would seek polit power democrat process meet pave way ban nazi parti lift februari howev inflammatori speech gave februari hitler bar public speak bavarian author ban remain place advanc polit ambit spite ban hitler appoint gregor strasser otto strasser joseph goebbel organis enlarg nazi parti northern germani gregor strasser steer independ polit cours emphasis socialist element parti programm stock market unit state crash octob impact germani dire million becam unemploy sever major bank collaps hitler nazi parti prepar take advantag emerg gain support parti promis repudi versail treati strengthen economi provid job great depress provid polit opportun hitler german ambival parliamentari republ face challeng extremist moder polit parti increasingli unabl stem tide extrem german referendum help elev nazi ideolog elect septemb result grand coalit replac minor cabinet leader chancellor heinrich brüning centr parti govern emerg decre presid paul von hindenburg govern decre becam new norm pave way authoritarian form govern nazi parti rose obscur win per cent vote parliamentari seat elect becom parti parliament hitler made promin appear trial two reichswehr offic lieuten richard schering hann ludin late charg membership nazi parti time illeg reichswehr personnel prosecut argu nazi parti extremist parti prompt defenc lawyer han frank call hitler testifi septemb hitler testifi parti would pursu polit power sole democrat elect mani support offic corp brüning auster measur brought littl econom improv extrem unpopular hitler exploit target polit messag specif peopl affect inflat depress farmer war veteran middl class although hitler termin austrian citizenship acquir german citizenship almost seven year meant stateless legal unabl run public offic still face risk deport februari interior minist brunswick dietrich klagg member nazi parti appoint hitler administr state deleg reichsrat berlin make hitler citizen brunswick thu germani hitler ran hindenburg presidenti elect speech industri club düsseldorf januari support mani germani power industrialist hindenburg support variou nationalist monarchist cathol republican parti social democrat hitler use campaign slogan hitler über deutschland hitler germani refer polit ambit campaign aircraft one first politician use aircraft travel campaign use effect hitler came second round elect garner per cent vote final elect although lost hindenburg elect establish hitler strong forc german polit absenc effect govern prompt two influenti politician franz von papen alfr hugenberg along sever industrialist businessmen write letter hindenburg signer urg hindenburg appoint hitler leader govern independ parliamentari parti could turn movement would enraptur million peopl hindenburg reluctantli agre appoint hitler chancellor two parliamentari juli novemb result format major govern hitler head coalit govern form nazi parti seat reichstag hugenberg parti german nation peopl parti dnvp januari new cabinet sworn brief ceremoni hindenburg offic nazi parti gain three post hitler name chancellor wilhelm frick minist interior hermann göring minist interior prussia hitler insist ministeri posit way gain control polic much germani chancellor hitler work attempt nazi parti oppon build major govern polit stalem ask hindenburg dissolv reichstag elect schedul earli march februari reichstag build set fire göring blame communist plot dutch communist marinu van der lubb found incrimin circumst insid burn build historian includ william shirer alan bullock thought nazi parti respons accord ian kershaw write view nearli modern historian van der lubb set fire alon need updat hitler urg hindenburg respond sign reichstag fire decre februari draft nazi suspend basic right allow detent without trial decre permit articl weimar constitut gave presid power take emerg measur protect public safeti order activ german communist parti kpd suppress kpd member arrest addit polit campaign nazi parti engag paramilitari violenc spread propaganda day preced elect elect day march nazi parti share vote increas per cent parti acquir largest number seat parliament hitler parti fail secur absolut major necessit anoth coalit dnvp march new reichstag constitut open ceremoni garrison church potsdam day potsdam held demonstr uniti nazi movement old prussian elit militari hitler appear morn coat humbl greet hindenburg achiev full polit control despit absolut major parliament hitler govern brought ermächtigungsgesetz enabl act vote newli elect reichstag titl gesetz zur behebung der von volk und reich law remedi distress peopl reich hitler cabinet power enact law without consent reichstag four year law could certain except deviat constitut sinc would affect constitut enabl act requir major pass leav noth chanc nazi use provis reichstag fire decre arrest communist deputi spite virul campaign parti nazi allow kpd contest elect prevent sever social democrat attend march reichstag assembl kroll opera hous turbul circumst rank sa men serv guard insid build larg group outsid oppos propos legisl shout slogan threat toward arriv member parliament hitler verbal promis centr parti leader ludwig kaa hindenburg would retain power veto kaa announc centr parti would support enabl act act pass vote parti except social democrat vote favour enabl act along reichstag fire decre transform hitler govern de facto legal dictatorship risk appear talk nonsens tell nation socialist movement go year forget peopl laugh year ago declar one day would govern germani laugh foolishli declar shall remain power achiev full control legisl execut branch govern hitler alli began suppress remain opposit social democrat parti made illeg asset seiz mani trade union deleg berlin may day activ sa stormtroop occupi union offic around countri may trade union forc dissolv leader arrest sent concentr camp german labour front form umbrella organis repres worker administr compani owner thu reflect concept nazism spirit hitler volksgemeinschaft peopl commun end june parti intimid disband includ nazi nomin coalit partner dnvp sa help hitler forc leader hugenberg resign june juli nazi parti declar legal polit parti germani demand sa polit militari power caus anxieti among militari industri polit leader respons hitler purg entir sa leadership night long knive took place june juli hitler target ernst röhm sa leader along number hitler polit adversari gregor strasser former chancellor kurt von schleicher round arrest shot intern commun german shock kill mani germani believ hitler restor order hindenburg die august previou day cabinet enact law concern head state german reich law state upon hindenburg death offic presid would abolish power merg chancellor hitler thu becam head state well head govern formal name führer und reichskanzl leader chancellor reich although reichskanzl eventu drop action hitler elimin last legal remedi could remov offic head state hitler becam arm forc immedi hindenburg death instig leadership reichswehr tradit loyalti oath soldier alter affirm loyalti hitler person name rather offic later renam suprem command state august merger presid chancellorship approv per cent elector vote plebiscit earli hitler use blackmail consolid hold militari instig affair hitler forc war minist field marshal werner von blomberg resign use polic dossier show blomberg new wife record prostitut armi command werner von fritsch remov schutzstaffel ss produc alleg engag homosexu relationship men fallen disfavour object hitler demand make wehrmacht readi war earli hitler assum blomberg titl thu take person command arm forc replac ministri war oberkommando der wehrmacht okw head gener wilhelm keitel day sixteen gener strip command transfer suspect suffici earli februari twelv gener remov hitler took care give dictatorship appear legal mani decre explicitli base reichstag fire decre henc articl weimar constitut reichstag renew enabl act twice time period elect reichstag still held voter present singl list nazi guest receiv well per cent vote sham elect held condit nazi threaten sever repris anyon vote vote august hitler appoint reichsbank presid hjalmar schacht minist econom follow year plenipotentiari war economi charg prepar economi war reconstruct rearmament financ mefo bill print money seiz asset peopl arrest enemi state includ jew number unemploy fell six million fewer one million hitler oversaw one largest infrastructur improv campaign german histori lead construct dam autobahn railroad civil work wage slightli lower mid late compar wage weimar republ cost live increas per cent averag work week increas shift war economi averag german work hour week hitler govern sponsor architectur immens scale albert speer instrument implement hitler classicist reinterpret german cultur place charg propos architectur renov berlin despit threaten boycott germani host olymp game hitler offici open ceremoni attend event winter game summer game berlin meet german militari leader februari hitler spoke conquest lebensraum east ruthless germanis ultim foreign polici object march princ bernhard wilhelm von bülow secretari foreign offic auswärtig amt issu statement major foreign polici aim anschluss austria restor germani nation border reject militari restrict treati versail return former german coloni africa german zone influenc eastern europ hitler found bülow goal modest speech period stress term peac goal polici willing work within intern agreement first meet cabinet hitler prioritis militari spend unemploy relief germani withdrew leagu nation world disarma confer octob januari per cent peopl saarland leagu nation administr vote unit germani march hitler announc expans wehrmacht time number permit versail treati includ develop air forc luftwaff increas size navi kriegsmarin britain franc itali leagu nation condemn violat treati noth stop naval agreement agna june allow german tonnag increas per cent british navi hitler call sign agna happiest day life believ agreement mark begin allianc predict mein kampf franc itali consult sign directli undermin leagu nation set treati versail path toward irrelev germani reoccupi demilitaris zone rhineland march violat versail treati hitler also sent troop spain support francisco franco spanish civil war receiv appeal help juli time hitler continu effort creat allianc august respons grow econom crisi caus rearmament effort hitler order göring implement four year plan prepar germani war within next four year plan envisag struggl german nazism hitler view requir commit effort rearmament regardless econom cost octob count galeazzo ciano foreign minist mussolini govern visit germani sign protocol express rapproch person meet hitler novemb mussolini declar axi germani itali novemb germani sign pact japan britain china itali poland also invit join pact itali sign hitler abandon plan allianc blame inadequ british leadership meet reich chancelleri foreign minist militari chief novemb hitler restat intent acquir lebensraum german peopl order prepar war east begin earli later event death confer minut record hossbach memorandum regard polit testament felt sever declin live standard germani result econom crisi could stop militari aggress aim seiz austria czechoslovakia hitler urg quick action britain franc gain perman lead arm race earli wake affair hitler assert control polici apparatu dismiss neurath foreign minist appoint war minist earli onward hitler carri foreign polici ultim aim war februari advic newli appoint foreign minist strongli joachim von ribbentrop hitler end allianc republ china instead enter allianc modern power empir japan hitler announc german recognit manchukuo japanes puppet state manchuria renounc german claim former coloni pacif held japan hitler order end arm shipment china recal german offic work chines armi retali chines gener chiang cancel econom agreement depriv german mani chines raw materi march hitler announc unif austria nazi germani anschluss hitler turn attent ethnic german popul sudetenland region czechoslovakia march hitler held seri secret meet berlin konrad henlein sudeten german parti largest ethnic german parti sudetenland men agre henlein would demand increas autonomi sudeten german czechoslovakian govern thu provid pretext german militari action czechoslovakia april henlein told foreign minist hungari whatev czech govern might offer would alway rais still higher demand want sabotag understand mean method blow czechoslovakia quickli privat hitler consid sudeten issu unimport real intent war conquest czechoslovakia april hitler order okw prepar fall grün case green code name invas czechoslovakia result intens french british diplomat pressur septemb czechoslovakian presid edvard beneš unveil fourth plan constitut reorganis countri agre henlein demand sudeten autonomi henlein parti respond beneš offer instig seri violent clash czechoslovakian polic led declar martial law certain sudeten district germani depend import oil confront britain czechoslovakian disput could curtail germani oil suppli forc hitler call fall grün origin plan octob septemb hitler nevil chamberlain édouard daladi mussolini attend confer munich led munich agreement hand sudetenland district germani chamberlain satisfi munich confer call outcom peac time hitler anger miss opportun war express disappoint speech octob saarbrücken hitler view peac although favour ostens german demand diplomat defeat spur intent limit british power pave way eastern expans germani result summit hitler select time magazin man year late earli continu econom crisi caus rearmament forc hitler make major defenc cut export die speech januari call econom offens increas german foreign exchang hold pay raw materi iron need militari weapon march threat hungari slovakia declar independ receiv protect germani next day violat munich agreement possibl result deepen econom crisi requir addit asset hitler order wehrmacht invad czech rump state pragu castl proclaim territori german protector privat discuss hitler declar britain main enemi defeat poland obliter necessari prelud goal eastern flank would secur land would ad germani lebensraum offend british guarante march polish independ said shall brew devil drink speech wilhelmshaven launch battleship tirpitz april threaten denounc naval agreement british continu guarante polish independ perceiv encircl polici poland either becom german satellit state would neutralis order secur reich eastern flank prevent possibl british blockad hitler initi favour idea satellit state upon reject polish govern decid invad made main foreign polici goal april hitler order militari prepar fall weiss case white plan invad poland august reichstag speech april renounc naval agreement pact historian william carr gerhard weinberg ian kershaw argu one reason hitler rush war fear earli death repeatedli claim must lead germani war got old successor might lack strength hitler concern militari attack poland could result prematur war britain hitler foreign minist former ambassador london joachim von ribbentrop assur neither britain franc would honour commit poland accordingli august hitler order militari mobilis poland plan requir tacit soviet support pact pact germani soviet union led joseph stalin includ secret agreement partit poland two countri contrari ribbentrop predict britain would sever tie britain poland sign allianc august along news itali mussolini would honour pact steel prompt hitler postpon attack poland august septemb hitler unsuccess tri manoeuvr british neutral offer guarante august instruct ribbentrop present peac plan imposs short time limit effort blame immin war british polish inact septemb germani invad western poland pretext deni claim free citi danzig right extraterritori road across polish corridor germani cede versail treati respons britain franc declar war germani septemb surpris hitler prompt angrili ask ribbentrop franc britain act declar immedi septemb soviet forc invad eastern poland fall poland follow contemporari journalist dub phoney war sitzkrieg sit war hitler instruct two newli appoint gauleit poland albert forster reichsgau prussia arthur greiser reichsgau wartheland germanis area question ask accomplish forster area ethnic pole mere sign form state german blood contrast greiser agre himmler carri ethnic cleans campaign toward pole greiser soon complain forster allow thousand pole accept racial german thu endang german racial puriti hitler refrain get involv inact advanc exampl theori work toward führer hitler issu vagu instruct expect subordin work polici anoth disput pitch one side repres heinrich himmler greiser champion ethnic cleans poland anoth repres göring han frank occupi poland call turn poland granari reich februari disput initi settl favour view end econom disrupt mass expuls may himmler issu memo entitl thought treatment alien popul east call expuls entir jewish popul europ africa reduct polish popul leaderless class labour hitler call himmler memo good correct ignor göring frank implement polici poland april german forc invad denmark norway day hitler proclaim birth greater german reich vision unit empir german nation europ dutch flemish scandinavian join racial pure politi german leadership may germani attack franc conquer luxembourg netherland belgium victori prompt mussolini itali join forc hitler june franc germani sign armistic june kershaw note hitler popular within german support peak return berlin juli tour pari follow unexpect swift victori hitler promot twelv gener rank field marshal field marshal ceremoni britain whose troop forc evacu franc sea dunkirk continu fight alongsid british dominion battl atlant hitler made peac overtur new british leader winston churchil upon reject order seri aerial attack royal air forc airbas radar station southeast england septemb systemat nightli bomb london began german luftwaff fail defeat royal air forc becam known battl britain end septemb hitler realis air superior invas britain oper sea lion could achiev order oper postpon nightli air raid british citi intensifi continu month includ london plymouth coventri septemb tripartit pact sign berlin saburō kurusu imperi japan hitler italian foreign minist ciano later expand includ hungari romania bulgaria thu yield axi power hitler attempt integr soviet union bloc fail inconclus talk hitler molotov berlin novemb order prepar invas soviet union earli german forc deploy north africa balkan middl east februari german forc arriv libya bolster italian presenc april hitler launch invas yugoslavia quickli follow invas greec may german forc sent support iraqi forc fight british invad crete june contraven pact three million axi troop attack soviet union offens codenam oper barbarossa intend destroy soviet union seiz natur resourc subsequ aggress western power action also part overal plan obtain live space german peopl hitler thought success invas would forc britain negoti surrend invas conquer huge area includ baltic republ belaru west ukrain earli august axi troop advanc km mile battl smolensk hitler order armi group centr temporarili halt advanc moscow divert panzer group aid encircl leningrad kiev gener disagre chang advanc within km mile moscow decis caus crisi among militari leadership paus provid red armi opportun mobilis fresh reserv historian russel stolfi consid one major factor caus failur moscow offens resum octob end disastr decemb crisi hitler appoint head oberkommando de heer decemb japan attack american fleet base pearl harbor hawaii four day later hitler declar war unit state decemb himmler ask hitler jew russia hitler repli al partisanen auszurotten extermin partisan isra historian yehuda bauer comment remark probabl close historian ever get definit order hitler genocid carri holocaust late german forc defeat second battl el alamein thwart hitler plan seiz suez canal middl east overconfid militari expertis follow earlier victori hitler becam distrust armi high command began interfer militari tactic plan damag consequ decemb januari hitler repeat refus allow withdraw battl stalingrad led almost total destruct armi axi soldier kill taken prison thereaft came decis strateg defeat battl kursk hitler militari judgement becam increasingli errat germani militari econom posit deterior hitler health follow alli invas sicili mussolini remov power king victor emmanuel iii vote confid grand council fascism marshal pietro badoglio place charg govern soon surrend alli throughout soviet union steadili forc hitler armi retreat along eastern front june western alli armi land northern franc one largest amphibi oper histori oper overlord mani german offic conclud defeat inevit continu hitler leadership would result complet destruct countri numer plan assassin hitler proceed signific degre signific juli plot came within germani least partli driven increas prospect german defeat war part oper valkyri plot involv clau von stauffenberg plant bomb one hitler headquart wolf lair rastenburg hitler narrowli surviv staff offic heinz brandt move briefcas contain bomb behind leg heavi confer tabl deflect much blast later hitler order savag repris result execut peopl hitler put unit nation war crime commiss first list war crimin decemb determin hitler could held crimin respons act nazi occupi countri march least seven indict file late red armi western alli advanc germani recognis strength determin red armi hitler decid use remain mobil reserv american british armi perceiv far weaker decemb launch ardenn offens incit disun among western alli perhap convinc join fight soviet temporari success offens fail much germani ruin januari hitler spoke radio howev grave crisi may moment despit everyth master unalter act view germani militari failur meant forfeit right surviv nation hitler order destruct german industri infrastructur could fall alli hand minist armament albert speer entrust execut scorch earth polici secretli disobey order hitler hope negoti peac unit state britain encourag death us presid franklin roosevelt april contrari expect caus rift among alli april final birthday hitler made last trip führerbunk surfac ruin garden reich chancelleri award iron cross boy soldier hitler youth fight red armi front near berlin april georgi zhukov belorussian front broken defenc gener gotthard heinrici armi group vistula battl seelow height advanc outskirt berlin denial dire situat hitler place hope underman armeeabteilung steiner armi detach steiner command felix steiner hitler order steiner attack northern flank salient german ninth armi order attack northward pincer attack militari confer april hitler inquir steiner offens inform attack launch soviet enter berlin hitler order everyon wilhelm keitel alfr jodl han kreb wilhelm burgdorf leav room launch tirad perceiv treacheri incompet gener culmin first everyth lost announc would stay berlin end shoot april red armi surround berlin goebbel made proclam urg citizen defend citi day göring sent telegram berchtesgaden argu hitler isol berlin göring assum leadership germani göring set deadlin would consid hitler incapacit hitler respond göring arrest last testament april remov göring govern posit april hitler discov himmler left berlin april attempt negoti surrend western alli consid treason order himmler arrest also order execut hermann fegelein himmler ss repres hitler headquart berlin desert midnight night april hitler marri eva braun small civil ceremoni führerbunk g later afternoon hitler inform mussolini execut italian resist movement previou day believ increas determin avoid captur april soviet troop within five hundr metr reich chancelleri hitler shot head braun bit cyanid capsul accord hitler wish corps carri outsid garden behind reich chancelleri place bomb crater dous petrol set fire red armi shell continu grand admir karl dönitz goebbel assum hitler role head state chancellor respect even may goebbel wife magda commit suicid reich chancelleri garden poison six children cyanid berlin surrend may remain goebbel famili gener han kreb commit suicid day hitler dog blondi repeatedli buri exhum soviet hitler braun remain alleg move well like soviet disinform evid identifi remain hitler except dental ever found news hitler death spread quickli death certif issu lengthi investig collect testimoni wit hitler death enter assumpt death base testimoni intern jewish financi outsid europ succeed plung nation world war result bolshevis earth thu victori jewri annihil jewish race europ holocaust germani war east base hitler view jew enemi german peopl lebensraum need germani expans focus eastern europ expans aim defeat poland soviet union remov kill jew slav generalplan ost gener plan east call deport popul occupi eastern europ soviet union west siberia use slave labour murder conquer territori colonis german germanis settler goal implement plan conquest soviet union fail hitler move plan forward januari decid jew slav deporte consid undesir kill h genocid organis execut heinrich himmler reinhard heydrich record wannse confer held januari led heydrich fifteen senior nazi offici particip provid clearest evid systemat plan holocaust februari hitler record say shall regain health elimin jew similarli meet juli lead functionari eastern territori hitler said easiest way quickli pacifi area would best achiev shoot everyon even look odd although direct order hitler authoris mass kill surfac public speech order gener diari nazi offici demonstr conceiv authoris extermin european jewri war hitler repeatedli state propheci fulfil name world war would bring annihil jewish race hitler approv squad follow german armi poland baltic soviet union well inform activ summer auschwitz concentr camp expand accommod larg number deporte murder enslav score concentr camp satellit camp set throughout europ sever camp devot exclus extermin schutzstaffel ss assist collaborationist govern recruit occupi countri respons death least eleven million includ murder million jew repres jewish popul europ romani peopl victim kill concentr extermin camp ghetto mass shoot mani victim holocaust murder ga chamber shot other die starvat diseas work slave labour addit elimin jew nazi plan reduc popul conquer territori million peopl starvat action call hunger plan food suppli would divert german armi german civilian citi would raze land allow return forest resettl german colonist togeth hunger plan generalplan ost would led starvat million peopl soviet union partial fulfil plan result addit death bring total number civilian prison war die democid estim million peopl hitler polici result kill nearli two million polish civilian three million soviet prison war communist polit oppon homosexu physic mental disabl jehovah wit adventist trade unionist hitler never spoke publicli kill seem never visit concentr camp nazi embrac concept racial hygien septemb hitler present two nuremberg reichstag law ban sexual relat marriag aryan jew later extend includ gypsi negro bastard offspr law strip german citizenship forbad employ women age jewish household hitler earli eugen polici target children physic development disabl programm dub action brandt later authoris euthanasia programm adult seriou mental physic disabl refer aktion hitler rule nazi parti autocrat assert führerprinzip leader principl principl reli absolut obedi subordin superior thu view govern structur pyramid infal apex rank parti determin fill appoint higher rank demand unquest obedi leader hitler leadership style give contradictori order subordin place posit duti respons overlap other stronger one job way hitler foster distrust competit infight among subordin consolid maximis power cabinet never met discourag minist meet independ hitler typic give written order instead commun verbal convey close associ martin bormann entrust bormann paperwork appoint person financ bormann use posit control flow inform access hitler hitler domin countri war effort world war ii greater extent nation leader strengthen control arm forc subsequ made major decis regard germani militari strategi decis mount riski seri offens norway franc low countri advic militari prove success though diplomat militari strategi employ attempt forc unit kingdom war end failur hitler deepen involv war effort appoint armi decemb point forward person direct war soviet union militari command face western alli retain degre autonomi hitler leadership becam increasingli disconnect realiti war turn germani militari defens strategi often hinder slow frequent direct hold unten posit nevertheless continu believ leadership could deliv victori final month war hitler refus consid peac negoti regard destruct germani prefer surrend militari challeng hitler domin war effort senior offic gener support enact decis hitler creat public imag celib man without domest life dedic entir polit mission nation met lover eva braun marri april one day commit suicid septemb geli raubal took life hitler gun munich apart rumour among contemporari geli romant relationship death sourc deep last pain paula hitler younger sister hitler last live member immedi famili die june hitler born practis cathol mother father leav home hitler never attend mass receiv sacrament albert speer state hitler rail church polit associ though never offici left church attach add hitler felt absenc organis religion peopl would turn mystic consid regress accord speer hitler believ japanes religi belief islam would suitabl religion german christian meek flabbi historian john conway state hitler fundament oppos christian church accord bullock hitler believ god anticler held christian ethic contempt contraven prefer view surviv fittest favour aspect protestant suit view adopt element cathol church hierarch organis liturgi phraseolog speech hitler state cathol declar german christian convers albert speer hitler said evangel church could becom establish church england hitler view church import polit conserv influenc societi adopt strateg relationship suit immedi polit purpos public hitler often prais christian heritag german christian cultur though profess belief aryan jesu fought jew public rhetor contradict privat statement describ christian absurd nonsens found lie accord us offic strateg servic oss report nazi master plan hitler plan destroy influenc christian church within reich eventu goal total elimin christian goal inform hitler movement earli saw inexpedi publicli express extrem posit accord bullock hitler want wait war execut plan speer wrote hitler neg view himmler alfr rosenberg mystic notion himmler attempt mythologis ss hitler pragmat ambit centr practic concern research various suggest hitler suffer irrit bowel syndrom skin lesion irregular heartbeat coronari sclerosi parkinson diseas syphili arter tinnitu monorch report prepar oss walter charl langer harvard univers describ hitler neurot psychopath book psychopath god adolf hitler historian robert wait propos hitler suffer borderlin person disord historian henrik eberl neumann consid suffer number ill includ parkinson diseas hitler experi patholog delus alway fulli awar therefor respons decis sometim hitler adopt mainli vegetarian diet avoid meat fish onward social event sometim gave graphic account slaughter anim effort make guest shun meat bormann greenhous construct near berghof near berchtesgaden ensur steadi suppli fresh fruit veget hitler hitler stop drink alcohol around time becam vegetarian thereaft occasion drank beer wine social occas adult life smoke heavili youth cigarett day eventu quit call habit wast money encourag close associ quit offer gold watch anyon abl break habit hitler began use amphetamin occasion becam addict late speer link use amphetamin hitler increasingli errat behaviour inflex exampl rare allow militari retreat prescrib medic war year person physician theodor morel hitler took mani pill day chronic stomach problem ailment regularli consum amphetamin barbitur opiat cocain well potassium bromid atropa belladonna latter form doktor koster antigaspil suffer ruptur eardrum result juli plot bomb blast wood splinter remov leg newsreel footag hitler show tremor left hand shuffl walk began war worsen toward end life schenck sever doctor met hitler last week life also form diagnosi parkinson diseas peac freedom democraci never fascism million dead warn us accord historian joachim fest hitler suicid liken numer contemporari spell broken similarli speer comment insid third reich emot day hitler suicid spell broken magic extinguish public support hitler collaps time death german mourn kershaw argu civilian militari personnel busi adjust collaps countri flee fight take interest accord historian john toland nazism burst like bubbl without leader kershaw describ hitler embodi modern polit evil never histori associ name one man add hitler polit programm brought world war leav behind devast impoverish eastern central europ germani suffer wholesal destruct characteris stund null zero hour hitler polici inflict human suffer unpreced scale accord rummel nazi regim respons democid kill estim million civilian prison war addit million soldier civilian die result militari action european theatr world war ii number civilian kill second world war unpreced histori warfar historian philosoph politician often use word evil describ nazi regim mani european countri criminalis promot nazism holocaust denial historian friedrich meineck describ hitler one great exampl singular incalcul power person histor life english historian hugh saw among simplifi histori systemat histor philosoph yet coarsest cruelest least magnanim conqueror world ever known historian john robert hitler defeat mark end phase european histori domin germani place emerg cold war global confront western bloc domin unit state nato nation eastern bloc domin soviet union historian sebastian haffner assert without hitler displac jew modern nation state israel would exist contend without hitler former european sphere influenc would postpon haffner claim alexand great hitler signific impact compar histor figur caus wide rang worldwid chang rel short time span hitler exploit documentari film newsreel inspir cult person involv appear seri propaganda film throughout polit career mani made leni riefenstahl regard pioneer modern filmmak hitler propaganda film appear includ final solut parti
Hitler (disambiguation),https://en.wikipedia.org/wiki/Hitler_(disambiguation),"
 Adolf Hitler (1889–1945) was the leader of Nazi Germany from 1933 to 1945.
 
Hitler may also refer to:
",adolf hitler leader nazi germani hitler may also refer
Führer (disambiguation),https://en.wikipedia.org/wiki/F%C3%BChrer_(disambiguation),"Führer is a German term meaning leader or guide. It was used as a political title, and later held as a government office by the Nazi Party leader Adolf Hitler during the Weimar and Nazi periods.
 Führer, Fuhrer or Fuehrer may also refer to:
",führer german term mean leader guid use polit titl later held govern offic nazi parti leader adolf hitler weimar nazi period führer fuhrer fuehrer may also refer
Führer,https://en.wikipedia.org/wiki/F%C3%BChrer_of_Germany,"
Führer (/ˈfjʊərər/ FURE-ər; German: [ˈfyːʁɐ] ⓘ) (spelled Fuehrer when the umlaut is unavailable) is a German word meaning ""leader"" or ""guide"". As a political title, it is strongly associated with Adolf Hitler, the dictator of Nazi Germany from 1933 to 1945. Hitler officially styled himself der Führer und Reichskanzler (the Leader and Chancellor of the Reich) after the death of President Paul von Hindenburg in 1934 and the subsequent merging of the offices of Reichspräsident and Reichskanzler. 
 Nazi Germany cultivated the Führerprinzip (""leader principle""), and Hitler was generally known as simply der Führer (""the Leader"").[1]
 In compound words, the use of Führer remains common in German and is used in words such as Reiseführer (travel guide), Museumsführer (museum docent), Bergführer (mountain guide), or Oppositionsführer (leader of the opposition). However, because of its strong association with Hitler, the isolated word itself usually has negative connotations when used with the meaning of ""leader"", especially in political contexts. 
 The word Führer has cognates in the Scandinavian languages, spelled fører in Danish and Norwegian. In Norwegian, the word has the same meaning as the German word. The Norwegian word for mayor is ordfører, literally meaning word leader. In Swedish Ordförande means Chairman and applies to a wide range of situations, for example in corporate boards or as the head of an official gathering of members. In Swedish and Danish, förare and fører normally means ""driver"" (of a vehicle), a meaning Führer can also have in German. However, in the compound word härförare and hærfører, that part does mean ""leader"", and is a cognate of the German Heerführer (military leader).[2]
 Führer has been used as a military title (compare Latin Dux) in Germany since at least the 18th century. The usage of the term ""Führer"" in the context of a company-sized military subunit in the German Army referred to a commander lacking the qualifications for permanent command. For example, the commanding officer of a company was titled ""Kompaniechef"" (lit. 'Company Chief'), but if he did not have the requisite rank or experience, or was only temporarily assigned to command, he was officially titled ""Kompanieführer"". Thus operational commands of various military echelons were typically referred to by their formation title followed by the title Führer, in connection with mission-type tactics used by the German military forces. The term Führer was also used at lower levels, regardless experience.
 The first example of the political use of Führer was with Austrian Georg Ritter von Schönerer (1842–1921), a major exponent of pan-Germanism and German nationalism in Austria, whose followers commonly referred to him as the Führer, and who also used the Roman salute – where the right arm and hand are held rigidly outstretched – which they called the ""German greeting"".[3] According to historian Richard J. Evans, this use of ""Führer"" by Schönerer's Pan-German Association, probably introduced the term to the German far-right, but its specific adoption by the Nazis may also have been influenced by the use in Italy of ""Duce"", also meaning ""leader"", as an informal title for Benito Mussolini, the Fascist Prime Minister, and later (from 1922) dictator, of that country.[4]
 Adolf Hitler took the title to denote his function as head of the Nazi Party; he received it in 1921 when, infuriated over party founder Anton Drexler's plan to merge with another antisemitic far-right nationalist party, he resigned from the party. Drexler and the party's Executive Committee then acquiesced to Hitler's demand to be made the chairman of the party with ""dictatorial powers"" as the condition for his return.[5]
 Final solution
 Parties
 In 1933, Hitler was appointed Reichskanzler (Chancellor of the Reich) by President Paul von Hindenburg.
 A month later, the decision to vote with the Nazi Party taken by the MPs of the Centre Party allowed the Nazi-dominated Reichstag to reach the qualified constitutional two-thirds majority required for passage of the Enabling Act allowing the cabinet to promulgate laws by decree, rendering in practice the system of checks and balances defunct. The Act became the official legal justification for such decrees later routinely issued by Hitler himself.
 One day before Hindenburg's death, Hitler and his cabinet decreed the ""Law Concerning the Head of State of the German Reich"", which stipulated that upon Hindenburg's death, the office of the president was to be merged with that of Chancellor.[8][9] Thus, upon Hindenburg's death, Hitler became Führer und Reichskanzler – although eventually Reichskanzler was quietly dropped from day-to-day usage and retained only in official documents.[10] Hitler therefore assumed the President's powers without assuming the office itself – ostensibly out of respect for Hindenburg's achievements as a heroic figure in World War I. The Enabling Act had specifically prohibited legislation that would affect the position or powers of the Reich President, but the first one-party Reichstag elected in November 1933 had passed an act on the first anniversary of Hitler's appointment as Chancellor, 30 January 1934, abolishing those restrictions. It was then approved by a referendum on 19 August.[6][7][11]
 The title was changed on 28 July 1942 to ""der Führer und Reichskanzler des Großdeutschen Reiches"" (Leader and Chancellor of the Greater German Reich).[12]
 According to the Weimar Constitution, the President was the Supreme Commander of the Armed Forces. Unlike ""President"", Hitler did take this title (Oberbefehlshaber) for himself. When conscription was reintroduced in 1935, Hitler created the title of Commander-in-Chief of the Armed Forces, a post held by the Minister of War. He retained the title of Supreme Commander for himself. Soldiers had to swear allegiance to Hitler as ""Führer des deutschen Reiches und Volkes"" (Leader of the German Reich and Nation). Field Marshal Werner von Blomberg, then the Minister of War and one of those who created the Hitler oath, or the personal oath of loyalty of the military to Hitler, became the Commander-in-Chief of the Armed Forces while Hitler remained Supreme Commander. Following the Blomberg–Fritsch affair in 1938, Hitler assumed the commander-in-chief's post as well and took personal command of the armed forces. However, he continued using the older formally higher title of Supreme Commander, which was thus filled with a somewhat new meaning. Combining it with ""Führer"", he used the style Führer und Oberster Befehlshaber der Wehrmacht (Leader and Supreme Commander of the Wehrmacht), yet a simple ""Führer"" after May 1942.
 Soldiers had to swear allegiance to Hitler as ""Führer des deutschen Reiches und Volkes"" (Leader of the German Reich and Nation).[13] In his political testament, Hitler also referred to himself as Führer der Nation (Leader of the Nation).[14]
 An additional title was adopted by Hitler on 23 June 1941 when he declared himself the ""Germanic Führer"" (Germanischer Führer), in addition to his duties as Führer of the German state and people.[15] This was done to emphasise Hitler's professed leadership of what the Nazis described as the ""Nordic-Germanic master race"", which was considered to include peoples such as the Norwegians, Danes, Swedes, Dutch, and others in addition to the Germans, and the intent to annex these countries to the German Reich to form the Greater Germanic Reich (Großgermanisches Reich deutscher Nation). Waffen-SS formations from these countries had to declare obedience to Hitler by addressing him in this fashion.[16] On 12 December 1941 Dutch fascist Anton Mussert also addressed him as such when he proclaimed his allegiance to Hitler during a visit to the Reich Chancellery in Berlin.[12] He had wanted to address Hitler as Führer aller Germanen (""Führer of all Germanics""), but Hitler personally decreed the former style.[12] Historian Loe de Jong speculates on the difference between the two: Führer aller Germanen implied a position separate from Hitler's role as Führer und Reichskanzler des Grossdeutschen Reiches (""Führer and Reich Chancellor of the Greater German Reich""), while germanischer Führer served more as an attribute of that main function.[12] As late as 1944, however, occasional propaganda publications continued to refer to him by this unofficial title.[17]
 One of the Nazis' most-repeated political slogans was Ein Volk, ein Reich, ein Führer – ""One People, One Empire, One Leader"". Historian Joseph Bendersky [de] says the slogan ""left an indelible mark on the minds of most Germans who lived through the Nazi years. It appeared on countless posters and in publications; it was heard constantly in radio broadcasts and speeches."" The slogan emphasised the absolute control of the leader over practically every sector of German society and culture – with the churches being formally the most notable exception.[13] The designation Führer itself was initially used only in the context of the Nazi Party, though its meaning gradually sprawled to cover the German state, the German Armed Forces, the German nation, and ultimately all the Germanic peoples.
 Hitler's word became in practice absolute and ultimate, even when incompatible with the constitution, as he saw himself as the sole source of power in Germany, similar to the Roman emperors and German early medieval leaders.[18] In spite of that, he took great care to maintain the pretence of legality of his dictatorship. He issued thousands of decrees that were based explicitly on the Reichstag Fire Decree. That decree itself was based on Article 48 of the constitution, which gave the president the power to take measures deemed necessary to protect public order. The Enabling Act was renewed in 1937 for four years and again in 1939 for four years by the Reichstag. In 1943, it was extended indefinitely by a decree from Hitler himself. Those extensions by the Reichstag were merely a formality with all other parties having been banned.
 However, Hitler had a narrow range of interest – mostly involving diplomacy and the military – and so his subordinates interpreted his vaguely formulated orders and wishes in a manner beneficial to their own interests or those of their organisations.[13] This led to vicious power wrangles that were immensely beneficial to Hitler in aiding him to ensure that no subordinate amassed enough power to challenge or jeopardise his absolute rule.
 Regional Nazi Party leaders were called Gauleiter, ""leiter"" also meaning ""leader"". Almost every Nazi paramilitary organisation, in particular the SS and SA, had Nazi Party paramilitary ranks incorporating the title of Führer. The SS including the Waffen-SS, like all paramilitary Nazi organisations, called all their members of any rank except the lowest one a Führer of something; thus confusingly, Gruppenführer was also an official rank title for a specific grade of general. The word Truppenführer was also a generic word referring to any commander or leader of troops and could be applied to NCOs or officers at many different levels of command.
 In Germany, the isolated word ""Führer"" is usually avoided in political contexts, due to its intimate connection with Nazi institutions and with Hitler personally. However, the suffix -führer is used in many compound words. Examples include Bergführer (mountain guide), Fremdenführer/Touristenführer (human tourist guide), Geschäftsführer (manager), Reiseführer (travel guidebook), Spielführer (team captain — also referred to as Mannschaftskapitän), and Wachführer (command duty officer/officer of the watch). 
 When used in the context of vehicles and traffic, it is often interchangeable with the suffix -fahrer (vehicle driver): Kraft(fahrzeug)führer/-fahrer (road vehicle driver), Lok(omotiv)führer/-fahrer (train driver), Sportbootführer/-fahrer (skipper); however, it is worth noticing the exception of the pair Autofahrer (car driver) and Autoführer (road guidebook). It may also be used in this context as a prefix such as in Führerschein (driver's license), Führerstand (train cabin), or Führerhaus (truck cabin). 
 Since German is a language with grammatical gender, Führer refers to a male leader; the feminine form is Führerin. 
 
The use of alternative terms like ""Chef"" (a borrowing from the French, as is the English ""chief"", e.g. Chef des Bundeskanzleramtes) or Leiter (often in compound words like Amtsleiter, Projektleiter, or Referatsleiter) is usually not the result of replacing of the word ""Führer"", but rather using terminology that existed before the Nazis. The use of Führer to refer to a political party leader is rare today and Vorsitzender (chairman) is the more common term. However, the word Oppositionsführer (""leader of the [parliamentary] opposition"") is more commonly used. In the manga series Fullmetal Alchemist (2001–2010), by Hiromu Arakawa the character of King Bradley (King being his given name) has the title Fuhrer-President of Amestris being that the country is a military dictatorship.
 Terms derived from Führer
  
 Other
  
 
 Final solution
 Parties
",führer german ˈfyːʁɐ spell fuehrer umlaut unavail german word mean leader guid polit titl strongli associ adolf hitler dictat nazi germani hitler offici style der führer und reichskanzl leader chancellor reich death presid paul von hindenburg subsequ merg offic reichspräsid reichskanzl nazi germani cultiv führerprinzip leader principl hitler gener known simpli der führer leader compound word use führer remain common german use word reiseführ travel guid museumsführ museum docent bergführer mountain guid oppositionsführ leader opposit howev strong associ hitler isol word usual neg connot use mean leader especi polit context word führer cognat scandinavian languag spell fører danish norwegian norwegian word mean german word norwegian word mayor ordfører liter mean word leader swedish ordförand mean chairman appli wide rang situat exampl corpor board head offici gather member swedish danish förare fører normal mean driver vehicl mean führer also german howev compound word härförare hærfører part mean leader cognat german heerführer militari leader führer use militari titl compar latin dux germani sinc least centuri usag term führer context militari subunit german armi refer command lack qualif perman command exampl command offic compani titl kompaniechef lit chief requisit rank experi temporarili assign command offici titl kompanieführ thu oper command variou militari echelon typic refer format titl follow titl führer connect tactic use german militari forc term führer also use lower level regardless experi first exampl polit use führer austrian georg ritter von schönerer major expon german nation austria whose follow commonli refer führer also use roman salut right arm hand held rigidli outstretch call german greet accord historian richard evan use führer schönerer associ probabl introduc term german specif adopt nazi may also influenc use itali duce also mean leader inform titl benito mussolini fascist prime minist later dictat countri adolf hitler took titl denot function head nazi parti receiv infuri parti founder anton drexler plan merg anoth antisemit nationalist parti resign parti drexler parti execut committe acquiesc hitler demand made chairman parti dictatori power condit return final solut parti hitler appoint reichskanzl chancellor reich presid paul von hindenburg month later decis vote nazi parti taken mp centr parti allow reichstag reach qualifi constitut major requir passag enabl act allow cabinet promulg law decre render practic system check balanc defunct act becam offici legal justif decre later routin issu hitler one day hindenburg death hitler cabinet decre law concern head state german reich stipul upon hindenburg death offic presid merg chancellor thu upon hindenburg death hitler becam führer und reichskanzl although eventu reichskanzl quietli drop usag retain offici document hitler therefor assum presid power without assum offic ostens respect hindenburg achiev heroic figur world war enabl act specif prohibit legisl would affect posit power reich presid first reichstag elect novemb pass act first anniversari hitler appoint chancellor januari abolish restrict approv referendum august titl chang juli der führer und reichskanzl de großdeutschen reich leader chancellor greater german reich accord weimar constitut presid suprem command arm forc unlik presid hitler take titl oberbefehlshab conscript reintroduc hitler creat titl arm forc post held minist war retain titl suprem command soldier swear allegi hitler führer de deutschen reich und volk leader german reich nation field marshal werner von blomberg minist war one creat hitler oath person oath loyalti militari hitler becam arm forc hitler remain suprem command follow affair hitler assum post well took person command arm forc howev continu use older formal higher titl suprem command thu fill somewhat new mean combin führer use style führer und oberst befehlshab der wehrmacht leader suprem command wehrmacht yet simpl führer may soldier swear allegi hitler führer de deutschen reich und volk leader german reich nation polit testament hitler also refer führer der nation leader nation addit titl adopt hitler june declar german führer germanisch führer addit duti führer german state peopl done emphasis hitler profess leadership nazi describ master race consid includ peopl norwegian dane swede dutch other addit german intent annex countri german reich form greater german reich großgermanisch reich deutscher nation format countri declar obedi hitler address fashion decemb dutch fascist anton mussert also address proclaim allegi hitler visit reich chancelleri berlin want address hitler führer aller germanen führer german hitler person decre former style historian loe de jong specul differ two führer aller germanen impli posit separ hitler role führer und reichskanzl de grossdeutschen reich führer reich chancellor greater german reich germanisch führer serv attribut main function late howev occasion propaganda public continu refer unoffici titl one nazi polit slogan ein volk ein reich ein führer one peopl one empir one leader historian joseph benderski de say slogan left indel mark mind german live nazi year appear countless poster public heard constantli radio broadcast speech slogan emphasis absolut control leader practic everi sector german societi cultur church formal notabl except design führer initi use context nazi parti though mean gradual sprawl cover german state german arm forc german nation ultim german peopl hitler word becam practic absolut ultim even incompat constitut saw sole sourc power germani similar roman emperor german earli mediev leader spite took great care maintain pretenc legal dictatorship issu thousand decre base explicitli reichstag fire decre decre base articl constitut gave presid power take measur deem necessari protect public order enabl act renew four year four year reichstag extend indefinit decre hitler extens reichstag mere formal parti ban howev hitler narrow rang interest mostli involv diplomaci militari subordin interpret vagu formul order wish manner benefici interest organis led viciou power wrangl immens benefici hitler aid ensur subordin amass enough power challeng jeopardis absolut rule region nazi parti leader call gauleit leiter also mean leader almost everi nazi paramilitari organis particular ss sa nazi parti paramilitari rank incorpor titl führer ss includ like paramilitari nazi organis call member rank except lowest one führer someth thu confusingli gruppenführ also offici rank titl specif grade gener word truppenführ also gener word refer command leader troop could appli nco offic mani differ level command germani isol word führer usual avoid polit context due intim connect nazi institut hitler person howev suffix use mani compound word exampl includ bergführer mountain guid human tourist guid geschäftsführer manag reiseführ travel guidebook spielführer team captain also refer mannschaftskapitän wachführer command duti watch use context vehicl traffic often interchang suffix vehicl driver kraft fahrzeug road vehicl driver lok omotiv train driver skipper howev worth notic except pair autofahr car driver autoführ road guidebook may also use context prefix führerschein driver licens führerstand train cabin führerhau truck cabin sinc german languag grammat gender führer refer male leader feminin form führerin use altern term like chef borrow french english chief chef de bundeskanzleramt leiter often compound word like amtsleit projektleit referatsleit usual result replac word führer rather use terminolog exist nazi use führer refer polit parti leader rare today vorsitzend chairman common term howev word oppositionsführ leader parliamentari opposit commonli use manga seri fullmet alchemist hiromu arakawa charact king bradley king given name titl amestri countri militari dictatorship term deriv führer final solut parti
Paul von Hindenburg,https://en.wikipedia.org/wiki/Paul_von_Hindenburg,"
 
 Paul Ludwig Hans Anton von Beneckendorff und von Hindenburg[a] (2 October 1847 – 2 August 1934) was a German military leader and statesman who led the Imperial German Army during the First World War[1] and later became president of Germany from 1925 until his death in 1934. He played a key role in the Nazi seizure of power in 1933 when he appointed Adolf Hitler as chancellor of Germany.[1]
 Hindenburg was born to a family of minor Prussian nobility in Posen. Upon completing his education as a cadet, he enlisted in the Third Regiment of Foot Guards as a second lieutenant. He saw combat during the Austro-Prussian and Franco-Prussian wars. In 1873, he was admitted to the prestigious War Academy in Berlin, where he studied before being appointed to the General Staff Corps. In 1885, he was promoted to major and became a member of the German General Staff. After teaching at the War Academy, Hindenburg rose to become a lieutenant general by 1900. In 1911, Hindenburg retired.
 After World War I began in 1914, Hindenburg was recalled and achieved fame on the Eastern Front as victor of Tannenberg. He oversaw crushing victories against the Russians that made him a national hero and the center of a pervasive cult of personality. By 1916, his popularity had risen to the point that he replaced General Erich von Falkenhayn as Chief of the Great General Staff.[2] Ultimately, he and his deputy, General Erich Ludendorff, exploited Emperor Wilhelm II's immense delegation of power to the Supreme Army Command to establish a de facto military dictatorship. Under their leadership, Germany secured Russia's defeat and achieved the largest advance on the Western Front since the conflict's outbreak. However, such improvements in Germany's fortunes were reversed after its Army was decisively defeated in the Second Battle of the Marne and the Allies' Hundred Days Offensive. Following the armistice, Hindenburg stepped down as the German Army's Chief of Staff before retiring again in 1919.
 In 1925, Hindenburg returned to public life to become the second elected president of the Weimar Republic. Opposed to Hitler and his Nazi Party, Hindenburg nonetheless played a major role in the instability that resulted in their rise to power. After twice dissolving the Reichstag in 1932, Hindenburg agreed in January 1933 to appoint Hitler as chancellor in coalition with the Deutschnationale Volkspartei. In response to the February 1933 Reichstag fire, Hindenburg approved the Reichstag Fire Decree which suspended various civil liberties. He likewise signed the Enabling Act of 1933 which gave the Nazi regime emergency powers. After Hindenburg died the following year, Hitler combined the presidency with the chancellery before declaring himself Führer (lit. 'Leader') of Germany and transforming the country into a totalitarian state.
 Hindenburg was born in Posen, Prussia, the son of Prussian junker Hans Robert Ludwig von Beneckendorff und von Hindenburg (1816–1902) and his wife Luise Schwickart (1825–1893),[1] the daughter of physician Karl Ludwig Schwickart and wife Julie Moennich. His paternal grandparents were Otto Ludwig Fady von Beneckendorff und von Hindenburg (1778–1855), through whom he was remotely descended from the illegitimate daughter of Count Heinrich VI of Waldeck, and his wife Eleonore von Brederfady (d. 1863).[clarification needed] Hindenburg's younger brothers and sister were Otto (b. 1849), Ida (b. 1851) and Bernhard (b. 1859). His family were all Lutheran Protestants in the Evangelical Church of Prussia, which since 1817 included both Calvinist and Lutheran parishioners.[citation needed]
 Hindenburg was proud of his family and could trace his ancestors back to 1289.[3] The dual surname was adopted in 1789 to secure an inheritance and appeared in formal documents, but in everyday life, they were von Beneckendorffs.[clarification needed] True to family tradition, his father supported his family as an infantry officer; he retired as a major. In the summer, they visited his grandfather at the Hindenburg estate of Neudeck in East Prussia. At age 11, Paul entered the Cadet Corps School at Wahlstatt (now Legnickie Pole, Poland).[1] At 16, he was transferred to the School in Berlin, and, at 18, he served as a page to the widow of King Frederick William IV of Prussia. Graduates entering the army were presented to King William I, who asked for their father's name and rank. He became a second lieutenant in the Third Regiment of Foot Guards.[citation needed]
 When the Austro-Prussian War of 1866 broke out, Hindenburg wrote to his parents: 'I rejoice in this bright-coloured future. For the soldier war is the normal state of things[...]If I fall, it is the most honorable and beautiful death.'[4] During the decisive Battle of Königgrätz, he was briefly knocked unconscious by a bullet that pierced his helmet and creased the top of his skull. Quickly regaining his senses, he wrapped his head in a towel and resumed leading his detachment, winning a decoration.[5] He was a battalion adjutant when the Franco-Prussian War (1870–71) broke out. After weeks of marching, the Guards attacked the village of Saint Privat (near Metz). Climbing a gentle slope, they came under heavy fire from the superior French rifles. After four hours the Prussian artillery came up to blast the French lines while the infantry, filled with the ""holy lust of battle"",[6] swept through the French lines. His regiment suffered 1096 casualties, and he became a regimental adjutant. The Guards were spectators at the Battle of Sedan and for the following months sat in the siege lines surrounding Paris. He was his regiment's elected representative at the Palace of Versailles when the German Empire was proclaimed on 18 January 1871; at 1.98m (6 feet 6 inches) tall with a muscular frame and striking blue eyes, he was an impressive figure.[7] After the French surrender, he watched from afar the suppression of the Paris Commune.[citation needed]
 In 1873, he passed the highly competitive entrance examination for admission to the Kriegsakademie in Berlin.[8] After three years of study, his grades were high enough for an appointment with the General Staff. He was promoted to captain in 1878 and assigned to the staff of the II Corps. He married the intelligent and accomplished Gertrud von Sperling (1860–1921), daughter of General Oskar von Sperling, in 1879. The couple would have two daughters, Irmengard Pauline (1880) and Annemaria (1891), and one son, Oskar (1883). Next, he commanded an infantry company, in which his men were ethnic Poles.[citation needed]
 He was transferred in 1885 to the General Staff and was promoted to major. His section was led by Count Alfred von Schlieffen, a student of encirclement battles like Cannae, whose Schlieffen Plan proposed to pocket the French Army. For five years Hindenburg also taught tactics at the Kriegsakademie. At the maneuvers of 1885, he met the future Kaiser Wilhelm II; they met again at the next year's war game in which Hindenburg commanded the ""Russian army"". He learned the topography of the lakes and sand barrens of East Prussia during the annual Great General Staff's ride in 1888. The following year, he moved to the War Ministry, to write the field service regulations on field-engineering and on the use of heavy artillery in field engagements; both were used during the First World War. He became a lieutenant colonel in 1891, and, two years later, was promoted to colonel, commanding an infantry regiment. He became chief of staff of the VIII Corps in 1896.[citation needed]
 Hindenburg became a major-general (equivalent to a British and US brigadier general) in 1897, and in 1900 he was promoted to lieutenant general (equivalent to major-general) and received command of the 28th Infantry Division. Five years later he was made commander of the IV Corps based in Magdeburg as a General of the Infantry (lieutenant-general; the German equivalent to four-star rank was Colonel-General). The annual maneuvers taught him how to maneuver a large force; in 1908 he defeated a corps commanded by the Kaiser.[9] Schlieffen recommended him as Chief of the General Staff in 1909, but he lost out to Helmuth von Moltke.[10] He retired in 1911 ""to make way for younger men"".[11] He had been in the army for 46 years, including 14 years in General Staff positions. During his career, Hindenburg did not have political ambitions and remained a staunch monarchist.[12]
 When WWI broke out, Hindenburg was retired in Hannover. On 22 August, due to the purge of German command[13] following Russian success in East Prussia, he was selected by the War Cabinet and the German Supreme Army Command (Oberste Heeresleitung, OHL) to assume command of the German Eighth Army in East Prussia, with General Erich Ludendorff as his chief of staff.[2][12] After the Eighth Army had been defeated by the Russian 1st Army at Gumbinnen, it had found itself in danger of encirclement as the Russian 2nd Army under General Alexander Samsonov advanced from the south towards the Vistula River. Momentarily panicked, Eighth Army commander Maximilian von Prittwitz notified OHL of his intent to withdraw his forces into Western Prussia.[14] The Chief of the German General Staff, Generaloberst Helmuth von Moltke, responded by relieving Prittwitz and replacing him with Hindenburg.[15]
 Upon arriving at Marienburg on 23 August, Hindenburg and Ludendorff were met by members of the 8th Army's staff led by Lieutenant Colonel Max Hoffmann, an expert on the Russian army. Hoffman informed them of his plans to shift part of the 8th Army south to attack the exposed left flank of the advancing Russian Second Army.[16] Agreeing with Hoffman's strategy, Hindenburg authorized Ludendorff to transfer most of the 8th Army south while leaving only two cavalry brigades to face the Russian First Army in the north.[17] In Hindenburg's words the line of soldiers defending Germany's border was ""thin, but not weak"", because the men were defending their homes.[18] If pushed too hard by the Second Army, he believed they would cede ground only gradually as German reinforcements continued to mass on the invading Russians' flanks before ultimately encircling and annihilating them.[19] On the eve of the ensuing battle, Hindenburg reportedly strolled close to the decaying walls of the fortress of the Knights of Prussia, recalling how the Knights of Prussia were defeated by the Slavs in 1410 at nearby Tannenberg.[20]
 On the night of 25 August, Hindenburg told his staff, ""Gentlemen, our preparations are so well in hand that we can sleep soundly tonight"".[21] On the day of the battle, Hindenburg reportedly watched from a hilltop as his forces' weak center gradually gave ground until the sudden roar of German guns to his right heralded the surprise attack on the Russians' flanks. Ultimately, the Battle of Tannenberg resulted in the destruction of the Russian 2nd Army, with 92,000 Russians captured together with four hundred guns,[22] while German casualties numbered only 14,000. According to British field marshal Edmund Ironside it was the ""greatest defeat suffered by any of the combatants during the war"".[23] Recognizing the victory's propaganda value, Hindenburg suggested naming the battle ""Tannenberg"" as a way of ""avenging"" the defeat inflicted on the Order of the Teutonic Knights by the Polish and Lithuanian knights in 1410, even though it was fought nowhere near the field of Tannenberg.[24]
 After this decisive victory, Hindenburg re-positioned the Eighth Army to face the Russian First Army. Hindenburg's tactics spurned head-on attacks all along the front in favor of schwerpunkte: sharp, localized hammer blows.[25] Two schwerpunkte struck in the First Battle of the Masurian Lakes. Two columns drove east from these breakthrough points to pocket the Russians led by General Paul von Rennenkampf, who managed to retreat 100 km (62 mi) with heavy losses. In the first six weeks of the war the Russians had lost more than 310,000 men.[26] Eight hundred thousand refugees were able to return to their East Prussian homes, thanks to victories that strikingly contrasted with the bloody deadlock of the Western Front following the failure of the Schlieffen Plan.[citation needed]
 The Hindenburg-Ludendorff duo's successful performance on the Eastern Front in 1914 marked the beginning of a military and political partnership that lasted until the end of the war. As Hindenburg wrote to the Kaiser a few months later: ""[Ludendorff] has become my faithful adviser and a friend who has my complete confidence and cannot be replaced by anyone.""[27] Despite their strikingly dissimilar temperaments, the older general's calm decisiveness proved to be an outstanding fit for Ludendorff's energy and tactical ingenuity. Ludendorff's nerves twice drove him to consider changing their plans for Tannenberg at the last minute; both times Hindenburg talked to him privately and his confidence wavered no further.[28]
 On the east bank of the Vistula in Poland the Russians were mobilizing new armies which were shielded from attack by the river; once assembled they would cross the river to march west into Silesia. To counter the Russians' pending invasion of Silesia, Hindenburg advanced into Poland and occupied the west bank of the Vistula opposite from where Russian forces were mobilizing. He set up headquarters at Posen in West Prussia, accompanied by Ludendorff and Hoffmann.[29] When the Russians attempted to cross the Vistula, the German forces under his command held firm, but the Russians were able to cross into the Austro-Hungarian sector to the south. Hindenburg retreated and destroyed all railways and bridges so that the Russians would be unable to advance beyond 120 km (75 mi) west of their railheads—well short of the German frontier.[citation needed]
 On 1 November 1914 Hindenburg was appointed Ober Ost (commander in the east) and was promoted to field marshal. To meet the Russians' renewed push into Silesia, Hindenburg moved the Ninth Army by rail north to Thorn and reinforced it with two corps from the Eighth Army. On 11 November, in a raging snowstorm, his forces surprised the Russian flank in the fierce Battle of Łódź, which ended the immediate Russian threat to Silesia and also captured Poland's second largest city.[citation needed]
 Hindenburg argued that the still miserably equipped Russians—some only carried spears—in the huge Polish salient were in a trap in which they could be snared in a cauldron by a southward pincer from East Prussia and a northward pincer from Galicia, using motor vehicles for speed,[30] even though the Russians outnumbered the Germans by three to one. From Hindenburg's point of view, such an overwhelming triumph could end the war in the Eastern Front.[31] Erich von Falkenhayn, the Chief of Germany's Great General Staff, rejected his plan as a pipe dream. Nevertheless, urged on by Ludendorff and Hoffman, Hindenburg spent the winter fighting for his strategy by badgering the Kaiser while his press officer recruited notables like the Kaiserin and the Crown Prince to ""stab the Kaiser in the back"".[32] The Kaiser compromised by keeping Falkenhayn in supreme command but replacing him as Prussian war minister. In retaliation, Falkenhayn reassigned some of Hindenburg's forces to a new army group under Prince Leopold of Bavaria and transferred Ludendorff to a new joint German and Austro-Hungarian Southern Army. Hindenburg and Ludendorff reacted by threatening to resign thereby resulting in Ludendorff's reinstatement under Hindenburg's command.[citation needed]
 Following his return, Ludendorff provided Hindenburg with a depressing evaluation of their allies' army, which already had lost many of their professional officers[33] and had been driven out of much of the Kingdom of Galicia and Lodomeria, their part of what once had been Poland. Meanwhile, the Russians were inexorably pushing from Galicia toward Hungary through the Carpathian passes. Under orders from Falkenhayn to contain the resurgent Russians, Hindenburg mounted an unsuccessful attack in Poland with his Ninth Army as well as an offensive by the newly formed Tenth Army which made only local gains. Following these setbacks, he set up temporary headquarters at Insterburg, and made plans to eliminate the Russians' remaining toehold in East Prussia by ensnaring them in a pincer movement between the Tenth Army in the north and Eighth Army in the south. The attack was launched on 7 February. Hindenburg's forces encircled an entire corps and captured more than 100,000 men in the Second Battle of the Masurian Lakes.[citation needed]
 Shortly thereafter, Hindenburg and Ludendorff played a key role in the Central Powers' Gorlice–Tarnów Offensive. After the Austro-Hungarian fortress of Przemyśl fell on 23 March, Austria-Hungary's high command pushed for a joint strike on the Russian right flank that could potentially drive their forces out of the Carpathians. Agreeing to the proposal, Falkenhayn moved OHL east to the castle of Pless while forming Army Group von Mackensen from a new German Eleventh Army and the Austro-Hungarian Fourth Army. As Field Marshal August von Mackensen broke through Russian lines between Gorlice and Tarnów, Hindenburg's Ninth and Tenth Army launched diversionary attacks that threatened Riga in the north.[34] In one of the war's most successful cavalry actions, three cavalry divisions swept east into Courland, the barren, sandy region near the Baltic coast. The cavalry's gains were held by Hindenburg's new Nieman army, named after the river.[citation needed]
 In June, the Supreme Army Command ordered Hindenburg to launch a Bug-Narew Offensive in Poland toward the Narew River north of Warsaw. Hindenburg created Army Group Gallwitz, named after its commander. Von Gallwitz was one of many able commanders selected by Hindenburg, who stayed at the new army's headquarters to be available if needed. (When Berlin approved the new army group, it became Twelfth Army.) The army group broke through the Russian lines after a brief, but intense, bombardment directed by Lieutenant Colonel Georg Bruchmüller, an artillery genius recalled from medical retirement. One-third of the opposing Russian First Army were casualties in the first five hours.[35] From then on Hindenburg often called on Bruchmüller. The Russians withdrew across the Narev River. However, steamroller frontal attacks cost dearly: by 20 August Gallwitz had lost 60,000 men.[citation needed]
 As the Russians withdrew from the Polish Salient, Falkenhayn insisted on pursuing them into Lithuania. However, Hindenburg and Ludendorff were dissatisfied with this plan. Hindenburg would later claim that he saw it as ""a pursuit in which the pursuer gets more exhausted than the pursued"".[36]
 On 1 June, Hindenburg's Nieman and Tenth Armies spearheaded attacks into Courland in an attempt to pocket the defenders. Ultimately, this plan was foiled by the prudent commander of the Fifth Russian Army who defied orders by withdrawing into defensible positions shielding Riga.[37]
 Despite the setback in Latvia, Hindenburg, and Ludendorff continued to rack up victories on the Eastern Front. In August, the Germans stormed the Novogeorgievsk fortress. Numerous Russian sources call the fall of Novogeorgievsk the most shameful page in the history of the Russian Imperial army.[38][39] The German Tenth Army besieged Kovno, a Lithuanian city on the Nieman River defended by a circle of forts. It fell on 17 August, along with 1,300 guns and almost 1 million shells. On 5 August his forces were consolidated into Army Group Hindenburg, which took the city of Grodno after bitter street fighting but could not trap the retreating defenders because the rail lines lacked the capacity to bring up the needed men. They occupied Vilnius on 18 September, then halted on ground favorable for a defensive line. On 6 August, German troops under Hindenburg used chlorine gas against Russian troops defending Osowiec Fortress. The Russians demolished much of Osowiec and withdrew on 18 August.[citation needed]
 In October, Hindenburg moved headquarters to Kovno. They were responsible for 108,800 km2 (42,000 mi2) of conquered Russian territory, which was home to three million people and became known as Ober Ost. The troops built fortifications on the eastern border while Ludendorff ""with his ruthless energy""[40] headed the civil government, using forced labor to repair the war damages and to dispatch useful products, like hogs, to Germany. A Hindenburg son-in-law, who was a reserve officer and a legal expert, joined the staff to write a new legal code.[citation needed] Baltic Germans who owned vast estates feted Hindenburg and he hunted their game preserves.[citation needed]
 Hindenburg would later judge German operations in 1915 to be ""unsatisfactory"". In his memoirs, he recounted that ""[t]he Russian bear had escaped our clutches""[41] and abandoning the Polish salient had shortened their lines substantially. Conversely, victorious Falkenhayn believed that ""The Russian Army has been so weakened by the blows it has suffered that Russia need not be seriously considered a danger in the foreseeable future"".[42] The Russians replaced their experienced supreme commander, Grand Duke Nicholas Nikolaevich, a man whose skill Hindenburg held in high regard,[43] with the Tsar.[citation needed]
 In the spring of 1916, the Central Powers experienced a military catastrophe in the East that left Germany bearing much of the war effort until the end of hostilities. On 4 June, the Russian Army began a massive offensive along 480 km (300 mi) of the southwestern front in present-day western Ukraine. In the ensuing onslaught, four armies commanded by General Aleksei Brusilov overwhelmed entrenchments that the Austro-Hungarians long regarded as impregnable.[44] Probing assault troops located three weak spots which then were struck in force. In nine days they captured more than 200,000 men and 200 guns and pushed into the open country.[citation needed]
 Under Hindenburg's command, Ober Ost desperately shored up weak points with soldiers stripped from less threatened positions. Ludendorff was so distraught on the phone to OHL that General Wilhelm Groener (who directed the army's railroads and had been a competitor with Ludendorff on the General Staff) was sent to evaluate his nerves, which were judged satisfactory.[45] For a week the Russians kept attacking: they lost 80,000 men; the defenders 16,000. On 16 July the Russians attacked the German lines west of Riga but were ultimately thwarted. When looking back on the Russian offensive, Hindenburg admitted that another attack of such scale and ferocity would have left his forces ""faced with the menace of a complete collapse.""[46]
 After having their strength decimated by the Russians in the Brusilov Offensive, the Austro-Hungarian forces submitted their Eastern Front forces to Hindenburg's command on 27 July (except for Archduke Karl's Army Group in southeast Galicia, in which General Hans von Seeckt was chief of staff). General von Eichhorn took over Army Group Hindenburg, while Hindenburg and Ludendorff, on a staff train equipped with the most advanced communication apparatus, visited their new forces. At threatened points, they formed mixed German and Austro-Hungarian units while other Austro-Hungarian formations were bolstered by a sprinkling of German officers. Officers were exchanged between the German and Austro-Hungarian armies for training. The derelict citadel of the Brest Fortress was refurbished as their headquarters. Their front was almost 1,000 km (620 mi) and their only reserves were a cavalry brigade plus some artillery and machine gunners.[47]
 In the west, the Germans were hemorrhaging in the battles of Verdun and the Somme. Influential Army officers, led by the artillery expert Lieutenant Colonel Max Bauer, a friend of Ludendorff's, lobbied against Falkenhayn, deploring his futile steamroller at Verdun and his inflexible defense along the Somme, where he packed troops into the front-line to be battered by the hail of shells and sacked commanders who lost their front-line trench. German leaders contrasted Falkenhayn's bludgeon with Hindenburg's deft parrying.[48]
The tipping point came when Falkenhayn ordered a spoiling attack by Bulgaria on Entente lines in Macedonia, which failed with heavy losses. Thus emboldened, Romania declared war on Austro-Hungary on 27 August, adding 650,000 trained enemies who invaded Hungarian Transylvania. Falkenhayn had been adamant that Romania would remain neutral. During the Kaiser's deliberations about who should command Falkenhayn said ""Well, if the Herr Field Marshall has the desire and the courage to take the post"". Hindenburg replied ""The desire, no, but the courage—yes"".[49] Chancellor Bethmann Hollweg favored Hindenburg, supposing him amenable to moderate peace terms,[50] mistaking his amiability as tractability and unaware that he was intent on enlarging Prussia.[citation needed]
 Hindenburg was summoned to Pless on 29 August where he was named Chief of the Great General Staff and, by extension, the Supreme Army Command. Ludendorff demanded joint responsibility for all decisions"";[51] Hindenburg did not demur. Henceforth, Ludendorff was entrusted with signing most orders, directives, and daily press reports. The eastern front was commanded by Leopold of Bavaria, with Hoffmann as his chief of staff. Hindenburg was also appointed the Supreme War Commander of the Central Powers, with nominal control over six million men. Until the end of the war, this arrangement formed the basis of Hindenburg's leadership which would come to be known as the Third OHL.[citation needed]
 The British were unimpressed: General Charteris, Haig's intelligence chief, wrote to his wife ""poor old Hindenburg is sixty-four years of age, and will not do very much.""[52] Conversely, the German War Cabinet was impressed by his swift decision-making. They credited ""Old Man Hindenburg"" with ending the ""Verdun folly"" and setting in motion the ""brilliant"" conquest of Romania.[53]
 Hindenburg and Ludendorff visited the Western Front in September, meeting the Army commanders and their staffs as well as their leaders: Crown Prince Rupprecht of Bavaria, Albrecht, Duke of Württemberg and Crown Prince Wilhelm of Prussia. Both crown princes, with Prussian chiefs of staff, commanded Army Groups. Rupprecht and Albrecht were presented with field marshal's batons. Hindenburg told them that they must stand on the defensive until Romania was dealt with, meanwhile defensive tactics must be improved—ideas were welcome.[54] A backup defensive line, which the Entente called the Hindenburg Line, would be constructed immediately. Ludendorff promised more arms. Rupprecht was delighted that two such competent men had ""replaced the dilettante 'Falkenhayn'.""[55] Bauer was impressed that Hindenburg ""saw everything only with the eye of the soldier.""[56]
 Under Field Marshal Hindenburg's leadership, the German Supreme Army Command issued a Textbook of Defensive Warfare that recommended fewer defenders in the front line relying on light machine guns. If pushed too hard, they were permitted to pull back. Front-line defenses were organized so that penetrating enemy forces found themselves cut down by machine gun fire and artillery from those who knew the ranges and location of their own strong points. Subsequently, the infantry would counterattack while the attacker's artillery was blind because they were unsure where their own men were. A reserve division was positioned immediately behind the line, if it entered the battle it was commanded by the division whose position had been penetrated. (Mobile defense was also used in World War II.) Responsibilities were reassigned to implement the new tactics: front-line commanders took over reserves ordered into the battle and for flexibility, infantry platoons were subdivided into eight-man units under a noncom.[citation needed]
 Field officers who visited headquarters often were invited to speak with Hindenburg, who inquired about their problems and recommendations. At this time he was especially curious about the eight-man units,[57] which he regarded as ""the greatest evidence of the confidence which we placed in the moral and mental powers of our army, down to its smallest unit.""[58] Revised Infantry Field Regulations were published and taught to all ranks, including at a school for division commanders, where they maneuvered a practice division. A monthly periodical informed artillery officers about new developments. In the last months of 1916, the British battering along the Somme produced fewer German casualties. Overall, ""In a fierce and obstinate conflict on the Somme, which lasted five months, the enemy pressed us back to a depth of about six miles on a stretch of nearly twenty-five miles""[59] Thirteen new divisions were created by reducing the number of men in infantry battalions, and divisions now had an artillery commander. Every regiment on the western front created an assault unit of stormtroopers selected from their fittest and most aggressive men.[60] Lieutenant General Ernst von Höppner was given responsibility for both aerial and antiaircraft forces; the army's vulnerable zeppelins went to the navy. Most cavalry regiments were dismounted and the artillery received their badly needed horses.[61]
 In October General Philippe Pétain began a series of limited attacks at Verdun, each starting with an intense bombardment coordinated by his artillery commander General Robert Nivelle. Then a double creeping barrage led the infantry into the shattered first German lines, where the attackers stopped to repel counterattacks.[62] With repeated nibbles by mid-December 1916 the French retook all the ground the Germans had paid for so dearly. Nivelle was given command of the French Army.[citation needed]
 Hindenburg's day at OHL began at 09:00 when he and Ludendorff discussed the reports—usually quickly agreeing on what was to be done.[63] Ludendorff would give their staff of about 40 officers their assignments, while Hindenburg walked for an hour or so, thinking or chatting with guests. After conferring again with Ludendorff, he heard reports from his departmental heads, met with visitors, and worked on correspondence. At noon Ludendorff gave the situation report to the Kaiser unless an important decision was required when Hindenburg took over. He lunched with his personal staff, which included a son-in-law who was an Army officer.[citation needed] Dinner at 20:00 was with the general staff officers of all ranks and guests—crowned heads, allied leaders, politicians, industrialists and scientists. They left the table to subdivide into informal chatting groups.[64] At 21:30 Ludendorff announced that time was up and they returned to work. After a junior officer summarized the daily reports, he might confer with Ludendorff again before retiring.[citation needed]
 Under Hindenburg, the Third OHL set ambitious benchmarks for arms production in what became known as the Hindenburg Programme, which was directed from the War Office by General Groener. Major goals included a new light machine gun, updated artillery, and motor transport, but no tanks because they considered them too vulnerable to artillery. To increase output they needed skilled workers. The army released a million men.[65] For total war, the Supreme Army Command wanted all German men and women from 15 to 60 enrolled for national service. Hindenburg also wanted the universities closed, except for medical training, so that empty places would not be filled by women. To swell the next generation of soldiers he wanted contraceptives banned and bachelors taxed.[66] When a Polish army was being formed he wanted Jews excluded.[67] Few of these ideas were adopted, because their political maneuvering was vigorous but inept, as Admiral Müller of the Military Cabinet observed ""Old Hindenburg, like Ludendorff, is no politician, and the latter is at the same time a hothead.""[68] For example, women were not included in the service law that ultimately passed, because in fact more women were already seeking employment than there were openings.[citation needed]
 Following the death of Austro-Hungarian emperor Franz Joseph on 21 November, Hindenburg met his successor Charles, who was frank about hoping to stop the fighting. Hindenburg's Eastern Front ran south from the Baltic to the Black Sea through what now are the Baltic States, Ukraine, and Romania. In Italy, the line ran from the Swiss border on the west to the Adriatic east of Venice. The Macedonian front extended along the Greek border from the Adriatic to the Aegean.[citation needed]
The line contested by the Russians and Ottomans between the Black and Caspian Sea ran along the heights of the Caucasus mountains. Hindenburg urged the Ottomans to pull their men off the heights before winter but they did not. In his memoirs, he would later allege this was because of their ""policy of massacre of the Armenians"".[69]
The front in Palestine ran from the Mediterranean to the southern end of the Dead Sea, and the defenders of Baghdad had a flank on the Tigris River. The Western Front ran southward from Belgium until near Laon, where it turned east to pass Verdun before again turning south to end at the Swiss Border. The remaining German enclaves in Africa were beyond his reach; an attempt to resupply them by dirigible failed. The Central Powers were surrounded and outnumbered.[citation needed]
 By the second quarter of 1917, Hindenburg and Ludendorff were able to assemble 680,000 more men in 53 new divisions[70] and provide them with an adequate supply of new light machine guns. Field guns were increased from 5,300 to 6,700 and heavies from 3,700 to 4,340. They tried to foster fighting spirit by ""patriotic instruction"" with lectures and films[71] to ""ensure that a fight is kept up against all agitators, croakers and weaklings"".[72] Meanwhile, to mitigate the risk of being attacked before their buildup was complete, Germany's new military leadership waged unrestricted submarine warfare on allied shipping, which they claimed would defeat the British in six months. Chancellor Bethmann Hollweg and his allies expressed opposition to this policy, not wanting to bring the United States and other neutrals into the war. After securing the Dutch and Danish borders, Hindenburg announced that unrestricted submarine warfare was imperative and Ludendorff added his voice. On 9 January the chancellor was forced to bow to their unsound military judgments.[citation needed]
 OHL moved west to the pleasant spa town of Bad Kreuznach in southwest Germany, which was on a main rail line. The Kaiser's quarters were in the spa building, staff offices were in the orange court, and the others lived in the hotel buildings. In February a third Army Group was formed on the Western Front to cover the front in Alsace-Lorraine, commanded by Archduke Albrecht of Württemberg. Some effective divisions from the east were exchanged for less competent divisions from the west. Since their disasters of the previous year, the Russian infantry had shown no fight and in March the revolution erupted in Russia. Shunning opportunity, the Central Powers stayed put; Hindenburg feared that invaders would resurrect the heroic resistance of 1812.[citation needed]
 On the Western Front, the Third OHL deduced the German Army's huge salient between the valley of the Somme and Laon obviously was vulnerable to a pincer attack, which indeed the French were planning. The new Hindenburg line ran across its base. Subsequently, On 16 March, Hindenburg authorized Operation Alberich whereby German forces were ordered to move out all able-bodied inhabitants and portable possessions to this line. In the process, they destroyed every building, leveled all roads and bridges, cut down every tree, fouled every well, and burned every combustible. In 39 days the Germans withdrew from a 1000 mi2 (2,590 km2) area, more ground than they had lost to all Allied offensives since 1914.[73] The cautiously following Allies also had to cope with booby traps, some exploding a month later. The new German front called the Hindenburg line was 42 km (26 mi) shorter freeing-up 14 German divisions.[citation needed]
 On 9 April, the British attacked at Arras and overtook two German lines while occupying part of a third as the Canadians swept the Germans completely off the Vimy Ridge. When the excitable Ludendorff became distraught over such developments, Hindenburg reportedly calmed his First Quartermaster-General by ""pressing his hand"" and assuring him, ""We have lived through more critical times than today together.""[74] Ultimately, the British tried to exploit their opening with a futile cavalry charge but did not press further. In the battle's aftermath, the Third OHL discovered one reason behind the British attack's success was that the Sixth Army commander, Ludwig von Falkenhausen, had failed to properly apply their instructions for a defense in depth by keeping reserve troops too far back from the front lines.[75][76] As a result of this failure, Falkenhausen along with several staff officers were stripped of their command.[77]
 After the Romanov dynasty's fall from power, Russia remained at war under the new revolutionary government led by Alexander Kerensky. In the Kerensky Offensive launched on 1 July, the Russian army pushed Austro-Hungarian forces in Galicia on 1 July. In order to counter this success, six German divisions mounted a counterattack on 18 July that tore a hole through the Russian front through which they sliced southward toward Tarnopol. The ensuing German advance threatened to encircle the Russian attackers, thereby causing them to retreat. At the end of August, the advancing Central Powers stopped at the frontier of Moldavia. To keep up the pressure and to seize ground he intended to keep, Hindenburg shifted north to the heavily fortified city of Riga (today in Latvia) which has the broad Dvina River as a moat. On 1 September the Eighth Army, led by Oskar von Hutier, attacked; Bruchmüller's bombardment, which included gas and smoke shells, drove the defenders from the far bank east of the city, the Germans crossed in barges and then bridged the river, immediately pressing forward to the Baltic coast, pocketing the defenders of the Riga salient. Next, a joint operation with the navy seized Oesel and two smaller islands in the Gulf of Riga. The Bolshevik revolution took Russia out of the war, and an armistice was signed on 16 December.[citation needed]
 Hindenburg detested Chancellor Bethmann Hollweg for arguing against unrestricted submarine warfare. Then in July, the Reichstag debated a resolution for peace without ""annexations or indemnities"". Colonel Bauer and the Crown Prince hurried to Berlin to block the move. The Minister of War urged Hindenburg and Ludendorff to join them, but when they arrived the Kaiser told them that ""there could be no justification for their presence in Berlin"". They should ""return in haste to Headquarters where they certainly would be much better occupied.""[78] In a letter to the Emperor dated 12 July 1917, Ludendorff threatened to resign, and Hindenburg joined in the ultimatum. The Kaiser declined to accept. By then the majority parties in the Reichstag saw Bethmann Hollweg as an unacceptable negotiator for peace because he had been chancellor too long and was too weak in his dealings with the Supreme Army Command. The crisis was resolved when Bethmann Hollweg voluntarily resigned. Ludendorff and Bauer wanted to replace both the Kaiser and chancellor with a dictator, but Hindenburg would not agree.[79] On 19 July, the Reichstag passed the resolution calling for a peace of understanding without ""territorial acquisitions achieved by force and violations of political, economic or financial integrity"",[80] which the new chancellor, Georg Michaelis, agreed to ""interpret"".[81] The policy of the peace resolution was therefore stillborn under Michaelis.[82]
 The resolution became advantageous in August when Pope Benedict XV called for peace. The German response cited the resolution to finesse specific questions like those about the future of Belgium. The industrialists opposed Groener's advocacy of an excess profits tax and insistence that workers take a part in company management.[83][84] Ludendorff relieved Groener by telegram and sent him off to command a division.[citation needed]
 Hindenburg's 70th birthday was celebrated lavishly all over Germany, 2 October was a public holiday, an honor that until then had been reserved only for the Kaiser.[85] Hindenburg published a birthday manifesto, which ended with the words:
 With God's help our German strength has withstood the tremendous attack of our enemies, because we were one, because each gave his all gladly. So it must stay to the end. 'Now thank we all our God' on the bloody battlefield! Take no thought for what is to be after the war! This only brings despondency into our ranks and strengthens the hopes of the enemy. Trust that Germany will achieve what she needs to stand there safe for all time, trust that the German oak will be given air and light for its free growth. Muscles tensed, nerves steeled, eyes front! We see before us the aim: Germany honored, free, and great! God will be with us to the end!""[86] Bavarian mountain warfare expert von Dellmensingen was sent to assess the Austro-Hungarian defenses in Italy, which he found poor. Then he scouted for a site from which an attack could be mounted against the Italians. Hindenburg created a new Fourteenth Army with ten Austro-Hungarian and seven German divisions and enough airplanes to control the air, commanded by Otto von Below. The attackers slipped undetected into the mountains opposite to the opening of the Soča valley. The attack began during the night when the defender's trenches in the valley were abruptly shrouded in a dense cloud of poison gas released from 894 canisters fired simultaneously from simple mortars. The defenders fled before their masks would fail. The artillery opened fire several hours later, hitting the Italian reinforcements hastening up to fill the gap. The attackers swept over the almost empty defenses and marched through the pass, while mountain troops cleared the heights on either side. The Italians fled west, too fast to be cut off. Entente divisions were rushed to Italy to stem the retreat by holding a line on the Piave River. Below's Army was dissolved and the German divisions returned to the Western Front, where in October Pétain had directed a successful limited objective attack in which six days of carefully planned bombardment left crater-free pathways for 68 tanks to lead the infantry forward on the Lassaux plateau south of Laon, which forced the Germans off of the entire ridge—the French Army had recovered.[citation needed]
 In the negotiations with Soviet Russia, Hindenburg wanted to retain control of all Russian territory that the Central Powers occupied, with German grand dukes ruling Courland and Lithuania, as well as a large slice of Poland. Their Polish plan was opposed by Foreign Minister Richard von Kühlmann, who encouraged the Kaiser to listen to the views of Max Hoffmann, chief of staff on the Eastern Front. Hoffmann demurred but when ordered argued that it would be a mistake to bring so many Slavs into Germany, when only a small slice of Poland was needed to improve defenses. Ludendorff was outraged that the Kaiser had consulted a subordinate, while Hindenburg complained that the Kaiser ""disregards our opinion in a matter of vital importance.""[87] The Kaiser backed off, but would not approve Ludendorff's order removing Hoffmann, who is not even mentioned in Hindenburg's memoir. When the Soviets refused the terms offered at Brest-Litovsk the Germans repudiated the armistice and in a week occupied the Baltic states, Belarus and Ukraine, which had signed the treaty as a separate entity. Now the Russians signed also. Hindenburg helped to force Kühlmann out in July 1918.[citation needed]
 In January more than half a million workers went on strike; among their demands was a peace without annexations. The strike collapsed when its leaders were arrested, the labor press suppressed, strikers in the reserve called for active duty, and seven great industrial concern taken under military control, which put their workers under martial law.[88] On 16 January Hindenburg demanded the replacement of Count von Valentini, the chief of the Civil Cabinet. The Kaiser bridled, responding ""I do not need your parental advice"",[89] but nonetheless fired his old friend. The Germans were unable to tender a plausible peace offer because OHL insisted on controlling Belgium and retaining the French coalfields. All of the Central Powers' cities were on the brink of starvation and their armies were on short rations. Hindenburg realized that ""empty stomachs prejudiced all higher impulses and tended to make men indifferent.""[90] He blamed his allies' hunger on poor organization and transportation, not realizing that the Germans would have enough to eat if they collected their harvest efficiently and rationed its distribution effectively.[91]
 German troops were in Finland, the Baltics, Poland, Belarus, Ukraine, much of Romania, the Crimea, and in a salient east of Ukraine extending east almost to the Volga and south into Georgia and Armenia. Hundreds of thousands of men were needed to hold and police these conquests. More Germans were in Macedonia and in Palestine, where the British were driving north; Falkenhayn was replaced by Otto Liman von Sanders, who had led the defense of Gallipoli. All Hindenburg required was that these fronts stand firm while the Germans won in the west, where now they outnumbered their opponents. He firmly believed that his opponents could be crushed by battlefield defeats regardless of their far superior resources.[citation needed]
 Offensive tactics were tailored to the defense. Their opponents were adopting defense in depth. He would attack the British because they were less skillful than the French.[93] The crucial blow would be in Flanders, along the River Lys, where the line was held by the Portuguese Army. However, winter mud prevented action there until April. Consequently, their first attack, named Michael, was on the southern part of the British line, at a projecting British salient near Saint-Quentin. Schwerpunkts would hit on either side of the salient's apex to pocket its defenders, the V Corps, as an overwhelming display of German power.[citation needed]
 Additional troops and skilled commanders, like von Hutier, were shifted from the east. Army Group von Gallwitz was formed in the west on 1 February. One quarter of the western divisions were designated for attack; to counter the elastic defense, during the winter each of them attended a four-week course on infiltration tactics.[94][page needed] Storm troops would slip through weak points in the front line and slice through the battle zone, bypassing strong points that would be mopped up by the mortars, flamethrowers, and manhandled field guns of the next wave. As always surprise was essential, so the artillery was slipped into attack positions at night, relying on camouflage for concealment; the British aerial photographers were allowed free rein before D-day. There would be no preliminary registration fire; the gunners were trained for map firing in schools established by Bruchmüller. In the short, intense bombardment each gun fired in a precise sequence, shifting back and forth between different targets, using many gas shells to keep defenders immersed in a toxic cloud. On D-day, the air force would establish air supremacy and strafe enemy strong points, and also update commanders on how far the attackers had penetrated. Signal lamps were used for messaging on the ground. Headquarters moved close to the front and as soon as possible would advance to pre-selected positions in newly occupied ground. OHL moved to Spa, Belgium while Hindenburg and Ludendorff were closer to the attack at Avesnes, France, which re-awoke his memories of occupied France 41 years before.[95]
 Operation Michael began on 21 March. The first day's reports were inconclusive, but by day two the Germans knew they had broken through some of the enemy artillery lines. But the encirclement failed because British stoutness gave their V Corps time to slip out of the targeted salient. On day four, German forces moved on into the open country, and the Kaiser prematurely celebrated by awarding Hindenburg the Grand Cross of the Iron Cross, a medal first created for von Blücher.[96] As usual, Hindenburg set objectives as the situation evolved. South of the salient, the Germans had almost destroyed the British Fifth Army, so they pushed west to cut between the French and British armies. However, they advanced too slowly through the broken terrain of the former Somme battlefields and the ground devastated when withdrawing the year before, and because troops stopped to loot food and clothing, and the Allies maintained a fluid defensive line, manned by troops brought up and supplied by rail and motor transport. Hindenburg hoped the Germans would get close enough to Amiens to bombard the railways with heavy artillery, but they were stopped just short, after having advanced a maximum of 65 km (40 mi). Hindenburg also hoped that civilian morale would crumble, because Paris was being shelled by naval guns mounted on rail carriages 120 km (75 mi) away, but he underestimated French resilience.[citation needed]
 The Allied command was dismayed. French headquarters realized: ""This much became clear from the terrible adventure, that our enemies were masters of a new method of warfare. ... What was even more serious was that it was perceived that the enemy's power was due to a thing that cannot be improvised, the training of officers and men.""[97]
 Prolonging Michael with the drive west delayed and weakened the attack in Flanders. Again the Germans broke through, smashing the Portuguese defenders and forcing the British from all of the ground they had paid so dearly for in 1917. However, French support enabled the British to save Hazebrouck, the rail junction that was the German goal. To draw the French reserves away from Flanders, the next attack was along the Aisne River where Nivelle had attacked the year before. Their success was dazzling. The defender's front was immersed in a gas cloud fired from simple mortars.[98] Within hours the Germans had reoccupied all the ground the French had taken by weeks of grinding, and they swept south through Champagne until they halted for resupply at the Marne River.[citation needed]
 However, the Germans had lost 977,555 of their best men between March and the end of July, while Allied ranks were swelling with Americans. Their dwindling stock of horses were on the verge of starvation, and the ragged troops thought continually of food. One of the most effective propaganda handbills, which the British showered on the German lines, listed the rations received by prisoners of war. The German troops resented their officers' better rations and reports of the ample meals at headquarters; in his memoirs, Ludendorff devotes six pages to defending officer's rations and perks.[99] After an attack, the survivors needed at least six weeks to recuperate, but now crack divisions were recommitted much sooner. Tens of thousands of men were skulking behind the lines. Determined to win, Hindenburg decided to expand the salient pointing toward Paris to strip more defenders from Flanders. The attack on Gouraud's French Fourth Army followed the now familiar scenario, but was met by a deceptive elastic defense and was decisively repelled at the French main line of resistance.[100] Hindenburg still intended to make a decisive attack in Flanders, but before the Germans could strike, the French and Americans, led by light tanks, smashed through the right flank of the German salient on the Marne. The German defense was halfhearted; they had lost. Hindenburg went on the defensive. The Germans withdrew one by one from the salients created by their victories, evacuating the wounded and supplies, and retiring to shortened lines. Hindenburg hoped to hold a line until their enemies were ready to bargain.[citation needed]
 After the retreat from the Marne, Ludendorff became distraught, shrieking orders and often in tears. At dinner on 19 July, he responded to a suggestion of Hindenburg's by shouting ""I have already told you that is impossible""—Hindenburg led him from the room.[101] On 8 August, the British completely surprised the Germans with a well-coordinated attack at Amiens, breaking well into the German lines. Most disquieting was that some German commanders surrendered their units and that reserves arriving at the front were taunted for prolonging the war. For Ludendorff, Amiens was the ""black day in the history of the German Army.""[102] Bauer and others wanted Ludendorff replaced, but Hindenburg stuck by his friend; he knew that ""Many a time has the soldier's calling exhausted strong characters.""[103] A sympathetic physician who was Ludendorff's friend persuaded him to leave headquarters temporarily to recuperate. (His breakdown is not mentioned in Hindenburg's or his own memoirs.) On 12 August, Army Group von Boehn was created to firm up the defenses in the Somme sector. On 29 September Hindenburg and Ludendorff told the incredulous Kaiser that the war was lost and that they must have an immediate armistice.[104]
 A new chancellor, Prince Maximilian of Baden, opened negotiations with President Woodrow Wilson, who would deal only with a democratic Germany. Prince Max told the Kaiser that he would resign unless Ludendorff was dismissed, but that Hindenburg was indispensable to hold the army together. On 26 October the Kaiser reprimanded Ludendorff before curtly accepting his resignation—then rejecting Hindenburg's.[105] Afterwards, Ludendorff refused to share Hindenburg's limousine.[106] Colonel Bauer was retired. Hindenburg promptly replaced Ludendorff with Wilhelm Groener.[107]
 The Germans were losing their allies. In June the Austro-Hungarians in Italy attacked the Entente lines along the Piave River but were repelled decisively. On 24 October the Italians crossed the river in the Battle of Vittorio Veneto. After a few days of resolute resistance the defense collapsed, weakened by the defection of men from the empire's subject nations and by starvation: the men in their Sixth Army had an average weight of 120 lb (54 kg).[108] On 14 October, Austria-Hungary asked for an armistice in Italy, but the fighting went on. In September the Entente and their Greek allies attacked in Macedonia. The Bulgarians begged for more Germans to stiffen their troops, but Hindenburg had none to spare. Many Bulgarian soldiers deserted as they retreated toward home, opening the road to Constantinople. The Austro-Hungarians were pushed back in Serbia, Albania and Montenegro, and signed an armistice on 3 November. The Ottomans were overextended, trying to defend Syria while exploiting the Russian collapse to move into the Caucasus, despite Hindenburg's urging them to defend what they had. The British and Arabs broke through in September, capturing Damascus. The Armistice of Mudros was signed on 30 October.[citation needed]
 Woodrow Wilson's 23 October diplomatic note to Germany had indirectly called for the Kaiser's abdication when it stated that the United States would negotiate only with representatives of the German people, not the monarchy.[109] Wilhelm, determined to lead the Army home in the event of disturbances in Berlin, refused to abdicate.[110] A week later, Admiral Franz von Hipper and Admiral Reinhard Scheer without authorization made plans to dispatch the Imperial Fleet on a last battle against the British. Sailors in Kiel mutinied and set up workers' and soldiers' councils that spread quickly across Germany, sparking the German revolution of 1918–1919.[111] On 8 November, Hindenburg and the Kaiser met with 39 regimental officers at Spa. There he delivered a situation report and answered questions.[112] Then Hindenburg left and Groener asked the officers to answer confidentially two questions about whether their troops would follow the Kaiser. The answers were decisive: the army would not. The Kaiser then agreed to abdicate without doing so at the time. In Berlin, however, Prince Max had already publicly announced the Kaiser's abdication and his own resignation, and that the Social Democrat leader Friedrich Ebert, was the new chancellor.[113] The Empire had crumbled all but bloodlessly. That evening Groener telephoned Ebert, whom he knew and trusted, and promised to support the new government, including with military force against revolutionaries on the left. In return Ebert promised that command of the troops would stay with the officer corps. Hindenburg remained head of the OHL to ensure and orderly return of the army.[114]
 The withdrawal became more fraught when the armistice obliged all German troops to leave Belgium, France, and Alsace-Lorraine in 14 days and to be behind the Rhine in 30 days. Stragglers would become prisoners. When the seven men from the executive committee of the soldiers' council formed at Spa arrived at OHL they were greeted politely by a lieutenant colonel, who acknowledged their leadership. When they broached the march home he took them to the map room, explaining allocation of roads, and scheduling unit departures, billeting, and feeding. They agreed that the existing staffs should make these arrangements.[115] To oversee the withdrawals OHL transferred headquarters from Belgium to Kassel in Germany, unsure how their officers would be received by the revolutionaries. They were greeted by the chairman of the workers' and soldiers' councils who proclaimed ""Hindenburg belongs to the German nation.""[116] His staff intended to billet him in the Kaiser's palace there, Wilhelmshöhe. Hindenburg refused because they did not have the Kaiser's permission, instead settling into a humble inn, thereby pleasing both his monarchist staff and the revolutionary masses. In the west 1.25 million men and 500,000 horses were brought home in the time allotted.[117]
 Hindenburg did not want to involve the Army in the defense of the new government against their civil enemies. Instead the Army supported the independent Freikorps (modeled on formations used in the Napoleonic wars), supplying them with weapons and equipment.[citation needed] In February 1919, OHL moved east to Kolberg to mount an offensive against impinging Soviet troops, but they were restrained by the Allied occupation administration, which in May 1919 ordered all German troops in the east home. On 25 June 1919, Hindenburg retired to Hanover once again. He settled in a splendid new villa, which was a gift of the city, despite his admittedly having ""lost the greatest war in history"".[118]
 ""Victory comes from movement"" was Schlieffen's principle for war.[119] Hindenburg expounded on Schlieffen's ideas as an instructor and later applied them during World War I. By employing such tactics, retreats and mobile defenses commanded by Hindenburg proved effective, and his Schwerpunkt attacks broke through the trench barrier on the Western Front. However, they failed to produce decisive victories because penetrating forces proved too slow to capitalize on their breakthroughs.[citation needed]
 Hindenburg has undergone a historical re-evaluation: his teaching of tactics and years on the General Staff have been less emphasised while he is remembered as a commander in Ludendorff's shadow. Winston Churchill in 1923, depicted Hindenburg as a figurehead awed by the mystique of the General Staff, concluding that ""Ludendorff throughout appears as the uncontested master.""[120] Parkinson stated that he is a ""beloved figurehead"",[121] while to Stallings he is ""an old military booby"".[122] These judgements stem from Ludendorff, who was famous during the war and immediately thereafter wrote his comprehensive memoir with himself center stage.[123][page needed] Hindenburg's less detailed memoir never disputed his colleague's claims, military decisions were made collectively not individually, and it is less useful to historians because it was written for general readers.[124] Ludendorff continued emphasising his preeminence in print,[125] which Hindenburg never disputed publicly.[citation needed]
 Others did though: the OHL officers who testified before the Reichstag committee investigating the collapse of 1918 agreed that Hindenburg was always in command.[126][127][128] He managed by setting objectives and appointing capable people to do their jobs, for instance ""giving full scope to the intellectual powers"" of Ludendorff.[129] These subordinates often felt he did little, even though he was setting the overall course. Ludendorff may have overrated himself, repressing repeated demonstrations that he lacked resilience essential to command.[130] Postwar he displayed poor judgment and an attraction to unusual ideas, contrasting with his former commander's adaptation to changing times.[citation needed]
 Most of their conferences were in private, but on 26 July 1918 the chief of staff of the Seventh Army, Fritz von Lossberg traveled to OHL to request permission to withdraw to a better position [131]
  Without knocking I entered Ludendorff's office and found him loudly arguing with the field marshal. I assumed it was over the situation at the Seventh Army. In any case as soon as I entered the field marshall asked me to give my assessment of the situation at the Seventh Army. I described it in short terms and emphasized especially that based on my own observations I thought the condition of the troops was cause for serious concern. For the past few days the Seventh Army commanding general, the staff, and I had all been recommending a withdrawal from the increasingly untenable front lines. I told Hindenburg that I had come to Avesenes with the concurrence of the Seventh Army commanding general to secure such an order. The field marshall turned to Ludendorff, saying something to the effect of 'Now Ludendorff, make sure that the order goes out immediately. ' He then left Ludendorff's office rather upset.  Hindenburg's record as a commander starting in the field at Tannenberg, then leading four national armies, culminating with breaking the trench deadlock in the west, and then holding a defeated army together, is unmatched by other soldiers in World War I.[citation needed] However, military skill is only one component of the record: ""... in general, the maladroit politics of Hindenburg and Ludendorff led directly to the collapse of 1918....""[132]
 Germany had not been allowed to take part in the negotiations that produced the Treaty of Versailles and received the terms as an accomplished fact on 7 May 1919. When in June the Weimar National Assembly, Germany's interim parliament, had still not accepted the treaty without demanding alterations, the Allies issued an ultimatum which included the threat of an invasion. Through General Groener, President Ebert asked Hindenburg whether the army was prepared to defend against an Allied invasion, which Ebert thought would be all but certain if the treaty were voted down. He promised to urge rejection of the treaty if there was even the slightest chance that the army could hold out. Under some prodding from Groener, Hindenburg concluded that the army could not resume the war under any circumstances, concluding with the statement, ""...as a soldier, I cannot help feeling that it were better to perish honorably than accept a disgraceful peace"". On 23 June, with just 19 minutes to spare, Ebert informed French premier Georges Clemenceau that Germany would ratify the treaty. It was signed on 28 June 1919.[133]
 Back in Hanover, as a field marshal he was provided with a staff that helped with his still extensive correspondence. He made few formal public appearances, but the streets around his house often were crowded with admirers when he took his afternoon walk. During the war he had left the newspaper reporters to Ludendorff; in retirement he was available. He hunted locally and elsewhere, including an annual chamois hunt in Bavaria. The yearly Tannenberg memorial celebration kept him in the public eye.[citation needed]
 A Berlin publisher urged him to produce his memoirs, which could educate and inspire by emphasizing his ethical and spiritual values. His story and ideas could be put on paper by a team of anonymous collaborators and the book would be translated immediately for the worldwide market.[134] Aus meinem Leben ('From My Life') was a huge bestseller, presenting to the world his carefully crafted image as a staunch, steadfast, uncomplicated soldier. Major themes were the need for Germany to maintain a strong military as the school teaching young German men moral values and the need to restore the monarchy, because only under the leadership of the House of Hohenzollern could Germany become great again, with ""the conviction that the subordination of the individual to the good of the community was not only a necessity, but a positive blessing ..."".[135] The Kaiser was treated throughout with great respect. Hindenburg concealed his cultural interests and assured his readers: ""It was against my inclination to take any interest in current politics.""[136] (This was in spite of what his intimates knew of his ""deep knowledge of Prussian political life"".)[137] Aus meinem Leben was dismissed by many military historians and critics as a boring apologia that skipped over the controversial issues, but it painted for the German public precisely the image he sought.[citation needed]
 In 1919, Hindenburg was subpoenaed to appear before the parliamentary commission investigating the responsibility for the outbreak of war in 1914 and for the defeat in 1918.[138] He was wary, as he had written: ""The only existing idol of the nation, undeservedly my humble self, runs the risk of being torn from its pedestal once it becomes the target of criticism.""[139] Ludendorff was also summoned. They had been strangers since Ludendorff's dismissal, but they prepared and arrived together on 18 November 1919. Hindenburg refused to take the oath until Ludendorff was permitted to read a statement that they were under no obligation to testify since their answers might expose them to criminal prosecution, but they were waiving their right of refusal. On the stand Hindenburg read through a prepared statement, ignoring the chairman's repeated demands that he answer questions. He testified that the German Army had been on the verge of winning the war in the autumn of 1918 and that the defeat had been precipitated by a stab in the back from disloyal elements on the home front and unpatriotic politicians, quoting a dinner conversation that Ludendorff had had with Sir Neill Malcolm.[140] When his reading was finished, Hindenburg walked out of the hearings despite being threatened with contempt, sure that they would not dare charge a war hero. His testimony gave additional weight to the stab-in-the-back myth, which was being adopted by nationalist and conservative politicians who sought to blame the socialist founders of the Weimar Republic for losing the war. Reviews in the German press that seriously misrepresented General Frederick Maurice's 1919 book about the last months of the war had been used by Ludendorff to convince Hindenburg that it was true.[141] A 1929 film glorifying his life as a dedicated patriot solidified his image.[142]
 Germany's first presidential election was constitutionally required to take place after the election for the first Reichstag, which was held on 6 June 1920. Hindenburg wrote to Wilhelm II, in exile in the Netherlands, for permission to run. Wilhelm approved, and on 8 March Hindenburg announced his intention to seek the presidency.[143] The Reichstag, however, repeatedly postponed the presidential election due to internal unrest and in October 1922 extended Ebert's term of office until 30 June 1925.[144] Hindenburg cut back on public appearances.[145]
 His serenity was shattered by the illness of his wife Gertrud, who died of cancer on 14 May 1921. He kept close to his three children, their spouses and his nine grandchildren. His son Oskar was at his side as the field marshal's liaison officer. Hindenburg was financially sustained by a fund set up by a group of admiring industrialists.[146]
 On 8 November 1923, Adolf Hitler, with Ludendorff at his side, launched the Beer Hall Putsch in Munich, which was suppressed by the Bavarian police. Hindenburg was not involved but inevitably was prominent in newspaper reports and issued a statement urging national unity.[147] At Tannenberg in August 1924 before a crowd of 50,000 Hindenburg laid the cornerstone for an imposing memorial at the site of the battle he had won in 1914.[147]
 President Ebert died on 28 February 1925. In the first round of the election to replace him, none of the candidates attained the required majority. For the second round, the Social Democrats, Catholic Centre Party and other democratic parties united to support the Centre's Wilhelm Marx. The Communist Party ran their own candidate, Ernst Thälmann. The parties on the right established a committee to select their strongest candidate. After a week's indecision they decided on Hindenburg, despite his advanced age and the fear, notably in Foreign Minister Gustav Stresemann, of unfavorable reactions by their former enemies. A delegation came to his home on 1 April. He stated his reservations but concluded, ""If you feel that my election is necessary for the sake of the Fatherland, I'll run, in God's name.""[148] Since some parties on the right still opposed him, he drafted a telegram declining the nomination, but before it was sent, Admiral Alfred von Tirpitz arrived in Hanover to persuade him to wait until the strength of his support was clearer. After his conservative opponents gave way, he consented on 9 April. Again he obtained Wilhelm II's approval. His campaign stressed his devotion to ""social justice, religious equality, genuine peace at home and abroad"".[149] He addressed only one public meeting, held in Hanover, and gave one radio address, on 11 April, calling for a ""national community"" (Volksgemeinschaft) under his leadership.[150] The second round required only a plurality to win, which Hindenburg obtained thanks to the support of the Bavarian People's Party (BVP), which had switched from Marx, and by the refusal of the Communists to withdraw their candidate.[151] In most of the world, including Great Britain, the victory of the aged field marshal was accepted with relative equanimity,[152][153] although there was a greater degree of apprehension in France.[154]
 Hindenburg took office on 12 May 1925 ""... offering [his] hand in this hour to every German"".[155] He moved into the elegant Presidential Palace on the Wilhelmstrasse, accompanied by his son Oskar (his military liaison officer) and Oskar's wife and their three children. The new president, always a stickler about uniforms, soon had the servants wearing new regalia with the shoe buckles appropriate for a court.[156] Nearby was the Reich chancellery, which during Hindenburg's tenure would have seven residents. He notified Chancellor Hans Luther that he would replace the head of Ebert's presidential staff, Dr. Otto Meissner, with his own choice, but Meissner was kept on and was Hindenburg's right-hand man throughout his presidency.[157]
 At his first meeting with Foreign Minister Stresemann, Hindenburg listened attentively and was persuaded that Stresemann's strategy of promoting friendly relations with the victors was correct.[158] He was cooler at their next, reacting to backlash from the right.[159] He nonetheless gave his support to the government's policy, and on 1 December 1925 the Locarno Treaties were signed. By agreeing to guarantee its post-war borders on the west, Germany took a significant step in restoring its position in Europe, although the German right was infuriated because the treaties accepted the loss of Alsace–Lorraine. Hindenburg countered their demands to restore the monarchy by arguing that the return of a Hohenzollern would block progress in revising Versailles.[160] He accepted the Republic as the mechanism for restoring Germany's position in Europe, although he was no Vernunftrepublikaner (republican by reason), since democracy was incompatible with the militaristic national community that would unite the people of Germany for future conflicts.[161]
 After the Luther government resigned over the signing of the Locarno Treaties, Hindenburg worked actively with the parties in the Reichstag to form a new coalition, an effort which took five weeks. The second Luther cabinet lasted just under four months and was followed by the return of Wilhelm Marx of the Centre Party as chancellor.[162] Stresemann stayed on as foreign minister in both cabinets.
 A major issue during Marx's chancellorship was whether the properties of the former rulers of the German states, including the Hohenzollerns, should be expropriated without compensation. Hindenburg felt that he could not speak out on the issue as president and talked of resigning so that he could express his opposition.[163] He nevertheless did not object when a private letter was made public in which he described the proposed referendum on the issue as a ""grave injustice"" that showed a ""deplorable lack of sense of tradition"" and ""gross ingratitude"".[164] The majority of those who voted in the referendum on expropriation favored the measure, but their numbers did not reach the voter participation threshold for it to pass. Hindenburg then urged the states, again under the threat of resigning, to reach fair settlements promptly, which for the most part they did.[165]
 The next crisis came in the autumn of 1926 when the chief of the army command, Hans von Seeckt, without first seeking government approval invited Prince Wilhelm, the grandson of the former emperor, to attend army maneuvres in uniform. It created a storm when the republican press publicized the transgression. Reichswehr Minister Otto Gessler told Hindenburg that Seeckt had to resign or he himself would. Since Gessler was supported by the cabinet, Hindenburg asked for Seeckt's resignation. In a painful final interview, Hindenburg emphasized that Seeckt had to go to keep the government from resigning, not because of his invitation to the Prince.[166] Seeckt's successor was Wilhelm Heye.
 Marx's government resigned after it was revealed that the Reichswehr, in violation of the Treaty of Versailles, was cooperating with the Red Army in producing poison gas and building a military aircraft factory in Soviet Russia.[167] The German Nationals (DNVP) agreed to join a revamped Marx cabinet, and a new government was in place on 31 January 1927. It legislated an overtime law[168] and introduced unemployment insurance.[169]
 On 18 September 1927 Hindenburg spoke at the dedication of the massive memorial at Tannenberg, outraging international opinion by denying Germany's responsibility for starting World War I as indicated in Article 231 of the Treaty of Versailles. He declared that Germany had entered the war as ""the means of self-assertion against a world full of enemies. Pure in heart we set off to the defence of the Fatherland and with clean hands the German army carried the sword.""[170] The Allied governments retaliated by not congratulating him on his eightieth birthday, although he was more upset by Ludendorff's refusal to have any contact at the ceremony. Most Germans did celebrate his birthday: his present was Neudeck, the ancestral East Prussian estate of the Hindenburgs, purchased with funds from a public subscription. Later it became known, or at least suspected, that the title was in Oskar's name to avoid a potential inheritance tax.[171]
 The Marx cabinet collapsed in February 1928. Hindenburg pressed it to promptly pass required legislation and then dissolved the Reichstag on 31 March 1928. His leadership during the crisis was widely applauded.[172] The Reichstag election on 20 May 1928 produced a shift to the left, although a handful of Nazis was elected as well. Hermann Müller of the SPD, whom Hindenburg found clever and agreeable, was appointed the new chancellor. Hindenburg later told Groener that Müller was his best chancellor.[173]
 The next crisis followed Stresemann's negotiation in 1929 of the Young Plan, which rescheduled reparations payments and opened the way for needed American loans. The right formed a committee to block adoption, which started by intensively lobbying Hindenburg using such powerful voices as that of Admiral Tirpitz. Hindenburg did not budge from his refusal to support the campaign.[174] The committee brought mainstream conservatives, such as the powerful newspaper owner Alfred Hugenberg, into alliance with the Nazis for the first time. Hugenberg called Hindenburg senile because of his opposition to the measure and claimed that he was a tool of the left and had no will of his own.[175] The committee submitted the issue to a national plebiscite, which failed because only 15% of eligible voters cast a ballot, far short of the 50% required.[176]
 In early 1930, the Müller government became embroiled in a dispute over how to pay for the rapidly rising costs of unemployment insurance. Müller asked Hindenburg to have his budget passed by presidential decree, which was allowed by Article 48 of the constitution, but Kurt von Schleicher persuaded Hindenburg to refuse.[177] Schleicher had for some time been planning a government in which the chancellor would be responsible to the president rather than the Reichstag. He thought that in such a presidential government the trained economist and leader of the Centre Party, Heinrich Brüning, would make an excellent chancellor and sounded him out on his willingness to take on the position under such terms.[178] Müller's government fell on 27 March 1930, and Hindenburg appointed Brüning chancellor. Brüning had hesitated because he lacked parliamentary support, but Hindenburg appealed to his sense of duty and threatened to resign if Brüning did not accept.[179] Only the four Social Democrats in the previous cabinet were replaced, resulting in what the press labeled the ""Hindenburg Cabinet"". The German historian Eberhard Jäckel concluded that presidential government was within the letter of the constitution but violated its spirit. Article 54 stated that the chancellor and his cabinet were responsible to the Reichstag, which made presidential government an end run around the constitution.[180]
 
Faced with declining tax revenues and mounting costs for unemployment insurance, Brüning introduced a deflationary austerity budget with major spending cuts and steep tax increases.[181] After his budget was defeated in the Reichstag in July 1930, Hindenburg signed it into law by invoking Article 48. The Reichstag voted to repeal the budget bill, so Hindenburg dissolved it just two years into its mandate and re-approved the budget using Article 48.[182] In the September 1930 election, the Nazis achieved an electoral breakthrough, gaining 18 percent of the vote to become the second strongest party in the Reichstag. The Communists made gains as well, moving up to third place with 13%. The SPD remained the strongest with 25% of the vote.[183] After the election, Brüning continued to govern largely through Article 48. His government was kept afloat by the Social Democrats, who voted against canceling his Article 48 bills in order to avoid another election that could only benefit the Nazis and the Communists.[184]
 Hindenburg met Adolf Hitler for the first time in October 1931 at a high-level conference in Berlin. Everyone present saw that they took an immediate dislike to one another. Afterwards Hindenburg privately often referred to Hitler as ""that Austrian corporal"", ""that Bohemian corporal"" or sometimes simply as ""the corporal"". He also ridiculed Hitler's Austrian accent.[185] Hitler for his part called Hindenburg ""that old fool"" or ""that old reactionary"" in private conversations.
 In foreign affairs Hindenburg spoke with hostility about Poland, often expressing a hope that it would disappear from the map of Europe ""at an appropriate moment"".[186]
 By January 1932, at the age of 84, Hindenburg was vacillating about running for a second term. Brüning recalled that once the president came to meet him at the railway station, but failed to recognize him.[187] On the other hand, Franz von Papen, a later chancellor, found that despite minor lapses with his short-term memory, the president remained competent until his last days.[188] Hindenburg was persuaded to run by the Kamarilla (his son Oskar, Groener, Meissner and Schleicher), and supported by the Centre Party, the Deutsche Volkspartei (DVP) and the Social Democratic Party of Germany (SPD), which regarded him as the only hope of defeating Hitler.[189] His fighting spirit was evoked by Nazi taunts when he appeared in public and in a few weeks three million Germans signed a petition urging him to carry on. Brüning proposed to the Reichstag that in light of the still-escalating economic disaster — now some of the largest banks had failed — the election should be postponed for two years, which would have required a two-thirds assent, to which the Nazis would never agree. Hitler was to be one of his opponents in the election. Hindenburg left most campaigning to others; in his single radio address he stressed the need for unity. ""I recall the spirit of 1914, and the mood at the front, which asked about the man, and not about his class or party.""[190] Hitler campaigned vigorously throughout Germany.[citation needed]
 In the first round of voting in March 1932, Hindenburg was front-runner, but failed to gain the required majority.[191] In the runoff the following month Hindenburg won with 53 percent of the vote. However, he was disappointed because he lost voters from the right, only winning by the support of those who had strongly opposed him seven years before. He wrote ""Despite all the blows in the neck I have taken, I will not abandon my efforts for a healthy move to the Right"".[192] He called in the party leaders for advice. During the meetings Meissner led the discussions while Hindenburg would only speak briefly on crucial points. Schleicher took the lead in choosing the cabinet, in which he was Reichswehr Minister. Groener was now even more unpopular to the right because he had banned wearing party uniforms in public. On 13 May 1932 Schleicher told Groener that he had ""lost the confidence of the Army"" and must resign at once.[193] Once Groener was gone, the ban was lifted and the Nazi brownshirts were back battling on the streets.[citation needed]
 To cope with mounting unemployment, Brüning desperately wanted an emergency decree to launch a program in which bankrupt estates would be carved up into small farms and turned over to unemployed settlers. When they met, Hindenburg read a statement that there would be no further decrees and insisted that the cabinet resign and that there must be a turn to the right. Brüning resigned on 1 June 1932. He was succeeded as chancellor by Papen from the Centre Party, who was Schleicher's choice; Hindenburg did not even ask the party leaders for advice. He was delighted with Papen, a rich, smooth aristocrat who had been a famous equestrian and a general staff officer; he soon became a Hindenburg family friend (Schleicher was no longer welcomed because he had quarreled with Oskar). The president was delighted to find that eight members of the new cabinet had served as officers during the war.[citation needed]
 The Social Democratic government of the State of Prussia was a caretaker, because it had lost its mandate in the preceding election. In his new role as chancellor, Papen accused it of failing to maintain public order, and removed it as the Prussian government on 20 July in what came to be known as the Prussian coup d'état. The national elections came eleven days later. Eight parties received substantial numbers of votes, but those supporting the government lost strength, while opponents on the right and left gained. The Nazis polled almost the same 37 percent they had in the presidential election, making them the largest party in the Reichstag. Schleicher negotiated with them, proposing that Hitler become vice-chancellor. Hitler demanded the chancellorship along with five cabinet positions and important posts in the state governments; additionally the Reichstag would have to pass an Enabling act giving a new government all needed powers, otherwise it would be dissolved. Around the country Nazi stormtroopers were running riot, attacking their political opponents. Hindenburg refused to make Hitler chancellor, so he met with Hitler to explain that he was unwilling to bring a single party to power, concluding with ""I want to extend my hand to you as a fellow soldier.""[194] The following morning he left for Neudeck; most of the newspapers praised his defense of the constitution. The constitution mandated a new election within sixty days, but owing to the crisis Hindenburg postponed it. Papen published an economic recovery plan that almost all of the parties and the labor unions lambasted. His scant support crumbled further.[citation needed]
 To add enough votes to gain a parliamentary mandate, Schleicher tried to persuade some of the Nazi leaders, like the war hero Hermann Göring, to defect and to take a position in his government. None of them would, so he became another presidential chancellor, still courting prominent Nazis—otherwise his days as chancellor were numbered. Papen continued to negotiate with Hitler, who moderated his conditions: he would settle for the chancellorship, the Reich Commissioner of Prussia and two cabinet positions: interior and a new slot for aviation. He also promised that he would respect the rights of the president, the Reichstag and the press, and Papen would be vice-chancellor. On these terms, Hindenburg allowed Oskar and Meissner to meet secretly with Hitler, culminating in an hour's tête-à-tête between Hitler and Oskar. Schleicher learned of the secret meeting and the following morning met with the president to demand emergency powers and the dissolution of the Reichstag. Hindenburg refused the powers but agreed to the election. Before a new government could be formed Hindenburg called General Werner von Blomberg, an opponent of Schleicher, back from a disarmament conference and appointed him Reichswehr minister, perhaps unaware that he was a Nazi sympathizer.[citation needed]
 To break the stalemate, Hindenburg proposed Hitler as chancellor, Papen as vice-chancellor and Reich commissioner of Prussia, and Göring as Prussian interior minister (who controlled the police). Two other cabinet ministers would be Nazis; the remaining eight would be from other parties. When Hindenburg met with Hitler, Papen would always be present. The new cabinet included only three Nazis: Hitler, Göring and Wilhelm Frick. Besides Hitler, Frick was the only Nazi with a portfolio; he held the nearly powerless Interior Ministry (unlike the rest of Europe, at the time the Interior Ministry had no power over the police, which was the responsibility of the Länder). Göring did not receive a portfolio, but critically was made Prussian interior minister, controlling the largest police force in which he promoted Nazis as commanders. Blomberg was Reichswehr minister, Hugenberg was both economics and agriculture minister, and Seldte (the leader of the first World War ex-servicemen's organization Der Stahlhelm) was labor minister. The other ministers were holdovers from the Papen and Schleicher cabinets.[citation needed]
 Hitler's first act as chancellor was to ask Hindenburg to dissolve the Reichstag, so that the Nazis and Deutschnationale Volkspartei (""German Nationalists"" or DNVP) could win an outright majority to pass the Enabling Act that would give the new government power to rule by decree, supposedly for the next four years. Unlike laws passed by Article 48, which could be cancelled by a majority in the Reichstag, under the Enabling Act the chancellor could pass laws by decree that could not be cancelled by a vote in the Reichstag. Hindenburg agreed to this request. In early February 1933, Papen asked for and received an Article 48 bill signed into law that sharply limited freedom of the press. After the Reichstag fire on 27 February, Hindenburg, at Hitler's urging, signed into law the Reichstag Fire Decree via Article 48, which effectively suspended all civil liberties in Germany. Göring as Prussian Interior Minister had enlisted thousands of Sturmabteilung (SA) men as auxiliary policemen, who attacked political opponents of the Nazis, with Communists and Social Democrats being singled out for particular abuse. Fritz Schäffer, a conservative Catholic and a leading politician of the Bavarian People's Party met Hindenburg on 17 February 1933 to complain about the ongoing campaign of terror against the SPD.[195] Schäffer told Hindenburg:
 We reject the notion that millions of Germans are not to be designated as national. The socialists served in the trenches and will serve in the trenches again. They voted for the banner of Hindenburg... I know many socialists who have earned acclaim for their service to Germany; I need only mention the name of Ebert.[192] Hindenburg, who had always hated the Social Democrats, rejected Schäffer's appeal, saying that the SPD were ""traitors"" who had ""stabbed the Fatherland in the back"" in 1918, and who could never belong to the volksgemeinschaft. Therefore, the Nazis had his full support in their campaign against the Social Democrats.[192] Hindenburg disliked Hitler, but he approved of his efforts to create the Volksgemeinschaft.[192] For Hindenburg, the ""Government of National Concentration"" headed by Hitler was the fulfillment of what he had been seeking since 1914, the creation of the Volksgemeinschaft.[192] Despite the ensuing anti-red hysteria, the Nazis received only 44% of the vote, though with the support of the DNVP they had a majority in the Reichstag.[citation needed]
 Hitler soon obtained Hindenburg's confidence, promising that after Germany regained full sovereignty, the monarchy would be restored; after a few weeks Hindenburg no longer asked Papen to join their meetings. The opening of the new Reichstag was celebrated with a Nazi extravaganza: Hindenburg descended into the crypt of the old garrison church in Potsdam to commune with the spirit of Frederick the Great at his grave, attended by Hitler who saluted the president as ""the custodian of the new rise of our people.""[196] An Enabling Act was prepared that transferred law-making from the Reichstag to the government, even if the new laws violated the constitution. With the Communist deputies and many Social Democrats kept out of the chamber (in violation of Articles 36 and 37 of the constitution), the Reichstag passed the act with well more than the needed two-thirds majority, effectively ending the Republic. As it turned out, that meeting took place in such an intimidating atmosphere that the Enabling Act would have garnered the required supermajority even with all deputies present and voting.[citation needed]
 During 1933 and 1934, Hitler was very aware that Hindenburg was the only check on his power. With the passage of the Enabling Act and the banning of all parties except the Nazis, Hindenburg's power to sack the chancellor was the only means by which Hitler could be legally removed from office. Given that Hindenburg was still a popular war hero and a revered figure in the Reichswehr, there was little doubt that the Reichswehr would side with Hindenburg if he ever decided to sack Hitler. Thus, as long as Hindenburg was alive, Hitler was always very careful to avoid offending him or the Army. Although Hindenburg was in increasingly bad health, the Nazis made sure that whenever Hindenburg did appear in public it was in Hitler's company. During these appearances, Hitler always made a point of showing him the utmost respect and deference.[citation needed]
 Economic austerity was abandoned as Hitler poured money into new programs hiring the unemployed, buying armaments, and building infrastructure—especially roads and autobahns.[197] Within a year, unemployment fell by almost 40%. Hitler gained the support of the armed forces by promising to rebuild their strength. The German states were taken over by the national government, the labor unions were suppressed, political opponents were imprisoned, and Jews were ejected from the civil service which included the universities. Hindenburg only objected about the treatment of Jews; he wanted war veterans retained, to which Hitler acceded. When Hitler moved to eject Hugenberg from the cabinet and to suppress the political parties, a trusted colleague of Hugenberg's was sent to Neudeck to appeal for assistance but only met with Oskar. Hindenburg delayed the appointment of one Nazi Gauleiter, but failed to obtain the installation of a Lutheran bishop he favored. The honor guard at Neudeck now were storm troopers. On 27 August at the stirring ceremonies at Tannenberg the president was presented with two large East Prussian properties near Neudeck. On the night before the plebiscite on Nazi rule scheduled for 11 November 1933, Hindenburg appealed to the voters to support their president and their chancellor, 95.1% of those voting did so. When a new commander of the army was to be appointed the president's choice won out over the chancellor's, but Hindenburg accepted a change in the military oath that eliminated obedience to the president and placed the swastika on military uniforms. By summer 1934, Hindenburg was dying of metastasized bladder cancer and his correspondence was dominated by complaints of Nazi stormtroopers running amok.[198]
 In the fall of 1933, a group of Hindenburg's friends led by General August von Cramon asked Hindenburg to restore the monarchy.[199] Hindenburg replied:
 Of course, I recognize your fidelity to our Kaiser, King and Lord without reservation. But precisely because I share this sentiment, I must urgently warn against the step you plan to take. ... The domestic crisis is not yet completely over, and foreign powers will have a hard time imagining me on the sidelines if it comes to a restoration of the monarchy. ... To say this is unbelievably painful for me.[199] During the summer of 1934, Hindenburg grew increasingly alarmed at Nazi excesses. With his support, Papen gave a speech at the University of Marburg on 17 June calling for an end to state terror and the restoration of some freedoms. When Propaganda Minister Joseph Goebbels got wind of it, he not only canceled a scheduled tape-delayed broadcast of the speech, but ordered the seizure of newspapers in which part of the text was printed.[200]
 Papen was furious, telling Hitler that he was acting as a ""trustee"" of Hindenburg, and that a ""junior minister"" like Goebbels had no right to silence him. He resigned and immediately notified Hindenburg about what happened. Hindenburg was equally outraged, and told Blomberg to give Hitler an ultimatum—unless Hitler took steps to end the growing tension in Germany and in the SA, Hindenburg would declare martial law and turn the government over to the army.[201] Not long afterward, Hitler carried out the Night of the Long Knives, in which the SA's leaders were murdered, for which he purportedly received Hindenburg's personal thanks in a telegram.[202] A day later, Hindenburg learned that Schleicher and his wife had been gunned down in their home. During the Nuremberg Trials, Göring admitted the telegram was never seen by Hindenburg, and was actually written by the Nazis.[203]
 Hindenburg remained in office until his death at the age of 86 from lung cancer at his home in Neudeck, East Prussia, on 2 August 1934. The day before, Hitler received word that Hindenburg was on his deathbed. He then had the cabinet pass the ""Law Concerning the Head of State of the German Reich,"" which stipulated that upon Hindenburg's death, the office of president would be abolished and its powers merged with those of the chancellor under the title of Führer und Reichskanzler (Leader and Chancellor of the Reich).[204]
 Three hours after Hindenburg's death, it was announced that as a result of this law, Hitler was now both Germany's head of state and head of government, thereby eliminating the last remedy by which he could be legally dismissed and cementing his status as the absolute dictator of Germany.[205] Publicly, Hitler announced that the presidency was ""inseparably united"" with Hindenburg, and it would not be appropriate for the title to ever be used again.[202]
 In truth, Hitler had known as early as April 1934 that Hindenburg would likely not survive the year. He worked feverishly to get the armed forces—the only group in Germany that would be nearly powerful enough to remove him with Hindenburg dead—to support his bid to become head of state after Hindenburg's death. In a meeting aboard the Deutschland on 11 April with Blomberg, army commander Werner von Fritsch and naval commander Erich Raeder, Hitler publicly proposed that he himself succeed Hindenburg. In return for the armed forces' support, he agreed to suppress the SA and promised that the armed forces would be the only bearers of arms in Germany under his watch. Raeder agreed right away, but Fritsch withheld his support until 16 May, when the senior generals unanimously agreed to back Hitler as Hindenburg's successor.[206]
 According to Günther von Tschirschky und Bögendorff, an interwar German diplomat and associate of Hindenburg who later defected to the United Kingdom, President Paul Von Hindenburg's last will and testament had criticised the Nazis and supported democracy. The defector said that it had also argued for the establishment of a constitutional monarchy with clear separation of powers along with the abolition of all forms of racial and religious discrimination. He alleged that the document had been handed over to Hitler by Hindenburg's Nazi supporting son. A few days after his death the Nazis released their own version of Hindenburg's final ""political testament"" which was complimentary of Hitler.[207]
 Hitler had a plebiscite held on 19 August 1934, in which the German people were asked if they approved of Hitler taking the office of Führer. The Ja (Yes) vote amounted to 90% of the vote. This referendum, as well as all efforts to make Hitler Hindenburg's successor, violated the Enabling Act. Although it gave Hitler the right to pass laws that were contrary to the constitution, it stated that the president's powers were to remain ""undisturbed"", which has long been interpreted to forbid any attempt to tamper with the presidency. The constitution had also previously been amended in 1932 to make the president of the High Court of Justice, not the chancellor, first in the line of succession to the presidency and even then only on an interim basis until fresh elections.[citation needed]
 Contrary to Hindenburg's will, he was interred with his wife in a magnificent ceremony at the Tannenberg Memorial. In 1944, as the Soviets approached, Generalleutnant Oskar von Hindenburg moved his parents' remains to western Germany. In January 1945, German troops blew up the memorial. In 1949, Polish authorities razed the site, leaving few traces. His remains were temporarily interred in Thuringia along with the remains of Frederick the Great, Frederick William I, the standards of the Imperial German Army from 1914 to 1918, the files of the Foreign Office, artworks from Prussian state museums, the library of Sanssouci and the Prussian crown jewels. By April 1945, the Monuments, Fine Arts, and Archives Section of the United States Army uncovered the remains and transported them to Marburg, where they were interred in St. Elizabeth's Church in Marburg, where they remain to this day. A plaque on his grave only commemorates the victims of war and violence, without mentioning Hindenburg's name.[208]
 Defunct
 Defunct
 On a visit to Hindenburg's headquarters, Crown Prince Wilhelm described the mood as family-like.[209] He reportedly had a good sense of humor and often made jokes at his own expense.[210] He also had a prodigious memory for names and faces, asking colleagues about their sons in the army, even recalling their ranks and units.[211]
 Despite this bonhomie, Hindenburg kept his own counsel. According to Kaiser Wilhelm II, ""Hindenburg never said more than half of what he really thought"".[212] When professor Hugo Vogel, commissioned to immortalize the victorious Tannenberg commanders in paint, arrived at headquarters most of his subjects begrudged posing.[213][214] Hindenburg visited most days, often staying for hours, which his staff attributed to ego, having no inkling that he and his wife collected paintings of the Virgin[215] nor that he was an amateur artist nor that he liked to discuss books—Schiller was his favorite author. After a painting was completed Hindenburg would periodically check on how many printed reproductions had been sold. Vogel was with him throughout the war and did his last portrait in 1934. Protecting his warrior image, Hindenburg wrote in his memoir that ""the artists were a distraction [with which] we would have preferred to dispense"".[216]
 After overseeing Germany's crushing victory at Tannenberg, Paul von Hindenburg became the center of a massive personality cult that persisted throughout his life. Henceforth, he was lauded as the living ideal of German masculinity and patriotism.[217] According to historian Anna Menge:
 The intensity, longevity, striking political and social breadth, and political deployment of the adulation for Hindenburg—in short, the power of the Hindenburg myth from 1914 until 1934 and beyond—was a political phenomenon of the first order....The Hindenburg myth was one of the central narratives in German public discourse during the First World War, the Weimar Republic, and the early years of Nazi rule. The striking polyvalence of the narrative—it extolled not only right-wing notions of authoritarian leadership but also more bi-partisan national values, such as salvaging something positive from war and defeat and self-affirmation in the face of crisis—meant that Hindenburg's myth could be deployed by different groups, at different times, and for different purposes. Although promoted first and foremost by German nationalists, especially in Weimar's early years, some elements of the Hindenburg myth had considerable cross-party appeal. That his initiation as a mythical figure rested on national defence and a battle fought against the arch-enemy of German Social Democracy, Tsarist Russia, had endeared him to many on the moderate left from 1914 onwards.[218] During World War I, the most celebrated tribute to Hindenburg was a 12 meter tall wooden likeness erected in Berlin. What admirers paid to drive in nails—ultimately 30 tons of them—went to war widows. Smaller versions were erected throughout Germany.[219] The wooden images and his photographs invariably portray a resolute, indomitable warrior, wearing a stern likeness.[citation needed]
 The famed zeppelin Hindenburg that was destroyed by fire in 1937 was named in his honor, as was the Hindenburgdamm, a causeway joining the island of Sylt to mainland Schleswig-Holstein that was built during his time in office. The previously Upper Silesian town of Zabrze (German: Hindenburg O.S.) was also renamed after him in 1915, as well as the SMS Hindenburg, a battlecruiser commissioned in the Imperial German Navy in 1917 and the last capital ship to enter service in the Imperial Navy. The Hindenburg Range in New Guinea, which includes perhaps one of the world's largest cliffs, the Hindenburg Wall, also bears his name.[citation needed]
 Historian Christopher Clark has criticized Hindenburg in his role as head of state for:
 withdrawing his solemn constitutional oaths of 1925 and 1932 to make common cause with the sworn enemies of the Republic. And then, having publicly declared that he would never consent to appoint Hitler to any post...levered the Nazi leader into the German Chancellery in January 1933. The Field Marshal had a high opinion of himself, and he doubtless sincerely believed that he personified a Prussian ""tradition"" of selfless service. But he was not, in truth, a man of tradition...As a military commander and later as Germany's head of state, Hindenburg broke virtually every bond he entered into. He was not the man of dogged, faithful service, but the man of image, manipulation and betrayal.[220] Hindenburg is a controversial figure in German history.[221] In recent years, numerous German local bodies have derecognized Hindenburg. In February 2020, Hindenburg's Berlin honorary citizenship had also been revoked.[222][223] The decision was passed by Berlin's left-wing coalition of Social Democrats, The Left and Greens.[224]
",paul ludwig han anton von beneckendorff und von hindenburg octob august german militari leader statesman led imperi german armi first world war later becam presid germani death play key role nazi seizur power appoint adolf hitler chancellor germani hindenburg born famili minor prussian nobil posen upon complet educ cadet enlist third regiment foot guard second lieuten saw combat war admit prestigi war academi berlin studi appoint gener staff corp promot major becam member german gener staff teach war academi hindenburg rose becom lieuten gener hindenburg retir world war began hindenburg recal achiev fame eastern front victor tannenberg oversaw crush victori russian made nation hero center pervas cult person popular risen point replac gener erich von falkenhayn chief great gener staff ultim deputi gener erich ludendorff exploit emperor wilhelm ii immens deleg power suprem armi command establish de facto militari dictatorship leadership germani secur russia defeat achiev largest advanc western front sinc conflict outbreak howev improv germani fortun revers armi decis defeat second battl marn alli hundr day offens follow armistic hindenburg step german armi chief staff retir hindenburg return public life becom second elect presid weimar republ oppos hitler nazi parti hindenburg nonetheless play major role instabl result rise power twice dissolv reichstag hindenburg agre januari appoint hitler chancellor coalit deutschnational volkspartei respons februari reichstag fire hindenburg approv reichstag fire decre suspend variou civil liberti likewis sign enabl act gave nazi regim emerg power hindenburg die follow year hitler combin presid chancelleri declar führer lit germani transform countri totalitarian state hindenburg born posen prussia son prussian junker han robert ludwig von beneckendorff und von hindenburg wife luis schwickart daughter physician karl ludwig schwickart wife juli moennich patern grandpar otto ludwig fadi von beneckendorff und von hindenburg remot descend illegitim daughter count heinrich vi waldeck wife eleonor von brederfadi clarif need hindenburg younger brother sister otto b ida b bernhard b famili lutheran protest evangel church prussia sinc includ calvinist lutheran parishion citat need hindenburg proud famili could trace ancestor back dual surnam adopt secur inherit appear formal document everyday life von beneckendorff clarif need true famili tradit father support famili infantri offic retir major summer visit grandfath hindenburg estat neudeck east prussia age paul enter cadet corp school wahlstatt legnicki pole poland transfer school berlin serv page widow king frederick william iv prussia graduat enter armi present king william ask father name rank becam second lieuten third regiment foot guard citat need war broke hindenburg wrote parent rejoic futur soldier war normal state thing fall honor beauti death decis battl königgrätz briefli knock unconsci bullet pierc helmet creas top skull quickli regain sens wrap head towel resum lead detach win decor battalion adjut war broke week march guard attack villag saint privat near metz climb gentl slope came heavi fire superior french rifl four hour prussian artilleri came blast french line infantri fill holi lust battl swept french line regiment suffer casualti becam regiment adjut guard spectat battl sedan follow month sat sieg line surround pari regiment elect repres palac versail german empir proclaim januari feet inch tall muscular frame strike blue eye impress figur french surrend watch afar suppress pari commun citat need pass highli competit entranc examin admiss kriegsakademi berlin three year studi grade high enough appoint gener staff promot captain assign staff ii corp marri intellig accomplish gertrud von sperl daughter gener oskar von sperl coupl would two daughter irmengard paulin annemaria one son oskar next command infantri compani men ethnic pole citat need transfer gener staff promot major section led count alfr von schlieffen student encircl battl like canna whose schlieffen plan propos pocket french armi five year hindenburg also taught tactic kriegsakademi maneuv met futur kaiser wilhelm ii met next year war game hindenburg command russian armi learn topographi lake sand barren east prussia annual great gener staff ride follow year move war ministri write field servic regul use heavi artilleri field engag use first world war becam lieuten colonel two year later promot colonel command infantri regiment becam chief staff viii corp citat need hindenburg becam equival british us brigadi gener promot lieuten gener equival receiv command infantri divis five year later made command iv corp base magdeburg gener infantri german equival rank annual maneuv taught maneuv larg forc defeat corp command kaiser schlieffen recommend chief gener staff lost helmuth von moltk retir make way younger men armi year includ year gener staff posit career hindenburg polit ambit remain staunch monarchist wwi broke hindenburg retir hannov august due purg german command follow russian success east prussia select war cabinet german suprem armi command oberst heeresleitung ohl assum command german eighth armi east prussia gener erich ludendorff chief staff eighth armi defeat russian armi gumbinnen found danger encircl russian armi gener alexand samsonov advanc south toward vistula river momentarili panick eighth armi command maximilian von prittwitz notifi ohl intent withdraw forc western prussia chief german gener staff generaloberst helmuth von moltk respond reliev prittwitz replac hindenburg upon arriv marienburg august hindenburg ludendorff met member armi staff led lieuten colonel max hoffmann expert russian armi hoffman inform plan shift part armi south attack expos left flank advanc russian second armi agre hoffman strategi hindenburg author ludendorff transfer armi south leav two cavalri brigad face russian first armi north hindenburg word line soldier defend germani border thin weak men defend home push hard second armi believ would cede ground gradual german reinforc continu mass invad russian flank ultim encircl annihil eve ensu battl hindenburg reportedli stroll close decay wall fortress knight prussia recal knight prussia defeat slav nearbi tannenberg night august hindenburg told staff gentlemen prepar well hand sleep soundli tonight day battl hindenburg reportedli watch hilltop forc weak center gradual gave ground sudden roar german gun right herald surpris attack russian flank ultim battl tannenberg result destruct russian armi russian captur togeth four hundr gun german casualti number accord british field marshal edmund ironsid greatest defeat suffer combat war recogn victori propaganda valu hindenburg suggest name battl tannenberg way aveng defeat inflict order teuton knight polish lithuanian knight even though fought nowher near field tannenberg decis victori hindenburg eighth armi face russian first armi hindenburg tactic spurn attack along front favor schwerpunkt sharp local hammer blow two schwerpunkt struck first battl masurian lake two column drove east breakthrough point pocket russian led gener paul von rennenkampf manag retreat km mi heavi loss first six week war russian lost men eight hundr thousand refuge abl return east prussian home thank victori strikingli contrast bloodi deadlock western front follow failur schlieffen plan citat need duo success perform eastern front mark begin militari polit partnership last end war hindenburg wrote kaiser month later ludendorff becom faith advis friend complet confid replac anyon despit strikingli dissimilar tempera older gener calm decis prove outstand fit ludendorff energi tactic ingenu ludendorff nerv twice drove consid chang plan tannenberg last minut time hindenburg talk privat confid waver east bank vistula poland russian mobil new armi shield attack river assembl would cross river march west silesia counter russian pend invas silesia hindenburg advanc poland occupi west bank vistula opposit russian forc mobil set headquart posen west prussia accompani ludendorff hoffmann russian attempt cross vistula german forc command held firm russian abl cross sector south hindenburg retreat destroy railway bridg russian would unabl advanc beyond km mi west short german frontier citat need novemb hindenburg appoint ober ost command east promot field marshal meet russian renew push silesia hindenburg move ninth armi rail north thorn reinforc two corp eighth armi novemb rage snowstorm forc surpris russian flank fierc battl łódź end immedi russian threat silesia also captur poland second largest citi citat need hindenburg argu still miser equip carri huge polish salient trap could snare cauldron southward pincer east prussia northward pincer galicia use motor vehicl speed even though russian outnumb german three one hindenburg point view overwhelm triumph could end war eastern front erich von falkenhayn chief germani great gener staff reject plan pipe dream nevertheless urg ludendorff hoffman hindenburg spent winter fight strategi badger kaiser press offic recruit notabl like kaiserin crown princ stab kaiser back kaiser compromis keep falkenhayn suprem command replac prussian war minist retali falkenhayn reassign hindenburg forc new armi group princ leopold bavaria transfer ludendorff new joint german southern armi hindenburg ludendorff react threaten resign therebi result ludendorff reinstat hindenburg command citat need follow return ludendorff provid hindenburg depress evalu alli armi alreadi lost mani profession offic driven much kingdom galicia lodomeria part poland meanwhil russian inexor push galicia toward hungari carpathian pass order falkenhayn contain resurg russian hindenburg mount unsuccess attack poland ninth armi well offens newli form tenth armi made local gain follow setback set temporari headquart insterburg made plan elimin russian remain toehold east prussia ensnar pincer movement tenth armi north eighth armi south attack launch februari hindenburg forc encircl entir corp captur men second battl masurian lake citat need shortli thereaft hindenburg ludendorff play key role central power offens fortress przemyśl fell march high command push joint strike russian right flank could potenti drive forc carpathian agre propos falkenhayn move ohl east castl pless form armi group von mackensen new german eleventh armi fourth armi field marshal august von mackensen broke russian line gorlic tarnów hindenburg ninth tenth armi launch diversionari attack threaten riga north one war success cavalri action three cavalri divis swept east courland barren sandi region near baltic coast cavalri gain held hindenburg new nieman armi name river citat need june suprem armi command order hindenburg launch offens poland toward narew river north warsaw hindenburg creat armi group gallwitz name command von gallwitz one mani abl command select hindenburg stay new armi headquart avail need berlin approv new armi group becam twelfth armi armi group broke russian line brief intens bombard direct lieuten colonel georg bruchmüller artilleri geniu recal medic retir oppos russian first armi casualti first five hour hindenburg often call bruchmüller russian withdrew across narev river howev steamrol frontal attack cost dearli august gallwitz lost men citat need russian withdrew polish salient falkenhayn insist pursu lithuania howev hindenburg ludendorff dissatisfi plan hindenburg would later claim saw pursuit pursuer get exhaust pursu june hindenburg nieman tenth armi spearhead attack courland attempt pocket defend ultim plan foil prudent command fifth russian armi defi order withdraw defens posit shield riga despit setback latvia hindenburg ludendorff continu rack victori eastern front august german storm novogeorgievsk fortress numer russian sourc call fall novogeorgievsk shame page histori russian imperi armi german tenth armi besieg kovno lithuanian citi nieman river defend circl fort fell august along gun almost million shell august forc consolid armi group hindenburg took citi grodno bitter street fight could trap retreat defend rail line lack capac bring need men occupi vilniu septemb halt ground favor defens line august german troop hindenburg use chlorin ga russian troop defend osowiec fortress russian demolish much osowiec withdrew august citat need octob hindenburg move headquart kovno respons conquer russian territori home three million peopl becam known ober ost troop built fortif eastern border ludendorff ruthless energi head civil govern use forc labor repair war damag dispatch use product like hog germani hindenburg reserv offic legal expert join staff write new legal code citat need baltic german own vast estat fete hindenburg hunt game preserv citat need hindenburg would later judg german oper unsatisfactori memoir recount russian bear escap clutch abandon polish salient shorten line substanti convers victori falkenhayn believ russian armi weaken blow suffer russia need serious consid danger forese futur russian replac experienc suprem command grand duke nichola nikolaevich man whose skill hindenburg held high regard tsar citat need spring central power experienc militari catastroph east left germani bear much war effort end hostil june russian armi began massiv offens along km mi southwestern front western ukrain ensu onslaught four armi command gener aleksei brusilov overwhelm entrench long regard impregn probe assault troop locat three weak spot struck forc nine day captur men gun push open countri citat need hindenburg command ober ost desper shore weak point soldier strip less threaten posit ludendorff distraught phone ohl gener wilhelm groener direct armi railroad competitor ludendorff gener staff sent evalu nerv judg satisfactori week russian kept attack lost men defend juli russian attack german line west riga ultim thwart look back russian offens hindenburg admit anoth attack scale feroc would left forc face menac complet collaps strength decim russian brusilov offens forc submit eastern front forc hindenburg command juli except archduk karl armi group southeast galicia gener han von seeckt chief staff gener von eichhorn took armi group hindenburg hindenburg ludendorff staff train equip advanc commun apparatu visit new forc threaten point form mix german unit format bolster sprinkl german offic offic exchang german armi train derelict citadel brest fortress refurbish headquart front almost km mi reserv cavalri brigad plu artilleri machin gunner west german hemorrhag battl verdun somm influenti armi offic led artilleri expert lieuten colonel max bauer friend ludendorff lobbi falkenhayn deplor futil steamrol verdun inflex defens along somm pack troop batter hail shell sack command lost trench german leader contrast falkenhayn bludgeon hindenburg deft parri tip point came falkenhayn order spoil attack bulgaria entent line macedonia fail heavi loss thu embolden romania declar war august ad train enemi invad hungarian transylvania falkenhayn adam romania would remain neutral kaiser deliber command falkenhayn said well herr field marshal desir courag take post hindenburg repli desir chancellor bethmann hollweg favor hindenburg suppos amen moder peac term mistak amiabl tractabl unawar intent enlarg prussia citat need hindenburg summon pless august name chief great gener staff extens suprem armi command ludendorff demand joint respons decis hindenburg demur henceforth ludendorff entrust sign order direct daili press report eastern front command leopold bavaria hoffmann chief staff hindenburg also appoint suprem war command central power nomin control six million men end war arrang form basi hindenburg leadership would come known third ohl citat need british unimpress gener charteri haig intellig chief wrote wife poor old hindenburg year age much convers german war cabinet impress swift credit old man hindenburg end verdun folli set motion brilliant conquest romania hindenburg ludendorff visit western front septemb meet armi command staff well leader crown princ rupprecht bavaria albrecht duke württemberg crown princ wilhelm prussia crown princ prussian chief staff command armi group rupprecht albrecht present field marshal baton hindenburg told must stand defens romania dealt meanwhil defens tactic must welcom backup defens line entent call hindenburg line would construct immedi ludendorff promis arm rupprecht delight two compet men replac dilettant bauer impress hindenburg saw everyth eye soldier field marshal hindenburg leadership german suprem armi command issu textbook defens warfar recommend fewer defend front line reli light machin gun push hard permit pull back defens organ penetr enemi forc found cut machin gun fire artilleri knew rang locat strong point subsequ infantri would counterattack attack artilleri blind unsur men reserv divis posit immedi behind line enter battl command divis whose posit penetr mobil defens also use world war ii respons reassign implement new tactic command took reserv order battl flexibl infantri platoon subdivid unit noncom citat need field offic visit headquart often invit speak hindenburg inquir problem recommend time especi curiou unit regard greatest evid confid place moral mental power armi smallest unit revis infantri field regul publish taught rank includ school divis command maneuv practic divis monthli period inform artilleri offic new develop last month british batter along somm produc fewer german casualti overal fierc obstin conflict somm last five month enemi press us back depth six mile stretch nearli mile thirteen new divis creat reduc number men infantri battalion divis artilleri command everi regiment western front creat assault unit stormtroop select fittest aggress men lieuten gener ernst von höppner given respons aerial antiaircraft forc armi vulner zeppelin went navi cavalri regiment dismount artilleri receiv badli need hors octob gener philipp pétain began seri limit attack verdun start intens bombard coordin artilleri command gener robert nivel doubl creep barrag led infantri shatter first german line attack stop repel counterattack repeat nibbl french retook ground german paid dearli nivel given command french armi citat need hindenburg day ohl began ludendorff discuss quickli agre done ludendorff would give staff offic assign hindenburg walk hour think chat guest confer ludendorff heard report department head met visitor work correspond noon ludendorff gave situat report kaiser unless import decis requir hindenburg took lunch person staff includ armi offic citat need dinner gener staff offic rank head alli leader politician industrialist scientist left tabl subdivid inform chat group ludendorff announc time return work junior offic summar daili report might confer ludendorff retir citat need hindenburg third ohl set ambiti benchmark arm product becam known hindenburg programm direct war offic gener groener major goal includ new light machin gun updat artilleri motor transport tank consid vulner artilleri increas output need skill worker armi releas million men total war suprem armi command want german men women enrol nation servic hindenburg also want univers close except medic train empti place would fill women swell next gener soldier want contracept ban bachelor tax polish armi form want jew exclud idea adopt polit maneuv vigor inept admir müller militari cabinet observ old hindenburg like ludendorff politician latter time hothead exampl women includ servic law ultim pass fact women alreadi seek employ open citat need follow death emperor franz joseph novemb hindenburg met successor charl frank hope stop fight hindenburg eastern front ran south baltic black sea baltic state ukrain romania itali line ran swiss border west adriat east venic macedonian front extend along greek border adriat aegean citat need line contest russian ottoman black caspian sea ran along height caucasu mountain hindenburg urg ottoman pull men height winter memoir would later alleg polici massacr armenian front palestin ran mediterranean southern end dead sea defend baghdad flank tigri river western front ran southward belgium near laon turn east pass verdun turn south end swiss border remain german enclav africa beyond reach attempt resuppli dirig fail central power surround outnumb citat need second quarter hindenburg ludendorff abl assembl men new divis provid adequ suppli new light machin gun field gun increas heavi tri foster fight spirit patriot instruct lectur film ensur fight kept agit croaker weakl meanwhil mitig risk attack buildup complet germani new militari leadership wage unrestrict submarin warfar alli ship claim would defeat british six month chancellor bethmann hollweg alli express opposit polici want bring unit state neutral war secur dutch danish border hindenburg announc unrestrict submarin warfar imper ludendorff ad voic januari chancellor forc bow unsound militari judgment citat need ohl move west pleasant spa town bad kreuznach southwest germani main rail line kaiser quarter spa build staff offic orang court other live hotel build februari third armi group form western front cover front command archduk albrecht württemberg effect divis east exchang less compet divis west sinc disast previou year russian infantri shown fight march revolut erupt russia shun opportun central power stay put hindenburg fear invad would resurrect heroic resist citat need western front third ohl deduc german armi huge salient valley somm laon obvious vulner pincer attack inde french plan new hindenburg line ran across base subsequ march hindenburg author oper alberich wherebi german forc order move inhabit portabl possess line process destroy everi build level road bridg cut everi tree foul everi well burn everi combust day german withdrew area ground lost alli offens sinc cautious follow alli also cope boobi trap explod month later new german front call hindenburg line km mi shorter german divis citat need april british attack arra overtook two german line occupi part third canadian swept german complet vimi ridg excit ludendorff becam distraught develop hindenburg reportedli calm first press hand assur live critic time today togeth ultim british tri exploit open futil cavalri charg press battl aftermath third ohl discov one reason behind british attack success sixth armi command ludwig von falkenhausen fail properli appli instruct defens depth keep reserv troop far back front line result failur falkenhausen along sever staff offic strip command romanov dynasti fall power russia remain war new revolutionari govern led alexand kerenski kerenski offens launch juli russian armi push forc galicia juli order counter success six german divis mount counterattack juli tore hole russian front slice southward toward tarnopol ensu german advanc threaten encircl russian attack therebi caus retreat end august advanc central power stop frontier moldavia keep pressur seiz ground intend keep hindenburg shift north heavili fortifi citi riga today latvia broad dvina river moat septemb eighth armi led oskar von hutier attack bruchmüller bombard includ ga smoke shell drove defend far bank east citi german cross barg bridg river immedi press forward baltic coast pocket defend riga salient next joint oper navi seiz oesel two smaller island gulf riga bolshevik revolut took russia war armistic sign decemb citat need hindenburg detest chancellor bethmann hollweg argu unrestrict submarin warfar juli reichstag debat resolut peac without annex indemn colonel bauer crown princ hurri berlin block move minist war urg hindenburg ludendorff join arriv kaiser told could justif presenc berlin return hast headquart certainli would much better occupi letter emperor date juli ludendorff threaten resign hindenburg join ultimatum kaiser declin accept major parti reichstag saw bethmann hollweg unaccept negoti peac chancellor long weak deal suprem armi command crisi resolv bethmann hollweg voluntarili resign ludendorff bauer want replac kaiser chancellor dictat hindenburg would agre juli reichstag pass resolut call peac understand without territori acquisit achiev forc violat polit econom financi integr new chancellor georg micha agre interpret polici peac resolut therefor stillborn micha resolut becam advantag august pope benedict xv call peac german respons cite resolut finess specif question like futur belgium industrialist oppos groener advocaci excess profit tax insist worker take part compani manag ludendorff reliev groener telegram sent command divis citat need hindenburg birthday celebr lavishli germani octob public holiday honor reserv kaiser hindenburg publish birthday manifesto end word god help german strength withstood tremend attack enemi one gave gladli must stay end thank god bloodi battlefield take thought war bring despond rank strengthen hope enemi trust germani achiev need stand safe time trust german oak given air light free growth muscl tens nerv steel eye front see us aim germani honor free great god us end bavarian mountain warfar expert von dellmensingen sent assess defens itali found poor scout site attack could mount italian hindenburg creat new fourteenth armi ten seven german divis enough airplan control air command otto von attack slip undetect mountain opposit open soča valley attack began night defend trench valley abruptli shroud dens cloud poison ga releas canist fire simultan simpl mortar defend fled mask would fail artilleri open fire sever hour later hit italian reinforc hasten fill gap attack swept almost empti defens march pass mountain troop clear height either side italian fled west fast cut entent divis rush itali stem retreat hold line piav river armi dissolv german divis return western front octob pétain direct success limit object attack six day care plan bombard left pathway tank lead infantri forward lassaux plateau south laon forc german entir french armi recov citat need negoti soviet russia hindenburg want retain control russian territori central power occupi german grand duke rule courland lithuania well larg slice poland polish plan oppos foreign minist richard von kühlmann encourag kaiser listen view max hoffmann chief staff eastern front hoffmann demur order argu would mistak bring mani slav germani small slice poland need improv defens ludendorff outrag kaiser consult subordin hindenburg complain kaiser disregard opinion matter vital import kaiser back would approv ludendorff order remov hoffmann even mention hindenburg memoir soviet refus term offer german repudi armistic week occupi baltic state belaru ukrain sign treati separ entiti russian sign also hindenburg help forc kühlmann juli citat need januari half million worker went strike among demand peac without annex strike collaps leader arrest labor press suppress striker reserv call activ duti seven great industri concern taken militari control put worker martial law januari hindenburg demand replac count von valentini chief civil cabinet kaiser bridl respond need parent advic nonetheless fire old friend german unabl tender plausibl peac offer ohl insist control belgium retain french coalfield central power citi brink starvat armi short ration hindenburg realiz empti stomach prejud higher impuls tend make men indiffer blame alli hunger poor organ transport realiz german would enough eat collect harvest effici ration distribut effect german troop finland baltic poland belaru ukrain much romania crimea salient east ukrain extend east almost volga south georgia armenia hundr thousand men need hold polic conquest german macedonia palestin british drive north falkenhayn replac otto liman von sander led defens gallipoli hindenburg requir front stand firm german west outnumb oppon firmli believ oppon could crush battlefield defeat regardless far superior resourc citat need offens tactic tailor defens oppon adopt defens depth would attack british less skill french crucial blow would flander along river ly line held portugues armi howev winter mud prevent action april consequ first attack name michael southern part british line project british salient near schwerpunkt would hit either side salient apex pocket defend v corp overwhelm display german power citat need addit troop skill command like von hutier shift east armi group von gallwitz form west februari one quarter western divis design attack counter elast defens winter attend cours infiltr tactic page need storm troop would slip weak point front line slice battl zone bypass strong point would mop mortar flamethrow manhandl field gun next wave alway surpris essenti artilleri slip attack posit night reli camouflag conceal british aerial photograph allow free rein would preliminari registr fire gunner train map fire school establish bruchmüller short intens bombard gun fire precis sequenc shift back forth differ target use mani ga shell keep defend immers toxic cloud air forc would establish air supremaci strafe enemi strong point also updat command far attack penetr signal lamp use messag ground headquart move close front soon possibl would advanc posit newli occupi ground ohl move spa belgium hindenburg ludendorff closer attack avesn franc memori occupi franc year oper michael began march first day report inconclus day two german knew broken enemi artilleri line encircl fail british stout gave v corp time slip target salient day four german forc move open countri kaiser prematur celebr award hindenburg grand cross iron cross medal first creat von blücher usual hindenburg set object situat evolv south salient german almost destroy british fifth armi push west cut french british armi howev advanc slowli broken terrain former somm battlefield ground devast withdraw year troop stop loot food cloth alli maintain fluid defens line man troop brought suppli rail motor transport hindenburg hope german would get close enough amien bombard railway heavi artilleri stop short advanc maximum km mi hindenburg also hope civilian moral would crumbl pari shell naval gun mount rail carriag km mi away underestim french resili citat need alli command dismay french headquart realiz much becam clear terribl adventur enemi master new method warfar even seriou perceiv enemi power due thing improvis train offic men prolong michael drive west delay weaken attack flander german broke smash portugues defend forc british ground paid dearli howev french support enabl british save hazebrouck rail junction german goal draw french reserv away flander next attack along aisn river nivel attack year success dazzl defend front immers ga cloud fire simpl mortar within hour german reoccupi ground french taken week grind swept south champagn halt resuppli marn river citat need howev german lost best men march end juli alli rank swell american dwindl stock hors verg starvat rag troop thought continu food one effect propaganda handbil british shower german line list ration receiv prison war german troop resent offic better ration report ampl meal headquart memoir ludendorff devot six page defend offic ration perk attack survivor need least six week recuper crack divis recommit much sooner ten thousand men skulk behind line determin win hindenburg decid expand salient point toward pari strip defend flander attack gouraud french fourth armi follow familiar scenario met decept elast defens decis repel french main line resist hindenburg still intend make decis attack flander german could strike french american led light tank smash right flank german salient marn german defens halfheart lost hindenburg went defens german withdrew one one salient creat victori evacu wound suppli retir shorten line hindenburg hope hold line enemi readi bargain citat need retreat marn ludendorff becam distraught shriek order often tear dinner juli respond suggest hindenburg shout alreadi told imposs led room august british complet surpris german attack amien break well german line disquiet german command surrend unit reserv arriv front taunt prolong war ludendorff amien black day histori german armi bauer other want ludendorff replac hindenburg stuck friend knew mani time soldier call exhaust strong charact sympathet physician ludendorff friend persuad leav headquart temporarili recuper breakdown mention hindenburg memoir august armi group von boehn creat firm defens somm sector septemb hindenburg ludendorff told incredul kaiser war lost must immedi armistic new chancellor princ maximilian baden open negoti presid woodrow wilson would deal democrat germani princ max told kaiser would resign unless ludendorff dismiss hindenburg indispens hold armi togeth octob kaiser reprimand ludendorff curtli accept reject hindenburg afterward ludendorff refus share hindenburg limousin colonel bauer retir hindenburg promptli replac ludendorff wilhelm groener german lose alli june itali attack entent line along piav river repel decis octob italian cross river battl vittorio veneto day resolut resist defens collaps weaken defect men empir subject nation starvat men sixth armi averag weight lb kg octob ask armistic itali fight went septemb entent greek alli attack macedonia bulgarian beg german stiffen troop hindenburg none spare mani bulgarian soldier desert retreat toward home open road constantinopl push back serbia albania montenegro sign armistic novemb ottoman overextend tri defend syria exploit russian collaps move caucasu despit hindenburg urg defend british arab broke septemb captur damascu armistic mudro sign octob citat need woodrow wilson octob diplomat note germani indirectli call kaiser abdic state unit state would negoti repres german peopl monarchi wilhelm determin lead armi home event disturb berlin refus abdic week later admir franz von hipper admir reinhard scheer without author made plan dispatch imperi fleet last battl british sailor kiel mutini set worker soldier council spread quickli across germani spark german revolut novemb hindenburg kaiser met regiment offic spa deliv situat report answer question hindenburg left groener ask offic answer confidenti two question whether troop would follow kaiser answer decis armi would kaiser agre abdic without time berlin howev princ max alreadi publicli announc kaiser abdic resign social democrat leader friedrich ebert new chancellor empir crumbl bloodlessli even groener telephon ebert knew trust promis support new govern includ militari forc revolutionari left return ebert promis command troop would stay offic corp hindenburg remain head ohl ensur orderli return armi withdraw becam fraught armistic oblig german troop leav belgium franc day behind rhine day straggler would becom prison seven men execut committe soldier council form spa arriv ohl greet polit lieuten colonel acknowledg leadership broach march home took map room explain alloc road schedul unit departur billet feed agre exist staff make arrang overse withdraw ohl transfer headquart belgium kassel germani unsur offic would receiv revolutionari greet chairman worker soldier council proclaim hindenburg belong german nation staff intend billet kaiser palac wilhelmshöh hindenburg refus kaiser permiss instead settl humbl inn therebi pleas monarchist staff revolutionari mass west million men hors brought home time allot hindenburg want involv armi defens new govern civil enemi instead armi support independ freikorp model format use napoleon war suppli weapon equip citat need februari ohl move east kolberg mount offens imping soviet troop restrain alli occup administr may order german troop east home june hindenburg retir hanov settl splendid new villa gift citi despit admittedli lost greatest war histori victori come movement schlieffen principl war hindenburg expound schlieffen idea instructor later appli world war employ tactic retreat mobil defens command hindenburg prove effect schwerpunkt attack broke trench barrier western front howev fail produc decis victori penetr forc prove slow capit breakthrough citat need hindenburg undergon histor teach tactic year gener staff less emphasis rememb command ludendorff shadow winston churchil depict hindenburg figurehead awe mystiqu gener staff conclud ludendorff throughout appear uncontest master parkinson state belov figurehead stall old militari boobi judgement stem ludendorff famou war immedi thereaft wrote comprehens memoir center stage page need hindenburg less detail memoir never disput colleagu claim militari decis made collect individu less use historian written gener reader ludendorff continu emphasis preemin print hindenburg never disput publicli citat need other though ohl offic testifi reichstag committe investig collaps agre hindenburg alway command manag set object appoint capabl peopl job instanc give full scope intellectu power ludendorff subordin often felt littl even though set overal cours ludendorff may overr repress repeat demonstr lack resili essenti command postwar display poor judgment attract unusu idea contrast former command adapt chang time citat need confer privat juli chief staff seventh armi fritz von lossberg travel ohl request permiss withdraw better posit without knock enter ludendorff offic found loudli argu field marshal assum situat seventh armi case soon enter field marshal ask give assess situat seventh armi describ short term emphas especi base observ thought condit troop caus seriou concern past day seventh armi command gener staff recommend withdraw increasingli unten front line told hindenburg come avesen concurr seventh armi command gener secur order field marshal turn ludendorff say someth effect ludendorff make sure order goe immedi left ludendorff offic rather upset hindenburg record command start field tannenberg lead four nation armi culmin break trench deadlock west hold defeat armi togeth unmatch soldier world war citat need howev militari skill one compon record gener maladroit polit hindenburg ludendorff led directli collaps germani allow take part negoti produc treati versail receiv term accomplish fact may june weimar nation assembl germani interim parliament still accept treati without demand alter alli issu ultimatum includ threat invas gener groener presid ebert ask hindenburg whether armi prepar defend alli invas ebert thought would certain treati vote promis urg reject treati even slightest chanc armi could hold prod groener hindenburg conclud armi could resum war circumst conclud statement soldier help feel better perish honor accept disgrac peac june minut spare ebert inform french premier georg clemenceau germani would ratifi treati sign june back hanov field marshal provid staff help still extens correspond made formal public appear street around hous often crowd admir took afternoon walk war left newspap report ludendorff retir avail hunt local elsewher includ annual chamoi hunt bavaria yearli tannenberg memori celebr kept public eye citat need berlin publish urg produc memoir could educ inspir emphas ethic spiritu valu stori idea could put paper team anonym collabor book would translat immedi worldwid market au meinem leben life huge bestsel present world care craft imag staunch steadfast uncompl soldier major theme need germani maintain strong militari school teach young german men moral valu need restor monarchi leadership hous hohenzollern could germani becom great convict subordin individu good commun necess posit bless kaiser treat throughout great respect hindenburg conceal cultur interest assur reader inclin take interest current polit spite intim knew deep knowledg prussian polit life au meinem leben dismiss mani militari historian critic bore apologia skip controversi issu paint german public precis imag sought citat need hindenburg subpoena appear parliamentari commiss investig respons outbreak war defeat wari written exist idol nation undeservedli humbl self run risk torn pedest becom target critic ludendorff also summon stranger sinc ludendorff dismiss prepar arriv togeth novemb hindenburg refus take oath ludendorff permit read statement oblig testifi sinc answer might expos crimin prosecut waiv right refus stand hindenburg read prepar statement ignor chairman repeat demand answer question testifi german armi verg win war autumn defeat precipit stab back disloy element home front unpatriot politician quot dinner convers ludendorff sir neill malcolm read finish hindenburg walk hear despit threaten contempt sure would dare charg war hero testimoni gave addit weight myth adopt nationalist conserv politician sought blame socialist founder weimar republ lose war review german press serious misrepres gener frederick mauric book last month war use ludendorff convinc hindenburg true film glorifi life dedic patriot solidifi imag germani first presidenti elect constitut requir take place elect first reichstag held june hindenburg wrote wilhelm ii exil netherland permiss run wilhelm approv march hindenburg announc intent seek presid reichstag howev repeatedli postpon presidenti elect due intern unrest octob extend ebert term offic june hindenburg cut back public appear seren shatter ill wife gertrud die cancer may kept close three children spous nine grandchildren son oskar side field marshal liaison offic hindenburg financi sustain fund set group admir industrialist novemb adolf hitler ludendorff side launch beer hall putsch munich suppress bavarian polic hindenburg involv inevit promin newspap report issu statement urg nation uniti tannenberg august crowd hindenburg laid cornerston impos memori site battl presid ebert die februari first round elect replac none candid attain requir major second round social democrat cathol centr parti democrat parti unit support centr wilhelm marx communist parti ran candid ernst thälmann parti right establish committe select strongest candid week indecis decid hindenburg despit advanc age fear notabl foreign minist gustav stresemann unfavor reaction former enemi deleg came home april state reserv conclud feel elect necessari sake fatherland run god name sinc parti right still oppos draft telegram declin nomin sent admir alfr von tirpitz arriv hanov persuad wait strength support clearer conserv oppon gave way consent april obtain wilhelm ii approv campaign stress devot social justic religi equal genuin peac home abroad address one public meet held hanov gave one radio address april call nation commun volksgemeinschaft leadership second round requir plural win hindenburg obtain thank support bavarian peopl parti bvp switch marx refus communist withdraw candid world includ great britain victori age field marshal accept rel equanim although greater degre apprehens franc hindenburg took offic may offer hand hour everi german move eleg presidenti palac wilhelmstrass accompani son oskar militari liaison offic oskar wife three children new presid alway stickler uniform soon servant wear new regalia shoe buckl appropri court nearbi reich chancelleri hindenburg tenur would seven resid notifi chancellor han luther would replac head ebert presidenti staff otto meissner choic meissner kept hindenburg man throughout presid first meet foreign minist stresemann hindenburg listen attent persuad stresemann strategi promot friendli relat victor correct cooler next react backlash right nonetheless gave support govern polici decemb locarno treati sign agre guarante border west germani took signific step restor posit europ although german right infuri treati accept loss hindenburg counter demand restor monarchi argu return hohenzollern would block progress revis versail accept republ mechan restor germani posit europ although vernunftrepublikan republican reason sinc democraci incompat militarist nation commun would unit peopl germani futur conflict luther govern resign sign locarno treati hindenburg work activ parti reichstag form new coalit effort took five week second luther cabinet last four month follow return wilhelm marx centr parti chancellor stresemann stay foreign minist cabinet major issu marx chancellorship whether properti former ruler german state includ hohenzollern expropri without compens hindenburg felt could speak issu presid talk resign could express opposit nevertheless object privat letter made public describ propos referendum issu grave injustic show deplor lack sens tradit gross ingratitud major vote referendum expropri favor measur number reach voter particip threshold pass hindenburg urg state threat resign reach fair settlement promptli part next crisi came autumn chief armi command han von seeckt without first seek govern approv invit princ wilhelm grandson former emperor attend armi maneuvr uniform creat storm republican press public transgress reichswehr minist otto gessler told hindenburg seeckt resign would sinc gessler support cabinet hindenburg ask seeckt resign pain final interview hindenburg emphas seeckt go keep govern resign invit princ seeckt successor wilhelm hey marx govern resign reveal reichswehr violat treati versail cooper red armi produc poison ga build militari aircraft factori soviet russia german nation dnvp agre join revamp marx cabinet new govern place januari legisl overtim law introduc unemploy insur septemb hindenburg spoke dedic massiv memori tannenberg outrag intern opinion deni germani respons start world war indic articl treati versail declar germani enter war mean world full enemi pure heart set defenc fatherland clean hand german armi carri sword alli govern retali congratul eightieth birthday although upset ludendorff refus contact ceremoni german celebr birthday present neudeck ancestr east prussian estat hindenburg purchas fund public subscript later becam known least suspect titl oskar name avoid potenti inherit tax marx cabinet collaps februari hindenburg press promptli pass requir legisl dissolv reichstag march leadership crisi wide applaud reichstag elect may produc shift left although hand nazi elect well hermann müller spd hindenburg found clever agreeabl appoint new chancellor hindenburg later told groener müller best chancellor next crisi follow stresemann negoti young plan reschedul repar payment open way need american loan right form committe block adopt start intens lobbi hindenburg use power voic admir tirpitz hindenburg budg refus support campaign committe brought mainstream conserv power newspap owner alfr hugenberg allianc nazi first time hugenberg call hindenburg senil opposit measur claim tool left committe submit issu nation plebiscit fail elig voter cast ballot far short requir earli müller govern becam embroil disput pay rapidli rise cost unemploy insur müller ask hindenburg budget pass presidenti decre allow articl constitut kurt von schleicher persuad hindenburg refus schleicher time plan govern chancellor would respons presid rather reichstag thought presidenti govern train economist leader centr parti heinrich brüning would make excel chancellor sound willing take posit term müller govern fell march hindenburg appoint brüning chancellor brüning hesit lack parliamentari support hindenburg appeal sens duti threaten resign brüning accept four social democrat previou cabinet replac result press label hindenburg cabinet german historian eberhard jäckel conclud presidenti govern within letter constitut violat spirit articl state chancellor cabinet respons reichstag made presidenti govern end run around constitut face declin tax revenu mount cost unemploy insur brüning introduc deflationari auster budget major spend cut steep tax increas budget defeat reichstag juli hindenburg sign law invok articl reichstag vote repeal budget bill hindenburg dissolv two year mandat budget use articl septemb elect nazi achiev elector breakthrough gain percent vote becom second strongest parti reichstag communist made gain well move third place spd remain strongest vote elect brüning continu govern larg articl govern kept afloat social democrat vote cancel articl bill order avoid anoth elect could benefit nazi communist hindenburg met adolf hitler first time octob confer berlin everyon present saw took immedi dislik one anoth afterward hindenburg privat often refer hitler austrian corpor bohemian corpor sometim simpli corpor also ridicul hitler austrian accent hitler part call hindenburg old fool old reactionari privat convers foreign affair hindenburg spoke hostil poland often express hope would disappear map europ appropri moment januari age hindenburg vacil run second term brüning recal presid came meet railway station fail recogn hand franz von papen later chancellor found despit minor laps memori presid remain compet last day hindenburg persuad run kamarilla son oskar groener meissner schleicher support centr parti deutsch volkspartei dvp social democrat parti germani spd regard hope defeat hitler fight spirit evok nazi taunt appear public week three million german sign petit urg carri brüning propos reichstag light econom disast largest bank fail elect postpon two year would requir assent nazi would never agre hitler one oppon elect hindenburg left campaign other singl radio address stress need uniti recal spirit mood front ask man class parti hitler campaign vigor throughout germani citat need first round vote march hindenburg fail gain requir major runoff follow month hindenburg percent vote howev disappoint lost voter right win support strongli oppos seven year wrote despit blow neck taken abandon effort healthi move right call parti leader advic meet meissner led discuss hindenburg would speak briefli crucial point schleicher took lead choos cabinet reichswehr minist groener even unpopular right ban wear parti uniform public may schleicher told groener lost confid armi must resign groener gone ban lift nazi brownshirt back battl street citat need cope mount unemploy brüning desper want emerg decre launch program bankrupt estat would carv small farm turn unemploy settler met hindenburg read statement would decre insist cabinet resign must turn right brüning resign june succeed chancellor papen centr parti schleicher choic hindenburg even ask parti leader advic delight papen rich smooth aristocrat famou equestrian gener staff offic soon becam hindenburg famili friend schleicher longer welcom quarrel oskar presid delight find eight member new cabinet serv offic war citat need social democrat govern state prussia caretak lost mandat preced elect new role chancellor papen accus fail maintain public order remov prussian govern juli came known prussian coup nation elect came eleven day later eight parti receiv substanti number vote support govern lost strength oppon right left gain nazi poll almost percent presidenti elect make largest parti reichstag schleicher negoti propos hitler becom hitler demand chancellorship along five cabinet posit import post state govern addit reichstag would pass enabl act give new govern need power otherwis would dissolv around countri nazi stormtroop run riot attack polit oppon hindenburg refus make hitler chancellor met hitler explain unwil bring singl parti power conclud want extend hand fellow soldier follow morn left neudeck newspap prais defens constitut constitut mandat new elect within sixti day owe crisi hindenburg postpon papen publish econom recoveri plan almost parti labor union lambast scant support crumbl citat need add enough vote gain parliamentari mandat schleicher tri persuad nazi leader like war hero hermann göring defect take posit govern none would becam anoth presidenti chancellor still court promin day chancellor number papen continu negoti hitler moder condit would settl chancellorship reich commission prussia two cabinet posit interior new slot aviat also promis would respect right presid reichstag press papen would term hindenburg allow oskar meissner meet secretli hitler culmin hour hitler oskar schleicher learn secret meet follow morn met presid demand emerg power dissolut reichstag hindenburg refus power agre elect new govern could form hindenburg call gener werner von blomberg oppon schleicher back disarma confer appoint reichswehr minist perhap unawar nazi sympath citat need break stalem hindenburg propos hitler chancellor papen reich commission prussia göring prussian interior minist control polic two cabinet minist would nazi remain eight would parti hindenburg met hitler papen would alway present new cabinet includ three nazi hitler göring wilhelm frick besid hitler frick nazi portfolio held nearli powerless interior ministri unlik rest europ time interior ministri power polic respons länder göring receiv portfolio critic made prussian interior minist control largest polic forc promot nazi command blomberg reichswehr minist hugenberg econom agricultur minist seldt leader first world war organ der stahlhelm labor minist minist holdov papen schleicher cabinet citat need hitler first act chancellor ask hindenburg dissolv reichstag nazi deutschnational volkspartei german nationalist dnvp could win outright major pass enabl act would give new govern power rule decre supposedli next four year unlik law pass articl could cancel major reichstag enabl act chancellor could pass law decre could cancel vote reichstag hindenburg agre request earli februari papen ask receiv articl bill sign law sharpli limit freedom press reichstag fire februari hindenburg hitler urg sign law reichstag fire decre via articl effect suspend civil liberti germani göring prussian interior minist enlist thousand sturmabteilung sa men auxiliari policemen attack polit oppon nazi communist social democrat singl particular abus fritz schäffer conserv cathol lead politician bavarian peopl parti met hindenburg februari complain ongo campaign terror spd schäffer told hindenburg reject notion million german design nation socialist serv trench serv trench vote banner hindenburg know mani socialist earn acclaim servic germani need mention name ebert hindenburg alway hate social democrat reject schäffer appeal say spd traitor stab fatherland back could never belong volksgemeinschaft therefor nazi full support campaign social democrat hindenburg dislik hitler approv effort creat volksgemeinschaft hindenburg govern nation concentr head hitler fulfil seek sinc creation volksgemeinschaft despit ensu hysteria nazi receiv vote though support dnvp major reichstag citat need hitler soon obtain hindenburg confid promis germani regain full sovereignti monarchi would restor week hindenburg longer ask papen join meet open new reichstag celebr nazi extravaganza hindenburg descend crypt old garrison church potsdam commun spirit frederick great grave attend hitler salut presid custodian new rise peopl enabl act prepar transfer reichstag govern even new law violat constitut communist deputi mani social democrat kept chamber violat articl constitut reichstag pass act well need major effect end republ turn meet took place intimid atmospher enabl act would garner requir supermajor even deputi present vote citat need hitler awar hindenburg check power passag enabl act ban parti except nazi hindenburg power sack chancellor mean hitler could legal remov offic given hindenburg still popular war hero rever figur reichswehr littl doubt reichswehr would side hindenburg ever decid sack hitler thu long hindenburg aliv hitler alway care avoid offend armi although hindenburg increasingli bad health nazi made sure whenev hindenburg appear public hitler compani appear hitler alway made point show utmost respect defer citat need econom auster abandon hitler pour money new program hire unemploy buy armament build road autobahn within year unemploy fell almost hitler gain support arm forc promis rebuild strength german state taken nation govern labor union suppress polit oppon imprison jew eject civil servic includ univers hindenburg object treatment jew want war veteran retain hitler acced hitler move eject hugenberg cabinet suppress polit parti trust colleagu hugenberg sent neudeck appeal assist met oskar hindenburg delay appoint one nazi gauleit fail obtain instal lutheran bishop favor honor guard neudeck storm trooper august stir ceremoni tannenberg presid present two larg east prussian properti near neudeck night plebiscit nazi rule schedul novemb hindenburg appeal voter support presid chancellor vote new command armi appoint presid choic chancellor hindenburg accept chang militari oath elimin obedi presid place swastika militari uniform summer hindenburg die metastas bladder cancer correspond domin complaint nazi stormtroop run amok fall group hindenburg friend led gener august von cramon ask hindenburg restor monarchi hindenburg repli cours recogn fidel kaiser king lord without reserv precis share sentiment must urgent warn step plan take domest crisi yet complet foreign power hard time imagin sidelin come restor monarchi say unbeliev pain summer hindenburg grew increasingli alarm nazi excess support papen gave speech univers marburg june call end state terror restor freedom propaganda minist joseph goebbel got wind cancel schedul broadcast speech order seizur newspap part text print papen furiou tell hitler act truste hindenburg junior minist like goebbel right silenc resign immedi notifi hindenburg happen hindenburg equal outrag told blomberg give hitler hitler took step end grow tension germani sa hindenburg would declar martial law turn govern armi long afterward hitler carri night long knive sa leader murder purportedli receiv hindenburg person thank telegram day later hindenburg learn schleicher wife gun home nuremberg trial göring admit telegram never seen hindenburg actual written nazi hindenburg remain offic death age lung cancer home neudeck east prussia august day hitler receiv word hindenburg deathb cabinet pass law concern head state german reich stipul upon hindenburg death offic presid would abolish power merg chancellor titl führer und reichskanzl leader chancellor reich three hour hindenburg death announc result law hitler germani head state head govern therebi elimin last remedi could legal dismiss cement statu absolut dictat germani publicli hitler announc presid insepar unit hindenburg would appropri titl ever use truth hitler known earli april hindenburg would like surviv year work feverishli get arm group germani would nearli power enough remov hindenburg support bid becom head state hindenburg death meet aboard deutschland april blomberg armi command werner von fritsch naval command erich raeder hitler publicli propos succeed hindenburg return arm forc support agre suppress sa promis arm forc would bearer arm germani watch raeder agre right away fritsch withheld support may senior gener unanim agre back hitler hindenburg successor accord günther von tschirschki und bögendorff interwar german diplomat associ hindenburg later defect unit kingdom presid paul von hindenburg last testament criticis nazi support democraci defector said also argu establish constitut monarchi clear separ power along abolit form racial religi discrimin alleg document hand hitler hindenburg nazi support son day death nazi releas version hindenburg final polit testament complimentari hitler hitler plebiscit held august german peopl ask approv hitler take offic führer ja ye vote amount vote referendum well effort make hitler hindenburg successor violat enabl act although gave hitler right pass law contrari constitut state presid power remain undisturb long interpret forbid attempt tamper presid constitut also previous amend make presid high court justic chancellor first line success presid even interim basi fresh elect citat need contrari hindenburg inter wife magnific ceremoni tannenberg memori soviet approach generalleutn oskar von hindenburg move parent remain western germani januari german troop blew memori polish author raze site leav trace remain temporarili inter thuringia along remain frederick great frederick william standard imperi german armi file foreign offic artwork prussian state museum librari sanssouci prussian crown jewel april monument fine art archiv section unit state armi uncov remain transport marburg inter elizabeth church marburg remain day plaqu grave commemor victim war violenc without mention hindenburg name defunct defunct visit hindenburg headquart crown princ wilhelm describ mood reportedli good sens humor often made joke expens also prodigi memori name face ask colleagu son armi even recal rank unit despit bonhomi hindenburg kept counsel accord kaiser wilhelm ii hindenburg never said half realli thought professor hugo vogel commiss immort victori tannenberg command paint arriv headquart subject begrudg pose hindenburg visit day often stay hour staff attribut ego inkl wife collect paint virgin amateur artist like discuss favorit author paint complet hindenburg would period check mani print reproduct sold vogel throughout war last portrait protect warrior imag hindenburg wrote memoir artist distract would prefer dispens overse germani crush victori tannenberg paul von hindenburg becam center massiv person cult persist throughout life henceforth laud live ideal german masculin patriot accord historian anna meng intens longev strike polit social breadth polit deploy adul short power hindenburg myth polit phenomenon first order hindenburg myth one central narr german public discours first world war weimar republ earli year nazi rule strike polyval extol notion authoritarian leadership also nation valu salvag someth posit war defeat face hindenburg myth could deploy differ group differ time differ purpos although promot first foremost german nationalist especi weimar earli year element hindenburg myth consider appeal initi mythic figur rest nation defenc battl fought german social democraci tsarist russia endear mani moder left onward world war celebr tribut hindenburg meter tall wooden like erect berlin admir paid drive ton war widow smaller version erect throughout germani wooden imag photograph invari portray resolut indomit warrior wear stern like citat need fame zeppelin hindenburg destroy fire name honor hindenburgdamm causeway join island sylt mainland built time offic previous upper silesian town zabrz german hindenburg also renam well sm hindenburg battlecruis commiss imperi german navi last capit ship enter servic imperi navi hindenburg rang new guinea includ perhap one world largest cliff hindenburg wall also bear name citat need historian christoph clark critic hindenburg role head state withdraw solemn constitut oath make common caus sworn enemi republ publicli declar would never consent appoint hitler post lever nazi leader german chancelleri januari field marshal high opinion doubtless sincer believ personifi prussian tradit selfless servic truth man tradit militari command later germani head state hindenburg broke virtual everi bond enter man dog faith servic man imag manipul betray hindenburg controversi figur german histori recent year numer german local bodi derecogn hindenburg februari hindenburg berlin honorari citizenship also revok decis pass berlin coalit social democrat left green
