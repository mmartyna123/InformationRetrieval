{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Wikipedia Article Recommendation system\n",
    "\n",
    "\n",
    "Authors:\n",
    "- Martyna Stasiak id.156071\n",
    "- Maria Musiał id.156062\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Introduction</b>\n",
    "The aim of this project is to implement a recommendation system to suggest similiar Wikipedia articles based on the ones that user have seen - user's history. <br>\n",
    "The system first creates a database containing <b>1000</b> wikipedia articles by crawling starting from the link provided by the user, then preprocesses them and computes similarities to generate miningful and reliable recommendations.\n",
    "\n",
    "#### <b>Objectives</b>\n",
    "1. <b>Crawling and Scraping</b>> - collect at least a 1000 Wikipedia articles \n",
    "2. <b>Preprocessing</b> - process and clean the data so that it will be suitable for the recommendation, including stemming, lemmatization\n",
    "3. <b>Similaries</b> - compute the similarities between articles in user's history and in the database\n",
    "4. <b>Recomendations</b> - recommend the best matching articles to the ones in the user's history \n",
    "<br> <br>\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries used in this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mmart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mmart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mmart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\mmart\\AppData\\Local\\Temp\\ipykernel_26364\\3925134912.py:21: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import random \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Crawling and saving our articles</b>\n",
    "In this part the csv file is created. It will work as a database containing wikipedia articles that may be recommended to the user.<br><br>\n",
    "It is done by crawling, that starts from some given URL; here we have chosen https://en.wikipedia.org/wiki/Machine_learning that is the article about Machine Learning. However it might be simply changed by providing a different link while invoking the function.<br><br>\n",
    "The `crawlArticles` function systematically collects articles by 'jumping' links found on Wikipedia pages. <br><br>\n",
    "<b>It fetches the title, link, and main content</b> of each article while filtering out irrelevant or special pages (like, disambiguation pages, main page, and links with special characters like ':'). <br><br>\n",
    "The function stores this information in a structured way, ensuring no duplicate visits, and includes a delay between requests to avoid overloading the server as it is requested by Wikipedia.<br><br> \n",
    "A couple of cells below the collected articles are saved as a CSV file, using custom saving function that is used throughout the project, forming our base dataset for the recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlArticles(start_url, max_articles):\n",
    "    visited = set() # set of visited pages to keep the ones that have already been crawled\n",
    "    to_visit = [start_url] # a list of pages yet to be crawled, starting with the start_url\n",
    "    articles = [] # a list to store the details of the articles (title, link and content)\n",
    "    \n",
    "    # a loop to check if there are pages to visit and if the number of articles is less than the max_articles\n",
    "    while to_visit and len(articles) < max_articles:\n",
    "        page = to_visit.pop(0) # get the first page in the list\n",
    "        if page in visited:\n",
    "            continue\n",
    "        visited.add(page) # add the page to the visited set\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(page) # geting the page\n",
    "            response.raise_for_status() \n",
    "            soup = BeautifulSoup(response.content, 'html.parser') # parsing the page\n",
    "            \n",
    "            # extracting the title and content of the article\n",
    "            title = soup.find('h1').text # article's title\n",
    "            paragraphs = soup.find_all('p') # article's paragraphs\n",
    "            content = ' '.join([p.text for p in paragraphs]) # article's content that is inside paragraphs\n",
    "            articles.append({\"title\": title, \"link\": page, \"content\": content})\n",
    "            \n",
    "            # extracting and filtering new links\n",
    "            for link in soup.find_all('a', href=True): # we look for all links in the page\n",
    "                href = link['href']\n",
    "                # we ensure that no page that has parts like 'disambiguation' or is a 'Main_Page' would be visited and added to the database\n",
    "                if href.startswith('/wiki/') and ':' not in href and '#' not in href and 'Main_Page' not in href and 'disambiguation' not in href:\n",
    "                    full_url = \"https://en.wikipedia.org\" + href\n",
    "                    if full_url not in visited:\n",
    "                        to_visit.append(full_url) # adding the new link to the list of pages to visit\n",
    "                        \n",
    "            time.sleep(0.5) # need to be polite to Wikipedia\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = crawlArticles(\"https://en.wikipedia.org/wiki/Machine_learning\", 1000)\n",
    "df = pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDatabase(df, fileName):\n",
    "    df.to_csv(fileName, index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDatabase(df, 'articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of articles: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we may see the first five articles (their title, link and content not yet processed) that are in our database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine learning</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Machine_learning</td>\n",
       "      <td>Machine learning (ML) is a field of study in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine Learning (journal)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Machine_Learning...</td>\n",
       "      <td>Machine Learning  is a peer-reviewed scientifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Statistical learning in language acquisition</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Statistical_lear...</td>\n",
       "      <td>Statistical learning is the ability for humans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data mining</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Data_mining</td>\n",
       "      <td>Data mining is the process of extracting and d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Supervised learning</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Supervised_learning</td>\n",
       "      <td>In machine learning, supervised learning (SL) ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0                              Machine learning   \n",
       "1                    Machine Learning (journal)   \n",
       "2  Statistical learning in language acquisition   \n",
       "3                                   Data mining   \n",
       "4                           Supervised learning   \n",
       "\n",
       "                                                link  \\\n",
       "0     https://en.wikipedia.org/wiki/Machine_learning   \n",
       "1  https://en.wikipedia.org/wiki/Machine_Learning...   \n",
       "2  https://en.wikipedia.org/wiki/Statistical_lear...   \n",
       "3          https://en.wikipedia.org/wiki/Data_mining   \n",
       "4  https://en.wikipedia.org/wiki/Supervised_learning   \n",
       "\n",
       "                                             content  \n",
       "0  Machine learning (ML) is a field of study in a...  \n",
       "1  Machine Learning  is a peer-reviewed scientifi...  \n",
       "2  Statistical learning is the ability for humans...  \n",
       "3  Data mining is the process of extracting and d...  \n",
       "4  In machine learning, supervised learning (SL) ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles in the starting database: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of articles in the starting database: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Preprocessing the Articles</b>\n",
    "In this step, we preprocess the articles to prepare them for the actual recommendation system. <br><br>\n",
    "Preprocessing ensures that the articles content is suitable for analysis, making comparisons between articles more meaningful. <br>The processed data is saved to the csv file.<br><br>\n",
    "\n",
    "\n",
    "We have created a function where the following techniques may be selected:\n",
    "\n",
    "1. Tokenization: It splits the text into individual words - tokens for further processing.<br>\n",
    "The tokenizers we chose are:\n",
    "    - word_tokenize,\n",
    "    - wordpunct_tokenize.\n",
    "When performing the preprocessing part it is possible to chose either of them, putting their name as a tokenizer parameter.\n",
    "2. Removing Stopwords: Stopwords like \"the,\" \"is,\" and \"and\" are the words that add little or no value to the analysis since they are so common. Removing them helps focus on the more informative parts of the text.\n",
    "3. Stemming: It reduces words to their root form, usually by chopping off the ends of the words (for example, \"automatic\" becomes \"automat\"). Stemming is computationally efficient but can sometimes produce non-standard roots.<br>\n",
    "Stemmer that we chose:\n",
    "    - Porter Stemmer,\n",
    "    - Lancaster Stemmer.\n",
    "Here same as in the tokenization the specific stemmer might be chosen, putting its name as a stemmer parameter. If we would like to use the stemming we have to provide stemmer's name since by the default there is none.<br>\n",
    "Also since the results given by the different stemmers may (and will) differ it is important to use the same one throught the preprocessing, for example when preprocessing the new article that is not in the database.\n",
    "4. Lemmatization: It is generally similar to stemming but ensures the root form is a valid word - it performs a proper reduction of a word to its dictionary form, i.e. lemma (for example, \"better\" becomes \"good\"). This is slightly more accurate but computationally intensive.<br>\n",
    "We chose the WordNet Lemmatizer. Here same as in the stemming case, we have to specify if we would like to use the lemmatizer since by the default function does not use it. Additionaly there is an extra parameter that we have to set to True if we want to use lemmatization.\n",
    "5. Lowercasing and Removing Non-Alphabetic Characters: Standardizing the text to lowercase and excluding numbers, punctuation, and special characters just to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessArticles(df, tokenizer=word_tokenize, stemmer=None, lemmatizer=None, useLemmatizer=False):\n",
    "    tokens = tokenizer(df['content'].lower())\n",
    "    terms = [word for word in tokens if word.isalpha() and word not in stopWords] # remove stopwords and non-alphabetic words\n",
    "    if stemmer:\n",
    "        processed = [stemmer.stem(word) for word in terms]\n",
    "    elif useLemmatizer and lemmatizer:\n",
    "        processed = [lemmatizer.lemmatize(word) for word in terms]\n",
    "    else:\n",
    "        processed = terms\n",
    "    return ' '.join(processed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here to compare the different text processing techniques we have displayed processed content of first five articles (in much shortened versions) just to see how they actually differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplifieddf = df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing variations\n",
    "variations_wordtokenizer = {\n",
    "    \"porter_stemmer\": lambda row: preprocessArticles(row, tokenizer=word_tokenize, stemmer=porter),\n",
    "    \"lancaster_stemmer\": lambda row: preprocessArticles(row, tokenizer=word_tokenize, stemmer=lancaster),\n",
    "    \"lemmatization\": lambda row: preprocessArticles(row, tokenizer=word_tokenize, lemmatizer=lemmatizer, useLemmatizer=True)\n",
    "}\n",
    "\n",
    "# Apply variations without modifying the original DataFrame\n",
    "results_wordtokenizer = pd.DataFrame({\n",
    "    \"title\": simplifieddf[\"title\"],\n",
    "    \"original_content\": simplifieddf[\"content\"]\n",
    "})\n",
    "\n",
    "for name, preprocess_function in variations_wordtokenizer.items():\n",
    "    # Apply each variation to the content column using the original function\n",
    "    results_wordtokenizer[name] = simplifieddf.apply(preprocess_function, axis=1)\n",
    "\n",
    "\n",
    "variations_wordpunct = {\n",
    "    \"porter_stemmer\": lambda row: preprocessArticles(row, tokenizer=wordpunct_tokenize, stemmer=porter),\n",
    "    \"lancaster_stemmer\": lambda row: preprocessArticles(row, tokenizer=wordpunct_tokenize, stemmer=lancaster),\n",
    "    \"lemmatization\": lambda row: preprocessArticles(row, tokenizer=wordpunct_tokenize, lemmatizer=lemmatizer, useLemmatizer=True)\n",
    "}\n",
    "\n",
    "# Apply variations without modifying the original DataFrame\n",
    "results_wordpunct = pd.DataFrame({\n",
    "    \"title\": simplifieddf[\"title\"],\n",
    "    \"original_content\": simplifieddf[\"content\"]\n",
    "})\n",
    "\n",
    "for name, preprocess_function in variations_wordpunct.items():\n",
    "    # Apply each variation to the content column using the original function\n",
    "    results_wordpunct[name] = simplifieddf.apply(preprocess_function, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenizer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original_content</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>lancaster_stemmer</th>\n",
       "      <th>lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine learning</td>\n",
       "      <td>Machine learning (ML) is a field of study in a...</td>\n",
       "      <td>machin learn ml field studi artifici intellig ...</td>\n",
       "      <td>machin learn ml field study art intellig conce...</td>\n",
       "      <td>machine learning ml field study artificial int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine Learning (journal)</td>\n",
       "      <td>Machine Learning  is a peer-reviewed scientifi...</td>\n",
       "      <td>machin learn scientif journal publish sinc for...</td>\n",
       "      <td>machin learn sci journ publ sint forty edit me...</td>\n",
       "      <td>machine learning scientific journal published ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Statistical learning in language acquisition</td>\n",
       "      <td>Statistical learning is the ability for humans...</td>\n",
       "      <td>statist learn abil human anim extract statist ...</td>\n",
       "      <td>stat learn abl hum anim extract stat regul wor...</td>\n",
       "      <td>statistical learning ability human animal extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data mining</td>\n",
       "      <td>Data mining is the process of extracting and d...</td>\n",
       "      <td>data mine process extract discov pattern larg ...</td>\n",
       "      <td>dat min process extract discov pattern larg da...</td>\n",
       "      <td>data mining process extracting discovering pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Supervised learning</td>\n",
       "      <td>In machine learning, supervised learning (SL) ...</td>\n",
       "      <td>machin learn supervis learn sl paradigm model ...</td>\n",
       "      <td>machin learn superv learn sl paradigm model tr...</td>\n",
       "      <td>machine learning supervised learning sl paradi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0                              Machine learning   \n",
       "1                    Machine Learning (journal)   \n",
       "2  Statistical learning in language acquisition   \n",
       "3                                   Data mining   \n",
       "4                           Supervised learning   \n",
       "\n",
       "                                    original_content  \\\n",
       "0  Machine learning (ML) is a field of study in a...   \n",
       "1  Machine Learning  is a peer-reviewed scientifi...   \n",
       "2  Statistical learning is the ability for humans...   \n",
       "3  Data mining is the process of extracting and d...   \n",
       "4  In machine learning, supervised learning (SL) ...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0  machin learn ml field studi artifici intellig ...   \n",
       "1  machin learn scientif journal publish sinc for...   \n",
       "2  statist learn abil human anim extract statist ...   \n",
       "3  data mine process extract discov pattern larg ...   \n",
       "4  machin learn supervis learn sl paradigm model ...   \n",
       "\n",
       "                                   lancaster_stemmer  \\\n",
       "0  machin learn ml field study art intellig conce...   \n",
       "1  machin learn sci journ publ sint forty edit me...   \n",
       "2  stat learn abl hum anim extract stat regul wor...   \n",
       "3  dat min process extract discov pattern larg da...   \n",
       "4  machin learn superv learn sl paradigm model tr...   \n",
       "\n",
       "                                       lemmatization  \n",
       "0  machine learning ml field study artificial int...  \n",
       "1  machine learning scientific journal published ...  \n",
       "2  statistical learning ability human animal extr...  \n",
       "3  data mining process extracting discovering pat...  \n",
       "4  machine learning supervised learning sl paradi...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Word Tokenizer\")\n",
    "columns_to_display = [\"title\", \"original_content\"] + list(variations_wordtokenizer.keys())\n",
    "results_wordtokenizer[columns_to_display]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Punct Tokenizer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original_content</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>lancaster_stemmer</th>\n",
       "      <th>lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine learning</td>\n",
       "      <td>Machine learning (ML) is a field of study in a...</td>\n",
       "      <td>machin learn ml field studi artifici intellig ...</td>\n",
       "      <td>machin learn ml field study art intellig conce...</td>\n",
       "      <td>machine learning ml field study artificial int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine Learning (journal)</td>\n",
       "      <td>Machine Learning  is a peer-reviewed scientifi...</td>\n",
       "      <td>machin learn peer review scientif journal publ...</td>\n",
       "      <td>machin learn peer review sci journ publ sint f...</td>\n",
       "      <td>machine learning peer reviewed scientific jour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Statistical learning in language acquisition</td>\n",
       "      <td>Statistical learning is the ability for humans...</td>\n",
       "      <td>statist learn abil human anim extract statist ...</td>\n",
       "      <td>stat learn abl hum anim extract stat regul wor...</td>\n",
       "      <td>statistical learning ability human animal extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data mining</td>\n",
       "      <td>Data mining is the process of extracting and d...</td>\n",
       "      <td>data mine process extract discov pattern larg ...</td>\n",
       "      <td>dat min process extract discov pattern larg da...</td>\n",
       "      <td>data mining process extracting discovering pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Supervised learning</td>\n",
       "      <td>In machine learning, supervised learning (SL) ...</td>\n",
       "      <td>machin learn supervis learn sl paradigm model ...</td>\n",
       "      <td>machin learn superv learn sl paradigm model tr...</td>\n",
       "      <td>machine learning supervised learning sl paradi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0                              Machine learning   \n",
       "1                    Machine Learning (journal)   \n",
       "2  Statistical learning in language acquisition   \n",
       "3                                   Data mining   \n",
       "4                           Supervised learning   \n",
       "\n",
       "                                    original_content  \\\n",
       "0  Machine learning (ML) is a field of study in a...   \n",
       "1  Machine Learning  is a peer-reviewed scientifi...   \n",
       "2  Statistical learning is the ability for humans...   \n",
       "3  Data mining is the process of extracting and d...   \n",
       "4  In machine learning, supervised learning (SL) ...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0  machin learn ml field studi artifici intellig ...   \n",
       "1  machin learn peer review scientif journal publ...   \n",
       "2  statist learn abil human anim extract statist ...   \n",
       "3  data mine process extract discov pattern larg ...   \n",
       "4  machin learn supervis learn sl paradigm model ...   \n",
       "\n",
       "                                   lancaster_stemmer  \\\n",
       "0  machin learn ml field study art intellig conce...   \n",
       "1  machin learn peer review sci journ publ sint f...   \n",
       "2  stat learn abl hum anim extract stat regul wor...   \n",
       "3  dat min process extract discov pattern larg da...   \n",
       "4  machin learn superv learn sl paradigm model tr...   \n",
       "\n",
       "                                       lemmatization  \n",
       "0  machine learning ml field study artificial int...  \n",
       "1  machine learning peer reviewed scientific jour...  \n",
       "2  statistical learning ability human animal extr...  \n",
       "3  data mining process extracting discovering pat...  \n",
       "4  machine learning supervised learning sl paradi...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Word Punct Tokenizer\")\n",
    "columns_to_display = [\"title\", \"original_content\"] + list(variations_wordpunct.keys())\n",
    "results_wordpunct[columns_to_display]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Since the lemmatization is more exhaustive we have chose to use the stemming with Porter's algorithm and a word_tokenizer.</b>\n",
    "However it is easy to change to a different method, just by adjusting the parameters. The processed data is now saved to the processedArticles.csv file, that has one more column than the articles.csv (or any different database that was provided) - processedContent that contains as the name suggests the processed content of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>processedContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine learning</td>\n",
       "      <td>Machine learning (ML) is a field of study in a...</td>\n",
       "      <td>machin learn ml field studi artifici intellig ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine Learning (journal)</td>\n",
       "      <td>Machine Learning  is a peer-reviewed scientifi...</td>\n",
       "      <td>machin learn scientif journal publish sinc for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Statistical learning in language acquisition</td>\n",
       "      <td>Statistical learning is the ability for humans...</td>\n",
       "      <td>statist learn abil human anim extract statist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data mining</td>\n",
       "      <td>Data mining is the process of extracting and d...</td>\n",
       "      <td>data mine process extract discov pattern larg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Supervised learning</td>\n",
       "      <td>In machine learning, supervised learning (SL) ...</td>\n",
       "      <td>machin learn supervis learn sl paradigm model ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0                              Machine learning   \n",
       "1                    Machine Learning (journal)   \n",
       "2  Statistical learning in language acquisition   \n",
       "3                                   Data mining   \n",
       "4                           Supervised learning   \n",
       "\n",
       "                                             content  \\\n",
       "0  Machine learning (ML) is a field of study in a...   \n",
       "1  Machine Learning  is a peer-reviewed scientifi...   \n",
       "2  Statistical learning is the ability for humans...   \n",
       "3  Data mining is the process of extracting and d...   \n",
       "4  In machine learning, supervised learning (SL) ...   \n",
       "\n",
       "                                    processedContent  \n",
       "0  machin learn ml field studi artifici intellig ...  \n",
       "1  machin learn scientif journal publish sinc for...  \n",
       "2  statist learn abil human anim extract statist ...  \n",
       "3  data mine process extract discov pattern larg ...  \n",
       "4  machin learn supervis learn sl paradigm model ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processedContent'] = df.apply(lambda row: preprocessArticles(row, tokenizer=word_tokenize, stemmer=porter, \n",
    "                                                          lemmatizer=None, useLemmatizer=False), axis=1)\n",
    "saveDatabase(df, 'processedArticles.csv')\n",
    "\n",
    "columnstoUse = ['title', 'content','processedContent']\n",
    "df[columnstoUse].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Term Frequency-Inverse Document Frequency Vectorization</b>\n",
    "In this part, we transform the preprocessed articles into numerical vectors using TF-IDF vectorization, that is Term Frequency-Inverse Document Frequency. <br><br>\n",
    "TF-IDF is a weighting scheme that is the best konown and most commonly used in the Information Retrieval field.<br>\n",
    "It quantifies the importance of a term in specific document or like in this case article relative to its importance in the entire corpus.<br><br>\n",
    "TF-IDF is composed of two parts:\n",
    "- Term Frequency (TF): Measures how frequently given term appears in the document or articte,\n",
    "- Inverse Domain Frequency (IDF): It reduces the importance of commonly occuring terms across the whole corpus and at the same time emphasizing the ones that are distinctive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(df):\n",
    "    tfidf = TfidfVectorizer(use_idf=True, smooth_idf=False)\n",
    "    tfidf_matrix = tfidf.fit_transform(df['processedContent'])\n",
    "\n",
    "    dfTFIDF = pd.DataFrame(tfidf_matrix.toarray(), index=df['title'], columns=tfidf.get_feature_names_out())\n",
    "    return tfidf, dfTFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaababbbbaab</th>\n",
       "      <th>aaagtctgac</th>\n",
       "      <th>aaai</th>\n",
       "      <th>aachen</th>\n",
       "      <th>aacl</th>\n",
       "      <th>aacsb</th>\n",
       "      <th>aad</th>\n",
       "      <th>aadhaar</th>\n",
       "      <th>...</th>\n",
       "      <th>積脊跡</th>\n",
       "      <th>精神</th>\n",
       "      <th>纽约时报中文</th>\n",
       "      <th>胡同</th>\n",
       "      <th>自由</th>\n",
       "      <th>良心</th>\n",
       "      <th>蝴蝶</th>\n",
       "      <th>賦有</th>\n",
       "      <th>關係</th>\n",
       "      <th>高尔夫</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Machine learning</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Machine Learning (journal)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Statistical learning in language acquisition</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data mining</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Supervised learning</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32089 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               aa  aaa  aaababbbbaab  \\\n",
       "title                                                                  \n",
       "Machine learning                              0.0  0.0           0.0   \n",
       "Machine Learning (journal)                    0.0  0.0           0.0   \n",
       "Statistical learning in language acquisition  0.0  0.0           0.0   \n",
       "Data mining                                   0.0  0.0           0.0   \n",
       "Supervised learning                           0.0  0.0           0.0   \n",
       "\n",
       "                                              aaagtctgac  aaai  aachen  aacl  \\\n",
       "title                                                                          \n",
       "Machine learning                                     0.0   0.0     0.0   0.0   \n",
       "Machine Learning (journal)                           0.0   0.0     0.0   0.0   \n",
       "Statistical learning in language acquisition         0.0   0.0     0.0   0.0   \n",
       "Data mining                                          0.0   0.0     0.0   0.0   \n",
       "Supervised learning                                  0.0   0.0     0.0   0.0   \n",
       "\n",
       "                                              aacsb  aad  aadhaar  ...  積脊跡  \\\n",
       "title                                                              ...        \n",
       "Machine learning                                0.0  0.0      0.0  ...  0.0   \n",
       "Machine Learning (journal)                      0.0  0.0      0.0  ...  0.0   \n",
       "Statistical learning in language acquisition    0.0  0.0      0.0  ...  0.0   \n",
       "Data mining                                     0.0  0.0      0.0  ...  0.0   \n",
       "Supervised learning                             0.0  0.0      0.0  ...  0.0   \n",
       "\n",
       "                                               精神  纽约时报中文   胡同   自由   良心   蝴蝶  \\\n",
       "title                                                                           \n",
       "Machine learning                              0.0     0.0  0.0  0.0  0.0  0.0   \n",
       "Machine Learning (journal)                    0.0     0.0  0.0  0.0  0.0  0.0   \n",
       "Statistical learning in language acquisition  0.0     0.0  0.0  0.0  0.0  0.0   \n",
       "Data mining                                   0.0     0.0  0.0  0.0  0.0  0.0   \n",
       "Supervised learning                           0.0     0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "                                               賦有   關係  高尔夫  \n",
       "title                                                        \n",
       "Machine learning                              0.0  0.0  0.0  \n",
       "Machine Learning (journal)                    0.0  0.0  0.0  \n",
       "Statistical learning in language acquisition  0.0  0.0  0.0  \n",
       "Data mining                                   0.0  0.0  0.0  \n",
       "Supervised learning                           0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 32089 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf, dfTFIDF = tf_idf(df)\n",
    "\n",
    "dfTFIDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Recommendation System</b> \n",
    "This section is  the `core` part of the recommendation system .<br>\n",
    "It uses all the methods defined and explained above, that is:  \n",
    "1. <b>Crawling</b>: to get the database of the articles avileable for recomendation, \n",
    "2. <b>Processing</b> of the article's content to work with the meaningful text, \n",
    "3. <b>TF-IDF vectorization</b> to get the importance of each term artice wise.<br><br>\n",
    "\n",
    "#### How it works\n",
    "The recommendation system takes the user's history (a list of titles the user has read or liked) and the article database as input and outputs the top_n recommendations – the articles most similar to the user's history.<br><br>\n",
    "To get the most meaningful recommendations recommender uses the cosine similarity.<br>\n",
    "It measures the angle between two vectors in high dimentional space; a smaller angle, that is closer to 1, means higher similarity.\n",
    "<br><br>\n",
    "The system fetches content of the articles that are in the user's history from the articles database and transforms it to a vector representation using the TF-IDF vectorization. Next it calculates the cosine similarity between the history vector and each of the articles in the database and returns the ones that are mostly similiar to the user's history, at the same time ensuring that no article from it will be in the recommendations.<br><br>\n",
    "The output has top_n rows corresponding to the top_n recommendations - the most similiar articles to the user's history, and 3 columns representing article's:\n",
    "- Title,\n",
    "- Link - a clickable link that redirects us directly to the recommended article (note that the url might differ from the article name, but on the Wikipedia website the article corresponds to the title),\n",
    "- Similarity score between given article and user's history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendArticles(history, df, top_n=5):\n",
    "    \n",
    "    if df.index.name != 'title':\n",
    "        df = df.set_index('title')\n",
    "    \n",
    "    tfidf, dfTFIDF = tf_idf(df.reset_index())\n",
    "\n",
    "    \n",
    "    \n",
    "    historyContent = ' '.join(df.loc[history,'processedContent'])\n",
    "    historyVector = tfidf.transform([historyContent]).toarray()[0]\n",
    "    \n",
    "    cosineDistance = dfTFIDF.apply(lambda row: cosine(row, historyVector), axis=1)\n",
    "    similarityScores = 1 - cosineDistance\n",
    "    \n",
    "    recommendations = pd.DataFrame({\n",
    "        'title': dfTFIDF.index, \n",
    "        'link': df['link'],\n",
    "        'similarity': similarityScores\n",
    "    })\n",
    "    \n",
    "    # we exclude the articles that user have already seen from the recommendations\n",
    "    recommendations = recommendations[~recommendations['title'].isin(history)]\n",
    "    \n",
    "    recommendations = recommendations.sort_values(by='similarity', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    recommendations = recommendations.head(top_n)\n",
    "\n",
    "    \n",
    "    recommendations = HTML(recommendations.to_html(render_links=True, escape=False))\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Outline of machine learning</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Outline_of_machine_learning\" target=\"_blank\">https://en.wikipedia.org/wiki/Outline_of_machine_learning</a></td>\n",
       "      <td>0.642366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meta-learning (computer science)</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Meta-learning_(computer_science)\" target=\"_blank\">https://en.wikipedia.org/wiki/Meta-learning_(computer_science)</a></td>\n",
       "      <td>0.592653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neural network (machine learning)</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Artificial_neural_network\" target=\"_blank\">https://en.wikipedia.org/wiki/Artificial_neural_network</a></td>\n",
       "      <td>0.534939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neural network (machine learning)</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Neural_network_(machine_learning)\" target=\"_blank\">https://en.wikipedia.org/wiki/Neural_network_(machine_learning)</a></td>\n",
       "      <td>0.534939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deep learning</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Deep_neural_network\" target=\"_blank\">https://en.wikipedia.org/wiki/Deep_neural_network</a></td>\n",
       "      <td>0.529459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = ['Machine learning']\n",
    "\n",
    "\n",
    "recommendations = recommendArticles(history, df, top_n=5)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Outline of machine learning</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Outline_of_machine_learning\" target=\"_blank\">https://en.wikipedia.org/wiki/Outline_of_machine_learning</a></td>\n",
       "      <td>0.627044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meta-learning (computer science)</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Meta-learning_(computer_science)\" target=\"_blank\">https://en.wikipedia.org/wiki/Meta-learning_(computer_science)</a></td>\n",
       "      <td>0.613893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Self-supervised learning</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Self-supervised_learning\" target=\"_blank\">https://en.wikipedia.org/wiki/Self-supervised_learning</a></td>\n",
       "      <td>0.535426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neural network (machine learning)</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Artificial_neural_network\" target=\"_blank\">https://en.wikipedia.org/wiki/Artificial_neural_network</a></td>\n",
       "      <td>0.530469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neural network (machine learning)</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Neural_network_(machine_learning)\" target=\"_blank\">https://en.wikipedia.org/wiki/Neural_network_(machine_learning)</a></td>\n",
       "      <td>0.530469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = ['Machine learning', 'Supervised learning']\n",
    "\n",
    "\n",
    "recommendations = recommendArticles(history, df, top_n=5)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Adding articles in user history that are not in the database</b>\n",
    "Since the Wikipedia has more than 6 million English articles we may encounter the situation that the user has seen the article that is not in our rather small database; <br>\n",
    "To addres this, we implemented a dynamic mechanism to handle such cases by fetching, processing, and expanding the database with new articles.\n",
    "#### Steps for adding a new article\n",
    "1. Fetch Article:<br>\n",
    "The fetchUnknownArticle function takes the title of the unknown article as input. It constructs the article's URL using the Wikipedia structure and fetches its content by making an HTTP request.\n",
    "    - The function extracts the title, link, and content of the article.\n",
    "    - The content is processed using the preprocessArticles function to generate a processedContent field.\n",
    "\n",
    "2. Expand with Related Articles <br>\n",
    "The additionalArticles function is invoked for the newly fetched article. This function uses the article's link as the starting point for crawling additional related articles.\n",
    "\n",
    "    - Newly crawled articles are filtered to ensure they aren't already in the database.\n",
    "    - Each article is processed (tokenized, stemmed/lemmatized) and added to the list of new articles.\n",
    "\n",
    "3. Update the Database\n",
    "The expandDatabase function consolidates the newly fetched articles (both the original and the crawled ones) and appends them to the existing database.\n",
    "\n",
    "    - The database is updated dynamically to include these new articles, ensuring no duplicates are introduced.\n",
    "    - The updated database is saved to the CSV file, ensuring persistence for future recommendations.\n",
    "\n",
    "----\n",
    "\n",
    "When the recommendation system is invoked:\n",
    "\n",
    "The user's history is examined to identify titles that are missing from the database.\n",
    "Missing articles are fetched, processed, and added to the database through the steps above.\n",
    "The expanded database is then used to generate recommendations, ensuring the system remains robust and comprehensive, even with articles outside the original dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = ['Machine learning', 'Aliens']\n",
    "# recommendations = recommendArticles(history, df, top_n=5)\n",
    "# recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processedArticles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchUnknownArticle(unknowTitle):\n",
    "    base_url = \"https://en.wikipedia.org/wiki/\"\n",
    "    url = base_url + unknowTitle.replace(' ', '_')\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = ' '.join([p.text for p in paragraphs if p.text])\n",
    "        \n",
    "        #to preprocess the content we simulate dataframe row\n",
    "        row = pd.Series({\"content\": content})\n",
    "        \n",
    "        processedArticle = preprocessArticles(row, tokenizer=word_tokenize, stemmer=porter, \n",
    "                                                          lemmatizer=None, useLemmatizer=False)\n",
    "        \n",
    "        \n",
    "        newArticle = {\"title\": unknowTitle, \"link\": url, \"content\": content, \n",
    "                      \"processedContent\": processedArticle}\n",
    "        return newArticle\n",
    "    except:\n",
    "        print(f\"Failed to fetch article {unknowTitle}. Please check the title and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additionalArticles(newArticle, df, maxExpansion=3):\n",
    "    mainArticle = fetchUnknownArticle(newArticle)\n",
    "    if mainArticle is None:\n",
    "        return []\n",
    "    \n",
    "    newCrawledArticles = crawlArticles(mainArticle['link'], max_articles=maxExpansion)\n",
    "    \n",
    "    processedNewArticles=[]\n",
    "    for article in newCrawledArticles:\n",
    "        if article['title'] not in df.index:\n",
    "            row = pd.Series({\"content\": article['content']})\n",
    "            article['processedContent'] = preprocessArticles(row, tokenizer=word_tokenize, stemmer=porter, \n",
    "                                                          lemmatizer=None, useLemmatizer=False)\n",
    "            processedNewArticles.append(article)\n",
    "            \n",
    "    return processedNewArticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expandDatabase(history, df):\n",
    "    unknownTitles = [title for title in history if title not in df.index]\n",
    "    allNewArticles = []\n",
    "    \n",
    "    for title in unknownTitles:\n",
    "        articleData = fetchUnknownArticle(title)\n",
    "        if articleData:\n",
    "            allNewArticles.append(articleData)\n",
    "            crawledArticles = additionalArticles(title, df)\n",
    "            allNewArticles.extend(crawledArticles)\n",
    "            \n",
    "    if allNewArticles:\n",
    "        additionaldf = pd.DataFrame(allNewArticles).set_index('title')\n",
    "        df = pd.concat([df, additionaldf])\n",
    "        df = df[~df.index.duplicated(keep='first')]\n",
    "        saveDatabase(df, 'processedArticles.csv')\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendArticles(history, df, top_n=5, explanation_terms=3):\n",
    "    \n",
    "    if df.index.name != 'title':\n",
    "        df = df.set_index('title')\n",
    "    \n",
    "    df = expandDatabase(history, df)\n",
    "    \n",
    "\n",
    "    \n",
    "    tfidf, dfTFIDF = tf_idf(df.reset_index())\n",
    "    \n",
    "    historyContent = ' '.join(df.loc[history,'processedContent'])\n",
    "    historyVector = tfidf.transform([historyContent]).toarray()[0]\n",
    "    \n",
    "    cosineDistance = dfTFIDF.apply(lambda row: cosine(row, historyVector), axis=1)\n",
    "    similarityScores = 1 - cosineDistance\n",
    "    \n",
    "    recommendations = pd.DataFrame({\n",
    "        'title': dfTFIDF.index, \n",
    "        'link': df['link'],\n",
    "        'similarity': similarityScores\n",
    "    })\n",
    "    \n",
    "    # we exclude the articles that user have already seen from the recommendations\n",
    "    recommendations = recommendations[~recommendations['title'].isin(history)]\n",
    "    \n",
    "    recommendations = recommendations.sort_values(by='similarity', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    recommendations = recommendations.head(top_n)\n",
    "\n",
    "    # Generate explanations\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    explanations = []\n",
    "    for _, row in recommendations.iterrows():\n",
    "        article_vector = dfTFIDF.loc[row['title']].values\n",
    "        article_vector_array = np.array(article_vector)  # Ensure it's a NumPy array\n",
    "\n",
    "        # Find the top terms based on their TF-IDF scores\n",
    "        top_terms_indices = np.argsort(article_vector_array)[-explanation_terms:][::-1]\n",
    "    \n",
    "        # Ensure the terms are properly indexed and converted to strings\n",
    "        top_terms = [str(feature_names[i]) for i in top_terms_indices]\n",
    "\n",
    "        explanations.append(\", \".join(top_terms) if top_terms else \"No meaningful terms\")\n",
    "\n",
    "    recommendations['explanation'] = explanations\n",
    "    \n",
    "    recommendations = HTML(recommendations.to_html(render_links=True, escape=False))\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from wordcloud import WordCloud\n",
    "\n",
    "# # Most Frequent Words\n",
    "# def plot_most_frequent_words(df, top_n=20):\n",
    "#     all_words = ' '.join(df['processedContent']).split()\n",
    "#     word_counts = pd.Series(all_words).value_counts().head(top_n)\n",
    "#     word_counts.plot(kind='bar', figsize=(10, 6), title='Most Frequent Words')\n",
    "#     plt.xlabel('Words')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.show()\n",
    "\n",
    "# # Word Cloud\n",
    "# def plot_word_cloud(df):\n",
    "#     all_words = ' '.join(df['processedContent'])\n",
    "#     wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_words)\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.imshow(wordcloud, interpolation='bilinear')\n",
    "#     plt.axis('off')\n",
    "#     plt.title('Word Cloud of Processed Content')\n",
    "#     plt.show()\n",
    "\n",
    "# # Article Length Distribution\n",
    "# def plot_article_length_distribution(df):\n",
    "#     article_lengths = df['processedContent'].apply(lambda x: len(x.split()))\n",
    "#     article_lengths.plot(kind='hist', bins=20, figsize=(10, 6), title='Article Length Distribution')\n",
    "#     plt.xlabel('Number of Words')\n",
    "#     plt.ylabel('Number of Articles')\n",
    "#     plt.show()\n",
    "\n",
    "# # Generate statistics\n",
    "# plot_most_frequent_words(df)\n",
    "# plot_word_cloud(df)\n",
    "# plot_article_length_distribution(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>similarity</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Führer</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/F%C3%BChrer_of_Germany\" target=\"_blank\">https://en.wikipedia.org/wiki/F%C3%BChrer_of_Germany</a></td>\n",
       "      <td>0.476054</td>\n",
       "      <td>führer, hitler, reich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paul von Hindenburg</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Paul_von_Hindenburg\" target=\"_blank\">https://en.wikipedia.org/wiki/Paul_von_Hindenburg</a></td>\n",
       "      <td>0.284168</td>\n",
       "      <td>hindenburg, ludendorff, armi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Life\" target=\"_blank\">https://en.wikipedia.org/wiki/Life</a></td>\n",
       "      <td>0.259254</td>\n",
       "      <td>life, cell, organ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Extraterrestrial intelligence</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Extraterrestrial_intelligence\" target=\"_blank\">https://en.wikipedia.org/wiki/Extraterrestrial_intelligence</a></td>\n",
       "      <td>0.231961</td>\n",
       "      <td>extraterrestri, civil, contact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial intelligence</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Artificial_intelligence\" target=\"_blank\">https://en.wikipedia.org/wiki/Artificial_intelligence</a></td>\n",
       "      <td>0.212300</td>\n",
       "      <td>ai, intellig, use</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = ['Machine learning', 'Extraterrestrial life', 'Adolf Hitler']\n",
    "recommendations = recommendArticles(history, df, top_n=5)\n",
    "# print(f\"Recommendations based on articles {history}:\")\n",
    "recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>similarity</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine learning</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Machine_learning\" target=\"_blank\">https://en.wikipedia.org/wiki/Machine_learning</a></td>\n",
       "      <td>0.626917</td>\n",
       "      <td>learn, machin, data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meta-learning (computer science)</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Meta-learning_(computer_science)\" target=\"_blank\">https://en.wikipedia.org/wiki/Meta-learning_(computer_science)</a></td>\n",
       "      <td>0.492054</td>\n",
       "      <td>learn, algorithm, network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Self-supervised learning</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Self-supervised_learning\" target=\"_blank\">https://en.wikipedia.org/wiki/Self-supervised_learning</a></td>\n",
       "      <td>0.486766</td>\n",
       "      <td>learn, ncssl, ssl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Outline of machine learning</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Outline_of_machine_learning\" target=\"_blank\">https://en.wikipedia.org/wiki/Outline_of_machine_learning</a></td>\n",
       "      <td>0.466568</td>\n",
       "      <td>learn, ml, machin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feature learning</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Feature_learning\" target=\"_blank\">https://en.wikipedia.org/wiki/Feature_learning</a></td>\n",
       "      <td>0.463154</td>\n",
       "      <td>represent, data, learn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = ['Supervised learning', 'Unsupervised learning']\n",
    "recommendations = recommendArticles(history, df, top_n=5)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>similarity</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>West Germany</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/West_Germany\" target=\"_blank\">https://en.wikipedia.org/wiki/West_Germany</a></td>\n",
       "      <td>0.553450</td>\n",
       "      <td>germani, west, german</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Supervised learning</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Supervised_learning\" target=\"_blank\">https://en.wikipedia.org/wiki/Supervised_learning</a></td>\n",
       "      <td>0.326064</td>\n",
       "      <td>learn, train, algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United States</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/United_States\" target=\"_blank\">https://en.wikipedia.org/wiki/United_States</a></td>\n",
       "      <td>0.324682</td>\n",
       "      <td>state, american, unit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Locomotive</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Locomotive\" target=\"_blank\">https://en.wikipedia.org/wiki/Locomotive</a></td>\n",
       "      <td>0.323156</td>\n",
       "      <td>locomot, steam, electr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Training</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Training\" target=\"_blank\">https://en.wikipedia.org/wiki/Training</a></td>\n",
       "      <td>0.323140</td>\n",
       "      <td>train, skill, traine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = ['Machine learning', 'Memtransistor', 'Train', 'Germany']\n",
    "recommendations = recommendArticles(history, df, top_n=5)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>similarity</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>West Germany</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/West_Germany\" target=\"_blank\">https://en.wikipedia.org/wiki/West_Germany</a></td>\n",
       "      <td>0.553450</td>\n",
       "      <td>germani, west, german</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Supervised learning</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Supervised_learning\" target=\"_blank\">https://en.wikipedia.org/wiki/Supervised_learning</a></td>\n",
       "      <td>0.326064</td>\n",
       "      <td>learn, train, algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United States</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/United_States\" target=\"_blank\">https://en.wikipedia.org/wiki/United_States</a></td>\n",
       "      <td>0.324682</td>\n",
       "      <td>state, american, unit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Locomotive</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Locomotive\" target=\"_blank\">https://en.wikipedia.org/wiki/Locomotive</a></td>\n",
       "      <td>0.323156</td>\n",
       "      <td>locomot, steam, electr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Training</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Training\" target=\"_blank\">https://en.wikipedia.org/wiki/Training</a></td>\n",
       "      <td>0.323140</td>\n",
       "      <td>train, skill, traine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = ['Memtransistor', 'Train', 'Germany', 'Machine learning']\n",
    "recommendations = recommendArticles(history, df, top_n=5)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>similarity</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Papuan mountain pigeon</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Papuan_mountain_pigeon\" target=\"_blank\">https://en.wikipedia.org/wiki/Papuan_mountain_pigeon</a></td>\n",
       "      <td>0.839441</td>\n",
       "      <td>pigeon, papuan, mountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Primate</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Primates\" target=\"_blank\">https://en.wikipedia.org/wiki/Primates</a></td>\n",
       "      <td>0.068627</td>\n",
       "      <td>primat, monkey, speci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Behaviorism</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Behaviorism\" target=\"_blank\">https://en.wikipedia.org/wiki/Behaviorism</a></td>\n",
       "      <td>0.061110</td>\n",
       "      <td>behavior, skinner, behaviorist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient descent</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Gradient_descent\" target=\"_blank\">https://en.wikipedia.org/wiki/Gradient_descent</a></td>\n",
       "      <td>0.058338</td>\n",
       "      <td>gradient, descent, converg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Taxonomy (biology)</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Taxonomy_(biology)\" target=\"_blank\">https://en.wikipedia.org/wiki/Taxonomy_(biology)</a></td>\n",
       "      <td>0.056120</td>\n",
       "      <td>taxonomi, taxonom, taxa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = ['Mountain pigeon']\n",
    "recommendations = recommendArticles(history, df, top_n=5)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>similarity</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Least-concern species</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Least_Concern\" target=\"_blank\">https://en.wikipedia.org/wiki/Least_Concern</a></td>\n",
       "      <td>0.171854</td>\n",
       "      <td>iucn, speci, taxa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conservation status</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Conservation_status\" target=\"_blank\">https://en.wikipedia.org/wiki/Conservation_status</a></td>\n",
       "      <td>0.165450</td>\n",
       "      <td>speci, conserv, endang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Primate</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Primates\" target=\"_blank\">https://en.wikipedia.org/wiki/Primates</a></td>\n",
       "      <td>0.098087</td>\n",
       "      <td>primat, monkey, speci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random forest</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Random_forest\" target=\"_blank\">https://en.wikipedia.org/wiki/Random_forest</a></td>\n",
       "      <td>0.083403</td>\n",
       "      <td>forest, tree, random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes classifier</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Naive_Bayes_classifier\" target=\"_blank\">https://en.wikipedia.org/wiki/Naive_Bayes_classifier</a></td>\n",
       "      <td>0.063258</td>\n",
       "      <td>bay, naiv, femal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = ['Papuan mountain pigeon']\n",
    "recommendations = recommendArticles(history, df, top_n=5)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>processedContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine learning</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Machine_learning</td>\n",
       "      <td>Machine learning (ML) is a field of study in a...</td>\n",
       "      <td>machin learn ml field studi artifici intellig ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine Learning (journal)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Machine_Learning...</td>\n",
       "      <td>Machine Learning  is a peer-reviewed scientifi...</td>\n",
       "      <td>machin learn scientif journal publish sinc for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Statistical learning in language acquisition</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Statistical_lear...</td>\n",
       "      <td>Statistical learning is the ability for humans...</td>\n",
       "      <td>statist learn abil human anim extract statist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data mining</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Data_mining</td>\n",
       "      <td>Data mining is the process of extracting and d...</td>\n",
       "      <td>data mine process extract discov pattern larg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Supervised learning</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Supervised_learning</td>\n",
       "      <td>In machine learning, supervised learning (SL) ...</td>\n",
       "      <td>machin learn supervis learn sl paradigm model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Limitations and exceptions to copyright</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Limitations_and_...</td>\n",
       "      <td>\\n Limitations and exceptions to copyright are...</td>\n",
       "      <td>limit except copyright provis local copyright ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Copyright and Information Society Directive 2001</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Information_Soci...</td>\n",
       "      <td>\\n The Copyright and Information Society Direc...</td>\n",
       "      <td>copyright inform societi direct direct europea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>European Commission</td>\n",
       "      <td>https://en.wikipedia.org/wiki/European_Commission</td>\n",
       "      <td>\\n The European Commission (EC) is the primary...</td>\n",
       "      <td>european commiss ec primari execut arm europea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Open access</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Open_access</td>\n",
       "      <td>\\n Open access (OA) is a set of principles and...</td>\n",
       "      <td>open access oa set principl rang practic nomin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Copyright law of the United States</td>\n",
       "      <td>https://en.wikipedia.org/wiki/US_copyright_law</td>\n",
       "      <td>\\n The copyright law of the United States gran...</td>\n",
       "      <td>copyright law unit state grant monopoli protec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0                                    Machine learning   \n",
       "1                          Machine Learning (journal)   \n",
       "2        Statistical learning in language acquisition   \n",
       "3                                         Data mining   \n",
       "4                                 Supervised learning   \n",
       "..                                                ...   \n",
       "995           Limitations and exceptions to copyright   \n",
       "996  Copyright and Information Society Directive 2001   \n",
       "997                               European Commission   \n",
       "998                                       Open access   \n",
       "999                Copyright law of the United States   \n",
       "\n",
       "                                                  link  \\\n",
       "0       https://en.wikipedia.org/wiki/Machine_learning   \n",
       "1    https://en.wikipedia.org/wiki/Machine_Learning...   \n",
       "2    https://en.wikipedia.org/wiki/Statistical_lear...   \n",
       "3            https://en.wikipedia.org/wiki/Data_mining   \n",
       "4    https://en.wikipedia.org/wiki/Supervised_learning   \n",
       "..                                                 ...   \n",
       "995  https://en.wikipedia.org/wiki/Limitations_and_...   \n",
       "996  https://en.wikipedia.org/wiki/Information_Soci...   \n",
       "997  https://en.wikipedia.org/wiki/European_Commission   \n",
       "998          https://en.wikipedia.org/wiki/Open_access   \n",
       "999     https://en.wikipedia.org/wiki/US_copyright_law   \n",
       "\n",
       "                                               content  \\\n",
       "0    Machine learning (ML) is a field of study in a...   \n",
       "1    Machine Learning  is a peer-reviewed scientifi...   \n",
       "2    Statistical learning is the ability for humans...   \n",
       "3    Data mining is the process of extracting and d...   \n",
       "4    In machine learning, supervised learning (SL) ...   \n",
       "..                                                 ...   \n",
       "995  \\n Limitations and exceptions to copyright are...   \n",
       "996  \\n The Copyright and Information Society Direc...   \n",
       "997  \\n The European Commission (EC) is the primary...   \n",
       "998  \\n Open access (OA) is a set of principles and...   \n",
       "999  \\n The copyright law of the United States gran...   \n",
       "\n",
       "                                      processedContent  \n",
       "0    machin learn ml field studi artifici intellig ...  \n",
       "1    machin learn scientif journal publish sinc for...  \n",
       "2    statist learn abil human anim extract statist ...  \n",
       "3    data mine process extract discov pattern larg ...  \n",
       "4    machin learn supervis learn sl paradigm model ...  \n",
       "..                                                 ...  \n",
       "995  limit except copyright provis local copyright ...  \n",
       "996  copyright inform societi direct direct europea...  \n",
       "997  european commiss ec primari execut arm europea...  \n",
       "998  open access oa set principl rang practic nomin...  \n",
       "999  copyright law unit state grant monopoli protec...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
