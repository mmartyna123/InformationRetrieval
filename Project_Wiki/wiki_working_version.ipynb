{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia Recommendation system\n",
    "\n",
    "-----\n",
    "\n",
    "Authors:\n",
    "- Martyna Stasiak id.156071"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to generate the recommendations on wikipedia articles basing on the ones that user have liked. <br>\n",
    "To do that we have used 10 000 initial articles that were obtained by web crawling, starting from the https://en.wikipedia.org/wiki/Machine_learning article; later they were saved in the csv file, so if there is a need the file working as our database might be changed.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries that we have used and are necessary for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mmart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mmart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mmart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\mmart\\AppData\\Local\\Temp\\ipykernel_10932\\1868813398.py:21: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling and saving our articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we create the file that will work as or database containing all possible wikipedia articles. <br>\n",
    "We perform the crawling by ...... <explain precisely> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlArticles(start_url, max_articles):\n",
    "    visited = set()\n",
    "    to_visit = [start_url]\n",
    "    articles = []\n",
    "    \n",
    "    while to_visit and len(articles) < max_articles:\n",
    "        page = to_visit.pop(0)\n",
    "        if page in visited:\n",
    "            continue\n",
    "        visited.add(page)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(page)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            title = soup.find('h1').text # article's title\n",
    "            paragraphs = soup.find_all('p') # article's paragraphs\n",
    "            content = ' '.join([p.text for p in paragraphs]) # article's content that is inside paragraphs\n",
    "            articles.append({\"title\": title, \"link\": page, \"content\": content})\n",
    "            \n",
    "            # extracting and filtering new links\n",
    "            for link in soup.find_all('a', href=True): # we look for all links in the page\n",
    "                href = link['href']\n",
    "                if href.startswith('/wiki/') and ':' not in href and '#' not in href and 'Main_Page' not in href:\n",
    "                    full_url = \"https://en.wikipedia.org\" + href\n",
    "                    if full_url not in visited:\n",
    "                        to_visit.append(full_url)\n",
    "            time.sleep(0.5) # be polite to Wikipedia\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = crawlArticles(\"https://en.wikipedia.org/wiki/Machine_learning\", 10)\n",
    "df = pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDatabase(df, fileName):\n",
    "    df.to_csv(fileName, index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDatabase(df, 'articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine learning</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Machine_learning</td>\n",
       "      <td>Machine learning (ML) is a field of study in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine Learning (journal)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Machine_Learning...</td>\n",
       "      <td>Machine Learning  is a peer-reviewed scientifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Statistical learning in language acquisition</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Statistical_lear...</td>\n",
       "      <td>Statistical learning is the ability for humans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data mining</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Data_mining</td>\n",
       "      <td>Data mining is the process of extracting and d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Supervised learning</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Supervised_learning</td>\n",
       "      <td>In machine learning, supervised learning (SL) ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0                              Machine learning   \n",
       "1                    Machine Learning (journal)   \n",
       "2  Statistical learning in language acquisition   \n",
       "3                                   Data mining   \n",
       "4                           Supervised learning   \n",
       "\n",
       "                                                link  \\\n",
       "0     https://en.wikipedia.org/wiki/Machine_learning   \n",
       "1  https://en.wikipedia.org/wiki/Machine_Learning...   \n",
       "2  https://en.wikipedia.org/wiki/Statistical_lear...   \n",
       "3          https://en.wikipedia.org/wiki/Data_mining   \n",
       "4  https://en.wikipedia.org/wiki/Supervised_learning   \n",
       "\n",
       "                                             content  \n",
       "0  Machine learning (ML) is a field of study in a...  \n",
       "1  Machine Learning  is a peer-reviewed scientifi...  \n",
       "2  Statistical learning is the ability for humans...  \n",
       "3  Data mining is the process of extracting and d...  \n",
       "4  In machine learning, supervised learning (SL) ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we have the database containing the articles we have to do the preprocessing; <br>\n",
    "we have done: \n",
    "- lemmatization\n",
    "- deleting the stopwords\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessArticles(df, tokenizer=word_tokenize, stemmer=None, lemmatizer=None, useLemmatizer=False):\n",
    "    tokens = tokenizer(df['content'].lower())\n",
    "    terms = [word for word in tokens if word.isalpha() and word not in stopWords] # remove stopwords and non-alphabetic words\n",
    "    if stemmer:\n",
    "        processed = [stemmer.stem(word) for word in terms]\n",
    "    elif useLemmatizer and lemmatizer:\n",
    "        processed = [lemmatizer.lemmatize(word) for word in terms]\n",
    "    else:\n",
    "        processed = terms\n",
    "    return ' '.join(processed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing variations\n",
    "variations_wordtokenizer = {\n",
    "    \"porter_stemmer\": lambda row: preprocessArticles(row, tokenizer=word_tokenize, stemmer=porter),\n",
    "    \"lancaster_stemmer\": lambda row: preprocessArticles(row, tokenizer=word_tokenize, stemmer=lancaster),\n",
    "    \"lemmatization\": lambda row: preprocessArticles(row, tokenizer=word_tokenize, lemmatizer=lemmatizer, useLemmatizer=True)\n",
    "}\n",
    "\n",
    "# Apply variations without modifying the original DataFrame\n",
    "results_wordtokenizer = pd.DataFrame({\n",
    "    \"title\": df[\"title\"],\n",
    "    \"original_content\": df[\"content\"]\n",
    "})\n",
    "\n",
    "for name, preprocess_function in variations_wordtokenizer.items():\n",
    "    # Apply each variation to the content column using the original function\n",
    "    results_wordtokenizer[name] = df.apply(preprocess_function, axis=1)\n",
    "\n",
    "\n",
    "variations_wordpunct = {\n",
    "    \"porter_stemmer\": lambda row: preprocessArticles(row, tokenizer=word_tokenize, stemmer=porter),\n",
    "    \"lancaster_stemmer\": lambda row: preprocessArticles(row, tokenizer=word_tokenize, stemmer=lancaster),\n",
    "    \"lemmatization\": lambda row: preprocessArticles(row, tokenizer=word_tokenize, lemmatizer=lemmatizer, useLemmatizer=True)\n",
    "}\n",
    "\n",
    "# Apply variations without modifying the original DataFrame\n",
    "results_wordpunct = pd.DataFrame({\n",
    "    \"title\": df[\"title\"],\n",
    "    \"original_content\": df[\"content\"]\n",
    "})\n",
    "\n",
    "for name, preprocess_function in variations_wordpunct.items():\n",
    "    # Apply each variation to the content column using the original function\n",
    "    results_wordpunct[name] = df.apply(preprocess_function, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenizer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original_content</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>lancaster_stemmer</th>\n",
       "      <th>lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine learning</td>\n",
       "      <td>Machine learning (ML) is a field of study in a...</td>\n",
       "      <td>machin learn ml field studi artifici intellig ...</td>\n",
       "      <td>machin learn ml field study art intellig conce...</td>\n",
       "      <td>machine learning ml field study artificial int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine Learning (journal)</td>\n",
       "      <td>Machine Learning  is a peer-reviewed scientifi...</td>\n",
       "      <td>machin learn scientif journal publish sinc for...</td>\n",
       "      <td>machin learn sci journ publ sint forty edit me...</td>\n",
       "      <td>machine learning scientific journal published ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Statistical learning in language acquisition</td>\n",
       "      <td>Statistical learning is the ability for humans...</td>\n",
       "      <td>statist learn abil human anim extract statist ...</td>\n",
       "      <td>stat learn abl hum anim extract stat regul wor...</td>\n",
       "      <td>statistical learning ability human animal extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data mining</td>\n",
       "      <td>Data mining is the process of extracting and d...</td>\n",
       "      <td>data mine process extract discov pattern larg ...</td>\n",
       "      <td>dat min process extract discov pattern larg da...</td>\n",
       "      <td>data mining process extracting discovering pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Supervised learning</td>\n",
       "      <td>In machine learning, supervised learning (SL) ...</td>\n",
       "      <td>machin learn supervis learn sl paradigm model ...</td>\n",
       "      <td>machin learn superv learn sl paradigm model tr...</td>\n",
       "      <td>machine learning supervised learning sl paradi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0                              Machine learning   \n",
       "1                    Machine Learning (journal)   \n",
       "2  Statistical learning in language acquisition   \n",
       "3                                   Data mining   \n",
       "4                           Supervised learning   \n",
       "\n",
       "                                    original_content  \\\n",
       "0  Machine learning (ML) is a field of study in a...   \n",
       "1  Machine Learning  is a peer-reviewed scientifi...   \n",
       "2  Statistical learning is the ability for humans...   \n",
       "3  Data mining is the process of extracting and d...   \n",
       "4  In machine learning, supervised learning (SL) ...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0  machin learn ml field studi artifici intellig ...   \n",
       "1  machin learn scientif journal publish sinc for...   \n",
       "2  statist learn abil human anim extract statist ...   \n",
       "3  data mine process extract discov pattern larg ...   \n",
       "4  machin learn supervis learn sl paradigm model ...   \n",
       "\n",
       "                                   lancaster_stemmer  \\\n",
       "0  machin learn ml field study art intellig conce...   \n",
       "1  machin learn sci journ publ sint forty edit me...   \n",
       "2  stat learn abl hum anim extract stat regul wor...   \n",
       "3  dat min process extract discov pattern larg da...   \n",
       "4  machin learn superv learn sl paradigm model tr...   \n",
       "\n",
       "                                       lemmatization  \n",
       "0  machine learning ml field study artificial int...  \n",
       "1  machine learning scientific journal published ...  \n",
       "2  statistical learning ability human animal extr...  \n",
       "3  data mining process extracting discovering pat...  \n",
       "4  machine learning supervised learning sl paradi...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Word Tokenizer\")\n",
    "columns_to_display = [\"title\", \"original_content\"] + list(variations_wordtokenizer.keys())\n",
    "results_wordtokenizer[columns_to_display].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Punct Tokenizer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original_content</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>lancaster_stemmer</th>\n",
       "      <th>lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine learning</td>\n",
       "      <td>Machine learning (ML) is a field of study in a...</td>\n",
       "      <td>machin learn ml field studi artifici intellig ...</td>\n",
       "      <td>machin learn ml field study art intellig conce...</td>\n",
       "      <td>machine learning ml field study artificial int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine Learning (journal)</td>\n",
       "      <td>Machine Learning  is a peer-reviewed scientifi...</td>\n",
       "      <td>machin learn scientif journal publish sinc for...</td>\n",
       "      <td>machin learn sci journ publ sint forty edit me...</td>\n",
       "      <td>machine learning scientific journal published ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Statistical learning in language acquisition</td>\n",
       "      <td>Statistical learning is the ability for humans...</td>\n",
       "      <td>statist learn abil human anim extract statist ...</td>\n",
       "      <td>stat learn abl hum anim extract stat regul wor...</td>\n",
       "      <td>statistical learning ability human animal extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data mining</td>\n",
       "      <td>Data mining is the process of extracting and d...</td>\n",
       "      <td>data mine process extract discov pattern larg ...</td>\n",
       "      <td>dat min process extract discov pattern larg da...</td>\n",
       "      <td>data mining process extracting discovering pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Supervised learning</td>\n",
       "      <td>In machine learning, supervised learning (SL) ...</td>\n",
       "      <td>machin learn supervis learn sl paradigm model ...</td>\n",
       "      <td>machin learn superv learn sl paradigm model tr...</td>\n",
       "      <td>machine learning supervised learning sl paradi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0                              Machine learning   \n",
       "1                    Machine Learning (journal)   \n",
       "2  Statistical learning in language acquisition   \n",
       "3                                   Data mining   \n",
       "4                           Supervised learning   \n",
       "\n",
       "                                    original_content  \\\n",
       "0  Machine learning (ML) is a field of study in a...   \n",
       "1  Machine Learning  is a peer-reviewed scientifi...   \n",
       "2  Statistical learning is the ability for humans...   \n",
       "3  Data mining is the process of extracting and d...   \n",
       "4  In machine learning, supervised learning (SL) ...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0  machin learn ml field studi artifici intellig ...   \n",
       "1  machin learn scientif journal publish sinc for...   \n",
       "2  statist learn abil human anim extract statist ...   \n",
       "3  data mine process extract discov pattern larg ...   \n",
       "4  machin learn supervis learn sl paradigm model ...   \n",
       "\n",
       "                                   lancaster_stemmer  \\\n",
       "0  machin learn ml field study art intellig conce...   \n",
       "1  machin learn sci journ publ sint forty edit me...   \n",
       "2  stat learn abl hum anim extract stat regul wor...   \n",
       "3  dat min process extract discov pattern larg da...   \n",
       "4  machin learn superv learn sl paradigm model tr...   \n",
       "\n",
       "                                       lemmatization  \n",
       "0  machine learning ml field study artificial int...  \n",
       "1  machine learning scientific journal published ...  \n",
       "2  statistical learning ability human animal extr...  \n",
       "3  data mining process extracting discovering pat...  \n",
       "4  machine learning supervised learning sl paradi...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Word Punct Tokenizer\")\n",
    "columns_to_display = [\"title\", \"original_content\"] + list(variations_wordpunct.keys())\n",
    "results_wordpunct[columns_to_display].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>processedContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine learning</td>\n",
       "      <td>Machine learning (ML) is a field of study in a...</td>\n",
       "      <td>machin learn ml field studi artifici intellig ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine Learning (journal)</td>\n",
       "      <td>Machine Learning  is a peer-reviewed scientifi...</td>\n",
       "      <td>machin learn scientif journal publish sinc for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Statistical learning in language acquisition</td>\n",
       "      <td>Statistical learning is the ability for humans...</td>\n",
       "      <td>statist learn abil human anim extract statist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data mining</td>\n",
       "      <td>Data mining is the process of extracting and d...</td>\n",
       "      <td>data mine process extract discov pattern larg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Supervised learning</td>\n",
       "      <td>In machine learning, supervised learning (SL) ...</td>\n",
       "      <td>machin learn supervis learn sl paradigm model ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0                              Machine learning   \n",
       "1                    Machine Learning (journal)   \n",
       "2  Statistical learning in language acquisition   \n",
       "3                                   Data mining   \n",
       "4                           Supervised learning   \n",
       "\n",
       "                                             content  \\\n",
       "0  Machine learning (ML) is a field of study in a...   \n",
       "1  Machine Learning  is a peer-reviewed scientifi...   \n",
       "2  Statistical learning is the ability for humans...   \n",
       "3  Data mining is the process of extracting and d...   \n",
       "4  In machine learning, supervised learning (SL) ...   \n",
       "\n",
       "                                    processedContent  \n",
       "0  machin learn ml field studi artifici intellig ...  \n",
       "1  machin learn scientif journal publish sinc for...  \n",
       "2  statist learn abil human anim extract statist ...  \n",
       "3  data mine process extract discov pattern larg ...  \n",
       "4  machin learn supervis learn sl paradigm model ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processedContent'] = df.apply(lambda row: preprocessArticles(row, tokenizer=word_tokenize, stemmer=porter, \n",
    "                                                          lemmatizer=None, useLemmatizer=False), axis=1)\n",
    "saveDatabase(df, 'processed_articles.csv')\n",
    "\n",
    "columnstoUse = ['title', 'content','processedContent']\n",
    "df[columnstoUse].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF and Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(df):\n",
    "    tfidf = TfidfVectorizer(use_idf=True, smooth_idf=False)\n",
    "    tfidf_matrix = tfidf.fit_transform(df['processedContent'])\n",
    "\n",
    "    dfTFIDF = pd.DataFrame(tfidf_matrix.toarray(), index=df['title'], columns=tfidf.get_feature_names_out())\n",
    "    return tfidf, dfTFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aahc</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>abnorm</th>\n",
       "      <th>absenc</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abus</th>\n",
       "      <th>academ</th>\n",
       "      <th>acceler</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>yield</th>\n",
       "      <th>yim</th>\n",
       "      <th>yoshua</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yu</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Machine learning</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005643</td>\n",
       "      <td>0.007322</td>\n",
       "      <td>0.007322</td>\n",
       "      <td>0.007141</td>\n",
       "      <td>0.004766</td>\n",
       "      <td>0.004766</td>\n",
       "      <td>0.007141</td>\n",
       "      <td>0.004766</td>\n",
       "      <td>0.007141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004766</td>\n",
       "      <td>0.014297</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004766</td>\n",
       "      <td>0.014283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Machine Learning (journal)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Statistical learning in language acquisition</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049931</td>\n",
       "      <td>0.116505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016249</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data mining</th>\n",
       "      <td>0.013043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006687</td>\n",
       "      <td>0.013374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Supervised learning</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2293 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  aahc   abandon      abil  \\\n",
       "title                                                                        \n",
       "Machine learning                              0.000000  0.005643  0.007322   \n",
       "Machine Learning (journal)                    0.000000  0.000000  0.000000   \n",
       "Statistical learning in language acquisition  0.000000  0.000000  0.049931   \n",
       "Data mining                                   0.013043  0.000000  0.006687   \n",
       "Supervised learning                           0.000000  0.000000  0.000000   \n",
       "\n",
       "                                                   abl    abnorm    absenc  \\\n",
       "title                                                                        \n",
       "Machine learning                              0.007322  0.007141  0.004766   \n",
       "Machine Learning (journal)                    0.000000  0.000000  0.000000   \n",
       "Statistical learning in language acquisition  0.116505  0.000000  0.000000   \n",
       "Data mining                                   0.013374  0.000000  0.000000   \n",
       "Supervised learning                           0.051182  0.000000  0.000000   \n",
       "\n",
       "                                              abstract      abus    academ  \\\n",
       "title                                                                        \n",
       "Machine learning                              0.004766  0.007141  0.004766   \n",
       "Machine Learning (journal)                    0.000000  0.000000  0.080419   \n",
       "Statistical learning in language acquisition  0.016249  0.000000  0.000000   \n",
       "Data mining                                   0.000000  0.000000  0.008704   \n",
       "Supervised learning                           0.000000  0.000000  0.000000   \n",
       "\n",
       "                                               acceler  ...      year  \\\n",
       "title                                                   ...             \n",
       "Machine learning                              0.007141  ...  0.004766   \n",
       "Machine Learning (journal)                    0.000000  ...  0.000000   \n",
       "Statistical learning in language acquisition  0.000000  ...  0.000000   \n",
       "Data mining                                   0.000000  ...  0.000000   \n",
       "Supervised learning                           0.000000  ...  0.000000   \n",
       "\n",
       "                                                   yet     yield       yim  \\\n",
       "title                                                                        \n",
       "Machine learning                              0.014297  0.009532  0.000000   \n",
       "Machine Learning (journal)                    0.000000  0.000000  0.000000   \n",
       "Statistical learning in language acquisition  0.005416  0.000000  0.008116   \n",
       "Data mining                                   0.000000  0.000000  0.000000   \n",
       "Supervised learning                           0.000000  0.000000  0.000000   \n",
       "\n",
       "                                              yoshua     young  youtub  \\\n",
       "title                                                                    \n",
       "Machine learning                                 0.0  0.000000     0.0   \n",
       "Machine Learning (journal)                       0.0  0.000000     0.0   \n",
       "Statistical learning in language acquisition     0.0  0.048697     0.0   \n",
       "Data mining                                      0.0  0.000000     0.0   \n",
       "Supervised learning                              0.0  0.000000     0.0   \n",
       "\n",
       "                                                    yu      zero       zip  \n",
       "title                                                                       \n",
       "Machine learning                              0.000000  0.004766  0.014283  \n",
       "Machine Learning (journal)                    0.000000  0.000000  0.000000  \n",
       "Statistical learning in language acquisition  0.024348  0.000000  0.000000  \n",
       "Data mining                                   0.000000  0.000000  0.000000  \n",
       "Supervised learning                           0.000000  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 2293 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf, dfTFIDF = tf_idf(df)\n",
    "\n",
    "dfTFIDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendArticles(history, df, top_n=5):\n",
    "    tfidf, dfTFIDF = tf_idf(df)\n",
    "    df = df.set_index('title')\n",
    "    \n",
    "    historyContent = ' '.join(df.loc[history,'processedContent'])\n",
    "    historyVector = tfidf.transform([historyContent]).toarray()[0]\n",
    "    \n",
    "    cosineDistance = dfTFIDF.apply(lambda row: cosine(row, historyVector), axis=1)\n",
    "    similarityScores = 1 - cosineDistance\n",
    "    \n",
    "    recommendations = pd.DataFrame({\n",
    "        'title': dfTFIDF.index, \n",
    "        'link': df.loc[dfTFIDF.index, 'link'],\n",
    "        'similarity': similarityScores\n",
    "    })\n",
    "    \n",
    "    # we exclude the articles that user have already seen from the recommendations\n",
    "    recommendations = recommendations[~recommendations['title'].isin(history)]\n",
    "    \n",
    "    recommendations = recommendations.sort_values(by='similarity', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    recommendations = recommendations.head(top_n)\n",
    "\n",
    "    \n",
    "    recommendations = HTML(recommendations.to_html(render_links=True, escape=False))\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meta-learning (computer science)</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Meta-learning_(computer_science)\" target=\"_blank\">https://en.wikipedia.org/wiki/Meta-learning_(computer_science)</a></td>\n",
       "      <td>0.574335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weak supervision</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Semi-supervised_learning\" target=\"_blank\">https://en.wikipedia.org/wiki/Semi-supervised_learning</a></td>\n",
       "      <td>0.535394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Self-supervised learning</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Self-supervised_learning\" target=\"_blank\">https://en.wikipedia.org/wiki/Self-supervised_learning</a></td>\n",
       "      <td>0.533141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unsupervised learning</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Unsupervised_learning\" target=\"_blank\">https://en.wikipedia.org/wiki/Unsupervised_learning</a></td>\n",
       "      <td>0.500827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reinforcement learning</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning\" target=\"_blank\">https://en.wikipedia.org/wiki/Reinforcement_learning</a></td>\n",
       "      <td>0.417073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = ['Machine learning', 'Supervised learning']\n",
    "recommendations = recommendArticles(history, df, top_n=5)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding articles in user history that are not in the database\n",
    "Since the Wikipedia has more than 6 million articles in english we may encounter the situation that the user has seen the article that is not in our rather small database; <br>\n",
    "Because of that this part is added to deal with such problems by adding to the database the title, link and content of unknown for us article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Aliens'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m history \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMachine learning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAliens\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mrecommendArticles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m recommendations\n",
      "Cell \u001b[1;32mIn [46], line 5\u001b[0m, in \u001b[0;36mrecommendArticles\u001b[1;34m(history, df, top_n)\u001b[0m\n\u001b[0;32m      2\u001b[0m tfidf, dfTFIDF \u001b[38;5;241m=\u001b[39m tf_idf(df)\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m historyContent \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessedContent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      6\u001b[0m historyVector \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mtransform([historyContent])\u001b[38;5;241m.\u001b[39mtoarray()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m cosineDistance \u001b[38;5;241m=\u001b[39m dfTFIDF\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: cosine(row, historyVector), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1368\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m   1367\u001b[0m     tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_ellipsis(tup)\n\u001b[1;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;66;03m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[0;32m   1371\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1089\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1087\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m section\n\u001b[0;32m   1088\u001b[0m         \u001b[38;5;66;03m# This is an elided recursive call to iloc/loc\u001b[39;00m\n\u001b[1;32m-> 1089\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot applicable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1363\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mmart\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Aliens'] not in index\""
     ]
    }
   ],
   "source": [
    "# history = ['Machine learning', 'Aliens']\n",
    "# recommendations = recommendArticles(history, df, top_n=5)\n",
    "# recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchUnknownArticle(unknowTitle):\n",
    "    base_url = \"https://en.wikipedia.org/wiki/\"\n",
    "    url = base_url + unknowTitle.replace(' ', '_')\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = ' '.join([p.text for p in paragraphs if p.text])\n",
    "        \n",
    "        #to preprocess the content we simulate dataframe row\n",
    "        row = pd.Series({\"content\": content})\n",
    "        \n",
    "        processedArticle = preprocessArticles(row, tokenizer=word_tokenize, stemmer=porter, \n",
    "                                                          lemmatizer=None, useLemmatizer=False)\n",
    "        \n",
    "        \n",
    "        newArticle = {\"title\": unknowTitle, \"link\": url, \"content\": content, \n",
    "                      \"processedContent\": processedArticle}\n",
    "        return newArticle\n",
    "    except:\n",
    "        print(f\"Failed to fetch article {unknowTitle}. Please check the title and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additionalArticles(newArticle, df, maxExpansion=5):\n",
    "    mainArticle = fetchUnknownArticle(newArticle)\n",
    "    if mainArticle is None:\n",
    "        return df\n",
    "    \n",
    "    newCrawledArticles = crawlArticles(mainArticle['link'], max_articles=maxExpansion)\n",
    "    \n",
    "    processedNewArticles=[]\n",
    "    for article in newCrawledArticles:\n",
    "        if article['title'] not in df['title'].values:\n",
    "            row = pd.Series({\"content\": article['content']})\n",
    "            article['processedContent'] = preprocessArticles(row, tokenizer=word_tokenize, stemmer=porter, \n",
    "                                                          lemmatizer=None, useLemmatizer=False)\n",
    "            processedNewArticles.append(article)\n",
    "            \n",
    "    return processedNewArticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expandDatabase(history, df):\n",
    "    unknownTitles = [title for title in history if title not in df['title'].values]\n",
    "    allNewArticles = []\n",
    "    \n",
    "    for title in unknownTitles:\n",
    "        articleData = fetchUnknownArticle(title)\n",
    "        if articleData:\n",
    "            allNewArticles.append(articleData)\n",
    "            crawledArticles = additionalArticles(title, df)\n",
    "            allNewArticles.extend(crawledArticles)\n",
    "            \n",
    "    if allNewArticles:\n",
    "        additionaldf = pd.DataFrame(allNewArticles)\n",
    "        df = pd.concat([df, additionaldf], ignore_index=True)\n",
    "        df = df.drop_duplicates(subset='title')\n",
    "        saveDatabase(df, 'articles.csv')\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendArticles(history, df, top_n=5):\n",
    "    \n",
    "    df = expandDatabase(history, df)\n",
    "    \n",
    "    tfidf, dfTFIDF = tf_idf(df)\n",
    "    df = df.set_index('title')\n",
    "    \n",
    "    historyContent = ' '.join(df.loc[history,'processedContent'])\n",
    "    historyVector = tfidf.transform([historyContent]).toarray()[0]\n",
    "    \n",
    "    cosineDistance = dfTFIDF.apply(lambda row: cosine(row, historyVector), axis=1)\n",
    "    similarityScores = 1 - cosineDistance\n",
    "    \n",
    "    recommendations = pd.DataFrame({\n",
    "        'title': dfTFIDF.index, \n",
    "        'link': df.loc[dfTFIDF.index, 'link'],\n",
    "        'similarity': similarityScores\n",
    "    })\n",
    "    \n",
    "    # we exclude the articles that user have already seen from the recommendations\n",
    "    recommendations = recommendations[~recommendations['title'].isin(history)]\n",
    "    \n",
    "    recommendations = recommendations.sort_values(by='similarity', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    recommendations = recommendations.head(top_n)\n",
    "\n",
    "    \n",
    "    recommendations = HTML(recommendations.to_html(render_links=True, escape=False))\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Life</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Life\" target=\"_blank\">https://en.wikipedia.org/wiki/Life</a></td>\n",
       "      <td>0.455563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meta-learning (computer science)</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Meta-learning_(computer_science)\" target=\"_blank\">https://en.wikipedia.org/wiki/Meta-learning_(computer_science)</a></td>\n",
       "      <td>0.446897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Self-supervised learning</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Self-supervised_learning\" target=\"_blank\">https://en.wikipedia.org/wiki/Self-supervised_learning</a></td>\n",
       "      <td>0.419621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Weak supervision</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Semi-supervised_learning\" target=\"_blank\">https://en.wikipedia.org/wiki/Semi-supervised_learning</a></td>\n",
       "      <td>0.411906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abiogenesis</td>\n",
       "      <td><a href=\"https://en.wikipedia.org/wiki/Abiogenesis\" target=\"_blank\">https://en.wikipedia.org/wiki/Abiogenesis</a></td>\n",
       "      <td>0.400445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = ['Machine learning', 'Extraterrestrial life']\n",
    "recommendations = recommendArticles(history, df, top_n=5)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
