{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia Recommendation system\n",
    "\n",
    "-----\n",
    "\n",
    "Authors:\n",
    "- Martyna Stasiak id.156071"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to generate the recommendations on wikipedia articles basing on the ones that user have liked. <br>\n",
    "To do that we have used 10 000 initial articles that were obtained by web crawling, starting from the https://en.wikipedia.org/wiki/Machine_learning article; later they were saved in the csv file, so if there is a need the file working as our database might be changed.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries that we have used and are necessary for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mmart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mmart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mmart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling and saving our articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we create the file that will work as or database containing all possible wikipedia articles. <br>\n",
    "We perform the crawling by ...... <explain precisely> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlArticles(start_url, max_articles):\n",
    "    visited = set()\n",
    "    to_visit = [start_url]\n",
    "    articles = []\n",
    "    \n",
    "    while to_visit and len(articles) < max_articles:\n",
    "        page = to_visit.pop(0)\n",
    "        if page in visited:\n",
    "            continue\n",
    "        visited.add(page)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(page)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            title = soup.find('h1').text # article's title\n",
    "            paragraphs = soup.find_all('p') # article's paragraphs\n",
    "            content = ' '.join([p.text for p in paragraphs]) # article's content that is inside paragraphs\n",
    "            articles.append({\"title\": title, \"link\": page, \"content\": content})\n",
    "            \n",
    "            # extracting and filtering new links\n",
    "            for link in soup.find_all('a', href=True): # we look for all links in the page\n",
    "                href = link['href']\n",
    "                if href.startswith('/wiki/') and ':' not in href and '#' not in href:\n",
    "                    full_url = \"https://en.wikipedia.org\" + href\n",
    "                    if full_url not in visited:\n",
    "                        to_visit.append(full_url)\n",
    "            time.sleep(0.5) # be polite to Wikipedia\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = crawlArticles(\"https://en.wikipedia.org/wiki/Machine_learning\", 10)\n",
    "df = pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createDatabase(start_url, max_articles=10):\n",
    "#     articles = crawlArticles(start_url, max_articles)\n",
    "#     df = pd.DataFrame(articles)\n",
    "#     # df.to_csv('articles.csv', index=False) no saving yet !!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDatabase(df, fileName):\n",
    "    df.to_csv(fileName, index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDatabase(df, 'articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine learning</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Machine_learning</td>\n",
       "      <td>Machine learning (ML) is a field of study in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Main Page</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Main_Page</td>\n",
       "      <td>Jochi (c. 1182 – c. 1225) was a prince in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine Learning (journal)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Machine_Learning...</td>\n",
       "      <td>Machine Learning  is a peer-reviewed scientifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Statistical learning in language acquisition</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Statistical_lear...</td>\n",
       "      <td>Statistical learning is the ability for humans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data mining</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Data_mining</td>\n",
       "      <td>Data mining is the process of extracting and d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0                              Machine learning   \n",
       "1                                     Main Page   \n",
       "2                    Machine Learning (journal)   \n",
       "3  Statistical learning in language acquisition   \n",
       "4                                   Data mining   \n",
       "\n",
       "                                                link  \\\n",
       "0     https://en.wikipedia.org/wiki/Machine_learning   \n",
       "1            https://en.wikipedia.org/wiki/Main_Page   \n",
       "2  https://en.wikipedia.org/wiki/Machine_Learning...   \n",
       "3  https://en.wikipedia.org/wiki/Statistical_lear...   \n",
       "4          https://en.wikipedia.org/wiki/Data_mining   \n",
       "\n",
       "                                             content  \n",
       "0  Machine learning (ML) is a field of study in a...  \n",
       "1  Jochi (c. 1182 – c. 1225) was a prince in the ...  \n",
       "2  Machine Learning  is a peer-reviewed scientifi...  \n",
       "3  Statistical learning is the ability for humans...  \n",
       "4  Data mining is the process of extracting and d...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Starting crawling from a Wikipedia article\n",
    "# start_url = \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
    "# articles = crawl_articles(start_url, max_articles=10)\n",
    "\n",
    "# # Save the articles\n",
    "# df = pd.DataFrame(articles)\n",
    "# df.to_csv(\"wikipedia_articles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we have the database containing the articles we have to do the preprocessing; <br>\n",
    "we have done: \n",
    "- lemmatization\n",
    "- deleting the stopwords\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessArticles(df, tokenizer=word_tokenize, stemmer=None, lemmatizer=None, useLemmatizer=False):\n",
    "    tokens = tokenizer(df['content'].lower())\n",
    "    terms = [word for word in tokens if word.isalpha() and word not in stopWords] # remove stopwords and non-alphabetic words\n",
    "    if stemmer:\n",
    "        processed = [stemmer.stem(word) for word in terms]\n",
    "    elif useLemmatizer and lemmatizer:\n",
    "        processed = [lemmatizer.lemmatize(word) for word in terms]\n",
    "    else:\n",
    "        processed = terms\n",
    "    return ' '.join(processed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original_content</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>lancaster_stemmer</th>\n",
       "      <th>lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine learning</td>\n",
       "      <td>Machine learning (ML) is a field of study in a...</td>\n",
       "      <td>machin learn ml field studi artifici intellig ...</td>\n",
       "      <td>machin learn ml field study art intellig conce...</td>\n",
       "      <td>machine learning ml field study artificial int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Main Page</td>\n",
       "      <td>Jochi (c. 1182 – c. 1225) was a prince in the ...</td>\n",
       "      <td>jochi princ mongol empir month birth mother bö...</td>\n",
       "      <td>joch print mongol empir month bir moth börte c...</td>\n",
       "      <td>jochi prince mongol empire month birth mother ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine Learning (journal)</td>\n",
       "      <td>Machine Learning  is a peer-reviewed scientifi...</td>\n",
       "      <td>machin learn scientif journal publish sinc for...</td>\n",
       "      <td>machin learn sci journ publ sint forty edit me...</td>\n",
       "      <td>machine learning scientific journal published ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Statistical learning in language acquisition</td>\n",
       "      <td>Statistical learning is the ability for humans...</td>\n",
       "      <td>statist learn abil human anim extract statist ...</td>\n",
       "      <td>stat learn abl hum anim extract stat regul wor...</td>\n",
       "      <td>statistical learning ability human animal extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data mining</td>\n",
       "      <td>Data mining is the process of extracting and d...</td>\n",
       "      <td>data mine process extract discov pattern larg ...</td>\n",
       "      <td>dat min process extract discov pattern larg da...</td>\n",
       "      <td>data mining process extracting discovering pat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0                              Machine learning   \n",
       "1                                     Main Page   \n",
       "2                    Machine Learning (journal)   \n",
       "3  Statistical learning in language acquisition   \n",
       "4                                   Data mining   \n",
       "\n",
       "                                    original_content  \\\n",
       "0  Machine learning (ML) is a field of study in a...   \n",
       "1  Jochi (c. 1182 – c. 1225) was a prince in the ...   \n",
       "2  Machine Learning  is a peer-reviewed scientifi...   \n",
       "3  Statistical learning is the ability for humans...   \n",
       "4  Data mining is the process of extracting and d...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0  machin learn ml field studi artifici intellig ...   \n",
       "1  jochi princ mongol empir month birth mother bö...   \n",
       "2  machin learn scientif journal publish sinc for...   \n",
       "3  statist learn abil human anim extract statist ...   \n",
       "4  data mine process extract discov pattern larg ...   \n",
       "\n",
       "                                   lancaster_stemmer  \\\n",
       "0  machin learn ml field study art intellig conce...   \n",
       "1  joch print mongol empir month bir moth börte c...   \n",
       "2  machin learn sci journ publ sint forty edit me...   \n",
       "3  stat learn abl hum anim extract stat regul wor...   \n",
       "4  dat min process extract discov pattern larg da...   \n",
       "\n",
       "                                       lemmatization  \n",
       "0  machine learning ml field study artificial int...  \n",
       "1  jochi prince mongol empire month birth mother ...  \n",
       "2  machine learning scientific journal published ...  \n",
       "3  statistical learning ability human animal extr...  \n",
       "4  data mining process extracting discovering pat...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define preprocessing variations\n",
    "variations = {\n",
    "    \"porter_stemmer\": lambda row: preprocessArticles(row, tokenizer=word_tokenize, stemmer=porter),\n",
    "    \"lancaster_stemmer\": lambda row: preprocessArticles(row, tokenizer=word_tokenize, stemmer=lancaster),\n",
    "    \"lemmatization\": lambda row: preprocessArticles(row, tokenizer=word_tokenize, lemmatizer=lemmatizer, useLemmatizer=True)\n",
    "}\n",
    "\n",
    "# Apply variations without modifying the original DataFrame\n",
    "results = pd.DataFrame({\n",
    "    \"title\": df[\"title\"],\n",
    "    \"original_content\": df[\"content\"]\n",
    "})\n",
    "\n",
    "for name, preprocess_function in variations.items():\n",
    "    # Apply each variation to the content column using the original function\n",
    "    results[name] = df.apply(preprocess_function, axis=1)\n",
    "\n",
    "# Display first few rows of variations\n",
    "columns_to_display = [\"title\", \"original_content\"] + list(variations.keys())\n",
    "results[columns_to_display].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>processedContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine learning</td>\n",
       "      <td>Machine learning (ML) is a field of study in a...</td>\n",
       "      <td>machin learn ml field studi artifici intellig ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Main Page</td>\n",
       "      <td>Jochi (c. 1182 – c. 1225) was a prince in the ...</td>\n",
       "      <td>jochi princ mongol empir month birth mother bö...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine Learning (journal)</td>\n",
       "      <td>Machine Learning  is a peer-reviewed scientifi...</td>\n",
       "      <td>machin learn scientif journal publish sinc for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Statistical learning in language acquisition</td>\n",
       "      <td>Statistical learning is the ability for humans...</td>\n",
       "      <td>statist learn abil human anim extract statist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data mining</td>\n",
       "      <td>Data mining is the process of extracting and d...</td>\n",
       "      <td>data mine process extract discov pattern larg ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0                              Machine learning   \n",
       "1                                     Main Page   \n",
       "2                    Machine Learning (journal)   \n",
       "3  Statistical learning in language acquisition   \n",
       "4                                   Data mining   \n",
       "\n",
       "                                             content  \\\n",
       "0  Machine learning (ML) is a field of study in a...   \n",
       "1  Jochi (c. 1182 – c. 1225) was a prince in the ...   \n",
       "2  Machine Learning  is a peer-reviewed scientifi...   \n",
       "3  Statistical learning is the ability for humans...   \n",
       "4  Data mining is the process of extracting and d...   \n",
       "\n",
       "                                    processedContent  \n",
       "0  machin learn ml field studi artifici intellig ...  \n",
       "1  jochi princ mongol empir month birth mother bö...  \n",
       "2  machin learn scientif journal publish sinc for...  \n",
       "3  statist learn abil human anim extract statist ...  \n",
       "4  data mine process extract discov pattern larg ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processedContent'] = df.apply(lambda row: preprocessArticles(row, tokenizer=word_tokenize, stemmer=porter, \n",
    "                                                          lemmatizer=lemmatizer, useLemmatizer=True), axis=1)\n",
    "saveDatabase(df, 'processed_articles.csv')\n",
    "\n",
    "columnstoUse = ['title', 'content','processedContent']\n",
    "df[columnstoUse].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF and Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   title                                               link  \\\n",
      "6  Unsupervised learning  https://en.wikipedia.org/wiki/Unsupervised_lea...   \n",
      "\n",
      "   mean_similarity  \n",
      "6         0.513629  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_wikipedia_articles.csv\")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['clean_content'])\n",
    "\n",
    "def recommend_articles(input_titles, top_n=5):\n",
    "    # Find indices of the input articles\n",
    "    input_indices = df[df['title'].isin(input_titles)].index\n",
    "    \n",
    "    # Compute the cosine similarity between input and all articles\n",
    "    input_vectors = tfidf_matrix[input_indices]\n",
    "    similarities = cosine_similarity(input_vectors, tfidf_matrix)\n",
    "    mean_similarity = similarities.mean(axis=0)\n",
    "    \n",
    "    \n",
    "    df['mean_similarity'] = mean_similarity\n",
    "    \n",
    "    recommendations = df.loc[~df.index.isin(input_indices)]\n",
    "    \n",
    "    recommend_articles = recommendations.nlargest(top_n, 'mean_similarity')\n",
    "    \n",
    "    return recommend_articles[['title', 'link', 'mean_similarity']]\n",
    "\n",
    "\n",
    "# Example: Recommend articles based on previously visited ones\n",
    "visited_titles = [\"Machine learning\", \"Artificial intelligence\"]\n",
    "recommended_articles = recommend_articles(visited_titles, top_n=1)\n",
    "print(recommended_articles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
